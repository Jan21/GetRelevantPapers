{
  "sections": {
    "graph_neural_reasoning_may_fail_in_certifying_bool": "## Introduction\n\n\nderived from the formulas, or whether the complex embedding schemes can be learned from backpropagation. Previous successes on SAT problems argued for the power of GNN, which can handle NP- complete problems [1, 12], whereas no evidences have been reported for solving semi- decidable predicate logic problems via GNN. The significant difficulty to prove the problems is the requirement of comprehensive reasoning over a search space, since a complete proof includes SAT and UNSAT (i.e., Boolean unsatisfiability). Perhaps disappointingly, this work presents some theoretical evidences that support a pessimistic conjecture: GNNs do not simulate the complete solver for UNSAT. Specifically, we discover that the neural reasoning procedure learned by GNNs does simulate the algorithms that may allow a CNF formula changing over iterations. Those complete SAT- solvers, e.g., DPLL and CDCL, are almost common in the operation that adaptively alters the original Boolean formula that eases the reasoning process. So GNNs do not learn to simulate their behaviors. Instead, we prove that by appropriately defining a specific structure of GNN that a parametrized GNN may learn, the local search heuristic in Walk SAT can be simulated by GNN. Towards these results, we believe that GNN can not solve UNSAT in existing logical reasoning problems. ## 2 Embedding Logic Formulae by GNNs Preliminary: Graph Neural Networks (GNNs). GNNs refer to the neural architectures devised to learn the embeddings of nodes and graphs via message- passing. Resembling the generic definition in [14], they consist of two successive operators to propagate the messages and evolve the embeddings over iterations: \\[m_{v}^{(k)} = \\mathrm{Aggregate}^{(k)}\\left(\\{h_{u}^{(k - 1)}:u\\in \\mathcal{N}(v)\\} \\right),\\quad h_{v}^{(k)} = \\mathrm{Combine}^{(k)}\\left(h_{v}^{(k - 1)},m_{v}^{(k)}\\right) \\quad (1)\\] where \\(h_{v}^{(k)}\\) denotes the hidden state (embedding) of node \\(v\\) in the \\(k^{t h}\\) iteration, and \\(\\mathcal{N}(v)\\) denotes the neighbors of node \\(v\\) . In each iteration, the Aggregate \\((k)(\\cdot)\\) aggregates hidden states from node \\(v\\) 's neighbors \\(\\{h_{u}^{(k - 1)}:u\\in \\mathcal{N}(v)\\}\\) to produce the new message (i.e., \\(m_{v}^{(k)}\\) ) for node \\(v\\) . Combine \\((k)(\\cdot ,\\cdot)\\) updates the embedding of \\(v\\) in terms of its previous state and its current message. After a specific number of iterations (e.g., \\(K\\) in our discussion), the embeddings should capture the global relational information of the nodes, which can be fed into other neural network modules for specific tasks. Significant successes about GNNs have been witnessed in relational reasoning [6, 17, 20], where an instance could be departed into multiple objects then encoded by a series of features with their relation. It typically suits representation in Eq. 1. Whereas in logical reasoning, a Boolean formula is in Conjunctive Normal Form (CNF) that consists of literal and clause items. In term of the independence among literals in CNF (so do clauses), [12] embeds a formula into a bipartite graph, where the nodes denote the clauses and literals that are disjoint, respectively. In this principle, given a literal \\(v\\) as a node, all the nodes of clauses that contains the literal are routinely treated as \\(v\\) 's neighbors, vice and versa for the node of each clause. We assume \\(\\Phi\\) is a logic formula in CNF, i.e., a set of clauses, and \\(\\Psi (v)\\in \\Phi\\) denote one of clauses within the logic formula \\(\\Phi\\) that contains literal \\(v\\) . Derived from Eq. 1, GNNs for logical reasoning can be further specified by \\[\\begin{array}{r l} & {m_{v}^{(k)} = \\mathrm{Aggregate}_{L}^{(k)}\\Big(\\{h_{\\Psi^{-1}}^{(k - 1)}:\\Psi (v)\\in \\Phi \\} \\Big),\\quad h_{v}^{(k)} = \\mathrm{Combine}_{L}^{(k)}\\Big(h_{v}^{(k - 1)},h_{v^{-1}}^{(k - 1)},m_{v}^{(k)}\\Big),}\\\\ & {m_{\\Psi^{(v)}}^{(k)} = \\mathrm{Aggregate}_{C}^{(k)}\\Big(\\{h_{u}^{(k - 1)}:u\\in \\Psi (v)\\} \\Big),\\quad h_{\\Psi^{(v)}}^{(k)} = \\mathrm{Combine}_{C}^{(k)}\\Big(h_{\\Psi^{(v)}}^{(k - 1)},m_{\\Psi^{(v)}}^{(k)}\\Big),}\\\\ & {\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad (2)} \\end{array} \\quad (2)\\] where \\(h_{v}^{(k)}\\) and \\(h_{\\Psi^{(v)}}^{(k)}\\) denote embeddings of the literal \\(v\\) and the clause \\(\\Psi (v)\\) in the \\(k^{t h}\\) iteration \\((h_{v^{- v}}^{(k)}\\) denotes the embedding of the negation of \\(v\\) ); \\(m_{v}^{(k)}\\) and \\(m_{\\Psi (v)}^{(k)}\\) refer to their propagated messages. Since the value of a Boolean formula is determined by the value assignment of the literal variables, Eq. 2 solely requires the final- state literal embeddings \\(\\{h_{u}^{(K)},u\\in L\\}\\) to predict the logical reasoning result. More specifically, we use \\(L\\) and \\(C\\) to denote a literal set and a clause set ( \\(L\\) and \\(C\\) may be different for each CNF formula), then \\(\\Psi (v)\\) is a clause and \\(\\Psi (v)\\) denotes a clause including the literal \\(v\\in L\\) . Note that the graph embeddings for SAT [7] and 2QBF [7] are generally represented by Eq.2. Hence our further analysis is based on Eq.2.\n\n## 3 Certifying UNSAT by GNNs may Fail Although existing researches showed that GNN can learn a well- performed solver for satisfiability problems, GNN- based SAT solvers actually have terrible performances in predicting unsatisfiability with high confidence [12] in a SAT formula, if the formula does not have a small unsatisfiable core (minimal number of clauses that is enough to cause unsatisfiability). In fact, some previous work [1] even completely removed unsatisfiable formulas from the training dataset, since they slowed down the whole training process. The difficulty in proving unsatisfiability is understandable, since constructing a proof of unsatisfiability demands a complete reasoning in the search space, which is more complex than constructing a proof of satisfiability that only requires a witness. Traditionally it relies on the recursive decision procedures that either traverse all possible assignments to construct the proof (DPLL [4]), or generate extra constraints from assignment trials that lead to conflicts, until some of the constraints contradict each other (CDCL [13]). The line of recursive algorithms include some operation branches that reconfigure the bipartite graph behind the CNF in each step while they search. In the terms of a graph that may iteratively change (e.g., DPLL), perhaps miserably, their recursive processes can not be simulated by GNNs. Observation 3.1. Given a recursive algorithm that iteratively reconfigures the graph, GNNs in Eq.2 can not simulate this recursive process. Proof. Associating the aggregate and combine functions in Eq. 2, we obtain the iterative update rule for the embedding of a literal \\(v\\) : \\[\\begin{array}{r l} & {h_{v}^{(k)} = \\mathrm{Combine}_{L}^{(k)}\\left(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},\\mathrm{Aggregate}_{L}^{(k)}\\left(\\{h_{\\Psi (v)}^{(k - 1)}:\\Psi (v)\\in \\Phi \\}\\right)\\right)}\\\\ & {\\quad = \\mathrm{Update}_{L}^{(k)}\\left(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},\\{h_{\\Psi (v)}^{(k - 1)}:\\Psi (v)\\in \\Phi \\}\\right),}\\\\ & {\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad s.t.v\\in L} \\end{array} \\quad (3)\\] Towards this principle, we observe that the embedding update of \\(v\\) in the current stage relies on the last- stage embeddings of \\(v\\) and its negation \\(\\neg v\\) , and the embeddings of all the clauses that include \\(v\\) in a CNF formula \\((\\Psi (v) \\in \\Phi)\\) . The literal \\(v\\) , \\(\\neg v\\) and the clauses containing \\(v\\) are consistent over iterations. Hence if the update function (Eq. 3) is consistent over the iterations in Eq.2, i.e., \\(\\forall k \\in \\mathbb{N}_{+}\\) , \\(\\mathrm{Update}_{L}^{(k)} = \\mathrm{Update}_{L}\\) , where \\(\\mathrm{Update}_{L}\\) means the update for literal embedding, GNNs derived from Eq. 3 receive a fixed graph generated by a CNF formula as input. However, if a recursive algorithm iteratively changes the graph that represents a CNF formula, it implies that there must be a clause that was changed (or eliminated) after this iteration, since clauses are permutation- invariant in a CNF formula. Accordingly there must be a literal embedding whose update process depends on a clause different from the previous iteration. It contradicts the literal embedding update function learned by Eq. 3 with \\(\\forall k \\in \\mathbb{N}_{+}\\) , \\(\\mathrm{Update}_{L}^{(k)} = \\mathrm{Update}_{L}\\) . Hence the message- passing in GNNs could not resemble the procedures in the complete SAT- solvers. In fact, GNNs are rather similar to learning a subfamily of incomplete SAT solvers (GSAT, Walk SAT [11]), which randomly assign variables and stochastically search for local witnesses. Observation 3.2. GNNs in Eq. 2 may simulate the local search in Walk SAT. Proof. Recall the iterative update routine of Walk SAT: starting by assigning a random value to each literal variable in a formula, it randomly chooses an unsatisfied clause in the formula and flips the value of a Boolean variable within that clause. Such process is repeated till the literal assignment satisfies all clauses in the formula. Here we construct the optimal aggregation and combine functions derived from Eq. 2, which are designed to simulate the procedure of Walk SAT. In this way, if the aggregation and combine functions in Eq. 2 approximate these optimal aggregation and combine functions, the GNN may simulate the local search in Walk SAT. Given a universe of literals in logical reasoning, we first initiate the embeddings of them and their negation, thus, \\(\\forall v \\in L\\) , random value of \\(h_{v}^{(0)}\\) and \\(h_{\\neg v}^{(0)}\\) are initiated. This assignment can be treated as the Boolean value that belong to different literals, which have been mapped from a binary vector into a real- value embedding space about the literals. We also randomly initiate the clause embeddings \\(h_{\\Psi (v)}^{(0)}\\) for reasoning each formula that contains the clause \\(\\Psi (v)\\) . Here we define the optimal\n\naggregation and combine functions that encode literals and clauses respectively, which GNNs in Eq. 2 may learn if they attempt to simulate Walk SAT: \\[\\begin{array}{r l} & {m_{v}^{(k)} = \\overline{{\\mathrm{Aggregate}}}_{L}\\Big(\\{h_{\\Psi^{(v)}}^{(k - 1)}:\\Psi (v)\\in \\Phi \\} \\Big),}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{\\epsilon^{(k)},} & {\\prod_{\\Psi (v)}||h_{\\Psi^{(v)}}^{(k - 1)}|| = 0}\\\\ {\\qquad 0,} & {\\prod_{\\Psi (v)}||h_{\\Psi^{(v)}}^{(k - 1)}||\\neq 0} \\end{array} \\right.} \\end{array} \\quad (4)\\] where \\(\\overline{{\\mathrm{Aggregate}}}_{L}(\\cdot)\\) denotes the optimal aggregation function to propagate literal messages and \\(m_{v}^{(k)}\\) denotes the optimally propagated message of literal \\(v\\) in the \\(k\\) iteration; \\(\\mathbf{0}\\) is a zero- value vector; \\(\\epsilon^{(k)}\\) denotes a bounded non- zero random vector generated in the \\(k\\) iteration; \\(||\\cdot ||\\) indicates a vector norm. \\[\\begin{array}{r l} & {h_{v}^{(k)} = \\overline{{\\mathrm{Combine}}}_{L}\\Big(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},m_{v}^{(k)}\\Big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{v - v}^{(k - 1)},} & {v = \\arg \\max \\{\\big||m_{u}^{(k)}\\big||\\} \\mathrm{and} \\big||m_{v}^{(k)}\\big|| > 0}\\\\ {h_{v}^{(k - 1)},} & {\\mathrm{otherwise}} \\end{array} \\right.} \\end{array} \\quad (5)\\] where \\(\\overline{{\\mathrm{Combine}}}_{L}(\\cdot)\\) denotes the optimal combine function that iteratively updates literal embeddings by the aid of the optimal message. Eq. 5 implies the local Boolean variable flipping in Walk SAT: if the norm of \\(m_{v}^{(k)}\\) is the maximum among all the optimal literal messages, its literal embedding would be replaced by the embedding of its negation, otherwise, keep the identical value. The maximization ensures only one literal embedding that would be \"flipped\" per iteration, which simulates the local search behavior. Besides, the literal embedding selected for update would not be \\(\\mathbf{0}\\) , which implies all the clauses containing this literal are satisfied (see the condition 2 in Eq. 4 ). Since all the satisfied clauses would not be selected in Walk SAT, this literal also would not be selected to update in this iteration. Finally, if a literal has been included by a clause that is unsatisfied, it would be randomly picked in some probability. The uncertainty is implied by the randomness of \\(\\epsilon^{(k)}\\) . \\[\\begin{array}{r l} & {m_{\\Psi^{(v)}}^{(k)} = \\overline{{\\mathrm{Aggregate}}}_{C}\\big(\\{h_{u}^{(k - 1)}:u\\in \\Psi (v)\\} \\big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{\\Psi^{(v)}}^{(0)},} & {\\mathrm{Sigmoid}\\Big(\\mathrm{MLP}_{2}^{*}\\Big(\\sum_{u\\in \\Psi (v)}\\mathrm{MLP}_{1}^{*}(h_{u}^{(k - 1)})\\Big)\\Big)\\geq 0.5}\\\\ {0,} & {\\mathrm{Sigmoid}\\Big(\\mathrm{MLP}_{2}^{*}\\Big(\\sum_{u\\in \\Psi (v)}\\mathbf{MLP}_{1}^{*}(h_{u}^{(k - 1)})\\Big)\\Big)< 0.5} \\end{array} \\right.} \\end{array} \\quad (6)\\] where \\(\\overline{{\\mathrm{Aggregate}}}_{C}(\\cdot)\\) denotes the optimal aggregation function that conveys the clause embedding messages during reasoning. Note that \\(\\mathrm{MLP}_{2}\\Big(\\sum_{u\\in \\Psi (v)}\\mathrm{MLP}_{1}(h_{u}^{(k - 1)})\\Big)\\) indicates Deep Sets [18], a neural network that encodes a literal embedding set \\(\\{h_{u}^{(k - 1)}\\}_{u\\in \\Psi (v)}\\) whose literals are included by a clause \\(\\Psi (v)\\) . The reduced feature would be fed into the sigmoid clause predictor. We use \\(\\mathrm{MLP}_{1}^{*}\\) and \\(\\mathrm{MLP}_{2}^{*}\\) to denote the implicit optimal prediction to each clause: given the arbitrarily initiated literal embeddings that denote the Boolean value assignment of literals, the optimal Deep Sets can predict whether the literal- derived clause is satisfied \\((\\geq 0.5)\\) or not \\((< 0.5)\\) . Since the predictor is permutation- invariant to the input, Propositions 3.1 in [15] promises that it can be approximated arbitrarily closely by graph convolution, which exactly corresponds to the parameterized clause aggregation functions in Eq.2. On the other hand, Eq. 5 promises the literal embeddings staying in their initiated values over iterations, hence the optimal Deep Sets may always judge whether a clause (the set of literals as the input of Deep Sets) is satisfied or not. \\[\\begin{array}{r l} & {h_{\\Psi^{(v)}}^{(k)} = \\overline{{\\mathrm{Combine}}}_{C}\\Big(h_{\\Psi^{(v)}}^{(k - 1)},m_{\\Psi^{(v)}}^{(k)}\\Big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{\\Psi^{(v)}}^{(k - 1)},} & {h_{\\Psi^{(v)}}^{(k - 1)} = m_{\\Psi^{(v)}}^{(k)}}\\\\ {h_{\\Psi^{(v)}}^{(0)},} & {||h_{\\Psi^{(v)}}^{(k - 1)}||< ||m_{\\Psi^{(v)}}^{(k)}||}\\\\ {0,} & {||h_{\\Psi^{(v)}}^{(k - 1)}||\\geq ||m_{\\Psi^{(v)}}^{(k)}||} \\end{array} \\right.} \\end{array} \\quad (7)\\] where \\(\\overline{{\\mathrm{Combine}}}_{C}(\\cdot)\\) denotes the optimal clause combine function. Based on the propagated messages conveyed by Eq. 2 , it determines how to iteratively update clause embeddings to simulate Walk SAT.\n\nHere we elaborate how the four optimal functions above cooperate to simulate an iteration of Walk- SAT. Since GNNs use literal embeddings as the initial input, we first analyze Eq. 6 and takes a literal \\(v\\) into our consideration. As we discussed, this function receives a set of literal embeddings that denotes a clause that contains \\(v\\) , and then, takes the optimal Deep Sets as an oracle to judge whether this clause is satisfied. The output, the optimal message about the clause, equals to the initiated embedding of the clause \\(h_{\\Psi (v)}\\) if it is satisfied, otherwise becomes 0. This process simulates the logical reasoning on a clause, which Walk SAT relies on to pick an unsatisfied clause and flip one of its variables (see Eq. 5). Based on \\(m_{\\Psi (v)}^{(k)}\\) , the optimal clause combine function (Eq. 7) updates an arbitrary clause embedding that contains \\(v\\) . The first branch states that, if the current clause message \\(m_{\\Psi (v)}^{(k)}\\) is consistent with the previous clause embedding \\(h_{\\Psi (v)}^{(k - 1)}\\) , it implies the satisfiability of the clause \\(\\Psi (v)\\) is not changed in this iteration (the previously satisfied clause is still satisfied, vice and versa). In this case the clause embedding would not be updated. The second and third branches imply that when \\(m_{\\Psi (v)}^{(k)}\\) and \\(h_{\\Psi (v)}^{(k - 1)}\\) are inconsistent, how to update the clause embedding \\(h_{\\Psi (v)}^{(k)}\\) to convey the current message about whether the clause \\(\\Psi (v)\\) is satisfied (return into the initial clause embeddings) or not (turn into 0). Therefore all updated embeddings about the clauses that contain \\(v\\) , as the neighbors of \\(v\\) , would be fed into the optimal aggregation function in Eq. 4. This function selects \\(v\\) that only exists in satisfied clauses, i.e., \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}||\\neq 0\\) (If there is an unsatisfied clauses, its embedding is 0 according to Eq. 7, and would lead to \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}|| = 0\\) ), then the embedding of \\(v\\) would become 0. The results by this operation are taken advantage by Eq. 5, which promises the literal that only exists in satisfied clauses would not be \"flipped\" (Walk SAT only chooses unsatisfied clause and select its variables to flip. If literals are not in any unsatisfied clauses, it would not be chosen). Towards the literal \\(v\\) contained by one unsatisfied clause at least \\((\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}|| = 0\\) since there exists a clause embedding equals to 0 according to Eq. 7), its literal message would be assigned by a random vector \\(\\epsilon^{(k)}\\) . It implies the randomness when Walk SAT try to select one of literal in unsatisfied clauses to flip its value. The flipping process is simulated by Eq. 6 as we have discussed. Here we further verify if a CNF formula could be satisfied, literal embeddings generated by the optimal aggregation and combine functions that represent the Boolean assignment of literal to satisfy this CNF formula, would converge over iterations (It corresponds to the stop criteria in Walk- SAT). Specifically suppose that in the \\(k\\) - 1 iteration, Eq. 5 have induced the literal embeddings so that all clauses with the literal in the formula have been satisfied. By Eq. 6 it is obvious that \\(\\forall v\\in L\\) , \\(m_{\\Psi (v)}^{(k)} = h_{\\Psi (v)}^{(0)}\\) . To this we have \\(h_{\\Psi (v)}^{(k - 1)} = m_{\\Psi (v)}^{(k)}\\) and \\(h_{\\Psi (v)}^{(k)} = h_{\\Psi (v)}^{(k - 1)} = h_{\\Psi (v)}^{(0)}\\) since all clauses in the formula have already been satisfied before the current iteration. In this case, it holds \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k - 1)}||\\neq 0\\) and leads to \\(\\forall v\\in L\\) , \\(m_{v}^{(k)} = 0\\) in this formula (Eq. 4). In term of this, Eq. 5 guarantees all the literal embeddings consistent with those in the previous iteration. Concluding the analysis above, we know that the optimal aggregation and combine functions (Eq. 4 5 6 7 ) are cooperated to simulate the local search in Walk SAT. Failure in 2QBF. Notably the failure in proving UNSAT would not be a problem for GNNs applied to solve SAT, as predicting satisfiability with high confidence has already been good enough for a binary distinction. However, 2QBF problems imply solving UNSAT, which inevitably makes GNNs unavailable in proving the relevant formulae. It probably explains the mystery in[7] about why GNNs purely learned by data- driven supervised learning lead to the same performances as random speculation [16]. ## 4 Further Discussion In this manuscript, we provide some discussions about the GNNs that consider the SAT and 2QBF problem as static graph, we haven't considered the shrinkage condition, which may apply dynamic GNN as [9], dues to the difficulty about proving the dynamic graph as we need to prove all the dynamic updating methods are impossible or not. Ought to be regarded that, this manuscript does not claim GNN is provably unable to achieve UNSAT, which remains an open issue.\n\nBelief propagation (BP) is a Bayesian message- passing method first proposed by [10], which is a useful approximation algorithm and has been applied to the SAT problems (specifically in 3- SAT [8]) and 2QBF problems [19]. BP can find the witnesses of unsatisfiability of 2QBF by adopting a bias estimation strategy. Each round of BP allows the user to select the most biased \\(\\forall\\) - variable and assign the biased value to the variable. After all the \\(\\forall\\) - variables are assigned, the formula is simplified by the assignment and sent to SAT solvers. The procedure returns the assignment as a witness of unsatisfiability if the simplified formula is unsatisfiable, or UNKNOWN otherwise. However, the fact that BP is used for each \\(\\forall\\) - variable assignment leads to high overhead, similar to the RL approach given by [5]. It is interesting, however, to see that with the added overhead, BP can find witnesses of unsatisfiability, which is what one- shot GNN- based embeddings cannot achieve. This manuscript revealed the previously unrecognized limitation of GNN in reasoning about unsatisfiability of SAT problems. This limitation is probably rooted in the simplicity of message- passing scheme, which is good enough for embedding graph features, but not for conducting complex reasoning on top of the graph structures. ## References [1] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit- SAT: An unsupervised differentiable approach. In International Conference on Learning Representations, 2019. [2] Alonzo Church. A note on the entscheidungsproblem. J. Symb. Log., 1(1):40- 41, 1936. [3] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, pages 151- 158, New York, NY, USA, 1971. ACM. [4] Martin Davis, George Logemann, and Donald W. Loveland. A machine program for theorem- proving. Commun. ACM, 5(7):394- 397, 1962. [5] Gil Lederman, Markus N. Rabe, and Sanjit A. Seshia. Learning heuristics for automated reasoning through deep reinforcement learning. Co RR, abs/1807.08058, 2018. [6] Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, and Shuicheng Yan. Semantic object parsing with graph LSTM. Co RR, abs/1603.07063, 2016. [7] Florian Lonsing, Uwe Egly, and Martina Seidl. Q- resolution with generalized axioms. In Nadia Creignou and Daniel Le Berre, editors, Theory and Applications of Satisfiability Testing - SAT 2016, pages 435- 452, Cham, 2016. Springer International Publishing.[8] M. \"M\u00e9zard\", G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812- 815, 2002. [9] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, and Charles E. Leisersen. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. Co RR, abs/1902.10191, 2019. [10] Judea Pearl. Reverend bayes on inference engines: A distributed hierarchical approach. In AAAI, pages 133- 136. AAAI Press, 1982. [11] Bart Selman, Henry A. Kautz, and Bram Cohen. Local search strategies for satisfiability testing. In Cliques, Coloring, and Satisfiability, volume 26 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 521- 531. DIMACS/AMS, 1993. [12] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single- bit supervision. In ICLR (Poster). Open- Review.net, 2019. [13] Jo\u00e3o P. Marques Silva, In\u00eas Lynce, and Sharad Malik. Conflict- driven clause learning SAT solvers. In Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications, pages 131- 153. IOS Press, 2009. [14] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR. Open Review.net, 2019. [15] Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken- ichi Kawarabayashi, and Stefanie Jegelka. What can neural networks reason about? Co RR, abs/1905.13211, 2019.\n\n[16] Zhanfu Yang, Fei Wang, Ziliang Chen, Guannan Wei, and Tiark Rompf. Graph neural reasoning for 2- quantified boolean formula solvers. Co RR, abs/1904.12084, 2019. [17] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. Neural- symbolic VQA: disentangling reasoning from vision and language understanding. Co RR, abs/1810.02338, 2018. [18] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pages 3391- 3401, 2017. [19] Pan Zhang, Abolfazl Ramezanpour, Lenka Zdeborov\u00e1, and Riccardo Zecchina. Message passing for quantified boolean formulas. Co RR, abs/1202.2536, 2012. [20] David Zheng, Vinson Luo, Jiajun Wu, and Joshua B. Tenenbaum. Unsupervised learning of latent physical properties using perception- prediction networks. Co RR, abs/1807.09244, 2018.",
    "introduction": "derived from the formulas, or whether the complex embedding schemes can be learned from backpropagation. Previous successes on SAT problems argued for the power of GNN, which can handle NP- complete problems [1, 12], whereas no evidences have been reported for solving semi- decidable predicate logic problems via GNN. The significant difficulty to prove the problems is the requirement of comprehensive reasoning over a search space, since a complete proof includes SAT and UNSAT (i.e., Boolean unsatisfiability). Perhaps disappointingly, this work presents some theoretical evidences that support a pessimistic conjecture: GNNs do not simulate the complete solver for UNSAT. Specifically, we discover that the neural reasoning procedure learned by GNNs does simulate the algorithms that may allow a CNF formula changing over iterations. Those complete SAT- solvers, e.g., DPLL and CDCL, are almost common in the operation that adaptively alters the original Boolean formula that eases the reasoning process. So GNNs do not learn to simulate their behaviors. Instead, we prove that by appropriately defining a specific structure of GNN that a parametrized GNN may learn, the local search heuristic in Walk SAT can be simulated by GNN. Towards these results, we believe that GNN can not solve UNSAT in existing logical reasoning problems. ## 2 Embedding Logic Formulae by GNNs Preliminary: Graph Neural Networks (GNNs). GNNs refer to the neural architectures devised to learn the embeddings of nodes and graphs via message- passing. Resembling the generic definition in [14], they consist of two successive operators to propagate the messages and evolve the embeddings over iterations: \\[m_{v}^{(k)} = \\mathrm{Aggregate}^{(k)}\\left(\\{h_{u}^{(k - 1)}:u\\in \\mathcal{N}(v)\\} \\right),\\quad h_{v}^{(k)} = \\mathrm{Combine}^{(k)}\\left(h_{v}^{(k - 1)},m_{v}^{(k)}\\right) \\quad (1)\\] where \\(h_{v}^{(k)}\\) denotes the hidden state (embedding) of node \\(v\\) in the \\(k^{t h}\\) iteration, and \\(\\mathcal{N}(v)\\) denotes the neighbors of node \\(v\\) . In each iteration, the Aggregate \\((k)(\\cdot)\\) aggregates hidden states from node \\(v\\) 's neighbors \\(\\{h_{u}^{(k - 1)}:u\\in \\mathcal{N}(v)\\}\\) to produce the new message (i.e., \\(m_{v}^{(k)}\\) ) for node \\(v\\) . Combine \\((k)(\\cdot ,\\cdot)\\) updates the embedding of \\(v\\) in terms of its previous state and its current message. After a specific number of iterations (e.g., \\(K\\) in our discussion), the embeddings should capture the global relational information of the nodes, which can be fed into other neural network modules for specific tasks. Significant successes about GNNs have been witnessed in relational reasoning [6, 17, 20], where an instance could be departed into multiple objects then encoded by a series of features with their relation. It typically suits representation in Eq. 1. Whereas in logical reasoning, a Boolean formula is in Conjunctive Normal Form (CNF) that consists of literal and clause items. In term of the independence among literals in CNF (so do clauses), [12] embeds a formula into a bipartite graph, where the nodes denote the clauses and literals that are disjoint, respectively. In this principle, given a literal \\(v\\) as a node, all the nodes of clauses that contains the literal are routinely treated as \\(v\\) 's neighbors, vice and versa for the node of each clause. We assume \\(\\Phi\\) is a logic formula in CNF, i.e., a set of clauses, and \\(\\Psi (v)\\in \\Phi\\) denote one of clauses within the logic formula \\(\\Phi\\) that contains literal \\(v\\) . Derived from Eq. 1, GNNs for logical reasoning can be further specified by \\[\\begin{array}{r l} & {m_{v}^{(k)} = \\mathrm{Aggregate}_{L}^{(k)}\\Big(\\{h_{\\Psi^{-1}}^{(k - 1)}:\\Psi (v)\\in \\Phi \\} \\Big),\\quad h_{v}^{(k)} = \\mathrm{Combine}_{L}^{(k)}\\Big(h_{v}^{(k - 1)},h_{v^{-1}}^{(k - 1)},m_{v}^{(k)}\\Big),}\\\\ & {m_{\\Psi^{(v)}}^{(k)} = \\mathrm{Aggregate}_{C}^{(k)}\\Big(\\{h_{u}^{(k - 1)}:u\\in \\Psi (v)\\} \\Big),\\quad h_{\\Psi^{(v)}}^{(k)} = \\mathrm{Combine}_{C}^{(k)}\\Big(h_{\\Psi^{(v)}}^{(k - 1)},m_{\\Psi^{(v)}}^{(k)}\\Big),}\\\\ & {\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad (2)} \\end{array} \\quad (2)\\] where \\(h_{v}^{(k)}\\) and \\(h_{\\Psi^{(v)}}^{(k)}\\) denote embeddings of the literal \\(v\\) and the clause \\(\\Psi (v)\\) in the \\(k^{t h}\\) iteration \\((h_{v^{- v}}^{(k)}\\) denotes the embedding of the negation of \\(v\\) ); \\(m_{v}^{(k)}\\) and \\(m_{\\Psi (v)}^{(k)}\\) refer to their propagated messages. Since the value of a Boolean formula is determined by the value assignment of the literal variables, Eq. 2 solely requires the final- state literal embeddings \\(\\{h_{u}^{(K)},u\\in L\\}\\) to predict the logical reasoning result. More specifically, we use \\(L\\) and \\(C\\) to denote a literal set and a clause set ( \\(L\\) and \\(C\\) may be different for each CNF formula), then \\(\\Psi (v)\\) is a clause and \\(\\Psi (v)\\) denotes a clause including the literal \\(v\\in L\\) . Note that the graph embeddings for SAT [7] and 2QBF [7] are generally represented by Eq.2. Hence our further analysis is based on Eq.2.",
    "3_certifying_unsat_by_gnns_may_fail_although_existing_researches_showed_that_gnn_can_learn_a_well-_performed_solver_for_satisfiability_problems_gnn-_based_sat_solvers_actually_have_terrible_performances_in_predicting_unsatisfiability_with_high_confidence_12_in_a_sat_formula_if_the_formula_does_not_have_a_small_unsatisfiable_core_minimal_number_of_clauses_that_is_enough_to_cause_unsatisfiability_in_fact_some_previous_work_1_even_completely_removed_unsatisfiable_formulas_from_the_training_dataset_since_they_slowed_down_the_whole_training_process_the_difficulty_in_proving_unsatisfiability_is_understandable_since_constructing_a_proof_of_unsatisfiability_demands_a_complete_reasoning_in_the_search_space_which_is_more_complex_than_constructing_a_proof_of_satisfiability_that_only_requires_a_witness_traditionally_it_relies_on_the_recursive_decision_procedures_that_either_traverse_all_possible_assignments_to_construct_the_proof_dpll_4_or_generate_extra_constraints_from_assignment_trials_that_lead_to_conflicts_until_some_of_the_constraints_contradict_each_other_cdcl_13_the_line_of_recursive_algorithms_include_some_operation_branches_that_reconfigure_the_bipartite_graph_behind_the_cnf_in_each_step_while_they_search_in_the_terms_of_a_graph_that_may_iteratively_change_eg_dpll_perhaps_miserably_their_recursive_processes_can_not_be_simulated_by_gnns_observation_31_given_a_recursive_algorithm_that_iteratively_reconfigures_the_graph_gnns_in_eq2_can_not_simulate_this_recursive_process_proof_associating_the_aggregate_and_combine_functions_in_eq_2_we_obtain_the_iterative_update_rule_for_the_embedding_of_a_literal_v_beginarrayr_l_h_vk_mathrmcombine_lklefth_vk_-_1h_v_-_vk_-_1mathrmaggregate_lklefth_psi_vk_-_1psi_vin_phi_rightright_quad_mathrmupdate_lklefth_vk_-_1h_v_-_vk_-_1h_psi_vk_-_1psi_vin_phi_right_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_stvin_l_endarray_quad_3_towards_this_principle_we_observe_that_the_embedding_update_of_v_in_the_current_stage_relies_on_the_last-_stage_embeddings_of_v_and_its_negation_neg_v_and_the_embeddings_of_all_the_clauses_that_include_v_in_a_cnf_formula_psi_v_in_phi_the_literal_v_neg_v_and_the_clauses_containing_v_are_consistent_over_iterations_hence_if_the_update_function_eq_3_is_consistent_over_the_iterations_in_eq2_ie_forall_k_in_mathbbn__mathrmupdate_lk_mathrmupdate_l_where_mathrmupdate_l_means_the_update_for_literal_embedding_gnns_derived_from_eq_3_receive_a_fixed_graph_generated_by_a_cnf_formula_as_input_however_if_a_recursive_algorithm_iteratively_changes_the_graph_that_represents_a_cnf_formula_it_implies_that_there_must_be_a_clause_that_was_changed_or_eliminated_after_this_iteration_since_clauses_are_permutation-_invariant_in_a_cnf_formula_accordingly_there_must_be_a_literal_embedding_whose_update_process_depends_on_a_clause_different_from_the_previous_iteration_it_contradicts_the_literal_embedding_update_function_learned_by_eq_3_with_forall_k_in_mathbbn__mathrmupdate_lk_mathrmupdate_l_hence_the_message-_passing_in_gnns_could_not_resemble_the_procedures_in_the_complete_sat-_solvers_in_fact_gnns_are_rather_similar_to_learning_a_subfamily_of_incomplete_sat_solvers_gsat_walk_sat_11_which_randomly_assign_variables_and_stochastically_search_for_local_witnesses_observation_32_gnns_in_eq_2_may_simulate_the_local_search_in_walk_sat_proof_recall_the_iterative_update_routine_of_walk_sat_starting_by_assigning_a_random_value_to_each_literal_variable_in_a_formula_it_randomly_chooses_an_unsatisfied_clause_in_the_formula_and_flips_the_value_of_a_boolean_variable_within_that_clause_such_process_is_repeated_till_the_literal_assignment_satisfies_all_clauses_in_the_formula_here_we_construct_the_optimal_aggregation_and_combine_functions_derived_from_eq_2_which_are_designed_to_simulate_the_procedure_of_walk_sat_in_this_way_if_the_aggregation_and_combine_functions_in_eq_2_approximate_these_optimal_aggregation_and_combine_functions_the_gnn_may_simulate_the_local_search_in_walk_sat_given_a_universe_of_literals_in_logical_reasoning_we_first_initiate_the_embeddings_of_them_and_their_negation_thus_forall_v_in_l_random_value_of_h_v0_and_h_neg_v0_are_initiated_this_assignment_can_be_treated_as_the_boolean_value_that_belong_to_different_literals_which_have_been_mapped_from_a_binary_vector_into_a_real-_value_embedding_space_about_the_literals_we_also_randomly_initiate_the_clause_embeddings_h_psi_v0_for_reasoning_each_formula_that_contains_the_clause_psi_v_here_we_define_the_optimal": "aggregation and combine functions that encode literals and clauses respectively, which GNNs in Eq. 2 may learn if they attempt to simulate Walk SAT: \\[\\begin{array}{r l} & {m_{v}^{(k)} = \\overline{{\\mathrm{Aggregate}}}_{L}\\Big(\\{h_{\\Psi^{(v)}}^{(k - 1)}:\\Psi (v)\\in \\Phi \\} \\Big),}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{\\epsilon^{(k)},} & {\\prod_{\\Psi (v)}||h_{\\Psi^{(v)}}^{(k - 1)}|| = 0}\\\\ {\\qquad 0,} & {\\prod_{\\Psi (v)}||h_{\\Psi^{(v)}}^{(k - 1)}||\\neq 0} \\end{array} \\right.} \\end{array} \\quad (4)\\] where \\(\\overline{{\\mathrm{Aggregate}}}_{L}(\\cdot)\\) denotes the optimal aggregation function to propagate literal messages and \\(m_{v}^{(k)}\\) denotes the optimally propagated message of literal \\(v\\) in the \\(k\\) iteration; \\(\\mathbf{0}\\) is a zero- value vector; \\(\\epsilon^{(k)}\\) denotes a bounded non- zero random vector generated in the \\(k\\) iteration; \\(||\\cdot ||\\) indicates a vector norm. \\[\\begin{array}{r l} & {h_{v}^{(k)} = \\overline{{\\mathrm{Combine}}}_{L}\\Big(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},m_{v}^{(k)}\\Big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{v - v}^{(k - 1)},} & {v = \\arg \\max \\{\\big||m_{u}^{(k)}\\big||\\} \\mathrm{and} \\big||m_{v}^{(k)}\\big|| > 0}\\\\ {h_{v}^{(k - 1)},} & {\\mathrm{otherwise}} \\end{array} \\right.} \\end{array} \\quad (5)\\] where \\(\\overline{{\\mathrm{Combine}}}_{L}(\\cdot)\\) denotes the optimal combine function that iteratively updates literal embeddings by the aid of the optimal message. Eq. 5 implies the local Boolean variable flipping in Walk SAT: if the norm of \\(m_{v}^{(k)}\\) is the maximum among all the optimal literal messages, its literal embedding would be replaced by the embedding of its negation, otherwise, keep the identical value. The maximization ensures only one literal embedding that would be \"flipped\" per iteration, which simulates the local search behavior. Besides, the literal embedding selected for update would not be \\(\\mathbf{0}\\) , which implies all the clauses containing this literal are satisfied (see the condition 2 in Eq. 4 ). Since all the satisfied clauses would not be selected in Walk SAT, this literal also would not be selected to update in this iteration. Finally, if a literal has been included by a clause that is unsatisfied, it would be randomly picked in some probability. The uncertainty is implied by the randomness of \\(\\epsilon^{(k)}\\) . \\[\\begin{array}{r l} & {m_{\\Psi^{(v)}}^{(k)} = \\overline{{\\mathrm{Aggregate}}}_{C}\\big(\\{h_{u}^{(k - 1)}:u\\in \\Psi (v)\\} \\big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{\\Psi^{(v)}}^{(0)},} & {\\mathrm{Sigmoid}\\Big(\\mathrm{MLP}_{2}^{*}\\Big(\\sum_{u\\in \\Psi (v)}\\mathrm{MLP}_{1}^{*}(h_{u}^{(k - 1)})\\Big)\\Big)\\geq 0.5}\\\\ {0,} & {\\mathrm{Sigmoid}\\Big(\\mathrm{MLP}_{2}^{*}\\Big(\\sum_{u\\in \\Psi (v)}\\mathbf{MLP}_{1}^{*}(h_{u}^{(k - 1)})\\Big)\\Big)< 0.5} \\end{array} \\right.} \\end{array} \\quad (6)\\] where \\(\\overline{{\\mathrm{Aggregate}}}_{C}(\\cdot)\\) denotes the optimal aggregation function that conveys the clause embedding messages during reasoning. Note that \\(\\mathrm{MLP}_{2}\\Big(\\sum_{u\\in \\Psi (v)}\\mathrm{MLP}_{1}(h_{u}^{(k - 1)})\\Big)\\) indicates Deep Sets [18], a neural network that encodes a literal embedding set \\(\\{h_{u}^{(k - 1)}\\}_{u\\in \\Psi (v)}\\) whose literals are included by a clause \\(\\Psi (v)\\) . The reduced feature would be fed into the sigmoid clause predictor. We use \\(\\mathrm{MLP}_{1}^{*}\\) and \\(\\mathrm{MLP}_{2}^{*}\\) to denote the implicit optimal prediction to each clause: given the arbitrarily initiated literal embeddings that denote the Boolean value assignment of literals, the optimal Deep Sets can predict whether the literal- derived clause is satisfied \\((\\geq 0.5)\\) or not \\((< 0.5)\\) . Since the predictor is permutation- invariant to the input, Propositions 3.1 in [15] promises that it can be approximated arbitrarily closely by graph convolution, which exactly corresponds to the parameterized clause aggregation functions in Eq.2. On the other hand, Eq. 5 promises the literal embeddings staying in their initiated values over iterations, hence the optimal Deep Sets may always judge whether a clause (the set of literals as the input of Deep Sets) is satisfied or not. \\[\\begin{array}{r l} & {h_{\\Psi^{(v)}}^{(k)} = \\overline{{\\mathrm{Combine}}}_{C}\\Big(h_{\\Psi^{(v)}}^{(k - 1)},m_{\\Psi^{(v)}}^{(k)}\\Big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{\\Psi^{(v)}}^{(k - 1)},} & {h_{\\Psi^{(v)}}^{(k - 1)} = m_{\\Psi^{(v)}}^{(k)}}\\\\ {h_{\\Psi^{(v)}}^{(0)},} & {||h_{\\Psi^{(v)}}^{(k - 1)}||< ||m_{\\Psi^{(v)}}^{(k)}||}\\\\ {0,} & {||h_{\\Psi^{(v)}}^{(k - 1)}||\\geq ||m_{\\Psi^{(v)}}^{(k)}||} \\end{array} \\right.} \\end{array} \\quad (7)\\] where \\(\\overline{{\\mathrm{Combine}}}_{C}(\\cdot)\\) denotes the optimal clause combine function. Based on the propagated messages conveyed by Eq. 2 , it determines how to iteratively update clause embeddings to simulate Walk SAT.\n\nHere we elaborate how the four optimal functions above cooperate to simulate an iteration of Walk- SAT. Since GNNs use literal embeddings as the initial input, we first analyze Eq. 6 and takes a literal \\(v\\) into our consideration. As we discussed, this function receives a set of literal embeddings that denotes a clause that contains \\(v\\) , and then, takes the optimal Deep Sets as an oracle to judge whether this clause is satisfied. The output, the optimal message about the clause, equals to the initiated embedding of the clause \\(h_{\\Psi (v)}\\) if it is satisfied, otherwise becomes 0. This process simulates the logical reasoning on a clause, which Walk SAT relies on to pick an unsatisfied clause and flip one of its variables (see Eq. 5). Based on \\(m_{\\Psi (v)}^{(k)}\\) , the optimal clause combine function (Eq. 7) updates an arbitrary clause embedding that contains \\(v\\) . The first branch states that, if the current clause message \\(m_{\\Psi (v)}^{(k)}\\) is consistent with the previous clause embedding \\(h_{\\Psi (v)}^{(k - 1)}\\) , it implies the satisfiability of the clause \\(\\Psi (v)\\) is not changed in this iteration (the previously satisfied clause is still satisfied, vice and versa). In this case the clause embedding would not be updated. The second and third branches imply that when \\(m_{\\Psi (v)}^{(k)}\\) and \\(h_{\\Psi (v)}^{(k - 1)}\\) are inconsistent, how to update the clause embedding \\(h_{\\Psi (v)}^{(k)}\\) to convey the current message about whether the clause \\(\\Psi (v)\\) is satisfied (return into the initial clause embeddings) or not (turn into 0). Therefore all updated embeddings about the clauses that contain \\(v\\) , as the neighbors of \\(v\\) , would be fed into the optimal aggregation function in Eq. 4. This function selects \\(v\\) that only exists in satisfied clauses, i.e., \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}||\\neq 0\\) (If there is an unsatisfied clauses, its embedding is 0 according to Eq. 7, and would lead to \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}|| = 0\\) ), then the embedding of \\(v\\) would become 0. The results by this operation are taken advantage by Eq. 5, which promises the literal that only exists in satisfied clauses would not be \"flipped\" (Walk SAT only chooses unsatisfied clause and select its variables to flip. If literals are not in any unsatisfied clauses, it would not be chosen). Towards the literal \\(v\\) contained by one unsatisfied clause at least \\((\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}|| = 0\\) since there exists a clause embedding equals to 0 according to Eq. 7), its literal message would be assigned by a random vector \\(\\epsilon^{(k)}\\) . It implies the randomness when Walk SAT try to select one of literal in unsatisfied clauses to flip its value. The flipping process is simulated by Eq. 6 as we have discussed. Here we further verify if a CNF formula could be satisfied, literal embeddings generated by the optimal aggregation and combine functions that represent the Boolean assignment of literal to satisfy this CNF formula, would converge over iterations (It corresponds to the stop criteria in Walk- SAT). Specifically suppose that in the \\(k\\) - 1 iteration, Eq. 5 have induced the literal embeddings so that all clauses with the literal in the formula have been satisfied. By Eq. 6 it is obvious that \\(\\forall v\\in L\\) , \\(m_{\\Psi (v)}^{(k)} = h_{\\Psi (v)}^{(0)}\\) . To this we have \\(h_{\\Psi (v)}^{(k - 1)} = m_{\\Psi (v)}^{(k)}\\) and \\(h_{\\Psi (v)}^{(k)} = h_{\\Psi (v)}^{(k - 1)} = h_{\\Psi (v)}^{(0)}\\) since all clauses in the formula have already been satisfied before the current iteration. In this case, it holds \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k - 1)}||\\neq 0\\) and leads to \\(\\forall v\\in L\\) , \\(m_{v}^{(k)} = 0\\) in this formula (Eq. 4). In term of this, Eq. 5 guarantees all the literal embeddings consistent with those in the previous iteration. Concluding the analysis above, we know that the optimal aggregation and combine functions (Eq. 4 5 6 7 ) are cooperated to simulate the local search in Walk SAT. Failure in 2QBF. Notably the failure in proving UNSAT would not be a problem for GNNs applied to solve SAT, as predicting satisfiability with high confidence has already been good enough for a binary distinction. However, 2QBF problems imply solving UNSAT, which inevitably makes GNNs unavailable in proving the relevant formulae. It probably explains the mystery in[7] about why GNNs purely learned by data- driven supervised learning lead to the same performances as random speculation [16]. ## 4 Further Discussion In this manuscript, we provide some discussions about the GNNs that consider the SAT and 2QBF problem as static graph, we haven't considered the shrinkage condition, which may apply dynamic GNN as [9], dues to the difficulty about proving the dynamic graph as we need to prove all the dynamic updating methods are impossible or not. Ought to be regarded that, this manuscript does not claim GNN is provably unable to achieve UNSAT, which remains an open issue.\n\nBelief propagation (BP) is a Bayesian message- passing method first proposed by [10], which is a useful approximation algorithm and has been applied to the SAT problems (specifically in 3- SAT [8]) and 2QBF problems [19]. BP can find the witnesses of unsatisfiability of 2QBF by adopting a bias estimation strategy. Each round of BP allows the user to select the most biased \\(\\forall\\) - variable and assign the biased value to the variable. After all the \\(\\forall\\) - variables are assigned, the formula is simplified by the assignment and sent to SAT solvers. The procedure returns the assignment as a witness of unsatisfiability if the simplified formula is unsatisfiable, or UNKNOWN otherwise. However, the fact that BP is used for each \\(\\forall\\) - variable assignment leads to high overhead, similar to the RL approach given by [5]. It is interesting, however, to see that with the added overhead, BP can find witnesses of unsatisfiability, which is what one- shot GNN- based embeddings cannot achieve. This manuscript revealed the previously unrecognized limitation of GNN in reasoning about unsatisfiability of SAT problems. This limitation is probably rooted in the simplicity of message- passing scheme, which is good enough for embedding graph features, but not for conducting complex reasoning on top of the graph structures. ## References [1] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit- SAT: An unsupervised differentiable approach. In International Conference on Learning Representations, 2019. [2] Alonzo Church. A note on the entscheidungsproblem. J. Symb. Log., 1(1):40- 41, 1936. [3] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, pages 151- 158, New York, NY, USA, 1971. ACM. [4] Martin Davis, George Logemann, and Donald W. Loveland. A machine program for theorem- proving. Commun. ACM, 5(7):394- 397, 1962. [5] Gil Lederman, Markus N. Rabe, and Sanjit A. Seshia. Learning heuristics for automated reasoning through deep reinforcement learning. Co RR, abs/1807.08058, 2018. [6] Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, and Shuicheng Yan. Semantic object parsing with graph LSTM. Co RR, abs/1603.07063, 2016. [7] Florian Lonsing, Uwe Egly, and Martina Seidl. Q- resolution with generalized axioms. In Nadia Creignou and Daniel Le Berre, editors, Theory and Applications of Satisfiability Testing - SAT 2016, pages 435- 452, Cham, 2016. Springer International Publishing.[8] M. \"M\u00e9zard\", G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812- 815, 2002. [9] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, and Charles E. Leisersen. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. Co RR, abs/1902.10191, 2019. [10] Judea Pearl. Reverend bayes on inference engines: A distributed hierarchical approach. In AAAI, pages 133- 136. AAAI Press, 1982. [11] Bart Selman, Henry A. Kautz, and Bram Cohen. Local search strategies for satisfiability testing. In Cliques, Coloring, and Satisfiability, volume 26 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 521- 531. DIMACS/AMS, 1993. [12] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single- bit supervision. In ICLR (Poster). Open- Review.net, 2019. [13] Jo\u00e3o P. Marques Silva, In\u00eas Lynce, and Sharad Malik. Conflict- driven clause learning SAT solvers. In Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications, pages 131- 153. IOS Press, 2009. [14] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR. Open Review.net, 2019. [15] Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken- ichi Kawarabayashi, and Stefanie Jegelka. What can neural networks reason about? Co RR, abs/1905.13211, 2019.\n\n[16] Zhanfu Yang, Fei Wang, Ziliang Chen, Guannan Wei, and Tiark Rompf. Graph neural reasoning for 2- quantified boolean formula solvers. Co RR, abs/1904.12084, 2019. [17] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. Neural- symbolic VQA: disentangling reasoning from vision and language understanding. Co RR, abs/1810.02338, 2018. [18] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pages 3391- 3401, 2017. [19] Pan Zhang, Abolfazl Ramezanpour, Lenka Zdeborov\u00e1, and Riccardo Zecchina. Message passing for quantified boolean formulas. Co RR, abs/1202.2536, 2012. [20] David Zheng, Vinson Luo, Jiajun Wu, and Joshua B. Tenenbaum. Unsupervised learning of latent physical properties using perception- prediction networks. Co RR, abs/1807.09244, 2018."
  },
  "section_objects": [
    {
      "heading": "Graph Neural Reasoning May Fail in Certifying Bool",
      "content": "## Introduction\n\n\nderived from the formulas, or whether the complex embedding schemes can be learned from backpropagation. Previous successes on SAT problems argued for the power of GNN, which can handle NP- complete problems [1, 12], whereas no evidences have been reported for solving semi- decidable predicate logic problems via GNN. The significant difficulty to prove the problems is the requirement of comprehensive reasoning over a search space, since a complete proof includes SAT and UNSAT (i.e., Boolean unsatisfiability). Perhaps disappointingly, this work presents some theoretical evidences that support a pessimistic conjecture: GNNs do not simulate the complete solver for UNSAT. Specifically, we discover that the neural reasoning procedure learned by GNNs does simulate the algorithms that may allow a CNF formula changing over iterations. Those complete SAT- solvers, e.g., DPLL and CDCL, are almost common in the operation that adaptively alters the original Boolean formula that eases the reasoning process. So GNNs do not learn to simulate their behaviors. Instead, we prove that by appropriately defining a specific structure of GNN that a parametrized GNN may learn, the local search heuristic in Walk SAT can be simulated by GNN. Towards these results, we believe that GNN can not solve UNSAT in existing logical reasoning problems. ## 2 Embedding Logic Formulae by GNNs Preliminary: Graph Neural Networks (GNNs). GNNs refer to the neural architectures devised to learn the embeddings of nodes and graphs via message- passing. Resembling the generic definition in [14], they consist of two successive operators to propagate the messages and evolve the embeddings over iterations: \\[m_{v}^{(k)} = \\mathrm{Aggregate}^{(k)}\\left(\\{h_{u}^{(k - 1)}:u\\in \\mathcal{N}(v)\\} \\right),\\quad h_{v}^{(k)} = \\mathrm{Combine}^{(k)}\\left(h_{v}^{(k - 1)},m_{v}^{(k)}\\right) \\quad (1)\\] where \\(h_{v}^{(k)}\\) denotes the hidden state (embedding) of node \\(v\\) in the \\(k^{t h}\\) iteration, and \\(\\mathcal{N}(v)\\) denotes the neighbors of node \\(v\\) . In each iteration, the Aggregate \\((k)(\\cdot)\\) aggregates hidden states from node \\(v\\) 's neighbors \\(\\{h_{u}^{(k - 1)}:u\\in \\mathcal{N}(v)\\}\\) to produce the new message (i.e., \\(m_{v}^{(k)}\\) ) for node \\(v\\) . Combine \\((k)(\\cdot ,\\cdot)\\) updates the embedding of \\(v\\) in terms of its previous state and its current message. After a specific number of iterations (e.g., \\(K\\) in our discussion), the embeddings should capture the global relational information of the nodes, which can be fed into other neural network modules for specific tasks. Significant successes about GNNs have been witnessed in relational reasoning [6, 17, 20], where an instance could be departed into multiple objects then encoded by a series of features with their relation. It typically suits representation in Eq. 1. Whereas in logical reasoning, a Boolean formula is in Conjunctive Normal Form (CNF) that consists of literal and clause items. In term of the independence among literals in CNF (so do clauses), [12] embeds a formula into a bipartite graph, where the nodes denote the clauses and literals that are disjoint, respectively. In this principle, given a literal \\(v\\) as a node, all the nodes of clauses that contains the literal are routinely treated as \\(v\\) 's neighbors, vice and versa for the node of each clause. We assume \\(\\Phi\\) is a logic formula in CNF, i.e., a set of clauses, and \\(\\Psi (v)\\in \\Phi\\) denote one of clauses within the logic formula \\(\\Phi\\) that contains literal \\(v\\) . Derived from Eq. 1, GNNs for logical reasoning can be further specified by \\[\\begin{array}{r l} & {m_{v}^{(k)} = \\mathrm{Aggregate}_{L}^{(k)}\\Big(\\{h_{\\Psi^{-1}}^{(k - 1)}:\\Psi (v)\\in \\Phi \\} \\Big),\\quad h_{v}^{(k)} = \\mathrm{Combine}_{L}^{(k)}\\Big(h_{v}^{(k - 1)},h_{v^{-1}}^{(k - 1)},m_{v}^{(k)}\\Big),}\\\\ & {m_{\\Psi^{(v)}}^{(k)} = \\mathrm{Aggregate}_{C}^{(k)}\\Big(\\{h_{u}^{(k - 1)}:u\\in \\Psi (v)\\} \\Big),\\quad h_{\\Psi^{(v)}}^{(k)} = \\mathrm{Combine}_{C}^{(k)}\\Big(h_{\\Psi^{(v)}}^{(k - 1)},m_{\\Psi^{(v)}}^{(k)}\\Big),}\\\\ & {\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad (2)} \\end{array} \\quad (2)\\] where \\(h_{v}^{(k)}\\) and \\(h_{\\Psi^{(v)}}^{(k)}\\) denote embeddings of the literal \\(v\\) and the clause \\(\\Psi (v)\\) in the \\(k^{t h}\\) iteration \\((h_{v^{- v}}^{(k)}\\) denotes the embedding of the negation of \\(v\\) ); \\(m_{v}^{(k)}\\) and \\(m_{\\Psi (v)}^{(k)}\\) refer to their propagated messages. Since the value of a Boolean formula is determined by the value assignment of the literal variables, Eq. 2 solely requires the final- state literal embeddings \\(\\{h_{u}^{(K)},u\\in L\\}\\) to predict the logical reasoning result. More specifically, we use \\(L\\) and \\(C\\) to denote a literal set and a clause set ( \\(L\\) and \\(C\\) may be different for each CNF formula), then \\(\\Psi (v)\\) is a clause and \\(\\Psi (v)\\) denotes a clause including the literal \\(v\\in L\\) . Note that the graph embeddings for SAT [7] and 2QBF [7] are generally represented by Eq.2. Hence our further analysis is based on Eq.2.\n\n## 3 Certifying UNSAT by GNNs may Fail Although existing researches showed that GNN can learn a well- performed solver for satisfiability problems, GNN- based SAT solvers actually have terrible performances in predicting unsatisfiability with high confidence [12] in a SAT formula, if the formula does not have a small unsatisfiable core (minimal number of clauses that is enough to cause unsatisfiability). In fact, some previous work [1] even completely removed unsatisfiable formulas from the training dataset, since they slowed down the whole training process. The difficulty in proving unsatisfiability is understandable, since constructing a proof of unsatisfiability demands a complete reasoning in the search space, which is more complex than constructing a proof of satisfiability that only requires a witness. Traditionally it relies on the recursive decision procedures that either traverse all possible assignments to construct the proof (DPLL [4]), or generate extra constraints from assignment trials that lead to conflicts, until some of the constraints contradict each other (CDCL [13]). The line of recursive algorithms include some operation branches that reconfigure the bipartite graph behind the CNF in each step while they search. In the terms of a graph that may iteratively change (e.g., DPLL), perhaps miserably, their recursive processes can not be simulated by GNNs. Observation 3.1. Given a recursive algorithm that iteratively reconfigures the graph, GNNs in Eq.2 can not simulate this recursive process. Proof. Associating the aggregate and combine functions in Eq. 2, we obtain the iterative update rule for the embedding of a literal \\(v\\) : \\[\\begin{array}{r l} & {h_{v}^{(k)} = \\mathrm{Combine}_{L}^{(k)}\\left(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},\\mathrm{Aggregate}_{L}^{(k)}\\left(\\{h_{\\Psi (v)}^{(k - 1)}:\\Psi (v)\\in \\Phi \\}\\right)\\right)}\\\\ & {\\quad = \\mathrm{Update}_{L}^{(k)}\\left(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},\\{h_{\\Psi (v)}^{(k - 1)}:\\Psi (v)\\in \\Phi \\}\\right),}\\\\ & {\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad s.t.v\\in L} \\end{array} \\quad (3)\\] Towards this principle, we observe that the embedding update of \\(v\\) in the current stage relies on the last- stage embeddings of \\(v\\) and its negation \\(\\neg v\\) , and the embeddings of all the clauses that include \\(v\\) in a CNF formula \\((\\Psi (v) \\in \\Phi)\\) . The literal \\(v\\) , \\(\\neg v\\) and the clauses containing \\(v\\) are consistent over iterations. Hence if the update function (Eq. 3) is consistent over the iterations in Eq.2, i.e., \\(\\forall k \\in \\mathbb{N}_{+}\\) , \\(\\mathrm{Update}_{L}^{(k)} = \\mathrm{Update}_{L}\\) , where \\(\\mathrm{Update}_{L}\\) means the update for literal embedding, GNNs derived from Eq. 3 receive a fixed graph generated by a CNF formula as input. However, if a recursive algorithm iteratively changes the graph that represents a CNF formula, it implies that there must be a clause that was changed (or eliminated) after this iteration, since clauses are permutation- invariant in a CNF formula. Accordingly there must be a literal embedding whose update process depends on a clause different from the previous iteration. It contradicts the literal embedding update function learned by Eq. 3 with \\(\\forall k \\in \\mathbb{N}_{+}\\) , \\(\\mathrm{Update}_{L}^{(k)} = \\mathrm{Update}_{L}\\) . Hence the message- passing in GNNs could not resemble the procedures in the complete SAT- solvers. In fact, GNNs are rather similar to learning a subfamily of incomplete SAT solvers (GSAT, Walk SAT [11]), which randomly assign variables and stochastically search for local witnesses. Observation 3.2. GNNs in Eq. 2 may simulate the local search in Walk SAT. Proof. Recall the iterative update routine of Walk SAT: starting by assigning a random value to each literal variable in a formula, it randomly chooses an unsatisfied clause in the formula and flips the value of a Boolean variable within that clause. Such process is repeated till the literal assignment satisfies all clauses in the formula. Here we construct the optimal aggregation and combine functions derived from Eq. 2, which are designed to simulate the procedure of Walk SAT. In this way, if the aggregation and combine functions in Eq. 2 approximate these optimal aggregation and combine functions, the GNN may simulate the local search in Walk SAT. Given a universe of literals in logical reasoning, we first initiate the embeddings of them and their negation, thus, \\(\\forall v \\in L\\) , random value of \\(h_{v}^{(0)}\\) and \\(h_{\\neg v}^{(0)}\\) are initiated. This assignment can be treated as the Boolean value that belong to different literals, which have been mapped from a binary vector into a real- value embedding space about the literals. We also randomly initiate the clause embeddings \\(h_{\\Psi (v)}^{(0)}\\) for reasoning each formula that contains the clause \\(\\Psi (v)\\) . Here we define the optimal\n\naggregation and combine functions that encode literals and clauses respectively, which GNNs in Eq. 2 may learn if they attempt to simulate Walk SAT: \\[\\begin{array}{r l} & {m_{v}^{(k)} = \\overline{{\\mathrm{Aggregate}}}_{L}\\Big(\\{h_{\\Psi^{(v)}}^{(k - 1)}:\\Psi (v)\\in \\Phi \\} \\Big),}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{\\epsilon^{(k)},} & {\\prod_{\\Psi (v)}||h_{\\Psi^{(v)}}^{(k - 1)}|| = 0}\\\\ {\\qquad 0,} & {\\prod_{\\Psi (v)}||h_{\\Psi^{(v)}}^{(k - 1)}||\\neq 0} \\end{array} \\right.} \\end{array} \\quad (4)\\] where \\(\\overline{{\\mathrm{Aggregate}}}_{L}(\\cdot)\\) denotes the optimal aggregation function to propagate literal messages and \\(m_{v}^{(k)}\\) denotes the optimally propagated message of literal \\(v\\) in the \\(k\\) iteration; \\(\\mathbf{0}\\) is a zero- value vector; \\(\\epsilon^{(k)}\\) denotes a bounded non- zero random vector generated in the \\(k\\) iteration; \\(||\\cdot ||\\) indicates a vector norm. \\[\\begin{array}{r l} & {h_{v}^{(k)} = \\overline{{\\mathrm{Combine}}}_{L}\\Big(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},m_{v}^{(k)}\\Big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{v - v}^{(k - 1)},} & {v = \\arg \\max \\{\\big||m_{u}^{(k)}\\big||\\} \\mathrm{and} \\big||m_{v}^{(k)}\\big|| > 0}\\\\ {h_{v}^{(k - 1)},} & {\\mathrm{otherwise}} \\end{array} \\right.} \\end{array} \\quad (5)\\] where \\(\\overline{{\\mathrm{Combine}}}_{L}(\\cdot)\\) denotes the optimal combine function that iteratively updates literal embeddings by the aid of the optimal message. Eq. 5 implies the local Boolean variable flipping in Walk SAT: if the norm of \\(m_{v}^{(k)}\\) is the maximum among all the optimal literal messages, its literal embedding would be replaced by the embedding of its negation, otherwise, keep the identical value. The maximization ensures only one literal embedding that would be \"flipped\" per iteration, which simulates the local search behavior. Besides, the literal embedding selected for update would not be \\(\\mathbf{0}\\) , which implies all the clauses containing this literal are satisfied (see the condition 2 in Eq. 4 ). Since all the satisfied clauses would not be selected in Walk SAT, this literal also would not be selected to update in this iteration. Finally, if a literal has been included by a clause that is unsatisfied, it would be randomly picked in some probability. The uncertainty is implied by the randomness of \\(\\epsilon^{(k)}\\) . \\[\\begin{array}{r l} & {m_{\\Psi^{(v)}}^{(k)} = \\overline{{\\mathrm{Aggregate}}}_{C}\\big(\\{h_{u}^{(k - 1)}:u\\in \\Psi (v)\\} \\big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{\\Psi^{(v)}}^{(0)},} & {\\mathrm{Sigmoid}\\Big(\\mathrm{MLP}_{2}^{*}\\Big(\\sum_{u\\in \\Psi (v)}\\mathrm{MLP}_{1}^{*}(h_{u}^{(k - 1)})\\Big)\\Big)\\geq 0.5}\\\\ {0,} & {\\mathrm{Sigmoid}\\Big(\\mathrm{MLP}_{2}^{*}\\Big(\\sum_{u\\in \\Psi (v)}\\mathbf{MLP}_{1}^{*}(h_{u}^{(k - 1)})\\Big)\\Big)< 0.5} \\end{array} \\right.} \\end{array} \\quad (6)\\] where \\(\\overline{{\\mathrm{Aggregate}}}_{C}(\\cdot)\\) denotes the optimal aggregation function that conveys the clause embedding messages during reasoning. Note that \\(\\mathrm{MLP}_{2}\\Big(\\sum_{u\\in \\Psi (v)}\\mathrm{MLP}_{1}(h_{u}^{(k - 1)})\\Big)\\) indicates Deep Sets [18], a neural network that encodes a literal embedding set \\(\\{h_{u}^{(k - 1)}\\}_{u\\in \\Psi (v)}\\) whose literals are included by a clause \\(\\Psi (v)\\) . The reduced feature would be fed into the sigmoid clause predictor. We use \\(\\mathrm{MLP}_{1}^{*}\\) and \\(\\mathrm{MLP}_{2}^{*}\\) to denote the implicit optimal prediction to each clause: given the arbitrarily initiated literal embeddings that denote the Boolean value assignment of literals, the optimal Deep Sets can predict whether the literal- derived clause is satisfied \\((\\geq 0.5)\\) or not \\((< 0.5)\\) . Since the predictor is permutation- invariant to the input, Propositions 3.1 in [15] promises that it can be approximated arbitrarily closely by graph convolution, which exactly corresponds to the parameterized clause aggregation functions in Eq.2. On the other hand, Eq. 5 promises the literal embeddings staying in their initiated values over iterations, hence the optimal Deep Sets may always judge whether a clause (the set of literals as the input of Deep Sets) is satisfied or not. \\[\\begin{array}{r l} & {h_{\\Psi^{(v)}}^{(k)} = \\overline{{\\mathrm{Combine}}}_{C}\\Big(h_{\\Psi^{(v)}}^{(k - 1)},m_{\\Psi^{(v)}}^{(k)}\\Big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{\\Psi^{(v)}}^{(k - 1)},} & {h_{\\Psi^{(v)}}^{(k - 1)} = m_{\\Psi^{(v)}}^{(k)}}\\\\ {h_{\\Psi^{(v)}}^{(0)},} & {||h_{\\Psi^{(v)}}^{(k - 1)}||< ||m_{\\Psi^{(v)}}^{(k)}||}\\\\ {0,} & {||h_{\\Psi^{(v)}}^{(k - 1)}||\\geq ||m_{\\Psi^{(v)}}^{(k)}||} \\end{array} \\right.} \\end{array} \\quad (7)\\] where \\(\\overline{{\\mathrm{Combine}}}_{C}(\\cdot)\\) denotes the optimal clause combine function. Based on the propagated messages conveyed by Eq. 2 , it determines how to iteratively update clause embeddings to simulate Walk SAT.\n\nHere we elaborate how the four optimal functions above cooperate to simulate an iteration of Walk- SAT. Since GNNs use literal embeddings as the initial input, we first analyze Eq. 6 and takes a literal \\(v\\) into our consideration. As we discussed, this function receives a set of literal embeddings that denotes a clause that contains \\(v\\) , and then, takes the optimal Deep Sets as an oracle to judge whether this clause is satisfied. The output, the optimal message about the clause, equals to the initiated embedding of the clause \\(h_{\\Psi (v)}\\) if it is satisfied, otherwise becomes 0. This process simulates the logical reasoning on a clause, which Walk SAT relies on to pick an unsatisfied clause and flip one of its variables (see Eq. 5). Based on \\(m_{\\Psi (v)}^{(k)}\\) , the optimal clause combine function (Eq. 7) updates an arbitrary clause embedding that contains \\(v\\) . The first branch states that, if the current clause message \\(m_{\\Psi (v)}^{(k)}\\) is consistent with the previous clause embedding \\(h_{\\Psi (v)}^{(k - 1)}\\) , it implies the satisfiability of the clause \\(\\Psi (v)\\) is not changed in this iteration (the previously satisfied clause is still satisfied, vice and versa). In this case the clause embedding would not be updated. The second and third branches imply that when \\(m_{\\Psi (v)}^{(k)}\\) and \\(h_{\\Psi (v)}^{(k - 1)}\\) are inconsistent, how to update the clause embedding \\(h_{\\Psi (v)}^{(k)}\\) to convey the current message about whether the clause \\(\\Psi (v)\\) is satisfied (return into the initial clause embeddings) or not (turn into 0). Therefore all updated embeddings about the clauses that contain \\(v\\) , as the neighbors of \\(v\\) , would be fed into the optimal aggregation function in Eq. 4. This function selects \\(v\\) that only exists in satisfied clauses, i.e., \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}||\\neq 0\\) (If there is an unsatisfied clauses, its embedding is 0 according to Eq. 7, and would lead to \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}|| = 0\\) ), then the embedding of \\(v\\) would become 0. The results by this operation are taken advantage by Eq. 5, which promises the literal that only exists in satisfied clauses would not be \"flipped\" (Walk SAT only chooses unsatisfied clause and select its variables to flip. If literals are not in any unsatisfied clauses, it would not be chosen). Towards the literal \\(v\\) contained by one unsatisfied clause at least \\((\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}|| = 0\\) since there exists a clause embedding equals to 0 according to Eq. 7), its literal message would be assigned by a random vector \\(\\epsilon^{(k)}\\) . It implies the randomness when Walk SAT try to select one of literal in unsatisfied clauses to flip its value. The flipping process is simulated by Eq. 6 as we have discussed. Here we further verify if a CNF formula could be satisfied, literal embeddings generated by the optimal aggregation and combine functions that represent the Boolean assignment of literal to satisfy this CNF formula, would converge over iterations (It corresponds to the stop criteria in Walk- SAT). Specifically suppose that in the \\(k\\) - 1 iteration, Eq. 5 have induced the literal embeddings so that all clauses with the literal in the formula have been satisfied. By Eq. 6 it is obvious that \\(\\forall v\\in L\\) , \\(m_{\\Psi (v)}^{(k)} = h_{\\Psi (v)}^{(0)}\\) . To this we have \\(h_{\\Psi (v)}^{(k - 1)} = m_{\\Psi (v)}^{(k)}\\) and \\(h_{\\Psi (v)}^{(k)} = h_{\\Psi (v)}^{(k - 1)} = h_{\\Psi (v)}^{(0)}\\) since all clauses in the formula have already been satisfied before the current iteration. In this case, it holds \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k - 1)}||\\neq 0\\) and leads to \\(\\forall v\\in L\\) , \\(m_{v}^{(k)} = 0\\) in this formula (Eq. 4). In term of this, Eq. 5 guarantees all the literal embeddings consistent with those in the previous iteration. Concluding the analysis above, we know that the optimal aggregation and combine functions (Eq. 4 5 6 7 ) are cooperated to simulate the local search in Walk SAT. Failure in 2QBF. Notably the failure in proving UNSAT would not be a problem for GNNs applied to solve SAT, as predicting satisfiability with high confidence has already been good enough for a binary distinction. However, 2QBF problems imply solving UNSAT, which inevitably makes GNNs unavailable in proving the relevant formulae. It probably explains the mystery in[7] about why GNNs purely learned by data- driven supervised learning lead to the same performances as random speculation [16]. ## 4 Further Discussion In this manuscript, we provide some discussions about the GNNs that consider the SAT and 2QBF problem as static graph, we haven't considered the shrinkage condition, which may apply dynamic GNN as [9], dues to the difficulty about proving the dynamic graph as we need to prove all the dynamic updating methods are impossible or not. Ought to be regarded that, this manuscript does not claim GNN is provably unable to achieve UNSAT, which remains an open issue.\n\nBelief propagation (BP) is a Bayesian message- passing method first proposed by [10], which is a useful approximation algorithm and has been applied to the SAT problems (specifically in 3- SAT [8]) and 2QBF problems [19]. BP can find the witnesses of unsatisfiability of 2QBF by adopting a bias estimation strategy. Each round of BP allows the user to select the most biased \\(\\forall\\) - variable and assign the biased value to the variable. After all the \\(\\forall\\) - variables are assigned, the formula is simplified by the assignment and sent to SAT solvers. The procedure returns the assignment as a witness of unsatisfiability if the simplified formula is unsatisfiable, or UNKNOWN otherwise. However, the fact that BP is used for each \\(\\forall\\) - variable assignment leads to high overhead, similar to the RL approach given by [5]. It is interesting, however, to see that with the added overhead, BP can find witnesses of unsatisfiability, which is what one- shot GNN- based embeddings cannot achieve. This manuscript revealed the previously unrecognized limitation of GNN in reasoning about unsatisfiability of SAT problems. This limitation is probably rooted in the simplicity of message- passing scheme, which is good enough for embedding graph features, but not for conducting complex reasoning on top of the graph structures. ## References [1] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit- SAT: An unsupervised differentiable approach. In International Conference on Learning Representations, 2019. [2] Alonzo Church. A note on the entscheidungsproblem. J. Symb. Log., 1(1):40- 41, 1936. [3] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, pages 151- 158, New York, NY, USA, 1971. ACM. [4] Martin Davis, George Logemann, and Donald W. Loveland. A machine program for theorem- proving. Commun. ACM, 5(7):394- 397, 1962. [5] Gil Lederman, Markus N. Rabe, and Sanjit A. Seshia. Learning heuristics for automated reasoning through deep reinforcement learning. Co RR, abs/1807.08058, 2018. [6] Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, and Shuicheng Yan. Semantic object parsing with graph LSTM. Co RR, abs/1603.07063, 2016. [7] Florian Lonsing, Uwe Egly, and Martina Seidl. Q- resolution with generalized axioms. In Nadia Creignou and Daniel Le Berre, editors, Theory and Applications of Satisfiability Testing - SAT 2016, pages 435- 452, Cham, 2016. Springer International Publishing.[8] M. \"M\u00e9zard\", G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812- 815, 2002. [9] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, and Charles E. Leisersen. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. Co RR, abs/1902.10191, 2019. [10] Judea Pearl. Reverend bayes on inference engines: A distributed hierarchical approach. In AAAI, pages 133- 136. AAAI Press, 1982. [11] Bart Selman, Henry A. Kautz, and Bram Cohen. Local search strategies for satisfiability testing. In Cliques, Coloring, and Satisfiability, volume 26 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 521- 531. DIMACS/AMS, 1993. [12] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single- bit supervision. In ICLR (Poster). Open- Review.net, 2019. [13] Jo\u00e3o P. Marques Silva, In\u00eas Lynce, and Sharad Malik. Conflict- driven clause learning SAT solvers. In Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications, pages 131- 153. IOS Press, 2009. [14] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR. Open Review.net, 2019. [15] Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken- ichi Kawarabayashi, and Stefanie Jegelka. What can neural networks reason about? Co RR, abs/1905.13211, 2019.\n\n[16] Zhanfu Yang, Fei Wang, Ziliang Chen, Guannan Wei, and Tiark Rompf. Graph neural reasoning for 2- quantified boolean formula solvers. Co RR, abs/1904.12084, 2019. [17] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. Neural- symbolic VQA: disentangling reasoning from vision and language understanding. Co RR, abs/1810.02338, 2018. [18] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pages 3391- 3401, 2017. [19] Pan Zhang, Abolfazl Ramezanpour, Lenka Zdeborov\u00e1, and Riccardo Zecchina. Message passing for quantified boolean formulas. Co RR, abs/1202.2536, 2012. [20] David Zheng, Vinson Luo, Jiajun Wu, and Joshua B. Tenenbaum. Unsupervised learning of latent physical properties using perception- prediction networks. Co RR, abs/1807.09244, 2018.",
      "level": 1,
      "line_start": 1,
      "line_end": 17
    },
    {
      "heading": "Introduction",
      "content": "derived from the formulas, or whether the complex embedding schemes can be learned from backpropagation. Previous successes on SAT problems argued for the power of GNN, which can handle NP- complete problems [1, 12], whereas no evidences have been reported for solving semi- decidable predicate logic problems via GNN. The significant difficulty to prove the problems is the requirement of comprehensive reasoning over a search space, since a complete proof includes SAT and UNSAT (i.e., Boolean unsatisfiability). Perhaps disappointingly, this work presents some theoretical evidences that support a pessimistic conjecture: GNNs do not simulate the complete solver for UNSAT. Specifically, we discover that the neural reasoning procedure learned by GNNs does simulate the algorithms that may allow a CNF formula changing over iterations. Those complete SAT- solvers, e.g., DPLL and CDCL, are almost common in the operation that adaptively alters the original Boolean formula that eases the reasoning process. So GNNs do not learn to simulate their behaviors. Instead, we prove that by appropriately defining a specific structure of GNN that a parametrized GNN may learn, the local search heuristic in Walk SAT can be simulated by GNN. Towards these results, we believe that GNN can not solve UNSAT in existing logical reasoning problems. ## 2 Embedding Logic Formulae by GNNs Preliminary: Graph Neural Networks (GNNs). GNNs refer to the neural architectures devised to learn the embeddings of nodes and graphs via message- passing. Resembling the generic definition in [14], they consist of two successive operators to propagate the messages and evolve the embeddings over iterations: \\[m_{v}^{(k)} = \\mathrm{Aggregate}^{(k)}\\left(\\{h_{u}^{(k - 1)}:u\\in \\mathcal{N}(v)\\} \\right),\\quad h_{v}^{(k)} = \\mathrm{Combine}^{(k)}\\left(h_{v}^{(k - 1)},m_{v}^{(k)}\\right) \\quad (1)\\] where \\(h_{v}^{(k)}\\) denotes the hidden state (embedding) of node \\(v\\) in the \\(k^{t h}\\) iteration, and \\(\\mathcal{N}(v)\\) denotes the neighbors of node \\(v\\) . In each iteration, the Aggregate \\((k)(\\cdot)\\) aggregates hidden states from node \\(v\\) 's neighbors \\(\\{h_{u}^{(k - 1)}:u\\in \\mathcal{N}(v)\\}\\) to produce the new message (i.e., \\(m_{v}^{(k)}\\) ) for node \\(v\\) . Combine \\((k)(\\cdot ,\\cdot)\\) updates the embedding of \\(v\\) in terms of its previous state and its current message. After a specific number of iterations (e.g., \\(K\\) in our discussion), the embeddings should capture the global relational information of the nodes, which can be fed into other neural network modules for specific tasks. Significant successes about GNNs have been witnessed in relational reasoning [6, 17, 20], where an instance could be departed into multiple objects then encoded by a series of features with their relation. It typically suits representation in Eq. 1. Whereas in logical reasoning, a Boolean formula is in Conjunctive Normal Form (CNF) that consists of literal and clause items. In term of the independence among literals in CNF (so do clauses), [12] embeds a formula into a bipartite graph, where the nodes denote the clauses and literals that are disjoint, respectively. In this principle, given a literal \\(v\\) as a node, all the nodes of clauses that contains the literal are routinely treated as \\(v\\) 's neighbors, vice and versa for the node of each clause. We assume \\(\\Phi\\) is a logic formula in CNF, i.e., a set of clauses, and \\(\\Psi (v)\\in \\Phi\\) denote one of clauses within the logic formula \\(\\Phi\\) that contains literal \\(v\\) . Derived from Eq. 1, GNNs for logical reasoning can be further specified by \\[\\begin{array}{r l} & {m_{v}^{(k)} = \\mathrm{Aggregate}_{L}^{(k)}\\Big(\\{h_{\\Psi^{-1}}^{(k - 1)}:\\Psi (v)\\in \\Phi \\} \\Big),\\quad h_{v}^{(k)} = \\mathrm{Combine}_{L}^{(k)}\\Big(h_{v}^{(k - 1)},h_{v^{-1}}^{(k - 1)},m_{v}^{(k)}\\Big),}\\\\ & {m_{\\Psi^{(v)}}^{(k)} = \\mathrm{Aggregate}_{C}^{(k)}\\Big(\\{h_{u}^{(k - 1)}:u\\in \\Psi (v)\\} \\Big),\\quad h_{\\Psi^{(v)}}^{(k)} = \\mathrm{Combine}_{C}^{(k)}\\Big(h_{\\Psi^{(v)}}^{(k - 1)},m_{\\Psi^{(v)}}^{(k)}\\Big),}\\\\ & {\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad (2)} \\end{array} \\quad (2)\\] where \\(h_{v}^{(k)}\\) and \\(h_{\\Psi^{(v)}}^{(k)}\\) denote embeddings of the literal \\(v\\) and the clause \\(\\Psi (v)\\) in the \\(k^{t h}\\) iteration \\((h_{v^{- v}}^{(k)}\\) denotes the embedding of the negation of \\(v\\) ); \\(m_{v}^{(k)}\\) and \\(m_{\\Psi (v)}^{(k)}\\) refer to their propagated messages. Since the value of a Boolean formula is determined by the value assignment of the literal variables, Eq. 2 solely requires the final- state literal embeddings \\(\\{h_{u}^{(K)},u\\in L\\}\\) to predict the logical reasoning result. More specifically, we use \\(L\\) and \\(C\\) to denote a literal set and a clause set ( \\(L\\) and \\(C\\) may be different for each CNF formula), then \\(\\Psi (v)\\) is a clause and \\(\\Psi (v)\\) denotes a clause including the literal \\(v\\in L\\) . Note that the graph embeddings for SAT [7] and 2QBF [7] are generally represented by Eq.2. Hence our further analysis is based on Eq.2.",
      "level": 2,
      "line_start": 4,
      "line_end": 8
    },
    {
      "heading": "3 Certifying UNSAT by GNNs may Fail Although existing researches showed that GNN can learn a well- performed solver for satisfiability problems, GNN- based SAT solvers actually have terrible performances in predicting unsatisfiability with high confidence [12] in a SAT formula, if the formula does not have a small unsatisfiable core (minimal number of clauses that is enough to cause unsatisfiability). In fact, some previous work [1] even completely removed unsatisfiable formulas from the training dataset, since they slowed down the whole training process. The difficulty in proving unsatisfiability is understandable, since constructing a proof of unsatisfiability demands a complete reasoning in the search space, which is more complex than constructing a proof of satisfiability that only requires a witness. Traditionally it relies on the recursive decision procedures that either traverse all possible assignments to construct the proof (DPLL [4]), or generate extra constraints from assignment trials that lead to conflicts, until some of the constraints contradict each other (CDCL [13]). The line of recursive algorithms include some operation branches that reconfigure the bipartite graph behind the CNF in each step while they search. In the terms of a graph that may iteratively change (e.g., DPLL), perhaps miserably, their recursive processes can not be simulated by GNNs. Observation 3.1. Given a recursive algorithm that iteratively reconfigures the graph, GNNs in Eq.2 can not simulate this recursive process. Proof. Associating the aggregate and combine functions in Eq. 2, we obtain the iterative update rule for the embedding of a literal \\(v\\) : \\[\\begin{array}{r l} & {h_{v}^{(k)} = \\mathrm{Combine}_{L}^{(k)}\\left(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},\\mathrm{Aggregate}_{L}^{(k)}\\left(\\{h_{\\Psi (v)}^{(k - 1)}:\\Psi (v)\\in \\Phi \\}\\right)\\right)}\\\\ & {\\quad = \\mathrm{Update}_{L}^{(k)}\\left(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},\\{h_{\\Psi (v)}^{(k - 1)}:\\Psi (v)\\in \\Phi \\}\\right),}\\\\ & {\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad s.t.v\\in L} \\end{array} \\quad (3)\\] Towards this principle, we observe that the embedding update of \\(v\\) in the current stage relies on the last- stage embeddings of \\(v\\) and its negation \\(\\neg v\\) , and the embeddings of all the clauses that include \\(v\\) in a CNF formula \\((\\Psi (v) \\in \\Phi)\\) . The literal \\(v\\) , \\(\\neg v\\) and the clauses containing \\(v\\) are consistent over iterations. Hence if the update function (Eq. 3) is consistent over the iterations in Eq.2, i.e., \\(\\forall k \\in \\mathbb{N}_{+}\\) , \\(\\mathrm{Update}_{L}^{(k)} = \\mathrm{Update}_{L}\\) , where \\(\\mathrm{Update}_{L}\\) means the update for literal embedding, GNNs derived from Eq. 3 receive a fixed graph generated by a CNF formula as input. However, if a recursive algorithm iteratively changes the graph that represents a CNF formula, it implies that there must be a clause that was changed (or eliminated) after this iteration, since clauses are permutation- invariant in a CNF formula. Accordingly there must be a literal embedding whose update process depends on a clause different from the previous iteration. It contradicts the literal embedding update function learned by Eq. 3 with \\(\\forall k \\in \\mathbb{N}_{+}\\) , \\(\\mathrm{Update}_{L}^{(k)} = \\mathrm{Update}_{L}\\) . Hence the message- passing in GNNs could not resemble the procedures in the complete SAT- solvers. In fact, GNNs are rather similar to learning a subfamily of incomplete SAT solvers (GSAT, Walk SAT [11]), which randomly assign variables and stochastically search for local witnesses. Observation 3.2. GNNs in Eq. 2 may simulate the local search in Walk SAT. Proof. Recall the iterative update routine of Walk SAT: starting by assigning a random value to each literal variable in a formula, it randomly chooses an unsatisfied clause in the formula and flips the value of a Boolean variable within that clause. Such process is repeated till the literal assignment satisfies all clauses in the formula. Here we construct the optimal aggregation and combine functions derived from Eq. 2, which are designed to simulate the procedure of Walk SAT. In this way, if the aggregation and combine functions in Eq. 2 approximate these optimal aggregation and combine functions, the GNN may simulate the local search in Walk SAT. Given a universe of literals in logical reasoning, we first initiate the embeddings of them and their negation, thus, \\(\\forall v \\in L\\) , random value of \\(h_{v}^{(0)}\\) and \\(h_{\\neg v}^{(0)}\\) are initiated. This assignment can be treated as the Boolean value that belong to different literals, which have been mapped from a binary vector into a real- value embedding space about the literals. We also randomly initiate the clause embeddings \\(h_{\\Psi (v)}^{(0)}\\) for reasoning each formula that contains the clause \\(\\Psi (v)\\) . Here we define the optimal",
      "content": "aggregation and combine functions that encode literals and clauses respectively, which GNNs in Eq. 2 may learn if they attempt to simulate Walk SAT: \\[\\begin{array}{r l} & {m_{v}^{(k)} = \\overline{{\\mathrm{Aggregate}}}_{L}\\Big(\\{h_{\\Psi^{(v)}}^{(k - 1)}:\\Psi (v)\\in \\Phi \\} \\Big),}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{\\epsilon^{(k)},} & {\\prod_{\\Psi (v)}||h_{\\Psi^{(v)}}^{(k - 1)}|| = 0}\\\\ {\\qquad 0,} & {\\prod_{\\Psi (v)}||h_{\\Psi^{(v)}}^{(k - 1)}||\\neq 0} \\end{array} \\right.} \\end{array} \\quad (4)\\] where \\(\\overline{{\\mathrm{Aggregate}}}_{L}(\\cdot)\\) denotes the optimal aggregation function to propagate literal messages and \\(m_{v}^{(k)}\\) denotes the optimally propagated message of literal \\(v\\) in the \\(k\\) iteration; \\(\\mathbf{0}\\) is a zero- value vector; \\(\\epsilon^{(k)}\\) denotes a bounded non- zero random vector generated in the \\(k\\) iteration; \\(||\\cdot ||\\) indicates a vector norm. \\[\\begin{array}{r l} & {h_{v}^{(k)} = \\overline{{\\mathrm{Combine}}}_{L}\\Big(h_{v}^{(k - 1)},h_{v - v}^{(k - 1)},m_{v}^{(k)}\\Big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{v - v}^{(k - 1)},} & {v = \\arg \\max \\{\\big||m_{u}^{(k)}\\big||\\} \\mathrm{and} \\big||m_{v}^{(k)}\\big|| > 0}\\\\ {h_{v}^{(k - 1)},} & {\\mathrm{otherwise}} \\end{array} \\right.} \\end{array} \\quad (5)\\] where \\(\\overline{{\\mathrm{Combine}}}_{L}(\\cdot)\\) denotes the optimal combine function that iteratively updates literal embeddings by the aid of the optimal message. Eq. 5 implies the local Boolean variable flipping in Walk SAT: if the norm of \\(m_{v}^{(k)}\\) is the maximum among all the optimal literal messages, its literal embedding would be replaced by the embedding of its negation, otherwise, keep the identical value. The maximization ensures only one literal embedding that would be \"flipped\" per iteration, which simulates the local search behavior. Besides, the literal embedding selected for update would not be \\(\\mathbf{0}\\) , which implies all the clauses containing this literal are satisfied (see the condition 2 in Eq. 4 ). Since all the satisfied clauses would not be selected in Walk SAT, this literal also would not be selected to update in this iteration. Finally, if a literal has been included by a clause that is unsatisfied, it would be randomly picked in some probability. The uncertainty is implied by the randomness of \\(\\epsilon^{(k)}\\) . \\[\\begin{array}{r l} & {m_{\\Psi^{(v)}}^{(k)} = \\overline{{\\mathrm{Aggregate}}}_{C}\\big(\\{h_{u}^{(k - 1)}:u\\in \\Psi (v)\\} \\big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{\\Psi^{(v)}}^{(0)},} & {\\mathrm{Sigmoid}\\Big(\\mathrm{MLP}_{2}^{*}\\Big(\\sum_{u\\in \\Psi (v)}\\mathrm{MLP}_{1}^{*}(h_{u}^{(k - 1)})\\Big)\\Big)\\geq 0.5}\\\\ {0,} & {\\mathrm{Sigmoid}\\Big(\\mathrm{MLP}_{2}^{*}\\Big(\\sum_{u\\in \\Psi (v)}\\mathbf{MLP}_{1}^{*}(h_{u}^{(k - 1)})\\Big)\\Big)< 0.5} \\end{array} \\right.} \\end{array} \\quad (6)\\] where \\(\\overline{{\\mathrm{Aggregate}}}_{C}(\\cdot)\\) denotes the optimal aggregation function that conveys the clause embedding messages during reasoning. Note that \\(\\mathrm{MLP}_{2}\\Big(\\sum_{u\\in \\Psi (v)}\\mathrm{MLP}_{1}(h_{u}^{(k - 1)})\\Big)\\) indicates Deep Sets [18], a neural network that encodes a literal embedding set \\(\\{h_{u}^{(k - 1)}\\}_{u\\in \\Psi (v)}\\) whose literals are included by a clause \\(\\Psi (v)\\) . The reduced feature would be fed into the sigmoid clause predictor. We use \\(\\mathrm{MLP}_{1}^{*}\\) and \\(\\mathrm{MLP}_{2}^{*}\\) to denote the implicit optimal prediction to each clause: given the arbitrarily initiated literal embeddings that denote the Boolean value assignment of literals, the optimal Deep Sets can predict whether the literal- derived clause is satisfied \\((\\geq 0.5)\\) or not \\((< 0.5)\\) . Since the predictor is permutation- invariant to the input, Propositions 3.1 in [15] promises that it can be approximated arbitrarily closely by graph convolution, which exactly corresponds to the parameterized clause aggregation functions in Eq.2. On the other hand, Eq. 5 promises the literal embeddings staying in their initiated values over iterations, hence the optimal Deep Sets may always judge whether a clause (the set of literals as the input of Deep Sets) is satisfied or not. \\[\\begin{array}{r l} & {h_{\\Psi^{(v)}}^{(k)} = \\overline{{\\mathrm{Combine}}}_{C}\\Big(h_{\\Psi^{(v)}}^{(k - 1)},m_{\\Psi^{(v)}}^{(k)}\\Big)}\\\\ & {\\qquad = \\left\\{ \\begin{array}{l l}{h_{\\Psi^{(v)}}^{(k - 1)},} & {h_{\\Psi^{(v)}}^{(k - 1)} = m_{\\Psi^{(v)}}^{(k)}}\\\\ {h_{\\Psi^{(v)}}^{(0)},} & {||h_{\\Psi^{(v)}}^{(k - 1)}||< ||m_{\\Psi^{(v)}}^{(k)}||}\\\\ {0,} & {||h_{\\Psi^{(v)}}^{(k - 1)}||\\geq ||m_{\\Psi^{(v)}}^{(k)}||} \\end{array} \\right.} \\end{array} \\quad (7)\\] where \\(\\overline{{\\mathrm{Combine}}}_{C}(\\cdot)\\) denotes the optimal clause combine function. Based on the propagated messages conveyed by Eq. 2 , it determines how to iteratively update clause embeddings to simulate Walk SAT.\n\nHere we elaborate how the four optimal functions above cooperate to simulate an iteration of Walk- SAT. Since GNNs use literal embeddings as the initial input, we first analyze Eq. 6 and takes a literal \\(v\\) into our consideration. As we discussed, this function receives a set of literal embeddings that denotes a clause that contains \\(v\\) , and then, takes the optimal Deep Sets as an oracle to judge whether this clause is satisfied. The output, the optimal message about the clause, equals to the initiated embedding of the clause \\(h_{\\Psi (v)}\\) if it is satisfied, otherwise becomes 0. This process simulates the logical reasoning on a clause, which Walk SAT relies on to pick an unsatisfied clause and flip one of its variables (see Eq. 5). Based on \\(m_{\\Psi (v)}^{(k)}\\) , the optimal clause combine function (Eq. 7) updates an arbitrary clause embedding that contains \\(v\\) . The first branch states that, if the current clause message \\(m_{\\Psi (v)}^{(k)}\\) is consistent with the previous clause embedding \\(h_{\\Psi (v)}^{(k - 1)}\\) , it implies the satisfiability of the clause \\(\\Psi (v)\\) is not changed in this iteration (the previously satisfied clause is still satisfied, vice and versa). In this case the clause embedding would not be updated. The second and third branches imply that when \\(m_{\\Psi (v)}^{(k)}\\) and \\(h_{\\Psi (v)}^{(k - 1)}\\) are inconsistent, how to update the clause embedding \\(h_{\\Psi (v)}^{(k)}\\) to convey the current message about whether the clause \\(\\Psi (v)\\) is satisfied (return into the initial clause embeddings) or not (turn into 0). Therefore all updated embeddings about the clauses that contain \\(v\\) , as the neighbors of \\(v\\) , would be fed into the optimal aggregation function in Eq. 4. This function selects \\(v\\) that only exists in satisfied clauses, i.e., \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}||\\neq 0\\) (If there is an unsatisfied clauses, its embedding is 0 according to Eq. 7, and would lead to \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}|| = 0\\) ), then the embedding of \\(v\\) would become 0. The results by this operation are taken advantage by Eq. 5, which promises the literal that only exists in satisfied clauses would not be \"flipped\" (Walk SAT only chooses unsatisfied clause and select its variables to flip. If literals are not in any unsatisfied clauses, it would not be chosen). Towards the literal \\(v\\) contained by one unsatisfied clause at least \\((\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k)}|| = 0\\) since there exists a clause embedding equals to 0 according to Eq. 7), its literal message would be assigned by a random vector \\(\\epsilon^{(k)}\\) . It implies the randomness when Walk SAT try to select one of literal in unsatisfied clauses to flip its value. The flipping process is simulated by Eq. 6 as we have discussed. Here we further verify if a CNF formula could be satisfied, literal embeddings generated by the optimal aggregation and combine functions that represent the Boolean assignment of literal to satisfy this CNF formula, would converge over iterations (It corresponds to the stop criteria in Walk- SAT). Specifically suppose that in the \\(k\\) - 1 iteration, Eq. 5 have induced the literal embeddings so that all clauses with the literal in the formula have been satisfied. By Eq. 6 it is obvious that \\(\\forall v\\in L\\) , \\(m_{\\Psi (v)}^{(k)} = h_{\\Psi (v)}^{(0)}\\) . To this we have \\(h_{\\Psi (v)}^{(k - 1)} = m_{\\Psi (v)}^{(k)}\\) and \\(h_{\\Psi (v)}^{(k)} = h_{\\Psi (v)}^{(k - 1)} = h_{\\Psi (v)}^{(0)}\\) since all clauses in the formula have already been satisfied before the current iteration. In this case, it holds \\(\\prod_{\\Psi (v)}||h_{\\Psi (v)}^{(k - 1)}||\\neq 0\\) and leads to \\(\\forall v\\in L\\) , \\(m_{v}^{(k)} = 0\\) in this formula (Eq. 4). In term of this, Eq. 5 guarantees all the literal embeddings consistent with those in the previous iteration. Concluding the analysis above, we know that the optimal aggregation and combine functions (Eq. 4 5 6 7 ) are cooperated to simulate the local search in Walk SAT. Failure in 2QBF. Notably the failure in proving UNSAT would not be a problem for GNNs applied to solve SAT, as predicting satisfiability with high confidence has already been good enough for a binary distinction. However, 2QBF problems imply solving UNSAT, which inevitably makes GNNs unavailable in proving the relevant formulae. It probably explains the mystery in[7] about why GNNs purely learned by data- driven supervised learning lead to the same performances as random speculation [16]. ## 4 Further Discussion In this manuscript, we provide some discussions about the GNNs that consider the SAT and 2QBF problem as static graph, we haven't considered the shrinkage condition, which may apply dynamic GNN as [9], dues to the difficulty about proving the dynamic graph as we need to prove all the dynamic updating methods are impossible or not. Ought to be regarded that, this manuscript does not claim GNN is provably unable to achieve UNSAT, which remains an open issue.\n\nBelief propagation (BP) is a Bayesian message- passing method first proposed by [10], which is a useful approximation algorithm and has been applied to the SAT problems (specifically in 3- SAT [8]) and 2QBF problems [19]. BP can find the witnesses of unsatisfiability of 2QBF by adopting a bias estimation strategy. Each round of BP allows the user to select the most biased \\(\\forall\\) - variable and assign the biased value to the variable. After all the \\(\\forall\\) - variables are assigned, the formula is simplified by the assignment and sent to SAT solvers. The procedure returns the assignment as a witness of unsatisfiability if the simplified formula is unsatisfiable, or UNKNOWN otherwise. However, the fact that BP is used for each \\(\\forall\\) - variable assignment leads to high overhead, similar to the RL approach given by [5]. It is interesting, however, to see that with the added overhead, BP can find witnesses of unsatisfiability, which is what one- shot GNN- based embeddings cannot achieve. This manuscript revealed the previously unrecognized limitation of GNN in reasoning about unsatisfiability of SAT problems. This limitation is probably rooted in the simplicity of message- passing scheme, which is good enough for embedding graph features, but not for conducting complex reasoning on top of the graph structures. ## References [1] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit- SAT: An unsupervised differentiable approach. In International Conference on Learning Representations, 2019. [2] Alonzo Church. A note on the entscheidungsproblem. J. Symb. Log., 1(1):40- 41, 1936. [3] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, pages 151- 158, New York, NY, USA, 1971. ACM. [4] Martin Davis, George Logemann, and Donald W. Loveland. A machine program for theorem- proving. Commun. ACM, 5(7):394- 397, 1962. [5] Gil Lederman, Markus N. Rabe, and Sanjit A. Seshia. Learning heuristics for automated reasoning through deep reinforcement learning. Co RR, abs/1807.08058, 2018. [6] Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, and Shuicheng Yan. Semantic object parsing with graph LSTM. Co RR, abs/1603.07063, 2016. [7] Florian Lonsing, Uwe Egly, and Martina Seidl. Q- resolution with generalized axioms. In Nadia Creignou and Daniel Le Berre, editors, Theory and Applications of Satisfiability Testing - SAT 2016, pages 435- 452, Cham, 2016. Springer International Publishing.[8] M. \"M\u00e9zard\", G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812- 815, 2002. [9] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, and Charles E. Leisersen. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. Co RR, abs/1902.10191, 2019. [10] Judea Pearl. Reverend bayes on inference engines: A distributed hierarchical approach. In AAAI, pages 133- 136. AAAI Press, 1982. [11] Bart Selman, Henry A. Kautz, and Bram Cohen. Local search strategies for satisfiability testing. In Cliques, Coloring, and Satisfiability, volume 26 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 521- 531. DIMACS/AMS, 1993. [12] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single- bit supervision. In ICLR (Poster). Open- Review.net, 2019. [13] Jo\u00e3o P. Marques Silva, In\u00eas Lynce, and Sharad Malik. Conflict- driven clause learning SAT solvers. In Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications, pages 131- 153. IOS Press, 2009. [14] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR. Open Review.net, 2019. [15] Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken- ichi Kawarabayashi, and Stefanie Jegelka. What can neural networks reason about? Co RR, abs/1905.13211, 2019.\n\n[16] Zhanfu Yang, Fei Wang, Ziliang Chen, Guannan Wei, and Tiark Rompf. Graph neural reasoning for 2- quantified boolean formula solvers. Co RR, abs/1904.12084, 2019. [17] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. Neural- symbolic VQA: disentangling reasoning from vision and language understanding. Co RR, abs/1810.02338, 2018. [18] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pages 3391- 3401, 2017. [19] Pan Zhang, Abolfazl Ramezanpour, Lenka Zdeborov\u00e1, and Riccardo Zecchina. Message passing for quantified boolean formulas. Co RR, abs/1202.2536, 2012. [20] David Zheng, Vinson Luo, Jiajun Wu, and Joshua B. Tenenbaum. Unsupervised learning of latent physical properties using perception- prediction networks. Co RR, abs/1807.09244, 2018.",
      "level": 2,
      "line_start": 9,
      "line_end": 17
    }
  ]
}