{
  "sections": {
    "a_unified_pre_training_and_adaptation_framework_fo": "## Introduction\n\n\n### 3.5 Inference with Local Search Method The last step of our model is to search the feasible solutions for CO problems with constraint conditions by using the predictions generated by our framework. We can also get close to optimal solutions with a search algorithm. For Max- Cut problems without hard clauses, our model can directly infer the truth assignment and obtain the target values. For problems with hard clauses, we introduce a heuristic local search method to discretely obtain feasible solutions that satisfy constraints and obtain the target values. Local search is a typical search method for discrete optimization. We adopt a 2- improvement local search algorithm [49,50] that iterates over all nodes in the graph and attempts to replace a 1- labeled node \\(v_{i}\\) with two 1- labeled nodes \\(v_{j}\\) and \\(v_{k}\\) . For the MIS problem, \\(v_{j}\\) and \\(v_{k}\\) must be the neighbors of \\(v_{i}\\) . These two nodes are 1- tight and are not connected. Here, a node is 1- tight if exactly one of its neighbors is 1- labeled. In other words, \\(v_{i}\\) is the only 1- labeled neighbor of \\(v_{j}\\) and \\(v_{k}\\) in the graph. Notice that this local search algorithm can find a valid 2- improvement in \\(O(E)\\) time if it exists. We introduce this method with fixed steps to find a feasible solution and evaluate the performance of our model. ### 3.6 Overall Algorithms Most of the previous works focus on modeling specific CO problems. In contrast, our method captures the common knowledge of COs and designs a unified learning method to utilize the knowledge to solve COs. To summarize, our framework has three essential processes. The problem transfer process leverages Max- SAT to bridge various COs. The COs are converted to Max- SAT with a general form that can capture logical information within CO problems. From the perspective of data transformation, the graphs in COs are firstly converted into clauses and then converted to bipartite graphs. The overall algorithm for problem transfer is listed in Algorithm 1. Algorithm 1 Problem Transfer via Max- SAT Input: A CO graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) Objective function: max \\(f(S)\\) Constraint condition: \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) Output: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) #From a graph to Max- SAT clauses# 1: Generate soft clauses \\(C_{s}\\) with objective function max \\(f(S)\\) 2: Generate hard clauses \\(C_{h}\\) with constraint condition \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) #From Max- SAT clauses to a bipartite graph# 3: Construct nodes \\(\\tilde{\\nu}_{x}\\) and \\(\\tilde{\\nu}_{c}\\) for each variable \\(v_{i}\\) and clause \\(c_{j}\\) 4: Construct edges \\(e_{(v_{i},c_{j})}^{\\prime}\\in \\tilde{\\mathcal{E}},\\forall v_{i}\\in c_{j}\\) The pre- training process uses samples from Max- SAT to learn generalizable features that can benefit all COs. The networks used for pre- training include the MLP, feature extraction backbones based on GNNs, and the classification network. The overall algorithm for pre- training is listed in Algorithm 2. The fine- tuning process uses samples from Max- SAT and target CO to build a domain adaptation architecture. Based on the pre- trained network, the fine- tuning network introduces an additional discriminator for domain classification, which can learn domain- invariant features to further improve the generalizability of features. The overall algorithm for fine- tuning is listed in Algorithm 3. ## 4 Experimental Results In this section, we evaluated the effectiveness of the proposed framework for three CO problems on graphs, i.e., Max- Cut, MIS, and MDS. Generally, we aimed to answer three essential questions by the experiments. Q1: Can we leverage Max- SAT to learn transferable and generalizable features that can improve the ability to solve different CO problems on graphs? Q2: How can we incorporate Max- SAT\n\nInput: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, True assignment \\(\\mathcal{V}\\) Number of pre- training layers \\(L\\) Number of iterations iters. Output: Pre- trained parameters \\(\\Theta_{p}\\) for MLP, Bip- GNN, and FC. 1: Initialize node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for nodes of variables and clauses; 4: for \\(l\\leqslant L\\) do 5: Extract features for clauses \\(\\mathbf{C}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 6: Extract features for variables \\(\\mathbf{X}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 7: end for 8: Map variable features to the predictions via \\(\\mathrm{FC}(\\mathbf{X}^{(L)})\\) 9: Compute the loss \\(\\mathcal{L}_{c}\\) in Eq. (3.14); 10: Backward and update parameters; 11: end for 12: Obtain the updated parameters \\(\\Theta_{p}\\) Algorithm 3 Fine- tuning Based on Domain Adaptation Input: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, Number of fine- tuning layers \\(L\\) Number of iterations iters, True assignment \\(\\mathcal{V}\\) Output: Predicted feasible solutions \\(\\mathcal{P} = \\{p_{i}\\}_{i = 1}^{n}\\) Fine- tuned parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, FC, and Dis. 1: Initialize node attributes of both domains \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for source variables and clauses; 4: Obtain features \\(\\mathbf{X}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for target variables and clauses; 5: for \\(l\\leqslant L\\) do 6: Extract features for clauses \\(\\mathbf{C}_{S}^{(l)}\\) and \\(\\mathbf{C}_{T}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 7: Extract features for variables \\(\\mathbf{X}_{S}^{(l)}\\) and \\(\\mathbf{X}_{T}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 8: end for 9: Map variable features to the assignment labels via \\(\\mathrm{FC}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{FC}(\\mathbf{X}_{T}^{(L)})\\) 10: Map variable features to the domain labels via \\(\\mathrm{Dis}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{Dis}(\\mathbf{X}_{T}^{(L)})\\) 11: Compute the loss \\(\\mathcal{L}_{ft}\\) in Eq. (3.15); 12: Updated parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, Dis, and FC; 13: end for 14: Obtain the optimal parameters \\(\\Theta_{ft}\\) 15: Predict feasible solutions \\(\\mathcal{P}\\) into the pipelines of GNN- based learning frameworks to fully utilize the information of Max- SAT? Q3: Do different pre- training and domain adaptation strategies matter for the ability of solving COs? ### 4.1 Experimental Setup Datasets We introduce the datasets that were used in our experiments in this part. In the pretraining stage, a large number of Max- SAT instances were required. We generated the clauses by running three representative generators with different distributions: the uniform distribution, the power- law distribution, and the double power- law distribution. A total number of 20,000 Max- SAT instances\n\nwere generated and the Max HS solver was used to obtain the solutions that served as labels for training. In the fine- tuning stage, according to the CO problems, different datasets are leveraged. For Max- Cut, we introduce the GSET benchmark \\(^{1)}\\) for evaluation. GSET is a set of 71 unweighted graphs that were commonly used for testing the algorithms to solve the Max- Cut problem. For MIS and MDS problems, we introduce the frb benchmark with four different instance scales for evaluation. Each frb dataset consists of five instances of hard CO problems and is regarded as the benchmark for CSP competition annually. Evaluation Metrics For different CO problems, we used the corresponding evaluation metrics based on their feasible solutions. For Max- Cut, \\(p\\) values [51] were used to evaluate the number of graph cuts. Concretely, \\(p\\) can be calculated by \\[p(z) = \\frac{z / n - \\gamma / 4}{\\sqrt{\\gamma / 4}}, \\quad (4.1)\\] where \\(z\\) is the predicted cut size for a \\(\\gamma\\) - regular graph with \\(n\\) nodes and \\(\\gamma\\) is the degree of nodes. For MIS and MDS, the number of cuts or sets is used for evaluation. Baseline methods To evaluate our method, we compare it with several baseline methods that have been widely used in solving COs. These methods can be roughly classified as traditional methods, heuristic methods, and learning- based methods. SDP [52] is a classical approach based on semi- definite programming. EO [53] is the extremal optimization method. BLS [54] is the breakout local search method. ECO- DQN [55] is an approach of exploratory CO with reinforcement learning. GMC- A and GMC- B [56] are two unsupervised GNN architectures for Max- CUT with different loss functions. RUN- CSP [57] is a recurrent unsupervised neural network for constraint satisfaction problems. PIGNN [58] is the physics- inspired GNNs for CO problems. MAXSAT [59] is a GNN- based framework designed for Max- SAT problems. Experimental Settings The proposed framework was implemented in Python with Py Torch. We summarized the hyper- parameters used in our paper as follows. The dimension of features \\(d\\) was set as 128. The number of GNN layers \\(L\\) for pre- training and fine- tuning was set as 5 and the first 2 layers were fixed during the fine- tuning. The number of layers for MLP was set as 2. The number of epochs for pre- training and fine- tuning were tuned and set as 400 and 100. The Adam optimizer was used for model training with a learning rate of \\(10^{- 5}\\) and a weight decay of \\(10^{- 10}\\) . The warm- up trick was also adopted. The number of FC layers for classification was set as 2 and the discriminator used a two- layer fully connected network to generate domain labels. The steps for local search for MIS and MDS were set as 120. All experiments were conducted on a workstation equipped with an Intel Xeon(R) Gold 6139M CPU 515 @ 2.30GHz and an NVIDIA RTX 3090 GPU with 24GB. Table 1 Results of solving Max-Cut with different approaches on random graphs. <table><tr><td rowspan=\"2\">Methods</td><td colspan=\"4\">n=100</td><td colspan=\"4\">n=200</td><td colspan=\"4\">n=500</td><td colspan=\"4\">n=800</td><td colspan=\"4\">n=1000</td></tr><tr><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td></td><td>\u03b3=10</td></tr><tr><td>SDP</td><td>0.709</td><td>0.697</td><td>0.689</td><td>0.709</td><td>0.702</td><td>0.692</td><td>0.702</td><td>0.690</td><td>0.682</td><td>0.701</td><td>0.688</td><td>0.679</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>0.712</td><td>0.708</td><td>0.703</td><td>0.721</td><td>0.723</td><td>0.724</td><td>0.727</td><td>0.737</td><td>0.735</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>0.712</td><td>0.707</td><td>0.704</td><td>0.720</td><td>0.721</td><td>0.719</td><td>0.722</td><td>0.725</td><td>0.721</td><td>0.720</td><td>0.726</td><td>0.717</td><td>0.721</td><td>0.725</td><td>0.720</td><td></td><td></td><td></td><td></td></tr><tr><td>ECO-DQN</td><td>0.713</td><td>0.705</td><td>0.707</td><td>0.718</td><td>0.723</td><td>0.720</td><td>0.725</td><td>0.727</td><td>0.725</td><td>0.722</td><td>0.721</td><td>0.722</td><td>0.726</td><td>0.726</td><td>0.721</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-A</td><td>0.691</td><td>0.655</td><td>0.637</td><td>0.701</td><td>0.683</td><td>0.660</td><td>0.693</td><td>0.668</td><td>0.599</td><td>0.691</td><td>0.666</td><td>0.602</td><td>0.688</td><td>0.662</td><td>0.601</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-B</td><td>0.698</td><td>0.660</td><td>0.630</td><td>0.708</td><td>0.667</td><td>0.646</td><td>0.707</td><td>0.701</td><td>0.670</td><td>0.699</td><td>0.696</td><td>0.653</td><td>0.702</td><td>0.694</td><td>0.651</td><td></td><td></td><td></td><td></td></tr><tr><td>RUN-CSP</td><td>0.702</td><td>0.706</td><td>0.704</td><td>0.711</td><td>0.715</td><td>0.714</td><td>0.714</td><td>0.726</td><td>0.710</td><td>0.704</td><td>0.713</td><td>0.705</td><td>0.705</td><td>0.711</td><td>0.702</td><td></td><td></td><td></td><td></td></tr><tr><td>PI-GNN</td><td>0.704</td><td>0.706</td><td>0.708</td><td>0.712</td><td>0.718</td><td>0.717</td><td>0.715</td><td>0.726</td><td>0.716</td><td>0.711</td><td>0.724</td><td>0.712</td><td>0.709</td><td>0.722</td><td>0.712</td><td></td><td></td><td></td><td></td></tr><tr><td>MAX-SAT</td><td>0.702</td><td>0.705</td><td>0.708</td><td>0.712</td><td>0.716</td><td>0.718</td><td>0.721</td><td>0.732</td><td>0.726</td><td>0.719</td><td>0.730</td><td>0.728</td><td>0.718</td><td>0.730</td><td>0.726</td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>0.702</td><td>0.708</td><td>0.707</td><td>0.722</td><td>0.721</td><td>0.725</td><td>0.732</td><td>0.738</td><td>0.733</td><td>0.734</td><td>0.739</td><td>0.733</td><td>0.732</td><td>0.738</td><td>0.733</td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 2 Results of solving Max-Cut with different approaches on the GSET benchmark. <table><tr><td colspan=\"5\">Max-Cut</td></tr><tr><td>Methods</td><td>G14</td><td>G15</td><td>G22</td><td>G55</td></tr><tr><td>V</td><td>800</td><td>800</td><td>2000</td><td>5000</td></tr><tr><td>E</td><td>4694</td><td>4661</td><td>19990</td><td>12468</td></tr><tr><td>SDP</td><td>2922</td><td>2938</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>2991</td><td>3047</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>3064</td><td>3050</td><td>-</td><td>-</td></tr><tr><td>ECO-DQN</td><td>3056</td><td>3028</td><td>13268</td><td>10198</td></tr><tr><td>GMC-A</td><td>2940</td><td>2948</td><td>12978</td><td>10094</td></tr><tr><td>GMC-B</td><td>2918</td><td>2906</td><td>13246</td><td>10104</td></tr><tr><td>RUN-CSP</td><td>2943</td><td>2928</td><td>13028</td><td>10116</td></tr><tr><td>PI-GNN</td><td>3026</td><td>2990</td><td>13181</td><td>10138</td></tr><tr><td>MAX-SAT</td><td>3046</td><td>3012</td><td>13250</td><td>10204</td></tr><tr><td>Ours</td><td>3052</td><td>3018</td><td>13269</td><td>10208</td></tr></table> Table 3 Results of solving MIS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Maximum Independent Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Redu MIS(Solver)</td><td>30.0</td><td>39.4</td><td>48.8</td><td>57.4</td></tr><tr><td>Greedy</td><td>24.6</td><td>33.0</td><td>42.2</td><td>48.0</td></tr><tr><td>GMC-A</td><td>21.2</td><td>27.7</td><td>40.2</td><td>45.2</td></tr><tr><td>GMC-B</td><td>20.8</td><td>28.1</td><td>40.6</td><td>46.0</td></tr><tr><td>RUN-CSP</td><td>25.8</td><td>33.6</td><td>42.2</td><td>49.4</td></tr><tr><td>PIGNN</td><td>26.5</td><td>35.1</td><td>44.4</td><td>51.8</td></tr><tr><td>MAX-SAT</td><td>27.9</td><td>36.6</td><td>44.4</td><td>51.8</td></tr><tr><td>Ours</td><td>27.8</td><td>36.8</td><td>44.8</td><td>52.2</td></tr></table> ### 4.2 Performance of Solving COs In this section, we evaluated the effectiveness of the proposed framework and aimed to answer Q1. Without loss of generality, Max- Cut, MIS, and MDS are selected for evaluation. Our framework is also adaptive to other COs that can be transformed as Max- SAT. For Max- Cut, we first reported the mean \\(p\\) - values across 1,000 regular graphs with 100, 200, 500, and 1000 nodes for three levels of node degrees in Table 1. Then, the experiments on the GSET benchmark were carried out and the number of cuts was reported in Table 2 for comparison. When encountering large- scale datasets (G22 and G55), traditional methods, e.g., EO and BLS, were out of memory and could not obtain the results. Based on the two tables, we have the following observations: (1) Our method obtained the best results compared with learning- based methods, which demonstrated that the proposed framework exhibited the powerful ability to learn useful information from CO problems. The results also showed that Max- SAT can boost the performance of solving COs; (2) Our method was superior in most cases compared with all baselines. The advantages of our method were mainly in large- scale Max- Cut problems (e.g., \\(n = 800\\) , G22, and G55) where the performance surpassed traditional methods;\n\nTable 4 Results of solving MDS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Minimum Dominated Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Greedy</td><td>191.1</td><td>255.0</td><td>296.5</td><td>284.8</td></tr><tr><td>HTS-DS</td><td>191.0</td><td>255.2</td><td>297.6</td><td>286.4</td></tr><tr><td>MWDS-CRO</td><td>191.0</td><td>255.1</td><td>298.4</td><td>285.5</td></tr><tr><td>GMC-A</td><td>201.2</td><td>268.1</td><td>301.3</td><td>292.2</td></tr><tr><td>GMC-B</td><td>192.1</td><td>261.4</td><td>299.1</td><td>291.6</td></tr><tr><td>RUN-CSP</td><td>190.8</td><td>255.4</td><td>299.3</td><td>278.6</td></tr><tr><td>PIGNN</td><td>191.1</td><td>255.3</td><td>299.1</td><td>280.2</td></tr><tr><td>MAX-SAT</td><td>191.0</td><td>255.3</td><td>298.8</td><td>278.8</td></tr><tr><td>Ours</td><td>190.8</td><td>255.1</td><td>298.5</td><td>278.1</td></tr></table> (3) Traditional or heuristic methods are very competitive in Max-Cut. However, the gaps between our method and these methods were small. Besides, the time of training and inference for our method was much less than these methods. For MIS and MDS, we reported the number of sets for MIS and the number of dominated nodes for MDS in Tables 3 and 4. \\(V\\) and \\(E\\) are the sets for nodes and edges. Greedy denotes the classical greedy algorithm. Larger and smaller values denote better results for MIS and MDS, respectively. The experiments were carried out on four frb datasets. Similar results can be observed in these two CO problems. Particularly, In MIS, our results were relatively closer to the results obtained by the solver, which showed the effectiveness of our framework. Based on the results obtained on the above three CO problems, we can answer Q1: Max- SAT offered common knowledge and the pre- training and adaptation pipeline built based on Max- SAT can learn transferable and generalizable features to improve the ability to solve different CO problems on graphs. <center>Figure 3 Results of ablation study with different variants of our model on Max-Cut. </center>\n\n<center>Figure 4 Heat maps of knowledge transferability from three different CO problems to the maximum independent set problem on two scales of nodes. </center> ### 4.3 Ablation Study In this part, we conducted ablation studies on the Max- Cut problem to verify the effectiveness of each module in our framework. We gradually removed each single module and kept the other modules unchanged. The variants of our frameworks are listed as follows. \\(\\mathbf{w} / \\mathbf{o}\\) pre- T denotes the model that removes the pre- training stage but keeps the domain adaptation module. \\(\\mathbf{w} / \\mathbf{o}\\) DA denotes the model that removes the domain adaptation module but keeps the pre- training stage. \\(\\mathbf{w} / \\mathbf{o}\\) Bi- G is the model that directly uses GNNs to extract the features in the original graphs represented from CO problems without converting them to MAX- SAT and bipartite graphs. \\(\\mathbf{w} / \\mathbf{o}\\) Att is the model that keeps bipartite GNNs without the attention mechanism. \\(\\mathbf{w} / \\mathbf{o}\\) Mult- Att is the model that keeps bipartite GNNs without the multi- head mechanism. We reported the \\(p\\) - values for all variants in Figure 3 and had the following observations. (1) All modules boosted the performance and played important roles in our framework. The overall model obtained the best results, which demonstrated that these modules were nicely incorporated into our framework. (2) Removing the problem transfer step experienced the largest performance degradation, which proved the effectiveness of Max- SAT and also answered Q1. (3) Pre- training and adaptation were both useful in improving the performance. Since both steps utilized Max- SAT to assist the learning of transferable features, we can answer Q2 and conclude that the combination of different strategies to use Max- SAT can largely benefit solving CO problems. (4) Attention and multi- head mechanisms also affected the results, which showed that better feature extraction backbones indeed helped the learning methods to solve CO problems. ### 4.4 Effects of Different Knowledge Transfer Strategies In this subsection, we verify the knowledge transfer ability of our model. we fixed the target domain as the MIS problem and used Max- SAT, Max- Cut, and MDS as source domains to check different degrees of transferability. To be concrete, we fed the data from different source domains into our framework for pre- training and domain adaptation and observed how the performance was affected by the selection of domains. We selected two scales of nodes, i.e., 450 variables and 760 variables, for demonstration. The results were shown in Figure 4 where darker colors indicated better performance and transferability.\n\n<center>Figure 5 Parameter analysis for feature dimension \\(d\\) , number of attention heads \\(h\\) , weight of loss \\(\\lambda\\) , and number of layers \\(L\\) . </center> From the heat maps, we can observe that Max- SAT \\(\\rightarrow\\) MIS always obtained the best results compared with the other two types of knowledge transfer, which showed that Max- SAT was more general and had the strongest transferability across different CO problems. Moreover, MDS is a more relevant CO problem to MIS compared with Max- Cut. Therefore, we observed better results from MDS \\(\\rightarrow\\) MIS than Max- Cut \\(\\rightarrow\\) MIS. This observation answered Q3 where different source domains do affect the performance of the target domain. From a practical viewpoint, selecting an appropriate source domain was always hard, and irrelevant tasks would generate negative transfers and eventually affect the performance of the target domain. ### 4.5 Parameter Analysis In this subsection, we analyzed the influence of several significant hyper- parameters on the model performance: the dimension of features \\(d\\) , the number of attention heads \\(h\\) , the weight of losses \\(\\lambda\\) , and the number of pertaining layers \\(L\\) . We recorded the performance of different \\(d\\) , \\(h\\) , \\(\\lambda\\) , \\(L\\) values on Max- CUT. The results were reported in Figure 5. It can be observed that with different dimensions of features, our model exhibited relatively stable performance when \\(d\\) was greater than or equal to 16. Our\n\nmodel achieved optimal performance when \\(d\\) was 128. Our model was not significantly affected by the number of attention heads. It achieved optimal performance when \\(k\\) was 10. The optimal value of weight \\(\\lambda\\) was obtained in 0.2. Pre- training led to better results, especially when the number of layers exceeded three. ## 5 Conclusion In this study, we aimed to discover the transferability of deep learning architectures for solving different CO problems on graphs. To achieve this goal, we leveraged the Max- SAT problem as a general formulation of original CO problems. The key advantages were two- fold. First, Max- SAT served as a bridge for various CO problems and provided a tool for discovering common properties in CO problems. Second, Max- SAT problems were represented as clauses that included logical information in addition to the simple correlations in graphs. By further constructing a pre- training and adaptation framework, we can extract transferable features that can be used for various CO problems. Experiments showed that incorporating Max- SAT indeed improved the ability to solve various CO problems on graphs. Our current work can be extended from the following directions. The pre- training stage is built on supervised tasks where labeled data samples are required. As a future direction, it is necessary to develop unsupervised pre- training paradigms that can fully leverage information from the data itself. Moreover, advanced GNNs that are more suitable for the properties of CO problems should be designed in the future. Acknowledgements This work was supported by the National Natural Science Foundation of China (Grant No. 11991021, No. 11991020, No. 12271503). ## References 1 Petrica C Pop, Ovidiu Cosma, Cosmin Sabo, and Corina Pop Sitar. A comprehensive survey on the generalized traveling salesman problem. European Journal of Operational Research, 2023. 2 Nathan Krislock, J\u00e9r\u00f4me Malick, and Fr\u00e9d\u00e9ric Roupin. Improved semidefinite bounding procedure for solving max- cut problems to optimality. Mathematical Programming, 143:61- 86, 2014. 3 Yiyuan Wang, Shaowei Cai, Shiwei Pan, Ximing Li, and Monghao Yin. Reduction and local search for weighted graph coloring problem. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 2433- 2441, 2020. 4 Marc Los and Christian Lardinois. Combinatorial programming, statistical optimization and the optimal transportation network problem. Transportation Research Part B: Methodological, 16(2):89- 124, 1982. 5 Simone L Martins and Celso C Ribeiro. Metaheuristics and applications to optimization problems in telecommunications. Handbook of Optimization in Telecommunications, pages 103- 128, 2006. 6 Eugene L Lawler and David E Wood. Branch- and- bound methods: A survey. Operations Research, 14(4):699- 719, 1966. 7 Steven J Benson, Yinyu Ye, and Xiong Zhang. Solving large- scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000. 8 Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263- 1272. PMLR, 2017. 9 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. 10 Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016. 11 Quentin Cappart, Didier Ch\u00e9telat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 12 Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in Neural Information Processing Systems, 30, 2017. 13 Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in Neural Information Processing Systems, 31, 2018.\n\n14 Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019. 15 Tianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang, Howard Heaton, Jialin Liu, and Wotao Yin. Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research, 23(1):8562- 8620, 2022. 16 Matteo Fischetti, Paolo Toth, and Daniele Vigo. A branch- and- bound algorithm for the capacitated vehicle routing problem on directed graphs. Operations Research, 42(5):846- 859, 1994. 17 Panos M Pardalos and Gregory P Rodgers. A branch and bound algorithm for the maximum clique problem. Computers & Operations Research, 19(5):363- 375, 1992. 18 Richard Bellman. Dynamic programming. Science, 153(3731):34- 37, 1966. 19 Michael Held and Richard M Karp. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1):196- 210, 1962. 20 Silvano Martello, David Pisinger, and Paolo Toth. Dynamic programming and strong bounds for the 0- 1 knapsack problem. Management Science, 45(3):414- 424, 1999. 21 Laurence A Wolsey and George L Nemhauser. Integer and combinatorial optimization, volume 55. John Wiley & Sons, 1999. 22 Dimitris Bertsimas and Ramazan Demir. An approximate dynamic programming approach to multidimensional knapsack problems. Management Science, 48(4):550- 565, 2002. 23 Paul Bouman, Niels Agatz, and Marie Schmidt. Dynamic programming approaches for the traveling salesman problem with drone. Networks, 72(4):528- 542, 2018. 24 Mutsunori Yagiura and Toshihide Ibaraki. On metaheuristic algorithms for combinatorial optimization problems. Systems and Computers in Japan, 32(3):33- 55, 2001. 25 James B Orlin, Abraham P Punnen, and Andreas S Schulz. Approximate local search in combinatorial optimization. SIAM Journal on Computing, 33(5):1201- 1214, 2004. 26 Emile HL Aarts, Jan HM Korst, and Peter JM van Laarhoven. A quantitative analysis of the simulated annealing algorithm: A case study for the traveling salesman problem. Journal of Statistical Physics, 50:187- 206, 1988. 27 Jean Berger and Mohamed Barkaoui. A hybrid genetic algorithm for the capacitated vehicle routing problem. In Genetic and Evolutionary Computation Conference, pages 646- 656. Springer, 2003. 28 Andr\u00e9 Chassein, Marc Goerigk, Adam Kasperski, and Pawel Zieli\u0144ski. Approximating combinatorial optimization problems with the ordered weighted averaging criterion. European Journal of Operational Research, 286(3):828- 838, 2020. 29 Henrique Lemos, Marcelo Prates, Pedro Avelar, and Luis Lamb. Graph coloring meets deep learning: Effective graph neural network models for combinatorial problems. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, pages 879- 885. IEEE, 2019. 30 Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym- nco: Leveraging symmetricity for neural combinatorial optimization. Advances in Neural Information Processing Systems, 35:1936- 1949, 2022. 31 Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2018. 32 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in Neural Information Processing Systems, 30, 2017. 33 Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. 34 Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. Gppt: Graph pre- training and prompt tuning to generalize graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1717- 1727, 2022. 35 Zixuan Yi, Iadh Ounis, and Craig Macdonald. Contrastive graph prompt- tuning for cross- domain recommendation. ACM Transactions on Information Systems, 2023. 36 Cristian Bodnar, Francesco Di Giovanni, Benjamin Chamberlain, Pietro Li\u00f2, and Michael Bronstein. Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns. Advances in Neural Information Processing Systems, 35:18527- 18541, 2022. 37 Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein. On over- squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865- 7885. PMLR, 2023. 38 Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 39 Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. In The Eleventh International Conference on Learning Representations, 2022. 40 Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, and Marinka Zitnik. Evaluating explainability for graph neural\n\nnetworks. Scientific Data, 10(1):144, 2023. 41 Jin Yuan, Feng Hou, Yangzhou Du, Zhongchao Shi, Xin Geng, Jianping Fan, and Yong Rui. Self- supervised graph neural network for multi- source domain adaptation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3907- 3916, 2022. 42 Quanyu Dai, Xiao- Ming Wu, Jiarren Xiao, Xiao Shen, and Dan Wang. Graph transfer learning via adversarial domain adaptation with graph convolution. IEEE Transactions on Knowledge and Data Engineering, 35(5):4908- 4922, 2022. 43 Ronghang Zhu, Xiaodong Jiang, Jiasen Lu, and Sheng Li. Cross- domain graph convolutions for adversarial unsupervised domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 34(8):3847- 3858, 2023. 44 Xinhong Ma, Tianzhu Zhang, and Changsheng Xu. Gcan: Graph convolutional adversarial network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8266- 8276, 2019. 45 Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and Pan Li. Structural re- weighting improves graph domain adaptation. In International Conference on Machine Learning, pages 21778- 21793, 2023. 46 Mehmet Pilanci and Elif Vural. Domain adaptation on graphs by learning aligned graph bases. IEEE Transactions on Knowledge and Data Engineering, 34(2):587- 600, 2022. 47 Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Graph domain adaptation via theory- grounded spectral regularization. In International Conference on Learning Representations, 2023. 48 Carlos Ans\u00f3tegui, Maria Luisa Bonet, and Jordi Levy. Towards industrial- like random sat instances. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, page 387- 392, 2009. 49 Diogo V Andrade, Mauricio GC Resende, and Renato F Werneck. Fast local search for the maximum independent set problem. Journal of Heuristics, 18:525- 547, 2012. 50 Thomas A Feo, Mauricio GC Resende, and Stuart H Smith. A greedy randomized adaptive search procedure for maximum independent set. Operations Research, 42(5):860- 878, 1994. 51 Amir Dembo, Andrea Montanari, and Subhabrata Sen. Extremal cuts of sparse random graphs. The Annals of Probability, pages 1190- 1217, 2017. 52 Changhui Choi and Yinyu Ye. Solving sparse semidefinite programs using the dual scaling algorithm with an iterative solver. Department of Management Sciences, University of Iowa, Iowa City, IA, 52242, 2000. 53 Stefan Boettcher and Allon G Percus. Extremal optimization for graph partitioning. Physical Review E, 64(2):026114, 2001. 54 Una Benlic and Jin- Kao Hao. Breakout local search for the max- cutproblem. Engineering Applications of Artificial Intelligence, 26(3):1162- 1173, 2013. 55 Thomas Barrett, William Clements, Jakob Foerster, and Alex Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3243- 3250, 2020. 56 Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max- cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242- 251. SPIE, 2019. 57 Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence, 3:580607, 2021. 58 Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics- inspired graph neural networks. Nature Machine Intelligence, 4(4):367- 377, 2022. 59 Minghao Liu, Pei Huang, Fuqi Jia, Fan Zhang, Yuchen Sun, Shaowei Cai, Feifei Ma, and Jian Zhang. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 16264- 16265, 2023.\n\n## 1 Introduction Combinatorial optimization (CO) is a multidisciplinary field ranging from optimization to artificial intelligence. The goal of CO is to find optimal solutions from finite possible options where exhaustive enumeration cannot be accomplished. Graphs are the most common research objects in CO. Typical CO problems on graphs include traveling salesman problems (TSP) [1], Max- Cut problems [2], and graph coloring problems [3]. Solving CO on graphs can benefit many real- world applications, such as transportation logistics [4] and telecommunications [5]. Solving COs on graphs is extremely difficult due to their discrete property and high computational costs. For many decades, researchers have made many efforts to develop various algorithms to solve COs on graphs. For example, branch and bound [6] is a traditional exact algorithm that gradually divides the solution space into subspaces and uses bounds to prune possible solutions that exceed the bounds. Unfortunately, these traditional exact algorithms encounter excessive computational burdens as the scales of problems increase [7]. Recently, deep learning has developed as an effective tool for COs, inspiring the emergence of the category of learning- based methods for solving COs. Particularly, graph neural networks (GNNs) [8- 10] are advanced deep learning models designed for graph- structure data, which have been successfully applied to COs on graphs [11- 13]. In general, the graphs in CO problems are first fed into GNNs so that the high- level features can be extracted. Then, the predictions generated from features serve as feasible solutions or can be further used to guide the searching process of the heuristic learning process [14, 15]. Learning- based methods extract more useful knowledge to reduce the search space, which can be used to effectively and efficiently solve COs on graphs. Besides, the data- driven manner is more adaptive to real- world problems when real data is available. However, current GNN models for CO problems still face several challenges. First, directly modeling CO with graphs is intuitive but not necessarily accurate. Although graphs can capture relationships between objects, they ignore the mathematical logicality and properties inside COs. Therefore, constructing learning methods over original graphs often results in insufficient problem- solving abilities. Second, current methods are usually designed to solve a single CO problem on graphs. Nevertheless, different COs on graphs may share the same graphs and leverage similar GNN backbones for feature extraction. It is quite beneficial to explore whether the transferability of knowledge exists among different problems. Moreover, for real- world problems on graphs, the data collected for training GNNs is often limited, it is essential to leverage data from other problems that may have common knowledge to improve the generalization ability of GNNs. To address the above issues, we propose in this paper a unified pre- training and adaptation framework for COs on graphs. The core idea is to develop a generalizable and transferable GNN framework from which all COs can be benefited. To achieve this goal, it is necessary to find out the common knowledge and properties of all CO problems on graphs that can be extracted by GNNs and used for different problems. We notice that using graphs to formulate CO omits logical information that is usually more general and common for all problems. Inspired by this observation, we propose to leverage the maximum satisfiability problems (Max- SAT) to bridge different CO problems on graphs. Max- SAT is a classic problem containing propositional logic formulas that can be used to describe additional logic information beyond simple relations on graphs. With this unified representation, we further adopt a pre- training process and a fine- tuning process based on domain adaptation to extract general features and utilize the transferability between different problems. Our framework can be adaptive to all CO problems that can be transferred to Max- SAT problems and is also suitable for various GNN architectures, which exhibit superior flexibility and versatility. To be concrete, we first use the Max- SAT problem as an intermediate problem by which we can transform graphs from different COs to clauses and formulas. We can construct bipartite graphs from these clauses in Max- SAT that not only capture logical information but also can be adaptive to different COs on graphs. In the training stage, we can generate abundant clauses from certain Max- SAT problems without considering the original CO and transform them into bipartite graphs for pre- training. Due to the generic nature of Max- SAT, the pre- trained model is equipped with better initialization. Then, we\n\nuse the original CO and Max- SAT to construct an adaptation framework with adversarial training for fine- tuning, which can further utilize the transferable information in Max- SAT. In both the pre- training and fine- tuning stages, the graph attention networks are leveraged as backbones to extract information in bipartite graphs transformed from COs on graphs and Max- SAT. The bipartite graph attention networks separately consider the aggregation of clauses and variables, which can better capture the dependencies between variables and clauses with different neighbor aggregation processes. To summarize, the contributions of this work are as follows: (1) We propose a unified pre-training and adaptation framework based on Max-SAT that can learn generalizable and transferable information to benefit the solving of COs on graphs. (2) We propose to use Max-SAT to bridge various COs on graphs, by which all problems can be represented as a unified form. The Max-SAT instances also carry additional logical relationships that can be further utilized by GNNs. (3) We design a pre-training and domain adaptation architecture to extract generalizable and transferable features based on instances generated from the Max-SAT and COs on graphs. This framework is versatile to various COs and can be used to boost the ability to solve these problems. (4) We evaluate our method on both synthetic datasets and open benchmarks. The numerical results demonstrated the effectiveness of our framework in solving COs on graphs. The remainder of this paper is organized as follows. Section 2 presents the learning methods for COs and popular GNN architectures for feature extraction on graphs. Section 3 introduces the problem transfer, pre-training, and fine-tuning procedures in our framework and summarizes the overall algorithms for training. The experiments that aim to address our three main claims are listed in Section 4. We conclude our work in Section 5. ## 2 Related Work ### 2.1 Learning Methods for Solving CO Problems Traditional methods for solving CO mainly include branch and bound, dynamic programming, and heuristic algorithms. Branch and bound is a branch- and- prune search method used to reduce the search space [6, 16, 17]. It progressively decomposes the problem into smaller subproblems and applies pruning strategies to exclude branches that cannot produce the optimal solution. Dynamic programming breaks down a large problem into smaller subproblems and solves them by utilizing the relationships between the subproblems [18- 21]. Dynamic programming is suitable for problems with overlapping subproblem structures, such as the knapsack problem [22] and TSP [23]. Heuristic methods employ heuristic strategies or random operations to search the solution space and continuously improve the current solution until a satisfactory solution is found or a predetermined stopping condition is met. Common heuristic algorithms include greedy algorithms [24], local search [25], simulated annealing [26], and genetic algorithms [27]. Traditional methods for solving CO problems have limitations including high computational complexity, lack of scalability for large- size problems, and limited solution quality in terms of finding globally optimal solutions. In recent years, deep learning methods have emerged as promising approaches to tackle these problems by leveraging the power of data- driven modeling and computational intelligence. One category of methods approximates the process of solving CO problems through deep learning methods [28]. By learning from a dataset of problem instances, these deep learning models can capture intrinsic patterns and dependencies in the problem and the predictions are then utilized to guide the search process toward better solutions. These approaches offer advantages in terms of speed and scalability as they reduce the requirements of expensive evaluations for the objective function during the search. Another category of methods is based on learning- based heuristics and meta- heuristics [12- 14, 29]. Deep learning models can be employed to create intelligent decision rules or policies to guide the search process. Reinforcement learning techniques have demonstrated success in learning effective exploration- exploitation strategies for\n\nCO problems [30,31]. By incorporating learning- based heuristics into traditional optimization algorithms, better- quality solutions can be obtained with reduced computational effort. ### 2.2 Graph Neural Networks GNNs belong to a typical deep- learning framework for graphs that follow a message- passing mechanism [8]. Graph convolution networks (GCNs) [10] define graph convolutions on the spectral domain so that the structural information from neighbors can be aggregated. Graph SAGE [32] proposes an inductive representation learning method based on aggregation and sampling directly on the spatial domain. Graph attention networks (GATs) [33] utilize self- attention mechanisms to learn node features where the importance of neighbors can be considered during the aggregation of information. More recently, prompt tuning has been used to generalize GNNs [34,35]. GPPT [34] proposes a graph prompting function for GNNs so that the pre- trained models can predict labels without fine- tuning. Since GNNs have been successfully used in various downstream tasks, researchers pay attention to the theories behind GNNs. For example, over- smoothing [36] and over- squashing [37] are the two main issues for GNNs. BORF [38] proposes a rewiring algorithm based on Ollivier- Ricci curvature to relieve both over- smoothing and over- squashing problems for GNNs. The expressive abilities of GNNs have also been studied to demonstrate why GNNs perform well in many tasks [9,39,40]. Recently, GNNs have been used in graph domain adaptation to address the domain shift on graphs [41]. Domain adaptation on graphs aims at leveraging knowledge learned from the source domain to improve the performance of a GNN model on a different target domain. Adversarial training is often used to build the alignment framework [42,43]. For example, TFLG [43] utilizes instance and class levels of structures and leverages adversarial training to learn domain- invariant features. Except for the features that can be easily used for domain alignment, many researchers seek to learn invariant information from different levels. GCAN [44] is a graph convolutional adversarial network introduced for unsupervised domain adaptation by jointly modeling structures, domain labels, and class labels. Stru RW [45] presents a structural re- weighting method to address the conditional structure shift problem. Since spectral properties play an important role in graph structures, many works also attempt to explore the alignment of spectral information [46,47]. ## 3 Methodology ### 3.1 CO Problems on Graphs Given a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) , where \\(\\nu\\) is a set of nodes and \\(\\mathcal{E}\\) is a set of edges between nodes. The CO problems on graphs are to find a subset of nodes \\(\\mathcal{S}\\) under given constraint conditions so that the number of edges between the nodes in \\(\\mathcal{S}\\) satisfies a specific condition and the objective function can be maximized or minimized. Generally, the CO can be expressed as: \\[\\begin{array}{r}{\\max f(\\mathcal{S}),}\\\\ {g_{i}(\\mathcal{S})\\leqslant b_{i},b_{i}\\in \\Omega ,} \\end{array} \\quad (3.1b)\\] where \\(f(\\mathcal{S})\\) is the objective function, \\(g_{i}(\\mathcal{S})\\) is the constraint condition function, \\(b_{i}\\) is the boundary of the constraint condition, and \\(\\Omega\\) is the set of constraint indicators. In the following part, we give three concrete CO problems on graphs. Max- Cut Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) , the goal of the Max- Cut problem is to find a partition that divides \\(\\nu\\) into subset \\(\\mathcal{S}\\) and its complementary set \\(\\mathcal{S}^{\\prime} = \\mathcal{V} - \\mathcal{S}\\) where the number of cuts between \\(\\mathcal{S}\\) and \\(\\mathcal{S}^{\\prime}\\) is maximized. The cuts refer to the edges between \\(\\mathcal{S}\\) and \\(\\mathcal{S}^{\\prime}\\) . The objective function of Max- Cut can be denoted as: \\[f(\\mathcal{S}) = \\sum_{e_{i,j}\\in \\mathcal{E}}\\big(v_{i} + v_{j} - 2v_{i}v_{j}\\big), \\quad (3.2)\\] where \\(v_{i} = 1\\) when \\(v_{i}\\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise, and \\(e_{i,j}\\) is the edge between nodes \\(v_{i}\\) and \\(v_{j}\\)\n\nMaximum Independent Set (MIS) Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) , the maximum independent set problem aims to find a node set \\(\\mathcal{S}\\) with a maximal number of nodes where there are no edges between any two nodes in this set. The objective function and constraint conditions of the maximum independent set problem can be denoted as: \\[\\begin{array}{l}{f(\\mathcal{S}) = \\sum_{v_{i}\\in \\mathcal{V}}v_{i},}\\\\ {v_{i} + v_{j}\\leqslant 1|\\forall e_{i,j}\\in \\mathcal{E},} \\end{array} \\quad (3.3a)\\] where \\(v_{i} = 1\\) when \\(v_{i} \\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise. Minimum Dominated Set (MDS) Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) , the goal of the minimum dominated set problem is to find a dominated set \\(\\mathcal{S}\\) where each node \\(v_{i} \\in \\mathcal{V}\\) or at least one of its neighbor nodes belongs to the dominated set \\(\\mathcal{S}\\) . The number of nodes in \\(\\mathcal{S}\\) should also be minimized. The objective function and constraint conditions of the minimum dominated set problem can be denoted as: \\[\\begin{array}{l}{f(\\mathcal{S}) = -\\sum_{v_{i}\\in \\mathcal{V}}v_{i},}\\\\ {v_{i} + v_{j}\\geqslant 1|e_{i,j}\\in \\mathcal{E},\\forall v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.4a)\\] where \\(v_{i} = 1\\) when \\(v_{i} \\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise. ### 3.2 Problem Transfer via Max-SAT Problem In this subsection, we introduce the details of leveraging the Max- SAT problem to bridge different CO problems. In general, to fully utilize the common logical information in COs, we first convert the original graphs of the CO problems into several clauses of Max- SAT problems and then transform these clauses into bipartite graphs. #### 3.2.1 From Graph to Clauses Given a graph from any CO, we use clauses with variables to describe the objective function and the constraint conditions. Max- SAT is an optimization problem that contains a set of clauses and variables. Define \\(m\\) clauses \\(\\mathcal{C} = \\{c_{1},c_{2},\\dots ,c_{m}\\}\\) and \\(n\\) variables \\(\\mathcal{X} = \\{x_{1},x_{2},\\dots ,x_{n}\\}\\) , the Max- SAT problem is to find a truth assignment of variables so that the number of satisfied clauses is maximized. To generate clauses, we define two rules for the objective function and constraint conditions respectively. The soft rule is based on the objective function and the hard rule is based on the constraint conditions. The difference between those two rules mainly lies in whether the clauses should be strictly satisfied. For clarification, we let the variable \\(x_{i}\\) correspond to the node \\(v_{i}\\) in graph \\(\\mathcal{G}\\) . For demonstration purposes, we provide the details of transformations for Max- Cut, MIS, and MDS problems. For every \\(v_{i} \\in \\mathcal{V}\\) in MIS problems, the clauses are generated by: \\[\\begin{array}{l}{c_{h} = \\neg x_{i}\\vee \\neg x_{j}|e_{i,j}\\in \\mathcal{E},}\\\\ {c_{s} = x_{i}|v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.5a)\\] where \\(c_{h} \\in \\mathcal{C}_{h}\\) denotes the hard clause which describes that the set cannot contain the node and its neighbors at the same time, \\(c_{s} \\in \\mathcal{C}_{s}\\) denotes the soft clause which describes that the number of nodes in the MIS problem should be maximized, \\(\\neg\\) and \\(\\vee\\) are the negation and disjunction. For every \\(v_{i} \\in \\mathcal{V}\\) in MDS problems, the clauses are generated by: \\[\\begin{array}{l}{c_{h} = x_{i}\\vee x_{j}|e_{i,j}\\in \\mathcal{E},}\\\\ {c_{s} = \\neg x_{i}|v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.6a)\\] where \\(c_{h} \\in \\mathcal{C}_{h}\\) describes that the node or at least one of its neighbor nodes belongs to MDS, and \\(c_{s} \\in \\mathcal{C}_{s}\\) describes that the number of nodes in MDS should be minimized.\n\nFor every \\(e_{i,j} \\in \\mathcal{E}\\) in Max- Cut problems, the clauses are generated by: \\[\\begin{array}{l}{c_{s} = x_{i}\\vee x_{j},}\\\\ {c_{s}^{\\prime} = \\neg x_{i}\\vee \\neg x_{j},} \\end{array} \\quad (3.7a)\\] where \\(c_{s}\\) and \\(c_{s}^{\\prime}\\) are all soft clauses belonging to \\(\\mathcal{C}_{s}\\) that describe the number of cut edges that should be maximized. #### 3.2.2 From Clauses to Bipartite Graph The soft and hard clauses describe the relations between clauses and variables, which reveals the logical dependencies. Then, we can use bipartite graphs to further represent these relations in Max- SAT. In general, the bipartite graph is denoted as \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) where \\(\\tilde{\\mathcal{V}}_{x}\\) and \\(\\tilde{\\mathcal{V}}_{c}\\) are node sets, and \\(\\tilde{\\mathcal{E}}\\) is the edge set. The main difference between graphs and bipartite graphs is that the nodes in bipartite graphs belong to two different types. The variables \\(\\mathcal{X}\\) correspond to the set of nodes \\(\\tilde{\\mathcal{V}}_{x}\\) and the clauses \\(\\mathcal{C}\\) correspond to the set of nodes \\(\\tilde{\\mathcal{V}}_{c}\\) . To generate the bipartite graph, the core step is to construct the edges between nodes. For each variable \\(x_{i}\\) in each clause \\(c_{i}\\) , an edge \\(\\tilde{e}_{i,j} \\in \\tilde{\\mathcal{E}}\\) corresponds to the affiliation between the variable and the clause. In this transfer process, we transfer the origin graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) of the CO problem into the bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) . A concrete transfer process can be shown in Figure 1. We choose Max- Cut as an example for demonstration. The first step is to transfer the graph in Max- Cut into a set of clauses in Max- SAT. We use the hard and soft rules to generate these clauses. After generating these clauses, we then transfer them into a bipartite graph. The variables and clauses are set as nodes and the affiliations between variables and clauses constitute edges. The new bipartite graph can be then used for the next feature extraction process. <center>Figure 1 Problem transfer process of our framework via Max-SAT. The input graph of CO problems is first transferred as Max-SAT clauses. Then, a bipartite graph is generated from these clauses by the affiliations between variables and clauses. </center> Notice that transferring graphs in original COs to bipartite graphs has two main advantages: First, original graphs only contain simple dependencies between variables. In contrast, the bipartite graphs transferred from Max- SAT clauses are then equipped with logical information from COs; Second, the bipartite graphs are general forms that can represent various COs on graphs since all of these COs can be transformed into Max- SAT problems. Therefore, we can easily construct a knowledge transfer framework to leverage the generalizable and transferable information of these problems with the help of bipartite graphs.\n\n### 3.3 Learning with Bipartite Graph Attention Networks In this subsection, we first introduce the backbone network that is used to extract features from bipartite graphs. Traditional GNNs for feature extractions from graphs mainly follow a message- passing scheme: \\[\\begin{array}{r l} & {\\mathbf{a}_{v}^{(l)} = \\mathrm{AGG}(\\mathbf{x}_{u}\\mid u\\in \\mathcal{N}(v)),}\\\\ & {\\mathbf{x}_{v}^{(l + 1)} = \\mathrm{COM}(\\mathbf{a}_{v}^{(l)},\\mathbf{x}_{v}^{(l)}),} \\end{array} \\quad (3.8)\\] where \\(u\\) and \\(v\\) denote the nodes, \\(\\mathbf{a}_{v}\\) denotes the feature vector accumulated from neighbor nodes, \\(\\mathbf{x}_{v}^{(l)}\\) denotes the features at layer \\(l\\) and \\(\\mathbf{x}_{v}^{(0)}\\) is the initial attributes, \\(\\mathrm{AGG}(\\cdot)\\) and \\(\\mathrm{COM}(\\cdot)\\) are the aggregation and combination functions for message passing and feature updating, and \\(\\mathcal{N}(v)\\) denotes the set of neighbors for \\(v\\) . The above process is only suitable for graphs with one type of nodes. Recall that a bipartite graph contains two sets of nodes that represent variables and clauses and a set of edges that connect nodes from these two sets. Due to the different properties of nodes, the traditional message- passing scheme in GNNs cannot be directly used to extract features. We particularly design a bipartite GNN with attentive message- passing schemes. For an input bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\nu}_{x},\\tilde{\\nu}_{c},\\tilde{\\mathcal{E}})\\) , the attributed matrices of variables and clauses are denoted as \\(\\mathbf{C}\\in \\mathbb{R}^{m\\times d}\\) and \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times d}\\) where \\(m\\) and \\(n\\) denote the numbers of clauses and variables, and \\(d\\) is the dimension of attributes. For symbolic convenience, we use \\(v_{x}(i)\\) and \\(v_{c}(i)\\) to denote nodes of variables and clauses in the bipartite graph. The variable and clause of these nodes are denoted as \\(x_{i}\\) and \\(c_{i}\\) and their attributed vectors are denoted as \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{c}_{i}\\) accordingly. Since the initial bipartite graph does not contain attributed information, the attributed matrices require initialization. There are different strategies for initialization, e.g., uniform distribution, normal distribution, and all- one matrices. Since no significant influences have been observed in our experiments, we simply initialize attributed matrices as all- one matrices \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) . The message- passing process of bipartite GNNs consists of two steps in each iteration, the clause- wise aggregation step and the variable- wise aggregation step. First, clause- wise aggregation updates the feature of the clause node by aggregating the features from the variable nodes. To further discriminate the importance of different neighbors, the attention mechanism is introduced to learn features during the aggregation. Given a clause \\(c_{i}\\) and its neighbor variable \\(x_{j}\\) , the layer- wise aggregation from \\(x_{j}\\) to \\(c_{i}\\) through attention is represented as: \\[\\begin{array}{r l} & {\\alpha_{x_{j}\\to c_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{x}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{x}(k)\\in\\mathcal{N}(v_{c}(i))}\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{X}_{k}^{(l)})\\rangle\\right)}}\\\\ & {\\mathbf{c}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{c}_{i}^{(l)},\\sum_{v_{x}(j)\\in\\mathcal{N}(v_{c}(i))}\\alpha_{x_{j}\\to c_{i}}^{(l)}(\\mathbf{w}_{V}^{(l)}\\mathbf{x}_{j}^{(l)})\\right),} \\end{array} \\quad (3.11)\\] where \\(\\alpha_{x_{j}\\to c_{i}}\\) denotes the attention score between clause \\(c_{i}\\) and variable \\(x_{j}\\) , \\(\\mathbf{w}_{Q}^{(l)}\\in \\mathbb{R}^{d}\\) , \\(\\mathbf{w}_{K}^{(l)}\\in \\mathbb{R}^{d}\\) , and \\(\\mathbf{w}_{V}^{(l)}\\in \\mathbb{R}^{d}\\) are learnable parameters of weights, \\(\\mathbf{c}_{i}^{(l)}\\) and \\(\\mathbf{x}_{j}^{(l)}\\) are features of \\(i\\) - th clause and \\(j\\) - th variable at layer \\(l\\) , \\(\\mathcal{N}(v_{c}(i))\\) is the set of neighbor variable nodes for clause \\(c_{i}\\) , \\(\\langle \\cdot ,\\cdot \\rangle\\) is the dot product operation, \\(\\mathbf{c}_{i}^{(l + 1)}\\) is the updated features for clause \\(c_{i}\\) , and MLP denotes the multi- layer perceptron. Second, the variable feature can be updated through variable- wise aggregation from the features of clauses that contain the variable. Similarly, for variable \\(x_{i}\\) , the layer- wise aggregation from \\(c_{j}\\) to \\(x_{i}\\) can be denoted as: \\[\\begin{array}{r l} & {\\alpha_{c_{j}\\to x_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{c}(k)\\in\\mathcal{N}(v_{x}(i))}\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{k}^{(l)})\\rangle\\right)},}\\\\ & {\\mathbf{x}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{x}_{i}^{(l)},\\sum_{v_{c}(j)\\in\\mathcal{N}(v_{x}(i))}\\alpha_{c_{j}\\to x_{i}}^{(l)}(\\tilde{\\mathbf{w}}_{V}^{(l)}\\mathbf{c}_{j}^{(l)})\\right),} \\end{array} \\quad (3.12)\\]\n\n<center>Figure 2 Illustration of the proposed pre-training and fine-tuning architecture. In the pre-training stage, the bipartite graphs generated from Max-SAT clauses are used to train the MLP, bipartite GNN backbone (Bip-GNN(\u00b7)), and fully connected classification layers \\(\\mathrm{FC}(\\cdot)\\) . In the fine-tuning stage, the bipartite graphs generated by Max-SAT and the CO are treated as source and target domains and are separately fed into the MLP and Bip-GNN to obtain features. A discriminator is to obtain domain labels for domain adaptation. The features are passed through the classification layer to predict the labels of variable nodes. </center> where \\(\\alpha_{c_{j}\\to x_{i}}\\) denotes the attention score between clause \\(c_{j}\\) and variable \\(x_{i}\\) , \\(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\) , \\(\\tilde{\\mathbf{w}}_{K}^{(l)}\\) , and \\(\\tilde{\\mathbf{w}}_{V}^{(l)}\\) are parameters, and \\(\\mathcal{N}(v_{x}(i))\\) is the neighbor set of clause nodes for variable \\(x_{i}\\) . After the two steps of aggregation, we can obtain the updated features of variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers for bipartite GNNs. The correlations between variables and clauses can be captured by the aggregation processes and the importance of neighbors can be learned by the attention mechanism. We can then build our framework based on the bipartite GNN backbone. ### 3.4 Pre-Training and Fine-Tuning with Domain Adaptation In this subsection, we introduce the framework of our work for training as depicted in Figure 2. Our framework consists of two stages. The first stage is to pre- train our model by using massive samples generated from Max- SAT and the second stage is to further fine- tune the model with domain adaptation that combines samples from Max- SAT and CO problems. #### 3.4.1 Pre-Training In the pre- training stage, the goal is to learn the general knowledge from Max- SAT and obtain better parameter initialization that can be used for solving different CO problems. We first generate massive clauses from Max- SAT and then convert them into bipartite graphs. To generate Max- SAT clauses, we follow [48] and select different distributions to simulate different scenarios of problems: (1) In uniform distribution, the variables and clauses appear with the same frequency. The sizes of clauses also appear with the same frequency. The clause size refers to the number of variables within a clause. For example, given a clause \\(x_{1} \\lor x_{2} \\lor \\neg x_{3}\\) , the clause size is three. (2) The single power- low distribution is a non- uniform distribution where most variables appear in low frequencies while only a few variables appear in high frequencies. The sizes of clauses also appear with the same frequency. (3) Double power- low distribution is also a non- uniform distribution that aims to simulate extremely uneven samples. Small clause size appears in high frequencies and the frequency decreases as the clause size increases.\n\nAfter obtaining the bipartite graphs, we can build our pre- training process. To be concrete, the model structure for pre- training contains three parts, an MLP that is used to map the initial node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) into low- dimensional features \\(\\mathbf{X}^{(0)}\\) and \\(\\mathbf{C}^{(0)}\\) ; the bipartite GNN backbone \\(\\mathrm{Bip - GNN}(\\cdot)\\) from section 3.3 extracts high- level node features for variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers; and fully connected prediction network \\(\\mathrm{FC}(\\cdot)\\) maps node features to their labels. To pre- train the three parts, we build a supervised loss function to update the parameters. We leverage the binary cross entropy (BCE) as the objective of classification: \\[\\mathcal{L}_{c} = \\sum_{i = 1}^{n}\\mathrm{BCE}(p_{i},y_{i}), \\quad (3.14)\\] where \\(y_{i}\\in \\{0,1\\}\\) is the label that describes the truth assignment of the variable generated from the MAXHS solver, and \\(p_{i}\\in \\{0,1\\}\\) is the prediction of the pre- training model from the prediction network \\(\\mathrm{FC}(\\cdot)\\) . It is worth noting that we only consider the classification loss under \\(n\\) variable nodes. Since the truth assignments of clauses are easily affected by variables, it is difficult to determine the labels of clauses. Therefore, to avoid the disturbance brought by inaccurate labeling, we only consider the classification of variables. #### 3.4.2 Fine-Tuning with Domain Adaptation After the pre- training process, we further leverage Max- SAT to build a fine- tuning process based on domain adaptation. In general, domain adaptation aims to learn domain- invariant features that can be used for different tasks. For a target CO problem, the proposed adaptation network treats Max- SAT as the source domain and learns transferable features by combining the samples from the CO and Max- SAT problems. Before the feature extraction, we can construct the data from source and target domains via transformation operations in section 3.2. The data for the source domain is generated directly from Max- SAT with different distributions following the pre- training process while the data of the target domain is generated from Max- SAT that is converted from a certain CO problem. The network architecture of fine- tuning is based on the pre- trained network. The pre- training network learns generalizable features while the fine- tuning network focuses on learning task- specific features. With the help of pre- training, the model can converge faster and achieve better generalization performance. Concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on MLP and \\(\\mathrm{Bip - GNN}(\\cdot)\\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \\(\\mathrm{Dis}(\\cdot)\\) is to classify the domain labels for each sample. The source domain and target domain share the same feature extraction backbone but have different classification networks. The domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training. Our work can also be extended to the unsupervised setting where the labels for the target domain are unavailable. Based on the three parts, the overall loss for domain adaptation in the fine- tuning stage can be denoted as \\[\\mathcal{L}_{ft} = \\mathcal{L}_{c} + \\lambda \\mathcal{L}_{d}, \\quad (3.15)\\] where \\(\\mathcal{L}_{c}\\) and \\(\\mathcal{L}_{d}\\) are the losses for classification and discrimination, and \\(\\lambda\\) is a positive hyper- parameter that controls the weight of losses. The discriminative loss that classifies nodes based on their domain labels can be denoted as \\[\\mathcal{L}_{d} = (\\mathbf{X}_{S},\\mathbf{X}_{T}) = \\mathbb{E}_{\\mathbf{x}_{S}\\in \\mathcal{D}_{S}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{S}))\\right)] + \\mathbb{E}_{\\mathbf{x}_{T}\\in \\mathcal{D}_{T}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{T}))\\right)], \\quad (3.16)\\] where \\(\\mathbf{X}_{S}\\) and \\(\\mathbf{X}_{D}\\) are the node features from source and target domains, \\(\\mathcal{D}_{S}\\) and \\(\\mathcal{D}_{T}\\) represent the data distribution of source and target domains, \\(\\mathrm{Dis}(\\cdot)\\) is the domain classifier, and \\(\\mathrm{Bip - GNN}(\\cdot)\\) is the feature extractor.",
    "introduction": "### 3.5 Inference with Local Search Method The last step of our model is to search the feasible solutions for CO problems with constraint conditions by using the predictions generated by our framework. We can also get close to optimal solutions with a search algorithm. For Max- Cut problems without hard clauses, our model can directly infer the truth assignment and obtain the target values. For problems with hard clauses, we introduce a heuristic local search method to discretely obtain feasible solutions that satisfy constraints and obtain the target values. Local search is a typical search method for discrete optimization. We adopt a 2- improvement local search algorithm [49,50] that iterates over all nodes in the graph and attempts to replace a 1- labeled node \\(v_{i}\\) with two 1- labeled nodes \\(v_{j}\\) and \\(v_{k}\\) . For the MIS problem, \\(v_{j}\\) and \\(v_{k}\\) must be the neighbors of \\(v_{i}\\) . These two nodes are 1- tight and are not connected. Here, a node is 1- tight if exactly one of its neighbors is 1- labeled. In other words, \\(v_{i}\\) is the only 1- labeled neighbor of \\(v_{j}\\) and \\(v_{k}\\) in the graph. Notice that this local search algorithm can find a valid 2- improvement in \\(O(E)\\) time if it exists. We introduce this method with fixed steps to find a feasible solution and evaluate the performance of our model. ### 3.6 Overall Algorithms Most of the previous works focus on modeling specific CO problems. In contrast, our method captures the common knowledge of COs and designs a unified learning method to utilize the knowledge to solve COs. To summarize, our framework has three essential processes. The problem transfer process leverages Max- SAT to bridge various COs. The COs are converted to Max- SAT with a general form that can capture logical information within CO problems. From the perspective of data transformation, the graphs in COs are firstly converted into clauses and then converted to bipartite graphs. The overall algorithm for problem transfer is listed in Algorithm 1. Algorithm 1 Problem Transfer via Max- SAT Input: A CO graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) Objective function: max \\(f(S)\\) Constraint condition: \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) Output: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) #From a graph to Max- SAT clauses# 1: Generate soft clauses \\(C_{s}\\) with objective function max \\(f(S)\\) 2: Generate hard clauses \\(C_{h}\\) with constraint condition \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) #From Max- SAT clauses to a bipartite graph# 3: Construct nodes \\(\\tilde{\\nu}_{x}\\) and \\(\\tilde{\\nu}_{c}\\) for each variable \\(v_{i}\\) and clause \\(c_{j}\\) 4: Construct edges \\(e_{(v_{i},c_{j})}^{\\prime}\\in \\tilde{\\mathcal{E}},\\forall v_{i}\\in c_{j}\\) The pre- training process uses samples from Max- SAT to learn generalizable features that can benefit all COs. The networks used for pre- training include the MLP, feature extraction backbones based on GNNs, and the classification network. The overall algorithm for pre- training is listed in Algorithm 2. The fine- tuning process uses samples from Max- SAT and target CO to build a domain adaptation architecture. Based on the pre- trained network, the fine- tuning network introduces an additional discriminator for domain classification, which can learn domain- invariant features to further improve the generalizability of features. The overall algorithm for fine- tuning is listed in Algorithm 3. ## 4 Experimental Results In this section, we evaluated the effectiveness of the proposed framework for three CO problems on graphs, i.e., Max- Cut, MIS, and MDS. Generally, we aimed to answer three essential questions by the experiments. Q1: Can we leverage Max- SAT to learn transferable and generalizable features that can improve the ability to solve different CO problems on graphs? Q2: How can we incorporate Max- SAT\n\nInput: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, True assignment \\(\\mathcal{V}\\) Number of pre- training layers \\(L\\) Number of iterations iters. Output: Pre- trained parameters \\(\\Theta_{p}\\) for MLP, Bip- GNN, and FC. 1: Initialize node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for nodes of variables and clauses; 4: for \\(l\\leqslant L\\) do 5: Extract features for clauses \\(\\mathbf{C}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 6: Extract features for variables \\(\\mathbf{X}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 7: end for 8: Map variable features to the predictions via \\(\\mathrm{FC}(\\mathbf{X}^{(L)})\\) 9: Compute the loss \\(\\mathcal{L}_{c}\\) in Eq. (3.14); 10: Backward and update parameters; 11: end for 12: Obtain the updated parameters \\(\\Theta_{p}\\) Algorithm 3 Fine- tuning Based on Domain Adaptation Input: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, Number of fine- tuning layers \\(L\\) Number of iterations iters, True assignment \\(\\mathcal{V}\\) Output: Predicted feasible solutions \\(\\mathcal{P} = \\{p_{i}\\}_{i = 1}^{n}\\) Fine- tuned parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, FC, and Dis. 1: Initialize node attributes of both domains \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for source variables and clauses; 4: Obtain features \\(\\mathbf{X}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for target variables and clauses; 5: for \\(l\\leqslant L\\) do 6: Extract features for clauses \\(\\mathbf{C}_{S}^{(l)}\\) and \\(\\mathbf{C}_{T}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 7: Extract features for variables \\(\\mathbf{X}_{S}^{(l)}\\) and \\(\\mathbf{X}_{T}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 8: end for 9: Map variable features to the assignment labels via \\(\\mathrm{FC}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{FC}(\\mathbf{X}_{T}^{(L)})\\) 10: Map variable features to the domain labels via \\(\\mathrm{Dis}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{Dis}(\\mathbf{X}_{T}^{(L)})\\) 11: Compute the loss \\(\\mathcal{L}_{ft}\\) in Eq. (3.15); 12: Updated parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, Dis, and FC; 13: end for 14: Obtain the optimal parameters \\(\\Theta_{ft}\\) 15: Predict feasible solutions \\(\\mathcal{P}\\) into the pipelines of GNN- based learning frameworks to fully utilize the information of Max- SAT? Q3: Do different pre- training and domain adaptation strategies matter for the ability of solving COs? ### 4.1 Experimental Setup Datasets We introduce the datasets that were used in our experiments in this part. In the pretraining stage, a large number of Max- SAT instances were required. We generated the clauses by running three representative generators with different distributions: the uniform distribution, the power- law distribution, and the double power- law distribution. A total number of 20,000 Max- SAT instances\n\nwere generated and the Max HS solver was used to obtain the solutions that served as labels for training. In the fine- tuning stage, according to the CO problems, different datasets are leveraged. For Max- Cut, we introduce the GSET benchmark \\(^{1)}\\) for evaluation. GSET is a set of 71 unweighted graphs that were commonly used for testing the algorithms to solve the Max- Cut problem. For MIS and MDS problems, we introduce the frb benchmark with four different instance scales for evaluation. Each frb dataset consists of five instances of hard CO problems and is regarded as the benchmark for CSP competition annually. Evaluation Metrics For different CO problems, we used the corresponding evaluation metrics based on their feasible solutions. For Max- Cut, \\(p\\) values [51] were used to evaluate the number of graph cuts. Concretely, \\(p\\) can be calculated by \\[p(z) = \\frac{z / n - \\gamma / 4}{\\sqrt{\\gamma / 4}}, \\quad (4.1)\\] where \\(z\\) is the predicted cut size for a \\(\\gamma\\) - regular graph with \\(n\\) nodes and \\(\\gamma\\) is the degree of nodes. For MIS and MDS, the number of cuts or sets is used for evaluation. Baseline methods To evaluate our method, we compare it with several baseline methods that have been widely used in solving COs. These methods can be roughly classified as traditional methods, heuristic methods, and learning- based methods. SDP [52] is a classical approach based on semi- definite programming. EO [53] is the extremal optimization method. BLS [54] is the breakout local search method. ECO- DQN [55] is an approach of exploratory CO with reinforcement learning. GMC- A and GMC- B [56] are two unsupervised GNN architectures for Max- CUT with different loss functions. RUN- CSP [57] is a recurrent unsupervised neural network for constraint satisfaction problems. PIGNN [58] is the physics- inspired GNNs for CO problems. MAXSAT [59] is a GNN- based framework designed for Max- SAT problems. Experimental Settings The proposed framework was implemented in Python with Py Torch. We summarized the hyper- parameters used in our paper as follows. The dimension of features \\(d\\) was set as 128. The number of GNN layers \\(L\\) for pre- training and fine- tuning was set as 5 and the first 2 layers were fixed during the fine- tuning. The number of layers for MLP was set as 2. The number of epochs for pre- training and fine- tuning were tuned and set as 400 and 100. The Adam optimizer was used for model training with a learning rate of \\(10^{- 5}\\) and a weight decay of \\(10^{- 10}\\) . The warm- up trick was also adopted. The number of FC layers for classification was set as 2 and the discriminator used a two- layer fully connected network to generate domain labels. The steps for local search for MIS and MDS were set as 120. All experiments were conducted on a workstation equipped with an Intel Xeon(R) Gold 6139M CPU 515 @ 2.30GHz and an NVIDIA RTX 3090 GPU with 24GB. Table 1 Results of solving Max-Cut with different approaches on random graphs. <table><tr><td rowspan=\"2\">Methods</td><td colspan=\"4\">n=100</td><td colspan=\"4\">n=200</td><td colspan=\"4\">n=500</td><td colspan=\"4\">n=800</td><td colspan=\"4\">n=1000</td></tr><tr><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td></td><td>\u03b3=10</td></tr><tr><td>SDP</td><td>0.709</td><td>0.697</td><td>0.689</td><td>0.709</td><td>0.702</td><td>0.692</td><td>0.702</td><td>0.690</td><td>0.682</td><td>0.701</td><td>0.688</td><td>0.679</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>0.712</td><td>0.708</td><td>0.703</td><td>0.721</td><td>0.723</td><td>0.724</td><td>0.727</td><td>0.737</td><td>0.735</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>0.712</td><td>0.707</td><td>0.704</td><td>0.720</td><td>0.721</td><td>0.719</td><td>0.722</td><td>0.725</td><td>0.721</td><td>0.720</td><td>0.726</td><td>0.717</td><td>0.721</td><td>0.725</td><td>0.720</td><td></td><td></td><td></td><td></td></tr><tr><td>ECO-DQN</td><td>0.713</td><td>0.705</td><td>0.707</td><td>0.718</td><td>0.723</td><td>0.720</td><td>0.725</td><td>0.727</td><td>0.725</td><td>0.722</td><td>0.721</td><td>0.722</td><td>0.726</td><td>0.726</td><td>0.721</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-A</td><td>0.691</td><td>0.655</td><td>0.637</td><td>0.701</td><td>0.683</td><td>0.660</td><td>0.693</td><td>0.668</td><td>0.599</td><td>0.691</td><td>0.666</td><td>0.602</td><td>0.688</td><td>0.662</td><td>0.601</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-B</td><td>0.698</td><td>0.660</td><td>0.630</td><td>0.708</td><td>0.667</td><td>0.646</td><td>0.707</td><td>0.701</td><td>0.670</td><td>0.699</td><td>0.696</td><td>0.653</td><td>0.702</td><td>0.694</td><td>0.651</td><td></td><td></td><td></td><td></td></tr><tr><td>RUN-CSP</td><td>0.702</td><td>0.706</td><td>0.704</td><td>0.711</td><td>0.715</td><td>0.714</td><td>0.714</td><td>0.726</td><td>0.710</td><td>0.704</td><td>0.713</td><td>0.705</td><td>0.705</td><td>0.711</td><td>0.702</td><td></td><td></td><td></td><td></td></tr><tr><td>PI-GNN</td><td>0.704</td><td>0.706</td><td>0.708</td><td>0.712</td><td>0.718</td><td>0.717</td><td>0.715</td><td>0.726</td><td>0.716</td><td>0.711</td><td>0.724</td><td>0.712</td><td>0.709</td><td>0.722</td><td>0.712</td><td></td><td></td><td></td><td></td></tr><tr><td>MAX-SAT</td><td>0.702</td><td>0.705</td><td>0.708</td><td>0.712</td><td>0.716</td><td>0.718</td><td>0.721</td><td>0.732</td><td>0.726</td><td>0.719</td><td>0.730</td><td>0.728</td><td>0.718</td><td>0.730</td><td>0.726</td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>0.702</td><td>0.708</td><td>0.707</td><td>0.722</td><td>0.721</td><td>0.725</td><td>0.732</td><td>0.738</td><td>0.733</td><td>0.734</td><td>0.739</td><td>0.733</td><td>0.732</td><td>0.738</td><td>0.733</td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 2 Results of solving Max-Cut with different approaches on the GSET benchmark. <table><tr><td colspan=\"5\">Max-Cut</td></tr><tr><td>Methods</td><td>G14</td><td>G15</td><td>G22</td><td>G55</td></tr><tr><td>V</td><td>800</td><td>800</td><td>2000</td><td>5000</td></tr><tr><td>E</td><td>4694</td><td>4661</td><td>19990</td><td>12468</td></tr><tr><td>SDP</td><td>2922</td><td>2938</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>2991</td><td>3047</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>3064</td><td>3050</td><td>-</td><td>-</td></tr><tr><td>ECO-DQN</td><td>3056</td><td>3028</td><td>13268</td><td>10198</td></tr><tr><td>GMC-A</td><td>2940</td><td>2948</td><td>12978</td><td>10094</td></tr><tr><td>GMC-B</td><td>2918</td><td>2906</td><td>13246</td><td>10104</td></tr><tr><td>RUN-CSP</td><td>2943</td><td>2928</td><td>13028</td><td>10116</td></tr><tr><td>PI-GNN</td><td>3026</td><td>2990</td><td>13181</td><td>10138</td></tr><tr><td>MAX-SAT</td><td>3046</td><td>3012</td><td>13250</td><td>10204</td></tr><tr><td>Ours</td><td>3052</td><td>3018</td><td>13269</td><td>10208</td></tr></table> Table 3 Results of solving MIS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Maximum Independent Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Redu MIS(Solver)</td><td>30.0</td><td>39.4</td><td>48.8</td><td>57.4</td></tr><tr><td>Greedy</td><td>24.6</td><td>33.0</td><td>42.2</td><td>48.0</td></tr><tr><td>GMC-A</td><td>21.2</td><td>27.7</td><td>40.2</td><td>45.2</td></tr><tr><td>GMC-B</td><td>20.8</td><td>28.1</td><td>40.6</td><td>46.0</td></tr><tr><td>RUN-CSP</td><td>25.8</td><td>33.6</td><td>42.2</td><td>49.4</td></tr><tr><td>PIGNN</td><td>26.5</td><td>35.1</td><td>44.4</td><td>51.8</td></tr><tr><td>MAX-SAT</td><td>27.9</td><td>36.6</td><td>44.4</td><td>51.8</td></tr><tr><td>Ours</td><td>27.8</td><td>36.8</td><td>44.8</td><td>52.2</td></tr></table> ### 4.2 Performance of Solving COs In this section, we evaluated the effectiveness of the proposed framework and aimed to answer Q1. Without loss of generality, Max- Cut, MIS, and MDS are selected for evaluation. Our framework is also adaptive to other COs that can be transformed as Max- SAT. For Max- Cut, we first reported the mean \\(p\\) - values across 1,000 regular graphs with 100, 200, 500, and 1000 nodes for three levels of node degrees in Table 1. Then, the experiments on the GSET benchmark were carried out and the number of cuts was reported in Table 2 for comparison. When encountering large- scale datasets (G22 and G55), traditional methods, e.g., EO and BLS, were out of memory and could not obtain the results. Based on the two tables, we have the following observations: (1) Our method obtained the best results compared with learning- based methods, which demonstrated that the proposed framework exhibited the powerful ability to learn useful information from CO problems. The results also showed that Max- SAT can boost the performance of solving COs; (2) Our method was superior in most cases compared with all baselines. The advantages of our method were mainly in large- scale Max- Cut problems (e.g., \\(n = 800\\) , G22, and G55) where the performance surpassed traditional methods;\n\nTable 4 Results of solving MDS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Minimum Dominated Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Greedy</td><td>191.1</td><td>255.0</td><td>296.5</td><td>284.8</td></tr><tr><td>HTS-DS</td><td>191.0</td><td>255.2</td><td>297.6</td><td>286.4</td></tr><tr><td>MWDS-CRO</td><td>191.0</td><td>255.1</td><td>298.4</td><td>285.5</td></tr><tr><td>GMC-A</td><td>201.2</td><td>268.1</td><td>301.3</td><td>292.2</td></tr><tr><td>GMC-B</td><td>192.1</td><td>261.4</td><td>299.1</td><td>291.6</td></tr><tr><td>RUN-CSP</td><td>190.8</td><td>255.4</td><td>299.3</td><td>278.6</td></tr><tr><td>PIGNN</td><td>191.1</td><td>255.3</td><td>299.1</td><td>280.2</td></tr><tr><td>MAX-SAT</td><td>191.0</td><td>255.3</td><td>298.8</td><td>278.8</td></tr><tr><td>Ours</td><td>190.8</td><td>255.1</td><td>298.5</td><td>278.1</td></tr></table> (3) Traditional or heuristic methods are very competitive in Max-Cut. However, the gaps between our method and these methods were small. Besides, the time of training and inference for our method was much less than these methods. For MIS and MDS, we reported the number of sets for MIS and the number of dominated nodes for MDS in Tables 3 and 4. \\(V\\) and \\(E\\) are the sets for nodes and edges. Greedy denotes the classical greedy algorithm. Larger and smaller values denote better results for MIS and MDS, respectively. The experiments were carried out on four frb datasets. Similar results can be observed in these two CO problems. Particularly, In MIS, our results were relatively closer to the results obtained by the solver, which showed the effectiveness of our framework. Based on the results obtained on the above three CO problems, we can answer Q1: Max- SAT offered common knowledge and the pre- training and adaptation pipeline built based on Max- SAT can learn transferable and generalizable features to improve the ability to solve different CO problems on graphs. <center>Figure 3 Results of ablation study with different variants of our model on Max-Cut. </center>\n\n<center>Figure 4 Heat maps of knowledge transferability from three different CO problems to the maximum independent set problem on two scales of nodes. </center> ### 4.3 Ablation Study In this part, we conducted ablation studies on the Max- Cut problem to verify the effectiveness of each module in our framework. We gradually removed each single module and kept the other modules unchanged. The variants of our frameworks are listed as follows. \\(\\mathbf{w} / \\mathbf{o}\\) pre- T denotes the model that removes the pre- training stage but keeps the domain adaptation module. \\(\\mathbf{w} / \\mathbf{o}\\) DA denotes the model that removes the domain adaptation module but keeps the pre- training stage. \\(\\mathbf{w} / \\mathbf{o}\\) Bi- G is the model that directly uses GNNs to extract the features in the original graphs represented from CO problems without converting them to MAX- SAT and bipartite graphs. \\(\\mathbf{w} / \\mathbf{o}\\) Att is the model that keeps bipartite GNNs without the attention mechanism. \\(\\mathbf{w} / \\mathbf{o}\\) Mult- Att is the model that keeps bipartite GNNs without the multi- head mechanism. We reported the \\(p\\) - values for all variants in Figure 3 and had the following observations. (1) All modules boosted the performance and played important roles in our framework. The overall model obtained the best results, which demonstrated that these modules were nicely incorporated into our framework. (2) Removing the problem transfer step experienced the largest performance degradation, which proved the effectiveness of Max- SAT and also answered Q1. (3) Pre- training and adaptation were both useful in improving the performance. Since both steps utilized Max- SAT to assist the learning of transferable features, we can answer Q2 and conclude that the combination of different strategies to use Max- SAT can largely benefit solving CO problems. (4) Attention and multi- head mechanisms also affected the results, which showed that better feature extraction backbones indeed helped the learning methods to solve CO problems. ### 4.4 Effects of Different Knowledge Transfer Strategies In this subsection, we verify the knowledge transfer ability of our model. we fixed the target domain as the MIS problem and used Max- SAT, Max- Cut, and MDS as source domains to check different degrees of transferability. To be concrete, we fed the data from different source domains into our framework for pre- training and domain adaptation and observed how the performance was affected by the selection of domains. We selected two scales of nodes, i.e., 450 variables and 760 variables, for demonstration. The results were shown in Figure 4 where darker colors indicated better performance and transferability.\n\n<center>Figure 5 Parameter analysis for feature dimension \\(d\\) , number of attention heads \\(h\\) , weight of loss \\(\\lambda\\) , and number of layers \\(L\\) . </center> From the heat maps, we can observe that Max- SAT \\(\\rightarrow\\) MIS always obtained the best results compared with the other two types of knowledge transfer, which showed that Max- SAT was more general and had the strongest transferability across different CO problems. Moreover, MDS is a more relevant CO problem to MIS compared with Max- Cut. Therefore, we observed better results from MDS \\(\\rightarrow\\) MIS than Max- Cut \\(\\rightarrow\\) MIS. This observation answered Q3 where different source domains do affect the performance of the target domain. From a practical viewpoint, selecting an appropriate source domain was always hard, and irrelevant tasks would generate negative transfers and eventually affect the performance of the target domain. ### 4.5 Parameter Analysis In this subsection, we analyzed the influence of several significant hyper- parameters on the model performance: the dimension of features \\(d\\) , the number of attention heads \\(h\\) , the weight of losses \\(\\lambda\\) , and the number of pertaining layers \\(L\\) . We recorded the performance of different \\(d\\) , \\(h\\) , \\(\\lambda\\) , \\(L\\) values on Max- CUT. The results were reported in Figure 5. It can be observed that with different dimensions of features, our model exhibited relatively stable performance when \\(d\\) was greater than or equal to 16. Our\n\nmodel achieved optimal performance when \\(d\\) was 128. Our model was not significantly affected by the number of attention heads. It achieved optimal performance when \\(k\\) was 10. The optimal value of weight \\(\\lambda\\) was obtained in 0.2. Pre- training led to better results, especially when the number of layers exceeded three. ## 5 Conclusion In this study, we aimed to discover the transferability of deep learning architectures for solving different CO problems on graphs. To achieve this goal, we leveraged the Max- SAT problem as a general formulation of original CO problems. The key advantages were two- fold. First, Max- SAT served as a bridge for various CO problems and provided a tool for discovering common properties in CO problems. Second, Max- SAT problems were represented as clauses that included logical information in addition to the simple correlations in graphs. By further constructing a pre- training and adaptation framework, we can extract transferable features that can be used for various CO problems. Experiments showed that incorporating Max- SAT indeed improved the ability to solve various CO problems on graphs. Our current work can be extended from the following directions. The pre- training stage is built on supervised tasks where labeled data samples are required. As a future direction, it is necessary to develop unsupervised pre- training paradigms that can fully leverage information from the data itself. Moreover, advanced GNNs that are more suitable for the properties of CO problems should be designed in the future. Acknowledgements This work was supported by the National Natural Science Foundation of China (Grant No. 11991021, No. 11991020, No. 12271503). ## References 1 Petrica C Pop, Ovidiu Cosma, Cosmin Sabo, and Corina Pop Sitar. A comprehensive survey on the generalized traveling salesman problem. European Journal of Operational Research, 2023. 2 Nathan Krislock, J\u00e9r\u00f4me Malick, and Fr\u00e9d\u00e9ric Roupin. Improved semidefinite bounding procedure for solving max- cut problems to optimality. Mathematical Programming, 143:61- 86, 2014. 3 Yiyuan Wang, Shaowei Cai, Shiwei Pan, Ximing Li, and Monghao Yin. Reduction and local search for weighted graph coloring problem. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 2433- 2441, 2020. 4 Marc Los and Christian Lardinois. Combinatorial programming, statistical optimization and the optimal transportation network problem. Transportation Research Part B: Methodological, 16(2):89- 124, 1982. 5 Simone L Martins and Celso C Ribeiro. Metaheuristics and applications to optimization problems in telecommunications. Handbook of Optimization in Telecommunications, pages 103- 128, 2006. 6 Eugene L Lawler and David E Wood. Branch- and- bound methods: A survey. Operations Research, 14(4):699- 719, 1966. 7 Steven J Benson, Yinyu Ye, and Xiong Zhang. Solving large- scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000. 8 Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263- 1272. PMLR, 2017. 9 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. 10 Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016. 11 Quentin Cappart, Didier Ch\u00e9telat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 12 Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in Neural Information Processing Systems, 30, 2017. 13 Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in Neural Information Processing Systems, 31, 2018.\n\n14 Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019. 15 Tianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang, Howard Heaton, Jialin Liu, and Wotao Yin. Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research, 23(1):8562- 8620, 2022. 16 Matteo Fischetti, Paolo Toth, and Daniele Vigo. A branch- and- bound algorithm for the capacitated vehicle routing problem on directed graphs. Operations Research, 42(5):846- 859, 1994. 17 Panos M Pardalos and Gregory P Rodgers. A branch and bound algorithm for the maximum clique problem. Computers & Operations Research, 19(5):363- 375, 1992. 18 Richard Bellman. Dynamic programming. Science, 153(3731):34- 37, 1966. 19 Michael Held and Richard M Karp. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1):196- 210, 1962. 20 Silvano Martello, David Pisinger, and Paolo Toth. Dynamic programming and strong bounds for the 0- 1 knapsack problem. Management Science, 45(3):414- 424, 1999. 21 Laurence A Wolsey and George L Nemhauser. Integer and combinatorial optimization, volume 55. John Wiley & Sons, 1999. 22 Dimitris Bertsimas and Ramazan Demir. An approximate dynamic programming approach to multidimensional knapsack problems. Management Science, 48(4):550- 565, 2002. 23 Paul Bouman, Niels Agatz, and Marie Schmidt. Dynamic programming approaches for the traveling salesman problem with drone. Networks, 72(4):528- 542, 2018. 24 Mutsunori Yagiura and Toshihide Ibaraki. On metaheuristic algorithms for combinatorial optimization problems. Systems and Computers in Japan, 32(3):33- 55, 2001. 25 James B Orlin, Abraham P Punnen, and Andreas S Schulz. Approximate local search in combinatorial optimization. SIAM Journal on Computing, 33(5):1201- 1214, 2004. 26 Emile HL Aarts, Jan HM Korst, and Peter JM van Laarhoven. A quantitative analysis of the simulated annealing algorithm: A case study for the traveling salesman problem. Journal of Statistical Physics, 50:187- 206, 1988. 27 Jean Berger and Mohamed Barkaoui. A hybrid genetic algorithm for the capacitated vehicle routing problem. In Genetic and Evolutionary Computation Conference, pages 646- 656. Springer, 2003. 28 Andr\u00e9 Chassein, Marc Goerigk, Adam Kasperski, and Pawel Zieli\u0144ski. Approximating combinatorial optimization problems with the ordered weighted averaging criterion. European Journal of Operational Research, 286(3):828- 838, 2020. 29 Henrique Lemos, Marcelo Prates, Pedro Avelar, and Luis Lamb. Graph coloring meets deep learning: Effective graph neural network models for combinatorial problems. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, pages 879- 885. IEEE, 2019. 30 Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym- nco: Leveraging symmetricity for neural combinatorial optimization. Advances in Neural Information Processing Systems, 35:1936- 1949, 2022. 31 Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2018. 32 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in Neural Information Processing Systems, 30, 2017. 33 Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. 34 Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. Gppt: Graph pre- training and prompt tuning to generalize graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1717- 1727, 2022. 35 Zixuan Yi, Iadh Ounis, and Craig Macdonald. Contrastive graph prompt- tuning for cross- domain recommendation. ACM Transactions on Information Systems, 2023. 36 Cristian Bodnar, Francesco Di Giovanni, Benjamin Chamberlain, Pietro Li\u00f2, and Michael Bronstein. Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns. Advances in Neural Information Processing Systems, 35:18527- 18541, 2022. 37 Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein. On over- squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865- 7885. PMLR, 2023. 38 Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 39 Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. In The Eleventh International Conference on Learning Representations, 2022. 40 Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, and Marinka Zitnik. Evaluating explainability for graph neural\n\nnetworks. Scientific Data, 10(1):144, 2023. 41 Jin Yuan, Feng Hou, Yangzhou Du, Zhongchao Shi, Xin Geng, Jianping Fan, and Yong Rui. Self- supervised graph neural network for multi- source domain adaptation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3907- 3916, 2022. 42 Quanyu Dai, Xiao- Ming Wu, Jiarren Xiao, Xiao Shen, and Dan Wang. Graph transfer learning via adversarial domain adaptation with graph convolution. IEEE Transactions on Knowledge and Data Engineering, 35(5):4908- 4922, 2022. 43 Ronghang Zhu, Xiaodong Jiang, Jiasen Lu, and Sheng Li. Cross- domain graph convolutions for adversarial unsupervised domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 34(8):3847- 3858, 2023. 44 Xinhong Ma, Tianzhu Zhang, and Changsheng Xu. Gcan: Graph convolutional adversarial network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8266- 8276, 2019. 45 Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and Pan Li. Structural re- weighting improves graph domain adaptation. In International Conference on Machine Learning, pages 21778- 21793, 2023. 46 Mehmet Pilanci and Elif Vural. Domain adaptation on graphs by learning aligned graph bases. IEEE Transactions on Knowledge and Data Engineering, 34(2):587- 600, 2022. 47 Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Graph domain adaptation via theory- grounded spectral regularization. In International Conference on Learning Representations, 2023. 48 Carlos Ans\u00f3tegui, Maria Luisa Bonet, and Jordi Levy. Towards industrial- like random sat instances. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, page 387- 392, 2009. 49 Diogo V Andrade, Mauricio GC Resende, and Renato F Werneck. Fast local search for the maximum independent set problem. Journal of Heuristics, 18:525- 547, 2012. 50 Thomas A Feo, Mauricio GC Resende, and Stuart H Smith. A greedy randomized adaptive search procedure for maximum independent set. Operations Research, 42(5):860- 878, 1994. 51 Amir Dembo, Andrea Montanari, and Subhabrata Sen. Extremal cuts of sparse random graphs. The Annals of Probability, pages 1190- 1217, 2017. 52 Changhui Choi and Yinyu Ye. Solving sparse semidefinite programs using the dual scaling algorithm with an iterative solver. Department of Management Sciences, University of Iowa, Iowa City, IA, 52242, 2000. 53 Stefan Boettcher and Allon G Percus. Extremal optimization for graph partitioning. Physical Review E, 64(2):026114, 2001. 54 Una Benlic and Jin- Kao Hao. Breakout local search for the max- cutproblem. Engineering Applications of Artificial Intelligence, 26(3):1162- 1173, 2013. 55 Thomas Barrett, William Clements, Jakob Foerster, and Alex Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3243- 3250, 2020. 56 Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max- cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242- 251. SPIE, 2019. 57 Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence, 3:580607, 2021. 58 Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics- inspired graph neural networks. Nature Machine Intelligence, 4(4):367- 377, 2022. 59 Minghao Liu, Pei Huang, Fuqi Jia, Fan Zhang, Yuchen Sun, Shaowei Cai, Feifei Ma, and Jian Zhang. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 16264- 16265, 2023.",
    "35_inference_with_local_search_method_the_last_step_of_our_model_is_to_search_the_feasible_solutions_for_co_problems_with_constraint_conditions_by_using_the_predictions_generated_by_our_framework_we_can_also_get_close_to_optimal_solutions_with_a_search_algorithm_for_max-_cut_problems_without_hard_clauses_our_model_can_directly_infer_the_truth_assignment_and_obtain_the_target_values_for_problems_with_hard_clauses_we_introduce_a_heuristic_local_search_method_to_discretely_obtain_feasible_solutions_that_satisfy_constraints_and_obtain_the_target_values_local_search_is_a_typical_search_method_for_discrete_optimization_we_adopt_a_2-_improvement_local_search_algorithm_4950_that_iterates_over_all_nodes_in_the_graph_and_attempts_to_replace_a_1-_labeled_node_v_i_with_two_1-_labeled_nodes_v_j_and_v_k_for_the_mis_problem_v_j_and_v_k_must_be_the_neighbors_of_v_i_these_two_nodes_are_1-_tight_and_are_not_connected_here_a_node_is_1-_tight_if_exactly_one_of_its_neighbors_is_1-_labeled_in_other_words_v_i_is_the_only_1-_labeled_neighbor_of_v_j_and_v_k_in_the_graph_notice_that_this_local_search_algorithm_can_find_a_valid_2-_improvement_in_oe_time_if_it_exists_we_introduce_this_method_with_fixed_steps_to_find_a_feasible_solution_and_evaluate_the_performance_of_our_model_36_overall_algorithms_most_of_the_previous_works_focus_on_modeling_specific_co_problems_in_contrast_our_method_captures_the_common_knowledge_of_cos_and_designs_a_unified_learning_method_to_utilize_the_knowledge_to_solve_cos_to_summarize_our_framework_has_three_essential_processes_the_problem_transfer_process_leverages_max-_sat_to_bridge_various_cos_the_cos_are_converted_to_max-_sat_with_a_general_form_that_can_capture_logical_information_within_co_problems_from_the_perspective_of_data_transformation_the_graphs_in_cos_are_firstly_converted_into_clauses_and_then_converted_to_bipartite_graphs_the_overall_algorithm_for_problem_transfer_is_listed_in_algorithm_1_algorithm_1_problem_transfer_via_max-_sat_input_a_co_graph_mathcalg_mathcalvmathcale_objective_function_max_fs_constraint_condition_g_isleqslant_b_ib_iin_omega_output_a_bipartite_graph_tildemathcalg_tildemathcalv_xtildemathcalv_ctildemathcale_from_a_graph_to_max-_sat_clauses_1_generate_soft_clauses_c_s_with_objective_function_max_fs_2_generate_hard_clauses_c_h_with_constraint_condition_g_isleqslant_b_ib_iin_omega_from_max-_sat_clauses_to_a_bipartite_graph_3_construct_nodes_tildenu_x_and_tildenu_c_for_each_variable_v_i_and_clause_c_j_4_construct_edges_e_v_ic_jprimein_tildemathcaleforall_v_iin_c_j_the_pre-_training_process_uses_samples_from_max-_sat_to_learn_generalizable_features_that_can_benefit_all_cos_the_networks_used_for_pre-_training_include_the_mlp_feature_extraction_backbones_based_on_gnns_and_the_classification_network_the_overall_algorithm_for_pre-_training_is_listed_in_algorithm_2_the_fine-_tuning_process_uses_samples_from_max-_sat_and_target_co_to_build_a_domain_adaptation_architecture_based_on_the_pre-_trained_network_the_fine-_tuning_network_introduces_an_additional_discriminator_for_domain_classification_which_can_learn_domain-_invariant_features_to_further_improve_the_generalizability_of_features_the_overall_algorithm_for_fine-_tuning_is_listed_in_algorithm_3_4_experimental_results_in_this_section_we_evaluated_the_effectiveness_of_the_proposed_framework_for_three_co_problems_on_graphs_ie_max-_cut_mis_and_mds_generally_we_aimed_to_answer_three_essential_questions_by_the_experiments_q1_can_we_leverage_max-_sat_to_learn_transferable_and_generalizable_features_that_can_improve_the_ability_to_solve_different_co_problems_on_graphs_q2_how_can_we_incorporate_max-_sat": "Input: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, True assignment \\(\\mathcal{V}\\) Number of pre- training layers \\(L\\) Number of iterations iters. Output: Pre- trained parameters \\(\\Theta_{p}\\) for MLP, Bip- GNN, and FC. 1: Initialize node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for nodes of variables and clauses; 4: for \\(l\\leqslant L\\) do 5: Extract features for clauses \\(\\mathbf{C}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 6: Extract features for variables \\(\\mathbf{X}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 7: end for 8: Map variable features to the predictions via \\(\\mathrm{FC}(\\mathbf{X}^{(L)})\\) 9: Compute the loss \\(\\mathcal{L}_{c}\\) in Eq. (3.14); 10: Backward and update parameters; 11: end for 12: Obtain the updated parameters \\(\\Theta_{p}\\) Algorithm 3 Fine- tuning Based on Domain Adaptation Input: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, Number of fine- tuning layers \\(L\\) Number of iterations iters, True assignment \\(\\mathcal{V}\\) Output: Predicted feasible solutions \\(\\mathcal{P} = \\{p_{i}\\}_{i = 1}^{n}\\) Fine- tuned parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, FC, and Dis. 1: Initialize node attributes of both domains \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for source variables and clauses; 4: Obtain features \\(\\mathbf{X}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for target variables and clauses; 5: for \\(l\\leqslant L\\) do 6: Extract features for clauses \\(\\mathbf{C}_{S}^{(l)}\\) and \\(\\mathbf{C}_{T}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 7: Extract features for variables \\(\\mathbf{X}_{S}^{(l)}\\) and \\(\\mathbf{X}_{T}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 8: end for 9: Map variable features to the assignment labels via \\(\\mathrm{FC}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{FC}(\\mathbf{X}_{T}^{(L)})\\) 10: Map variable features to the domain labels via \\(\\mathrm{Dis}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{Dis}(\\mathbf{X}_{T}^{(L)})\\) 11: Compute the loss \\(\\mathcal{L}_{ft}\\) in Eq. (3.15); 12: Updated parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, Dis, and FC; 13: end for 14: Obtain the optimal parameters \\(\\Theta_{ft}\\) 15: Predict feasible solutions \\(\\mathcal{P}\\) into the pipelines of GNN- based learning frameworks to fully utilize the information of Max- SAT? Q3: Do different pre- training and domain adaptation strategies matter for the ability of solving COs? ### 4.1 Experimental Setup Datasets We introduce the datasets that were used in our experiments in this part. In the pretraining stage, a large number of Max- SAT instances were required. We generated the clauses by running three representative generators with different distributions: the uniform distribution, the power- law distribution, and the double power- law distribution. A total number of 20,000 Max- SAT instances\n\nwere generated and the Max HS solver was used to obtain the solutions that served as labels for training. In the fine- tuning stage, according to the CO problems, different datasets are leveraged. For Max- Cut, we introduce the GSET benchmark \\(^{1)}\\) for evaluation. GSET is a set of 71 unweighted graphs that were commonly used for testing the algorithms to solve the Max- Cut problem. For MIS and MDS problems, we introduce the frb benchmark with four different instance scales for evaluation. Each frb dataset consists of five instances of hard CO problems and is regarded as the benchmark for CSP competition annually. Evaluation Metrics For different CO problems, we used the corresponding evaluation metrics based on their feasible solutions. For Max- Cut, \\(p\\) values [51] were used to evaluate the number of graph cuts. Concretely, \\(p\\) can be calculated by \\[p(z) = \\frac{z / n - \\gamma / 4}{\\sqrt{\\gamma / 4}}, \\quad (4.1)\\] where \\(z\\) is the predicted cut size for a \\(\\gamma\\) - regular graph with \\(n\\) nodes and \\(\\gamma\\) is the degree of nodes. For MIS and MDS, the number of cuts or sets is used for evaluation. Baseline methods To evaluate our method, we compare it with several baseline methods that have been widely used in solving COs. These methods can be roughly classified as traditional methods, heuristic methods, and learning- based methods. SDP [52] is a classical approach based on semi- definite programming. EO [53] is the extremal optimization method. BLS [54] is the breakout local search method. ECO- DQN [55] is an approach of exploratory CO with reinforcement learning. GMC- A and GMC- B [56] are two unsupervised GNN architectures for Max- CUT with different loss functions. RUN- CSP [57] is a recurrent unsupervised neural network for constraint satisfaction problems. PIGNN [58] is the physics- inspired GNNs for CO problems. MAXSAT [59] is a GNN- based framework designed for Max- SAT problems. Experimental Settings The proposed framework was implemented in Python with Py Torch. We summarized the hyper- parameters used in our paper as follows. The dimension of features \\(d\\) was set as 128. The number of GNN layers \\(L\\) for pre- training and fine- tuning was set as 5 and the first 2 layers were fixed during the fine- tuning. The number of layers for MLP was set as 2. The number of epochs for pre- training and fine- tuning were tuned and set as 400 and 100. The Adam optimizer was used for model training with a learning rate of \\(10^{- 5}\\) and a weight decay of \\(10^{- 10}\\) . The warm- up trick was also adopted. The number of FC layers for classification was set as 2 and the discriminator used a two- layer fully connected network to generate domain labels. The steps for local search for MIS and MDS were set as 120. All experiments were conducted on a workstation equipped with an Intel Xeon(R) Gold 6139M CPU 515 @ 2.30GHz and an NVIDIA RTX 3090 GPU with 24GB. Table 1 Results of solving Max-Cut with different approaches on random graphs. <table><tr><td rowspan=\"2\">Methods</td><td colspan=\"4\">n=100</td><td colspan=\"4\">n=200</td><td colspan=\"4\">n=500</td><td colspan=\"4\">n=800</td><td colspan=\"4\">n=1000</td></tr><tr><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td></td><td>\u03b3=10</td></tr><tr><td>SDP</td><td>0.709</td><td>0.697</td><td>0.689</td><td>0.709</td><td>0.702</td><td>0.692</td><td>0.702</td><td>0.690</td><td>0.682</td><td>0.701</td><td>0.688</td><td>0.679</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>0.712</td><td>0.708</td><td>0.703</td><td>0.721</td><td>0.723</td><td>0.724</td><td>0.727</td><td>0.737</td><td>0.735</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>0.712</td><td>0.707</td><td>0.704</td><td>0.720</td><td>0.721</td><td>0.719</td><td>0.722</td><td>0.725</td><td>0.721</td><td>0.720</td><td>0.726</td><td>0.717</td><td>0.721</td><td>0.725</td><td>0.720</td><td></td><td></td><td></td><td></td></tr><tr><td>ECO-DQN</td><td>0.713</td><td>0.705</td><td>0.707</td><td>0.718</td><td>0.723</td><td>0.720</td><td>0.725</td><td>0.727</td><td>0.725</td><td>0.722</td><td>0.721</td><td>0.722</td><td>0.726</td><td>0.726</td><td>0.721</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-A</td><td>0.691</td><td>0.655</td><td>0.637</td><td>0.701</td><td>0.683</td><td>0.660</td><td>0.693</td><td>0.668</td><td>0.599</td><td>0.691</td><td>0.666</td><td>0.602</td><td>0.688</td><td>0.662</td><td>0.601</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-B</td><td>0.698</td><td>0.660</td><td>0.630</td><td>0.708</td><td>0.667</td><td>0.646</td><td>0.707</td><td>0.701</td><td>0.670</td><td>0.699</td><td>0.696</td><td>0.653</td><td>0.702</td><td>0.694</td><td>0.651</td><td></td><td></td><td></td><td></td></tr><tr><td>RUN-CSP</td><td>0.702</td><td>0.706</td><td>0.704</td><td>0.711</td><td>0.715</td><td>0.714</td><td>0.714</td><td>0.726</td><td>0.710</td><td>0.704</td><td>0.713</td><td>0.705</td><td>0.705</td><td>0.711</td><td>0.702</td><td></td><td></td><td></td><td></td></tr><tr><td>PI-GNN</td><td>0.704</td><td>0.706</td><td>0.708</td><td>0.712</td><td>0.718</td><td>0.717</td><td>0.715</td><td>0.726</td><td>0.716</td><td>0.711</td><td>0.724</td><td>0.712</td><td>0.709</td><td>0.722</td><td>0.712</td><td></td><td></td><td></td><td></td></tr><tr><td>MAX-SAT</td><td>0.702</td><td>0.705</td><td>0.708</td><td>0.712</td><td>0.716</td><td>0.718</td><td>0.721</td><td>0.732</td><td>0.726</td><td>0.719</td><td>0.730</td><td>0.728</td><td>0.718</td><td>0.730</td><td>0.726</td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>0.702</td><td>0.708</td><td>0.707</td><td>0.722</td><td>0.721</td><td>0.725</td><td>0.732</td><td>0.738</td><td>0.733</td><td>0.734</td><td>0.739</td><td>0.733</td><td>0.732</td><td>0.738</td><td>0.733</td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 2 Results of solving Max-Cut with different approaches on the GSET benchmark. <table><tr><td colspan=\"5\">Max-Cut</td></tr><tr><td>Methods</td><td>G14</td><td>G15</td><td>G22</td><td>G55</td></tr><tr><td>V</td><td>800</td><td>800</td><td>2000</td><td>5000</td></tr><tr><td>E</td><td>4694</td><td>4661</td><td>19990</td><td>12468</td></tr><tr><td>SDP</td><td>2922</td><td>2938</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>2991</td><td>3047</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>3064</td><td>3050</td><td>-</td><td>-</td></tr><tr><td>ECO-DQN</td><td>3056</td><td>3028</td><td>13268</td><td>10198</td></tr><tr><td>GMC-A</td><td>2940</td><td>2948</td><td>12978</td><td>10094</td></tr><tr><td>GMC-B</td><td>2918</td><td>2906</td><td>13246</td><td>10104</td></tr><tr><td>RUN-CSP</td><td>2943</td><td>2928</td><td>13028</td><td>10116</td></tr><tr><td>PI-GNN</td><td>3026</td><td>2990</td><td>13181</td><td>10138</td></tr><tr><td>MAX-SAT</td><td>3046</td><td>3012</td><td>13250</td><td>10204</td></tr><tr><td>Ours</td><td>3052</td><td>3018</td><td>13269</td><td>10208</td></tr></table> Table 3 Results of solving MIS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Maximum Independent Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Redu MIS(Solver)</td><td>30.0</td><td>39.4</td><td>48.8</td><td>57.4</td></tr><tr><td>Greedy</td><td>24.6</td><td>33.0</td><td>42.2</td><td>48.0</td></tr><tr><td>GMC-A</td><td>21.2</td><td>27.7</td><td>40.2</td><td>45.2</td></tr><tr><td>GMC-B</td><td>20.8</td><td>28.1</td><td>40.6</td><td>46.0</td></tr><tr><td>RUN-CSP</td><td>25.8</td><td>33.6</td><td>42.2</td><td>49.4</td></tr><tr><td>PIGNN</td><td>26.5</td><td>35.1</td><td>44.4</td><td>51.8</td></tr><tr><td>MAX-SAT</td><td>27.9</td><td>36.6</td><td>44.4</td><td>51.8</td></tr><tr><td>Ours</td><td>27.8</td><td>36.8</td><td>44.8</td><td>52.2</td></tr></table> ### 4.2 Performance of Solving COs In this section, we evaluated the effectiveness of the proposed framework and aimed to answer Q1. Without loss of generality, Max- Cut, MIS, and MDS are selected for evaluation. Our framework is also adaptive to other COs that can be transformed as Max- SAT. For Max- Cut, we first reported the mean \\(p\\) - values across 1,000 regular graphs with 100, 200, 500, and 1000 nodes for three levels of node degrees in Table 1. Then, the experiments on the GSET benchmark were carried out and the number of cuts was reported in Table 2 for comparison. When encountering large- scale datasets (G22 and G55), traditional methods, e.g., EO and BLS, were out of memory and could not obtain the results. Based on the two tables, we have the following observations: (1) Our method obtained the best results compared with learning- based methods, which demonstrated that the proposed framework exhibited the powerful ability to learn useful information from CO problems. The results also showed that Max- SAT can boost the performance of solving COs; (2) Our method was superior in most cases compared with all baselines. The advantages of our method were mainly in large- scale Max- Cut problems (e.g., \\(n = 800\\) , G22, and G55) where the performance surpassed traditional methods;\n\nTable 4 Results of solving MDS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Minimum Dominated Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Greedy</td><td>191.1</td><td>255.0</td><td>296.5</td><td>284.8</td></tr><tr><td>HTS-DS</td><td>191.0</td><td>255.2</td><td>297.6</td><td>286.4</td></tr><tr><td>MWDS-CRO</td><td>191.0</td><td>255.1</td><td>298.4</td><td>285.5</td></tr><tr><td>GMC-A</td><td>201.2</td><td>268.1</td><td>301.3</td><td>292.2</td></tr><tr><td>GMC-B</td><td>192.1</td><td>261.4</td><td>299.1</td><td>291.6</td></tr><tr><td>RUN-CSP</td><td>190.8</td><td>255.4</td><td>299.3</td><td>278.6</td></tr><tr><td>PIGNN</td><td>191.1</td><td>255.3</td><td>299.1</td><td>280.2</td></tr><tr><td>MAX-SAT</td><td>191.0</td><td>255.3</td><td>298.8</td><td>278.8</td></tr><tr><td>Ours</td><td>190.8</td><td>255.1</td><td>298.5</td><td>278.1</td></tr></table> (3) Traditional or heuristic methods are very competitive in Max-Cut. However, the gaps between our method and these methods were small. Besides, the time of training and inference for our method was much less than these methods. For MIS and MDS, we reported the number of sets for MIS and the number of dominated nodes for MDS in Tables 3 and 4. \\(V\\) and \\(E\\) are the sets for nodes and edges. Greedy denotes the classical greedy algorithm. Larger and smaller values denote better results for MIS and MDS, respectively. The experiments were carried out on four frb datasets. Similar results can be observed in these two CO problems. Particularly, In MIS, our results were relatively closer to the results obtained by the solver, which showed the effectiveness of our framework. Based on the results obtained on the above three CO problems, we can answer Q1: Max- SAT offered common knowledge and the pre- training and adaptation pipeline built based on Max- SAT can learn transferable and generalizable features to improve the ability to solve different CO problems on graphs. <center>Figure 3 Results of ablation study with different variants of our model on Max-Cut. </center>\n\n<center>Figure 4 Heat maps of knowledge transferability from three different CO problems to the maximum independent set problem on two scales of nodes. </center> ### 4.3 Ablation Study In this part, we conducted ablation studies on the Max- Cut problem to verify the effectiveness of each module in our framework. We gradually removed each single module and kept the other modules unchanged. The variants of our frameworks are listed as follows. \\(\\mathbf{w} / \\mathbf{o}\\) pre- T denotes the model that removes the pre- training stage but keeps the domain adaptation module. \\(\\mathbf{w} / \\mathbf{o}\\) DA denotes the model that removes the domain adaptation module but keeps the pre- training stage. \\(\\mathbf{w} / \\mathbf{o}\\) Bi- G is the model that directly uses GNNs to extract the features in the original graphs represented from CO problems without converting them to MAX- SAT and bipartite graphs. \\(\\mathbf{w} / \\mathbf{o}\\) Att is the model that keeps bipartite GNNs without the attention mechanism. \\(\\mathbf{w} / \\mathbf{o}\\) Mult- Att is the model that keeps bipartite GNNs without the multi- head mechanism. We reported the \\(p\\) - values for all variants in Figure 3 and had the following observations. (1) All modules boosted the performance and played important roles in our framework. The overall model obtained the best results, which demonstrated that these modules were nicely incorporated into our framework. (2) Removing the problem transfer step experienced the largest performance degradation, which proved the effectiveness of Max- SAT and also answered Q1. (3) Pre- training and adaptation were both useful in improving the performance. Since both steps utilized Max- SAT to assist the learning of transferable features, we can answer Q2 and conclude that the combination of different strategies to use Max- SAT can largely benefit solving CO problems. (4) Attention and multi- head mechanisms also affected the results, which showed that better feature extraction backbones indeed helped the learning methods to solve CO problems. ### 4.4 Effects of Different Knowledge Transfer Strategies In this subsection, we verify the knowledge transfer ability of our model. we fixed the target domain as the MIS problem and used Max- SAT, Max- Cut, and MDS as source domains to check different degrees of transferability. To be concrete, we fed the data from different source domains into our framework for pre- training and domain adaptation and observed how the performance was affected by the selection of domains. We selected two scales of nodes, i.e., 450 variables and 760 variables, for demonstration. The results were shown in Figure 4 where darker colors indicated better performance and transferability.\n\n<center>Figure 5 Parameter analysis for feature dimension \\(d\\) , number of attention heads \\(h\\) , weight of loss \\(\\lambda\\) , and number of layers \\(L\\) . </center> From the heat maps, we can observe that Max- SAT \\(\\rightarrow\\) MIS always obtained the best results compared with the other two types of knowledge transfer, which showed that Max- SAT was more general and had the strongest transferability across different CO problems. Moreover, MDS is a more relevant CO problem to MIS compared with Max- Cut. Therefore, we observed better results from MDS \\(\\rightarrow\\) MIS than Max- Cut \\(\\rightarrow\\) MIS. This observation answered Q3 where different source domains do affect the performance of the target domain. From a practical viewpoint, selecting an appropriate source domain was always hard, and irrelevant tasks would generate negative transfers and eventually affect the performance of the target domain. ### 4.5 Parameter Analysis In this subsection, we analyzed the influence of several significant hyper- parameters on the model performance: the dimension of features \\(d\\) , the number of attention heads \\(h\\) , the weight of losses \\(\\lambda\\) , and the number of pertaining layers \\(L\\) . We recorded the performance of different \\(d\\) , \\(h\\) , \\(\\lambda\\) , \\(L\\) values on Max- CUT. The results were reported in Figure 5. It can be observed that with different dimensions of features, our model exhibited relatively stable performance when \\(d\\) was greater than or equal to 16. Our\n\nmodel achieved optimal performance when \\(d\\) was 128. Our model was not significantly affected by the number of attention heads. It achieved optimal performance when \\(k\\) was 10. The optimal value of weight \\(\\lambda\\) was obtained in 0.2. Pre- training led to better results, especially when the number of layers exceeded three. ## 5 Conclusion In this study, we aimed to discover the transferability of deep learning architectures for solving different CO problems on graphs. To achieve this goal, we leveraged the Max- SAT problem as a general formulation of original CO problems. The key advantages were two- fold. First, Max- SAT served as a bridge for various CO problems and provided a tool for discovering common properties in CO problems. Second, Max- SAT problems were represented as clauses that included logical information in addition to the simple correlations in graphs. By further constructing a pre- training and adaptation framework, we can extract transferable features that can be used for various CO problems. Experiments showed that incorporating Max- SAT indeed improved the ability to solve various CO problems on graphs. Our current work can be extended from the following directions. The pre- training stage is built on supervised tasks where labeled data samples are required. As a future direction, it is necessary to develop unsupervised pre- training paradigms that can fully leverage information from the data itself. Moreover, advanced GNNs that are more suitable for the properties of CO problems should be designed in the future. Acknowledgements This work was supported by the National Natural Science Foundation of China (Grant No. 11991021, No. 11991020, No. 12271503). ## References 1 Petrica C Pop, Ovidiu Cosma, Cosmin Sabo, and Corina Pop Sitar. A comprehensive survey on the generalized traveling salesman problem. European Journal of Operational Research, 2023. 2 Nathan Krislock, J\u00e9r\u00f4me Malick, and Fr\u00e9d\u00e9ric Roupin. Improved semidefinite bounding procedure for solving max- cut problems to optimality. Mathematical Programming, 143:61- 86, 2014. 3 Yiyuan Wang, Shaowei Cai, Shiwei Pan, Ximing Li, and Monghao Yin. Reduction and local search for weighted graph coloring problem. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 2433- 2441, 2020. 4 Marc Los and Christian Lardinois. Combinatorial programming, statistical optimization and the optimal transportation network problem. Transportation Research Part B: Methodological, 16(2):89- 124, 1982. 5 Simone L Martins and Celso C Ribeiro. Metaheuristics and applications to optimization problems in telecommunications. Handbook of Optimization in Telecommunications, pages 103- 128, 2006. 6 Eugene L Lawler and David E Wood. Branch- and- bound methods: A survey. Operations Research, 14(4):699- 719, 1966. 7 Steven J Benson, Yinyu Ye, and Xiong Zhang. Solving large- scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000. 8 Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263- 1272. PMLR, 2017. 9 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. 10 Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016. 11 Quentin Cappart, Didier Ch\u00e9telat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 12 Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in Neural Information Processing Systems, 30, 2017. 13 Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in Neural Information Processing Systems, 31, 2018.\n\n14 Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019. 15 Tianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang, Howard Heaton, Jialin Liu, and Wotao Yin. Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research, 23(1):8562- 8620, 2022. 16 Matteo Fischetti, Paolo Toth, and Daniele Vigo. A branch- and- bound algorithm for the capacitated vehicle routing problem on directed graphs. Operations Research, 42(5):846- 859, 1994. 17 Panos M Pardalos and Gregory P Rodgers. A branch and bound algorithm for the maximum clique problem. Computers & Operations Research, 19(5):363- 375, 1992. 18 Richard Bellman. Dynamic programming. Science, 153(3731):34- 37, 1966. 19 Michael Held and Richard M Karp. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1):196- 210, 1962. 20 Silvano Martello, David Pisinger, and Paolo Toth. Dynamic programming and strong bounds for the 0- 1 knapsack problem. Management Science, 45(3):414- 424, 1999. 21 Laurence A Wolsey and George L Nemhauser. Integer and combinatorial optimization, volume 55. John Wiley & Sons, 1999. 22 Dimitris Bertsimas and Ramazan Demir. An approximate dynamic programming approach to multidimensional knapsack problems. Management Science, 48(4):550- 565, 2002. 23 Paul Bouman, Niels Agatz, and Marie Schmidt. Dynamic programming approaches for the traveling salesman problem with drone. Networks, 72(4):528- 542, 2018. 24 Mutsunori Yagiura and Toshihide Ibaraki. On metaheuristic algorithms for combinatorial optimization problems. Systems and Computers in Japan, 32(3):33- 55, 2001. 25 James B Orlin, Abraham P Punnen, and Andreas S Schulz. Approximate local search in combinatorial optimization. SIAM Journal on Computing, 33(5):1201- 1214, 2004. 26 Emile HL Aarts, Jan HM Korst, and Peter JM van Laarhoven. A quantitative analysis of the simulated annealing algorithm: A case study for the traveling salesman problem. Journal of Statistical Physics, 50:187- 206, 1988. 27 Jean Berger and Mohamed Barkaoui. A hybrid genetic algorithm for the capacitated vehicle routing problem. In Genetic and Evolutionary Computation Conference, pages 646- 656. Springer, 2003. 28 Andr\u00e9 Chassein, Marc Goerigk, Adam Kasperski, and Pawel Zieli\u0144ski. Approximating combinatorial optimization problems with the ordered weighted averaging criterion. European Journal of Operational Research, 286(3):828- 838, 2020. 29 Henrique Lemos, Marcelo Prates, Pedro Avelar, and Luis Lamb. Graph coloring meets deep learning: Effective graph neural network models for combinatorial problems. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, pages 879- 885. IEEE, 2019. 30 Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym- nco: Leveraging symmetricity for neural combinatorial optimization. Advances in Neural Information Processing Systems, 35:1936- 1949, 2022. 31 Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2018. 32 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in Neural Information Processing Systems, 30, 2017. 33 Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. 34 Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. Gppt: Graph pre- training and prompt tuning to generalize graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1717- 1727, 2022. 35 Zixuan Yi, Iadh Ounis, and Craig Macdonald. Contrastive graph prompt- tuning for cross- domain recommendation. ACM Transactions on Information Systems, 2023. 36 Cristian Bodnar, Francesco Di Giovanni, Benjamin Chamberlain, Pietro Li\u00f2, and Michael Bronstein. Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns. Advances in Neural Information Processing Systems, 35:18527- 18541, 2022. 37 Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein. On over- squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865- 7885. PMLR, 2023. 38 Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 39 Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. In The Eleventh International Conference on Learning Representations, 2022. 40 Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, and Marinka Zitnik. Evaluating explainability for graph neural\n\nnetworks. Scientific Data, 10(1):144, 2023. 41 Jin Yuan, Feng Hou, Yangzhou Du, Zhongchao Shi, Xin Geng, Jianping Fan, and Yong Rui. Self- supervised graph neural network for multi- source domain adaptation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3907- 3916, 2022. 42 Quanyu Dai, Xiao- Ming Wu, Jiarren Xiao, Xiao Shen, and Dan Wang. Graph transfer learning via adversarial domain adaptation with graph convolution. IEEE Transactions on Knowledge and Data Engineering, 35(5):4908- 4922, 2022. 43 Ronghang Zhu, Xiaodong Jiang, Jiasen Lu, and Sheng Li. Cross- domain graph convolutions for adversarial unsupervised domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 34(8):3847- 3858, 2023. 44 Xinhong Ma, Tianzhu Zhang, and Changsheng Xu. Gcan: Graph convolutional adversarial network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8266- 8276, 2019. 45 Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and Pan Li. Structural re- weighting improves graph domain adaptation. In International Conference on Machine Learning, pages 21778- 21793, 2023. 46 Mehmet Pilanci and Elif Vural. Domain adaptation on graphs by learning aligned graph bases. IEEE Transactions on Knowledge and Data Engineering, 34(2):587- 600, 2022. 47 Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Graph domain adaptation via theory- grounded spectral regularization. In International Conference on Learning Representations, 2023. 48 Carlos Ans\u00f3tegui, Maria Luisa Bonet, and Jordi Levy. Towards industrial- like random sat instances. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, page 387- 392, 2009. 49 Diogo V Andrade, Mauricio GC Resende, and Renato F Werneck. Fast local search for the maximum independent set problem. Journal of Heuristics, 18:525- 547, 2012. 50 Thomas A Feo, Mauricio GC Resende, and Stuart H Smith. A greedy randomized adaptive search procedure for maximum independent set. Operations Research, 42(5):860- 878, 1994. 51 Amir Dembo, Andrea Montanari, and Subhabrata Sen. Extremal cuts of sparse random graphs. The Annals of Probability, pages 1190- 1217, 2017. 52 Changhui Choi and Yinyu Ye. Solving sparse semidefinite programs using the dual scaling algorithm with an iterative solver. Department of Management Sciences, University of Iowa, Iowa City, IA, 52242, 2000. 53 Stefan Boettcher and Allon G Percus. Extremal optimization for graph partitioning. Physical Review E, 64(2):026114, 2001. 54 Una Benlic and Jin- Kao Hao. Breakout local search for the max- cutproblem. Engineering Applications of Artificial Intelligence, 26(3):1162- 1173, 2013. 55 Thomas Barrett, William Clements, Jakob Foerster, and Alex Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3243- 3250, 2020. 56 Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max- cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242- 251. SPIE, 2019. 57 Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence, 3:580607, 2021. 58 Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics- inspired graph neural networks. Nature Machine Intelligence, 4(4):367- 377, 2022. 59 Minghao Liu, Pei Huang, Fuqi Jia, Fan Zhang, Yuchen Sun, Shaowei Cai, Feifei Ma, and Jian Zhang. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 16264- 16265, 2023.",
    "1_introduction_combinatorial_optimization_co_is_a_multidisciplinary_field_ranging_from_optimization_to_artificial_intelligence_the_goal_of_co_is_to_find_optimal_solutions_from_finite_possible_options_where_exhaustive_enumeration_cannot_be_accomplished_graphs_are_the_most_common_research_objects_in_co_typical_co_problems_on_graphs_include_traveling_salesman_problems_tsp_1_max-_cut_problems_2_and_graph_coloring_problems_3_solving_co_on_graphs_can_benefit_many_real-_world_applications_such_as_transportation_logistics_4_and_telecommunications_5_solving_cos_on_graphs_is_extremely_difficult_due_to_their_discrete_property_and_high_computational_costs_for_many_decades_researchers_have_made_many_efforts_to_develop_various_algorithms_to_solve_cos_on_graphs_for_example_branch_and_bound_6_is_a_traditional_exact_algorithm_that_gradually_divides_the_solution_space_into_subspaces_and_uses_bounds_to_prune_possible_solutions_that_exceed_the_bounds_unfortunately_these_traditional_exact_algorithms_encounter_excessive_computational_burdens_as_the_scales_of_problems_increase_7_recently_deep_learning_has_developed_as_an_effective_tool_for_cos_inspiring_the_emergence_of_the_category_of_learning-_based_methods_for_solving_cos_particularly_graph_neural_networks_gnns_8-_10_are_advanced_deep_learning_models_designed_for_graph-_structure_data_which_have_been_successfully_applied_to_cos_on_graphs_11-_13_in_general_the_graphs_in_co_problems_are_first_fed_into_gnns_so_that_the_high-_level_features_can_be_extracted_then_the_predictions_generated_from_features_serve_as_feasible_solutions_or_can_be_further_used_to_guide_the_searching_process_of_the_heuristic_learning_process_14_15_learning-_based_methods_extract_more_useful_knowledge_to_reduce_the_search_space_which_can_be_used_to_effectively_and_efficiently_solve_cos_on_graphs_besides_the_data-_driven_manner_is_more_adaptive_to_real-_world_problems_when_real_data_is_available_however_current_gnn_models_for_co_problems_still_face_several_challenges_first_directly_modeling_co_with_graphs_is_intuitive_but_not_necessarily_accurate_although_graphs_can_capture_relationships_between_objects_they_ignore_the_mathematical_logicality_and_properties_inside_cos_therefore_constructing_learning_methods_over_original_graphs_often_results_in_insufficient_problem-_solving_abilities_second_current_methods_are_usually_designed_to_solve_a_single_co_problem_on_graphs_nevertheless_different_cos_on_graphs_may_share_the_same_graphs_and_leverage_similar_gnn_backbones_for_feature_extraction_it_is_quite_beneficial_to_explore_whether_the_transferability_of_knowledge_exists_among_different_problems_moreover_for_real-_world_problems_on_graphs_the_data_collected_for_training_gnns_is_often_limited_it_is_essential_to_leverage_data_from_other_problems_that_may_have_common_knowledge_to_improve_the_generalization_ability_of_gnns_to_address_the_above_issues_we_propose_in_this_paper_a_unified_pre-_training_and_adaptation_framework_for_cos_on_graphs_the_core_idea_is_to_develop_a_generalizable_and_transferable_gnn_framework_from_which_all_cos_can_be_benefited_to_achieve_this_goal_it_is_necessary_to_find_out_the_common_knowledge_and_properties_of_all_co_problems_on_graphs_that_can_be_extracted_by_gnns_and_used_for_different_problems_we_notice_that_using_graphs_to_formulate_co_omits_logical_information_that_is_usually_more_general_and_common_for_all_problems_inspired_by_this_observation_we_propose_to_leverage_the_maximum_satisfiability_problems_max-_sat_to_bridge_different_co_problems_on_graphs_max-_sat_is_a_classic_problem_containing_propositional_logic_formulas_that_can_be_used_to_describe_additional_logic_information_beyond_simple_relations_on_graphs_with_this_unified_representation_we_further_adopt_a_pre-_training_process_and_a_fine-_tuning_process_based_on_domain_adaptation_to_extract_general_features_and_utilize_the_transferability_between_different_problems_our_framework_can_be_adaptive_to_all_co_problems_that_can_be_transferred_to_max-_sat_problems_and_is_also_suitable_for_various_gnn_architectures_which_exhibit_superior_flexibility_and_versatility_to_be_concrete_we_first_use_the_max-_sat_problem_as_an_intermediate_problem_by_which_we_can_transform_graphs_from_different_cos_to_clauses_and_formulas_we_can_construct_bipartite_graphs_from_these_clauses_in_max-_sat_that_not_only_capture_logical_information_but_also_can_be_adaptive_to_different_cos_on_graphs_in_the_training_stage_we_can_generate_abundant_clauses_from_certain_max-_sat_problems_without_considering_the_original_co_and_transform_them_into_bipartite_graphs_for_pre-_training_due_to_the_generic_nature_of_max-_sat_the_pre-_trained_model_is_equipped_with_better_initialization_then_we": "use the original CO and Max- SAT to construct an adaptation framework with adversarial training for fine- tuning, which can further utilize the transferable information in Max- SAT. In both the pre- training and fine- tuning stages, the graph attention networks are leveraged as backbones to extract information in bipartite graphs transformed from COs on graphs and Max- SAT. The bipartite graph attention networks separately consider the aggregation of clauses and variables, which can better capture the dependencies between variables and clauses with different neighbor aggregation processes. To summarize, the contributions of this work are as follows: (1) We propose a unified pre-training and adaptation framework based on Max-SAT that can learn generalizable and transferable information to benefit the solving of COs on graphs. (2) We propose to use Max-SAT to bridge various COs on graphs, by which all problems can be represented as a unified form. The Max-SAT instances also carry additional logical relationships that can be further utilized by GNNs. (3) We design a pre-training and domain adaptation architecture to extract generalizable and transferable features based on instances generated from the Max-SAT and COs on graphs. This framework is versatile to various COs and can be used to boost the ability to solve these problems. (4) We evaluate our method on both synthetic datasets and open benchmarks. The numerical results demonstrated the effectiveness of our framework in solving COs on graphs. The remainder of this paper is organized as follows. Section 2 presents the learning methods for COs and popular GNN architectures for feature extraction on graphs. Section 3 introduces the problem transfer, pre-training, and fine-tuning procedures in our framework and summarizes the overall algorithms for training. The experiments that aim to address our three main claims are listed in Section 4. We conclude our work in Section 5. ## 2 Related Work ### 2.1 Learning Methods for Solving CO Problems Traditional methods for solving CO mainly include branch and bound, dynamic programming, and heuristic algorithms. Branch and bound is a branch- and- prune search method used to reduce the search space [6, 16, 17]. It progressively decomposes the problem into smaller subproblems and applies pruning strategies to exclude branches that cannot produce the optimal solution. Dynamic programming breaks down a large problem into smaller subproblems and solves them by utilizing the relationships between the subproblems [18- 21]. Dynamic programming is suitable for problems with overlapping subproblem structures, such as the knapsack problem [22] and TSP [23]. Heuristic methods employ heuristic strategies or random operations to search the solution space and continuously improve the current solution until a satisfactory solution is found or a predetermined stopping condition is met. Common heuristic algorithms include greedy algorithms [24], local search [25], simulated annealing [26], and genetic algorithms [27]. Traditional methods for solving CO problems have limitations including high computational complexity, lack of scalability for large- size problems, and limited solution quality in terms of finding globally optimal solutions. In recent years, deep learning methods have emerged as promising approaches to tackle these problems by leveraging the power of data- driven modeling and computational intelligence. One category of methods approximates the process of solving CO problems through deep learning methods [28]. By learning from a dataset of problem instances, these deep learning models can capture intrinsic patterns and dependencies in the problem and the predictions are then utilized to guide the search process toward better solutions. These approaches offer advantages in terms of speed and scalability as they reduce the requirements of expensive evaluations for the objective function during the search. Another category of methods is based on learning- based heuristics and meta- heuristics [12- 14, 29]. Deep learning models can be employed to create intelligent decision rules or policies to guide the search process. Reinforcement learning techniques have demonstrated success in learning effective exploration- exploitation strategies for\n\nCO problems [30,31]. By incorporating learning- based heuristics into traditional optimization algorithms, better- quality solutions can be obtained with reduced computational effort. ### 2.2 Graph Neural Networks GNNs belong to a typical deep- learning framework for graphs that follow a message- passing mechanism [8]. Graph convolution networks (GCNs) [10] define graph convolutions on the spectral domain so that the structural information from neighbors can be aggregated. Graph SAGE [32] proposes an inductive representation learning method based on aggregation and sampling directly on the spatial domain. Graph attention networks (GATs) [33] utilize self- attention mechanisms to learn node features where the importance of neighbors can be considered during the aggregation of information. More recently, prompt tuning has been used to generalize GNNs [34,35]. GPPT [34] proposes a graph prompting function for GNNs so that the pre- trained models can predict labels without fine- tuning. Since GNNs have been successfully used in various downstream tasks, researchers pay attention to the theories behind GNNs. For example, over- smoothing [36] and over- squashing [37] are the two main issues for GNNs. BORF [38] proposes a rewiring algorithm based on Ollivier- Ricci curvature to relieve both over- smoothing and over- squashing problems for GNNs. The expressive abilities of GNNs have also been studied to demonstrate why GNNs perform well in many tasks [9,39,40]. Recently, GNNs have been used in graph domain adaptation to address the domain shift on graphs [41]. Domain adaptation on graphs aims at leveraging knowledge learned from the source domain to improve the performance of a GNN model on a different target domain. Adversarial training is often used to build the alignment framework [42,43]. For example, TFLG [43] utilizes instance and class levels of structures and leverages adversarial training to learn domain- invariant features. Except for the features that can be easily used for domain alignment, many researchers seek to learn invariant information from different levels. GCAN [44] is a graph convolutional adversarial network introduced for unsupervised domain adaptation by jointly modeling structures, domain labels, and class labels. Stru RW [45] presents a structural re- weighting method to address the conditional structure shift problem. Since spectral properties play an important role in graph structures, many works also attempt to explore the alignment of spectral information [46,47]. ## 3 Methodology ### 3.1 CO Problems on Graphs Given a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) , where \\(\\nu\\) is a set of nodes and \\(\\mathcal{E}\\) is a set of edges between nodes. The CO problems on graphs are to find a subset of nodes \\(\\mathcal{S}\\) under given constraint conditions so that the number of edges between the nodes in \\(\\mathcal{S}\\) satisfies a specific condition and the objective function can be maximized or minimized. Generally, the CO can be expressed as: \\[\\begin{array}{r}{\\max f(\\mathcal{S}),}\\\\ {g_{i}(\\mathcal{S})\\leqslant b_{i},b_{i}\\in \\Omega ,} \\end{array} \\quad (3.1b)\\] where \\(f(\\mathcal{S})\\) is the objective function, \\(g_{i}(\\mathcal{S})\\) is the constraint condition function, \\(b_{i}\\) is the boundary of the constraint condition, and \\(\\Omega\\) is the set of constraint indicators. In the following part, we give three concrete CO problems on graphs. Max- Cut Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) , the goal of the Max- Cut problem is to find a partition that divides \\(\\nu\\) into subset \\(\\mathcal{S}\\) and its complementary set \\(\\mathcal{S}^{\\prime} = \\mathcal{V} - \\mathcal{S}\\) where the number of cuts between \\(\\mathcal{S}\\) and \\(\\mathcal{S}^{\\prime}\\) is maximized. The cuts refer to the edges between \\(\\mathcal{S}\\) and \\(\\mathcal{S}^{\\prime}\\) . The objective function of Max- Cut can be denoted as: \\[f(\\mathcal{S}) = \\sum_{e_{i,j}\\in \\mathcal{E}}\\big(v_{i} + v_{j} - 2v_{i}v_{j}\\big), \\quad (3.2)\\] where \\(v_{i} = 1\\) when \\(v_{i}\\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise, and \\(e_{i,j}\\) is the edge between nodes \\(v_{i}\\) and \\(v_{j}\\)\n\nMaximum Independent Set (MIS) Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) , the maximum independent set problem aims to find a node set \\(\\mathcal{S}\\) with a maximal number of nodes where there are no edges between any two nodes in this set. The objective function and constraint conditions of the maximum independent set problem can be denoted as: \\[\\begin{array}{l}{f(\\mathcal{S}) = \\sum_{v_{i}\\in \\mathcal{V}}v_{i},}\\\\ {v_{i} + v_{j}\\leqslant 1|\\forall e_{i,j}\\in \\mathcal{E},} \\end{array} \\quad (3.3a)\\] where \\(v_{i} = 1\\) when \\(v_{i} \\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise. Minimum Dominated Set (MDS) Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) , the goal of the minimum dominated set problem is to find a dominated set \\(\\mathcal{S}\\) where each node \\(v_{i} \\in \\mathcal{V}\\) or at least one of its neighbor nodes belongs to the dominated set \\(\\mathcal{S}\\) . The number of nodes in \\(\\mathcal{S}\\) should also be minimized. The objective function and constraint conditions of the minimum dominated set problem can be denoted as: \\[\\begin{array}{l}{f(\\mathcal{S}) = -\\sum_{v_{i}\\in \\mathcal{V}}v_{i},}\\\\ {v_{i} + v_{j}\\geqslant 1|e_{i,j}\\in \\mathcal{E},\\forall v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.4a)\\] where \\(v_{i} = 1\\) when \\(v_{i} \\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise. ### 3.2 Problem Transfer via Max-SAT Problem In this subsection, we introduce the details of leveraging the Max- SAT problem to bridge different CO problems. In general, to fully utilize the common logical information in COs, we first convert the original graphs of the CO problems into several clauses of Max- SAT problems and then transform these clauses into bipartite graphs. #### 3.2.1 From Graph to Clauses Given a graph from any CO, we use clauses with variables to describe the objective function and the constraint conditions. Max- SAT is an optimization problem that contains a set of clauses and variables. Define \\(m\\) clauses \\(\\mathcal{C} = \\{c_{1},c_{2},\\dots ,c_{m}\\}\\) and \\(n\\) variables \\(\\mathcal{X} = \\{x_{1},x_{2},\\dots ,x_{n}\\}\\) , the Max- SAT problem is to find a truth assignment of variables so that the number of satisfied clauses is maximized. To generate clauses, we define two rules for the objective function and constraint conditions respectively. The soft rule is based on the objective function and the hard rule is based on the constraint conditions. The difference between those two rules mainly lies in whether the clauses should be strictly satisfied. For clarification, we let the variable \\(x_{i}\\) correspond to the node \\(v_{i}\\) in graph \\(\\mathcal{G}\\) . For demonstration purposes, we provide the details of transformations for Max- Cut, MIS, and MDS problems. For every \\(v_{i} \\in \\mathcal{V}\\) in MIS problems, the clauses are generated by: \\[\\begin{array}{l}{c_{h} = \\neg x_{i}\\vee \\neg x_{j}|e_{i,j}\\in \\mathcal{E},}\\\\ {c_{s} = x_{i}|v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.5a)\\] where \\(c_{h} \\in \\mathcal{C}_{h}\\) denotes the hard clause which describes that the set cannot contain the node and its neighbors at the same time, \\(c_{s} \\in \\mathcal{C}_{s}\\) denotes the soft clause which describes that the number of nodes in the MIS problem should be maximized, \\(\\neg\\) and \\(\\vee\\) are the negation and disjunction. For every \\(v_{i} \\in \\mathcal{V}\\) in MDS problems, the clauses are generated by: \\[\\begin{array}{l}{c_{h} = x_{i}\\vee x_{j}|e_{i,j}\\in \\mathcal{E},}\\\\ {c_{s} = \\neg x_{i}|v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.6a)\\] where \\(c_{h} \\in \\mathcal{C}_{h}\\) describes that the node or at least one of its neighbor nodes belongs to MDS, and \\(c_{s} \\in \\mathcal{C}_{s}\\) describes that the number of nodes in MDS should be minimized.\n\nFor every \\(e_{i,j} \\in \\mathcal{E}\\) in Max- Cut problems, the clauses are generated by: \\[\\begin{array}{l}{c_{s} = x_{i}\\vee x_{j},}\\\\ {c_{s}^{\\prime} = \\neg x_{i}\\vee \\neg x_{j},} \\end{array} \\quad (3.7a)\\] where \\(c_{s}\\) and \\(c_{s}^{\\prime}\\) are all soft clauses belonging to \\(\\mathcal{C}_{s}\\) that describe the number of cut edges that should be maximized. #### 3.2.2 From Clauses to Bipartite Graph The soft and hard clauses describe the relations between clauses and variables, which reveals the logical dependencies. Then, we can use bipartite graphs to further represent these relations in Max- SAT. In general, the bipartite graph is denoted as \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) where \\(\\tilde{\\mathcal{V}}_{x}\\) and \\(\\tilde{\\mathcal{V}}_{c}\\) are node sets, and \\(\\tilde{\\mathcal{E}}\\) is the edge set. The main difference between graphs and bipartite graphs is that the nodes in bipartite graphs belong to two different types. The variables \\(\\mathcal{X}\\) correspond to the set of nodes \\(\\tilde{\\mathcal{V}}_{x}\\) and the clauses \\(\\mathcal{C}\\) correspond to the set of nodes \\(\\tilde{\\mathcal{V}}_{c}\\) . To generate the bipartite graph, the core step is to construct the edges between nodes. For each variable \\(x_{i}\\) in each clause \\(c_{i}\\) , an edge \\(\\tilde{e}_{i,j} \\in \\tilde{\\mathcal{E}}\\) corresponds to the affiliation between the variable and the clause. In this transfer process, we transfer the origin graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) of the CO problem into the bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) . A concrete transfer process can be shown in Figure 1. We choose Max- Cut as an example for demonstration. The first step is to transfer the graph in Max- Cut into a set of clauses in Max- SAT. We use the hard and soft rules to generate these clauses. After generating these clauses, we then transfer them into a bipartite graph. The variables and clauses are set as nodes and the affiliations between variables and clauses constitute edges. The new bipartite graph can be then used for the next feature extraction process. <center>Figure 1 Problem transfer process of our framework via Max-SAT. The input graph of CO problems is first transferred as Max-SAT clauses. Then, a bipartite graph is generated from these clauses by the affiliations between variables and clauses. </center> Notice that transferring graphs in original COs to bipartite graphs has two main advantages: First, original graphs only contain simple dependencies between variables. In contrast, the bipartite graphs transferred from Max- SAT clauses are then equipped with logical information from COs; Second, the bipartite graphs are general forms that can represent various COs on graphs since all of these COs can be transformed into Max- SAT problems. Therefore, we can easily construct a knowledge transfer framework to leverage the generalizable and transferable information of these problems with the help of bipartite graphs.\n\n### 3.3 Learning with Bipartite Graph Attention Networks In this subsection, we first introduce the backbone network that is used to extract features from bipartite graphs. Traditional GNNs for feature extractions from graphs mainly follow a message- passing scheme: \\[\\begin{array}{r l} & {\\mathbf{a}_{v}^{(l)} = \\mathrm{AGG}(\\mathbf{x}_{u}\\mid u\\in \\mathcal{N}(v)),}\\\\ & {\\mathbf{x}_{v}^{(l + 1)} = \\mathrm{COM}(\\mathbf{a}_{v}^{(l)},\\mathbf{x}_{v}^{(l)}),} \\end{array} \\quad (3.8)\\] where \\(u\\) and \\(v\\) denote the nodes, \\(\\mathbf{a}_{v}\\) denotes the feature vector accumulated from neighbor nodes, \\(\\mathbf{x}_{v}^{(l)}\\) denotes the features at layer \\(l\\) and \\(\\mathbf{x}_{v}^{(0)}\\) is the initial attributes, \\(\\mathrm{AGG}(\\cdot)\\) and \\(\\mathrm{COM}(\\cdot)\\) are the aggregation and combination functions for message passing and feature updating, and \\(\\mathcal{N}(v)\\) denotes the set of neighbors for \\(v\\) . The above process is only suitable for graphs with one type of nodes. Recall that a bipartite graph contains two sets of nodes that represent variables and clauses and a set of edges that connect nodes from these two sets. Due to the different properties of nodes, the traditional message- passing scheme in GNNs cannot be directly used to extract features. We particularly design a bipartite GNN with attentive message- passing schemes. For an input bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\nu}_{x},\\tilde{\\nu}_{c},\\tilde{\\mathcal{E}})\\) , the attributed matrices of variables and clauses are denoted as \\(\\mathbf{C}\\in \\mathbb{R}^{m\\times d}\\) and \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times d}\\) where \\(m\\) and \\(n\\) denote the numbers of clauses and variables, and \\(d\\) is the dimension of attributes. For symbolic convenience, we use \\(v_{x}(i)\\) and \\(v_{c}(i)\\) to denote nodes of variables and clauses in the bipartite graph. The variable and clause of these nodes are denoted as \\(x_{i}\\) and \\(c_{i}\\) and their attributed vectors are denoted as \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{c}_{i}\\) accordingly. Since the initial bipartite graph does not contain attributed information, the attributed matrices require initialization. There are different strategies for initialization, e.g., uniform distribution, normal distribution, and all- one matrices. Since no significant influences have been observed in our experiments, we simply initialize attributed matrices as all- one matrices \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) . The message- passing process of bipartite GNNs consists of two steps in each iteration, the clause- wise aggregation step and the variable- wise aggregation step. First, clause- wise aggregation updates the feature of the clause node by aggregating the features from the variable nodes. To further discriminate the importance of different neighbors, the attention mechanism is introduced to learn features during the aggregation. Given a clause \\(c_{i}\\) and its neighbor variable \\(x_{j}\\) , the layer- wise aggregation from \\(x_{j}\\) to \\(c_{i}\\) through attention is represented as: \\[\\begin{array}{r l} & {\\alpha_{x_{j}\\to c_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{x}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{x}(k)\\in\\mathcal{N}(v_{c}(i))}\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{X}_{k}^{(l)})\\rangle\\right)}}\\\\ & {\\mathbf{c}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{c}_{i}^{(l)},\\sum_{v_{x}(j)\\in\\mathcal{N}(v_{c}(i))}\\alpha_{x_{j}\\to c_{i}}^{(l)}(\\mathbf{w}_{V}^{(l)}\\mathbf{x}_{j}^{(l)})\\right),} \\end{array} \\quad (3.11)\\] where \\(\\alpha_{x_{j}\\to c_{i}}\\) denotes the attention score between clause \\(c_{i}\\) and variable \\(x_{j}\\) , \\(\\mathbf{w}_{Q}^{(l)}\\in \\mathbb{R}^{d}\\) , \\(\\mathbf{w}_{K}^{(l)}\\in \\mathbb{R}^{d}\\) , and \\(\\mathbf{w}_{V}^{(l)}\\in \\mathbb{R}^{d}\\) are learnable parameters of weights, \\(\\mathbf{c}_{i}^{(l)}\\) and \\(\\mathbf{x}_{j}^{(l)}\\) are features of \\(i\\) - th clause and \\(j\\) - th variable at layer \\(l\\) , \\(\\mathcal{N}(v_{c}(i))\\) is the set of neighbor variable nodes for clause \\(c_{i}\\) , \\(\\langle \\cdot ,\\cdot \\rangle\\) is the dot product operation, \\(\\mathbf{c}_{i}^{(l + 1)}\\) is the updated features for clause \\(c_{i}\\) , and MLP denotes the multi- layer perceptron. Second, the variable feature can be updated through variable- wise aggregation from the features of clauses that contain the variable. Similarly, for variable \\(x_{i}\\) , the layer- wise aggregation from \\(c_{j}\\) to \\(x_{i}\\) can be denoted as: \\[\\begin{array}{r l} & {\\alpha_{c_{j}\\to x_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{c}(k)\\in\\mathcal{N}(v_{x}(i))}\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{k}^{(l)})\\rangle\\right)},}\\\\ & {\\mathbf{x}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{x}_{i}^{(l)},\\sum_{v_{c}(j)\\in\\mathcal{N}(v_{x}(i))}\\alpha_{c_{j}\\to x_{i}}^{(l)}(\\tilde{\\mathbf{w}}_{V}^{(l)}\\mathbf{c}_{j}^{(l)})\\right),} \\end{array} \\quad (3.12)\\]\n\n<center>Figure 2 Illustration of the proposed pre-training and fine-tuning architecture. In the pre-training stage, the bipartite graphs generated from Max-SAT clauses are used to train the MLP, bipartite GNN backbone (Bip-GNN(\u00b7)), and fully connected classification layers \\(\\mathrm{FC}(\\cdot)\\) . In the fine-tuning stage, the bipartite graphs generated by Max-SAT and the CO are treated as source and target domains and are separately fed into the MLP and Bip-GNN to obtain features. A discriminator is to obtain domain labels for domain adaptation. The features are passed through the classification layer to predict the labels of variable nodes. </center> where \\(\\alpha_{c_{j}\\to x_{i}}\\) denotes the attention score between clause \\(c_{j}\\) and variable \\(x_{i}\\) , \\(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\) , \\(\\tilde{\\mathbf{w}}_{K}^{(l)}\\) , and \\(\\tilde{\\mathbf{w}}_{V}^{(l)}\\) are parameters, and \\(\\mathcal{N}(v_{x}(i))\\) is the neighbor set of clause nodes for variable \\(x_{i}\\) . After the two steps of aggregation, we can obtain the updated features of variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers for bipartite GNNs. The correlations between variables and clauses can be captured by the aggregation processes and the importance of neighbors can be learned by the attention mechanism. We can then build our framework based on the bipartite GNN backbone. ### 3.4 Pre-Training and Fine-Tuning with Domain Adaptation In this subsection, we introduce the framework of our work for training as depicted in Figure 2. Our framework consists of two stages. The first stage is to pre- train our model by using massive samples generated from Max- SAT and the second stage is to further fine- tune the model with domain adaptation that combines samples from Max- SAT and CO problems. #### 3.4.1 Pre-Training In the pre- training stage, the goal is to learn the general knowledge from Max- SAT and obtain better parameter initialization that can be used for solving different CO problems. We first generate massive clauses from Max- SAT and then convert them into bipartite graphs. To generate Max- SAT clauses, we follow [48] and select different distributions to simulate different scenarios of problems: (1) In uniform distribution, the variables and clauses appear with the same frequency. The sizes of clauses also appear with the same frequency. The clause size refers to the number of variables within a clause. For example, given a clause \\(x_{1} \\lor x_{2} \\lor \\neg x_{3}\\) , the clause size is three. (2) The single power- low distribution is a non- uniform distribution where most variables appear in low frequencies while only a few variables appear in high frequencies. The sizes of clauses also appear with the same frequency. (3) Double power- low distribution is also a non- uniform distribution that aims to simulate extremely uneven samples. Small clause size appears in high frequencies and the frequency decreases as the clause size increases.\n\nAfter obtaining the bipartite graphs, we can build our pre- training process. To be concrete, the model structure for pre- training contains three parts, an MLP that is used to map the initial node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) into low- dimensional features \\(\\mathbf{X}^{(0)}\\) and \\(\\mathbf{C}^{(0)}\\) ; the bipartite GNN backbone \\(\\mathrm{Bip - GNN}(\\cdot)\\) from section 3.3 extracts high- level node features for variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers; and fully connected prediction network \\(\\mathrm{FC}(\\cdot)\\) maps node features to their labels. To pre- train the three parts, we build a supervised loss function to update the parameters. We leverage the binary cross entropy (BCE) as the objective of classification: \\[\\mathcal{L}_{c} = \\sum_{i = 1}^{n}\\mathrm{BCE}(p_{i},y_{i}), \\quad (3.14)\\] where \\(y_{i}\\in \\{0,1\\}\\) is the label that describes the truth assignment of the variable generated from the MAXHS solver, and \\(p_{i}\\in \\{0,1\\}\\) is the prediction of the pre- training model from the prediction network \\(\\mathrm{FC}(\\cdot)\\) . It is worth noting that we only consider the classification loss under \\(n\\) variable nodes. Since the truth assignments of clauses are easily affected by variables, it is difficult to determine the labels of clauses. Therefore, to avoid the disturbance brought by inaccurate labeling, we only consider the classification of variables. #### 3.4.2 Fine-Tuning with Domain Adaptation After the pre- training process, we further leverage Max- SAT to build a fine- tuning process based on domain adaptation. In general, domain adaptation aims to learn domain- invariant features that can be used for different tasks. For a target CO problem, the proposed adaptation network treats Max- SAT as the source domain and learns transferable features by combining the samples from the CO and Max- SAT problems. Before the feature extraction, we can construct the data from source and target domains via transformation operations in section 3.2. The data for the source domain is generated directly from Max- SAT with different distributions following the pre- training process while the data of the target domain is generated from Max- SAT that is converted from a certain CO problem. The network architecture of fine- tuning is based on the pre- trained network. The pre- training network learns generalizable features while the fine- tuning network focuses on learning task- specific features. With the help of pre- training, the model can converge faster and achieve better generalization performance. Concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on MLP and \\(\\mathrm{Bip - GNN}(\\cdot)\\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \\(\\mathrm{Dis}(\\cdot)\\) is to classify the domain labels for each sample. The source domain and target domain share the same feature extraction backbone but have different classification networks. The domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training. Our work can also be extended to the unsupervised setting where the labels for the target domain are unavailable. Based on the three parts, the overall loss for domain adaptation in the fine- tuning stage can be denoted as \\[\\mathcal{L}_{ft} = \\mathcal{L}_{c} + \\lambda \\mathcal{L}_{d}, \\quad (3.15)\\] where \\(\\mathcal{L}_{c}\\) and \\(\\mathcal{L}_{d}\\) are the losses for classification and discrimination, and \\(\\lambda\\) is a positive hyper- parameter that controls the weight of losses. The discriminative loss that classifies nodes based on their domain labels can be denoted as \\[\\mathcal{L}_{d} = (\\mathbf{X}_{S},\\mathbf{X}_{T}) = \\mathbb{E}_{\\mathbf{x}_{S}\\in \\mathcal{D}_{S}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{S}))\\right)] + \\mathbb{E}_{\\mathbf{x}_{T}\\in \\mathcal{D}_{T}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{T}))\\right)], \\quad (3.16)\\] where \\(\\mathbf{X}_{S}\\) and \\(\\mathbf{X}_{D}\\) are the node features from source and target domains, \\(\\mathcal{D}_{S}\\) and \\(\\mathcal{D}_{T}\\) represent the data distribution of source and target domains, \\(\\mathrm{Dis}(\\cdot)\\) is the domain classifier, and \\(\\mathrm{Bip - GNN}(\\cdot)\\) is the feature extractor.",
    "33_learning_with_bipartite_graph_attention_networks_in_this_subsection_we_first_introduce_the_backbone_network_that_is_used_to_extract_features_from_bipartite_graphs_traditional_gnns_for_feature_extractions_from_graphs_mainly_follow_a_message-_passing_scheme_beginarrayr_l_mathbfa_vl_mathrmaggmathbfx_umid_uin_mathcalnv_mathbfx_vl_1_mathrmcommathbfa_vlmathbfx_vl_endarray_quad_38_where_u_and_v_denote_the_nodes_mathbfa_v_denotes_the_feature_vector_accumulated_from_neighbor_nodes_mathbfx_vl_denotes_the_features_at_layer_l_and_mathbfx_v0_is_the_initial_attributes_mathrmaggcdot_and_mathrmcomcdot_are_the_aggregation_and_combination_functions_for_message_passing_and_feature_updating_and_mathcalnv_denotes_the_set_of_neighbors_for_v_the_above_process_is_only_suitable_for_graphs_with_one_type_of_nodes_recall_that_a_bipartite_graph_contains_two_sets_of_nodes_that_represent_variables_and_clauses_and_a_set_of_edges_that_connect_nodes_from_these_two_sets_due_to_the_different_properties_of_nodes_the_traditional_message-_passing_scheme_in_gnns_cannot_be_directly_used_to_extract_features_we_particularly_design_a_bipartite_gnn_with_attentive_message-_passing_schemes_for_an_input_bipartite_graph_tildemathcalg_tildenu_xtildenu_ctildemathcale_the_attributed_matrices_of_variables_and_clauses_are_denoted_as_mathbfcin_mathbbrmtimes_d_and_mathbfxin_mathbbrntimes_d_where_m_and_n_denote_the_numbers_of_clauses_and_variables_and_d_is_the_dimension_of_attributes_for_symbolic_convenience_we_use_v_xi_and_v_ci_to_denote_nodes_of_variables_and_clauses_in_the_bipartite_graph_the_variable_and_clause_of_these_nodes_are_denoted_as_x_i_and_c_i_and_their_attributed_vectors_are_denoted_as_mathbfx_i_and_mathbfc_i_accordingly_since_the_initial_bipartite_graph_does_not_contain_attributed_information_the_attributed_matrices_require_initialization_there_are_different_strategies_for_initialization_eg_uniform_distribution_normal_distribution_and_all-_one_matrices_since_no_significant_influences_have_been_observed_in_our_experiments_we_simply_initialize_attributed_matrices_as_all-_one_matrices_mathbfx_ini_and_mathbfc_ini_the_message-_passing_process_of_bipartite_gnns_consists_of_two_steps_in_each_iteration_the_clause-_wise_aggregation_step_and_the_variable-_wise_aggregation_step_first_clause-_wise_aggregation_updates_the_feature_of_the_clause_node_by_aggregating_the_features_from_the_variable_nodes_to_further_discriminate_the_importance_of_different_neighbors_the_attention_mechanism_is_introduced_to_learn_features_during_the_aggregation_given_a_clause_c_i_and_its_neighbor_variable_x_j_the_layer-_wise_aggregation_from_x_j_to_c_i_through_attention_is_represented_as_beginarrayr_l_alpha_x_jto_c_il_fracexpleftlanglemathbfw_qlmathbfc_ilmathbfw_klmathbfx_jlranglerightsum_v_xkinmathcalnv_ciexpleftlanglemathbfw_qlmathbfc_ilmathbfw_klmathbfx_klrangleright_mathbfc_il_1_mathrmmlpleftmathbfc_ilsum_v_xjinmathcalnv_cialpha_x_jto_c_ilmathbfw_vlmathbfx_jlright_endarray_quad_311_where_alpha_x_jto_c_i_denotes_the_attention_score_between_clause_c_i_and_variable_x_j_mathbfw_qlin_mathbbrd_mathbfw_klin_mathbbrd_and_mathbfw_vlin_mathbbrd_are_learnable_parameters_of_weights_mathbfc_il_and_mathbfx_jl_are_features_of_i_-_th_clause_and_j_-_th_variable_at_layer_l_mathcalnv_ci_is_the_set_of_neighbor_variable_nodes_for_clause_c_i_langle_cdot_cdot_rangle_is_the_dot_product_operation_mathbfc_il_1_is_the_updated_features_for_clause_c_i_and_mlp_denotes_the_multi-_layer_perceptron_second_the_variable_feature_can_be_updated_through_variable-_wise_aggregation_from_the_features_of_clauses_that_contain_the_variable_similarly_for_variable_x_i_the_layer-_wise_aggregation_from_c_j_to_x_i_can_be_denoted_as_beginarrayr_l_alpha_c_jto_x_il_fracexpleftlangletildemathbfw_qlmathbfx_iltildemathbfw_klmathbfc_jlranglerightsum_v_ckinmathcalnv_xiexpleftlangletildemathbfw_qlmathbfx_iltildemathbfw_klmathbfc_klrangleright_mathbfx_il_1_mathrmmlpleftmathbfx_ilsum_v_cjinmathcalnv_xialpha_c_jto_x_iltildemathbfw_vlmathbfc_jlright_endarray_quad_312": "<center>Figure 2 Illustration of the proposed pre-training and fine-tuning architecture. In the pre-training stage, the bipartite graphs generated from Max-SAT clauses are used to train the MLP, bipartite GNN backbone (Bip-GNN(\u00b7)), and fully connected classification layers \\(\\mathrm{FC}(\\cdot)\\) . In the fine-tuning stage, the bipartite graphs generated by Max-SAT and the CO are treated as source and target domains and are separately fed into the MLP and Bip-GNN to obtain features. A discriminator is to obtain domain labels for domain adaptation. The features are passed through the classification layer to predict the labels of variable nodes. </center> where \\(\\alpha_{c_{j}\\to x_{i}}\\) denotes the attention score between clause \\(c_{j}\\) and variable \\(x_{i}\\) , \\(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\) , \\(\\tilde{\\mathbf{w}}_{K}^{(l)}\\) , and \\(\\tilde{\\mathbf{w}}_{V}^{(l)}\\) are parameters, and \\(\\mathcal{N}(v_{x}(i))\\) is the neighbor set of clause nodes for variable \\(x_{i}\\) . After the two steps of aggregation, we can obtain the updated features of variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers for bipartite GNNs. The correlations between variables and clauses can be captured by the aggregation processes and the importance of neighbors can be learned by the attention mechanism. We can then build our framework based on the bipartite GNN backbone. ### 3.4 Pre-Training and Fine-Tuning with Domain Adaptation In this subsection, we introduce the framework of our work for training as depicted in Figure 2. Our framework consists of two stages. The first stage is to pre- train our model by using massive samples generated from Max- SAT and the second stage is to further fine- tune the model with domain adaptation that combines samples from Max- SAT and CO problems. #### 3.4.1 Pre-Training In the pre- training stage, the goal is to learn the general knowledge from Max- SAT and obtain better parameter initialization that can be used for solving different CO problems. We first generate massive clauses from Max- SAT and then convert them into bipartite graphs. To generate Max- SAT clauses, we follow [48] and select different distributions to simulate different scenarios of problems: (1) In uniform distribution, the variables and clauses appear with the same frequency. The sizes of clauses also appear with the same frequency. The clause size refers to the number of variables within a clause. For example, given a clause \\(x_{1} \\lor x_{2} \\lor \\neg x_{3}\\) , the clause size is three. (2) The single power- low distribution is a non- uniform distribution where most variables appear in low frequencies while only a few variables appear in high frequencies. The sizes of clauses also appear with the same frequency. (3) Double power- low distribution is also a non- uniform distribution that aims to simulate extremely uneven samples. Small clause size appears in high frequencies and the frequency decreases as the clause size increases.\n\nAfter obtaining the bipartite graphs, we can build our pre- training process. To be concrete, the model structure for pre- training contains three parts, an MLP that is used to map the initial node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) into low- dimensional features \\(\\mathbf{X}^{(0)}\\) and \\(\\mathbf{C}^{(0)}\\) ; the bipartite GNN backbone \\(\\mathrm{Bip - GNN}(\\cdot)\\) from section 3.3 extracts high- level node features for variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers; and fully connected prediction network \\(\\mathrm{FC}(\\cdot)\\) maps node features to their labels. To pre- train the three parts, we build a supervised loss function to update the parameters. We leverage the binary cross entropy (BCE) as the objective of classification: \\[\\mathcal{L}_{c} = \\sum_{i = 1}^{n}\\mathrm{BCE}(p_{i},y_{i}), \\quad (3.14)\\] where \\(y_{i}\\in \\{0,1\\}\\) is the label that describes the truth assignment of the variable generated from the MAXHS solver, and \\(p_{i}\\in \\{0,1\\}\\) is the prediction of the pre- training model from the prediction network \\(\\mathrm{FC}(\\cdot)\\) . It is worth noting that we only consider the classification loss under \\(n\\) variable nodes. Since the truth assignments of clauses are easily affected by variables, it is difficult to determine the labels of clauses. Therefore, to avoid the disturbance brought by inaccurate labeling, we only consider the classification of variables. #### 3.4.2 Fine-Tuning with Domain Adaptation After the pre- training process, we further leverage Max- SAT to build a fine- tuning process based on domain adaptation. In general, domain adaptation aims to learn domain- invariant features that can be used for different tasks. For a target CO problem, the proposed adaptation network treats Max- SAT as the source domain and learns transferable features by combining the samples from the CO and Max- SAT problems. Before the feature extraction, we can construct the data from source and target domains via transformation operations in section 3.2. The data for the source domain is generated directly from Max- SAT with different distributions following the pre- training process while the data of the target domain is generated from Max- SAT that is converted from a certain CO problem. The network architecture of fine- tuning is based on the pre- trained network. The pre- training network learns generalizable features while the fine- tuning network focuses on learning task- specific features. With the help of pre- training, the model can converge faster and achieve better generalization performance. Concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on MLP and \\(\\mathrm{Bip - GNN}(\\cdot)\\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \\(\\mathrm{Dis}(\\cdot)\\) is to classify the domain labels for each sample. The source domain and target domain share the same feature extraction backbone but have different classification networks. The domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training. Our work can also be extended to the unsupervised setting where the labels for the target domain are unavailable. Based on the three parts, the overall loss for domain adaptation in the fine- tuning stage can be denoted as \\[\\mathcal{L}_{ft} = \\mathcal{L}_{c} + \\lambda \\mathcal{L}_{d}, \\quad (3.15)\\] where \\(\\mathcal{L}_{c}\\) and \\(\\mathcal{L}_{d}\\) are the losses for classification and discrimination, and \\(\\lambda\\) is a positive hyper- parameter that controls the weight of losses. The discriminative loss that classifies nodes based on their domain labels can be denoted as \\[\\mathcal{L}_{d} = (\\mathbf{X}_{S},\\mathbf{X}_{T}) = \\mathbb{E}_{\\mathbf{x}_{S}\\in \\mathcal{D}_{S}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{S}))\\right)] + \\mathbb{E}_{\\mathbf{x}_{T}\\in \\mathcal{D}_{T}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{T}))\\right)], \\quad (3.16)\\] where \\(\\mathbf{X}_{S}\\) and \\(\\mathbf{X}_{D}\\) are the node features from source and target domains, \\(\\mathcal{D}_{S}\\) and \\(\\mathcal{D}_{T}\\) represent the data distribution of source and target domains, \\(\\mathrm{Dis}(\\cdot)\\) is the domain classifier, and \\(\\mathrm{Bip - GNN}(\\cdot)\\) is the feature extractor."
  },
  "section_objects": [
    {
      "heading": "A unified pre training and adaptation framework fo",
      "content": "## Introduction\n\n\n### 3.5 Inference with Local Search Method The last step of our model is to search the feasible solutions for CO problems with constraint conditions by using the predictions generated by our framework. We can also get close to optimal solutions with a search algorithm. For Max- Cut problems without hard clauses, our model can directly infer the truth assignment and obtain the target values. For problems with hard clauses, we introduce a heuristic local search method to discretely obtain feasible solutions that satisfy constraints and obtain the target values. Local search is a typical search method for discrete optimization. We adopt a 2- improvement local search algorithm [49,50] that iterates over all nodes in the graph and attempts to replace a 1- labeled node \\(v_{i}\\) with two 1- labeled nodes \\(v_{j}\\) and \\(v_{k}\\) . For the MIS problem, \\(v_{j}\\) and \\(v_{k}\\) must be the neighbors of \\(v_{i}\\) . These two nodes are 1- tight and are not connected. Here, a node is 1- tight if exactly one of its neighbors is 1- labeled. In other words, \\(v_{i}\\) is the only 1- labeled neighbor of \\(v_{j}\\) and \\(v_{k}\\) in the graph. Notice that this local search algorithm can find a valid 2- improvement in \\(O(E)\\) time if it exists. We introduce this method with fixed steps to find a feasible solution and evaluate the performance of our model. ### 3.6 Overall Algorithms Most of the previous works focus on modeling specific CO problems. In contrast, our method captures the common knowledge of COs and designs a unified learning method to utilize the knowledge to solve COs. To summarize, our framework has three essential processes. The problem transfer process leverages Max- SAT to bridge various COs. The COs are converted to Max- SAT with a general form that can capture logical information within CO problems. From the perspective of data transformation, the graphs in COs are firstly converted into clauses and then converted to bipartite graphs. The overall algorithm for problem transfer is listed in Algorithm 1. Algorithm 1 Problem Transfer via Max- SAT Input: A CO graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) Objective function: max \\(f(S)\\) Constraint condition: \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) Output: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) #From a graph to Max- SAT clauses# 1: Generate soft clauses \\(C_{s}\\) with objective function max \\(f(S)\\) 2: Generate hard clauses \\(C_{h}\\) with constraint condition \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) #From Max- SAT clauses to a bipartite graph# 3: Construct nodes \\(\\tilde{\\nu}_{x}\\) and \\(\\tilde{\\nu}_{c}\\) for each variable \\(v_{i}\\) and clause \\(c_{j}\\) 4: Construct edges \\(e_{(v_{i},c_{j})}^{\\prime}\\in \\tilde{\\mathcal{E}},\\forall v_{i}\\in c_{j}\\) The pre- training process uses samples from Max- SAT to learn generalizable features that can benefit all COs. The networks used for pre- training include the MLP, feature extraction backbones based on GNNs, and the classification network. The overall algorithm for pre- training is listed in Algorithm 2. The fine- tuning process uses samples from Max- SAT and target CO to build a domain adaptation architecture. Based on the pre- trained network, the fine- tuning network introduces an additional discriminator for domain classification, which can learn domain- invariant features to further improve the generalizability of features. The overall algorithm for fine- tuning is listed in Algorithm 3. ## 4 Experimental Results In this section, we evaluated the effectiveness of the proposed framework for three CO problems on graphs, i.e., Max- Cut, MIS, and MDS. Generally, we aimed to answer three essential questions by the experiments. Q1: Can we leverage Max- SAT to learn transferable and generalizable features that can improve the ability to solve different CO problems on graphs? Q2: How can we incorporate Max- SAT\n\nInput: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, True assignment \\(\\mathcal{V}\\) Number of pre- training layers \\(L\\) Number of iterations iters. Output: Pre- trained parameters \\(\\Theta_{p}\\) for MLP, Bip- GNN, and FC. 1: Initialize node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for nodes of variables and clauses; 4: for \\(l\\leqslant L\\) do 5: Extract features for clauses \\(\\mathbf{C}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 6: Extract features for variables \\(\\mathbf{X}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 7: end for 8: Map variable features to the predictions via \\(\\mathrm{FC}(\\mathbf{X}^{(L)})\\) 9: Compute the loss \\(\\mathcal{L}_{c}\\) in Eq. (3.14); 10: Backward and update parameters; 11: end for 12: Obtain the updated parameters \\(\\Theta_{p}\\) Algorithm 3 Fine- tuning Based on Domain Adaptation Input: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, Number of fine- tuning layers \\(L\\) Number of iterations iters, True assignment \\(\\mathcal{V}\\) Output: Predicted feasible solutions \\(\\mathcal{P} = \\{p_{i}\\}_{i = 1}^{n}\\) Fine- tuned parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, FC, and Dis. 1: Initialize node attributes of both domains \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for source variables and clauses; 4: Obtain features \\(\\mathbf{X}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for target variables and clauses; 5: for \\(l\\leqslant L\\) do 6: Extract features for clauses \\(\\mathbf{C}_{S}^{(l)}\\) and \\(\\mathbf{C}_{T}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 7: Extract features for variables \\(\\mathbf{X}_{S}^{(l)}\\) and \\(\\mathbf{X}_{T}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 8: end for 9: Map variable features to the assignment labels via \\(\\mathrm{FC}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{FC}(\\mathbf{X}_{T}^{(L)})\\) 10: Map variable features to the domain labels via \\(\\mathrm{Dis}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{Dis}(\\mathbf{X}_{T}^{(L)})\\) 11: Compute the loss \\(\\mathcal{L}_{ft}\\) in Eq. (3.15); 12: Updated parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, Dis, and FC; 13: end for 14: Obtain the optimal parameters \\(\\Theta_{ft}\\) 15: Predict feasible solutions \\(\\mathcal{P}\\) into the pipelines of GNN- based learning frameworks to fully utilize the information of Max- SAT? Q3: Do different pre- training and domain adaptation strategies matter for the ability of solving COs? ### 4.1 Experimental Setup Datasets We introduce the datasets that were used in our experiments in this part. In the pretraining stage, a large number of Max- SAT instances were required. We generated the clauses by running three representative generators with different distributions: the uniform distribution, the power- law distribution, and the double power- law distribution. A total number of 20,000 Max- SAT instances\n\nwere generated and the Max HS solver was used to obtain the solutions that served as labels for training. In the fine- tuning stage, according to the CO problems, different datasets are leveraged. For Max- Cut, we introduce the GSET benchmark \\(^{1)}\\) for evaluation. GSET is a set of 71 unweighted graphs that were commonly used for testing the algorithms to solve the Max- Cut problem. For MIS and MDS problems, we introduce the frb benchmark with four different instance scales for evaluation. Each frb dataset consists of five instances of hard CO problems and is regarded as the benchmark for CSP competition annually. Evaluation Metrics For different CO problems, we used the corresponding evaluation metrics based on their feasible solutions. For Max- Cut, \\(p\\) values [51] were used to evaluate the number of graph cuts. Concretely, \\(p\\) can be calculated by \\[p(z) = \\frac{z / n - \\gamma / 4}{\\sqrt{\\gamma / 4}}, \\quad (4.1)\\] where \\(z\\) is the predicted cut size for a \\(\\gamma\\) - regular graph with \\(n\\) nodes and \\(\\gamma\\) is the degree of nodes. For MIS and MDS, the number of cuts or sets is used for evaluation. Baseline methods To evaluate our method, we compare it with several baseline methods that have been widely used in solving COs. These methods can be roughly classified as traditional methods, heuristic methods, and learning- based methods. SDP [52] is a classical approach based on semi- definite programming. EO [53] is the extremal optimization method. BLS [54] is the breakout local search method. ECO- DQN [55] is an approach of exploratory CO with reinforcement learning. GMC- A and GMC- B [56] are two unsupervised GNN architectures for Max- CUT with different loss functions. RUN- CSP [57] is a recurrent unsupervised neural network for constraint satisfaction problems. PIGNN [58] is the physics- inspired GNNs for CO problems. MAXSAT [59] is a GNN- based framework designed for Max- SAT problems. Experimental Settings The proposed framework was implemented in Python with Py Torch. We summarized the hyper- parameters used in our paper as follows. The dimension of features \\(d\\) was set as 128. The number of GNN layers \\(L\\) for pre- training and fine- tuning was set as 5 and the first 2 layers were fixed during the fine- tuning. The number of layers for MLP was set as 2. The number of epochs for pre- training and fine- tuning were tuned and set as 400 and 100. The Adam optimizer was used for model training with a learning rate of \\(10^{- 5}\\) and a weight decay of \\(10^{- 10}\\) . The warm- up trick was also adopted. The number of FC layers for classification was set as 2 and the discriminator used a two- layer fully connected network to generate domain labels. The steps for local search for MIS and MDS were set as 120. All experiments were conducted on a workstation equipped with an Intel Xeon(R) Gold 6139M CPU 515 @ 2.30GHz and an NVIDIA RTX 3090 GPU with 24GB. Table 1 Results of solving Max-Cut with different approaches on random graphs. <table><tr><td rowspan=\"2\">Methods</td><td colspan=\"4\">n=100</td><td colspan=\"4\">n=200</td><td colspan=\"4\">n=500</td><td colspan=\"4\">n=800</td><td colspan=\"4\">n=1000</td></tr><tr><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td></td><td>\u03b3=10</td></tr><tr><td>SDP</td><td>0.709</td><td>0.697</td><td>0.689</td><td>0.709</td><td>0.702</td><td>0.692</td><td>0.702</td><td>0.690</td><td>0.682</td><td>0.701</td><td>0.688</td><td>0.679</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>0.712</td><td>0.708</td><td>0.703</td><td>0.721</td><td>0.723</td><td>0.724</td><td>0.727</td><td>0.737</td><td>0.735</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>0.712</td><td>0.707</td><td>0.704</td><td>0.720</td><td>0.721</td><td>0.719</td><td>0.722</td><td>0.725</td><td>0.721</td><td>0.720</td><td>0.726</td><td>0.717</td><td>0.721</td><td>0.725</td><td>0.720</td><td></td><td></td><td></td><td></td></tr><tr><td>ECO-DQN</td><td>0.713</td><td>0.705</td><td>0.707</td><td>0.718</td><td>0.723</td><td>0.720</td><td>0.725</td><td>0.727</td><td>0.725</td><td>0.722</td><td>0.721</td><td>0.722</td><td>0.726</td><td>0.726</td><td>0.721</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-A</td><td>0.691</td><td>0.655</td><td>0.637</td><td>0.701</td><td>0.683</td><td>0.660</td><td>0.693</td><td>0.668</td><td>0.599</td><td>0.691</td><td>0.666</td><td>0.602</td><td>0.688</td><td>0.662</td><td>0.601</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-B</td><td>0.698</td><td>0.660</td><td>0.630</td><td>0.708</td><td>0.667</td><td>0.646</td><td>0.707</td><td>0.701</td><td>0.670</td><td>0.699</td><td>0.696</td><td>0.653</td><td>0.702</td><td>0.694</td><td>0.651</td><td></td><td></td><td></td><td></td></tr><tr><td>RUN-CSP</td><td>0.702</td><td>0.706</td><td>0.704</td><td>0.711</td><td>0.715</td><td>0.714</td><td>0.714</td><td>0.726</td><td>0.710</td><td>0.704</td><td>0.713</td><td>0.705</td><td>0.705</td><td>0.711</td><td>0.702</td><td></td><td></td><td></td><td></td></tr><tr><td>PI-GNN</td><td>0.704</td><td>0.706</td><td>0.708</td><td>0.712</td><td>0.718</td><td>0.717</td><td>0.715</td><td>0.726</td><td>0.716</td><td>0.711</td><td>0.724</td><td>0.712</td><td>0.709</td><td>0.722</td><td>0.712</td><td></td><td></td><td></td><td></td></tr><tr><td>MAX-SAT</td><td>0.702</td><td>0.705</td><td>0.708</td><td>0.712</td><td>0.716</td><td>0.718</td><td>0.721</td><td>0.732</td><td>0.726</td><td>0.719</td><td>0.730</td><td>0.728</td><td>0.718</td><td>0.730</td><td>0.726</td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>0.702</td><td>0.708</td><td>0.707</td><td>0.722</td><td>0.721</td><td>0.725</td><td>0.732</td><td>0.738</td><td>0.733</td><td>0.734</td><td>0.739</td><td>0.733</td><td>0.732</td><td>0.738</td><td>0.733</td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 2 Results of solving Max-Cut with different approaches on the GSET benchmark. <table><tr><td colspan=\"5\">Max-Cut</td></tr><tr><td>Methods</td><td>G14</td><td>G15</td><td>G22</td><td>G55</td></tr><tr><td>V</td><td>800</td><td>800</td><td>2000</td><td>5000</td></tr><tr><td>E</td><td>4694</td><td>4661</td><td>19990</td><td>12468</td></tr><tr><td>SDP</td><td>2922</td><td>2938</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>2991</td><td>3047</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>3064</td><td>3050</td><td>-</td><td>-</td></tr><tr><td>ECO-DQN</td><td>3056</td><td>3028</td><td>13268</td><td>10198</td></tr><tr><td>GMC-A</td><td>2940</td><td>2948</td><td>12978</td><td>10094</td></tr><tr><td>GMC-B</td><td>2918</td><td>2906</td><td>13246</td><td>10104</td></tr><tr><td>RUN-CSP</td><td>2943</td><td>2928</td><td>13028</td><td>10116</td></tr><tr><td>PI-GNN</td><td>3026</td><td>2990</td><td>13181</td><td>10138</td></tr><tr><td>MAX-SAT</td><td>3046</td><td>3012</td><td>13250</td><td>10204</td></tr><tr><td>Ours</td><td>3052</td><td>3018</td><td>13269</td><td>10208</td></tr></table> Table 3 Results of solving MIS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Maximum Independent Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Redu MIS(Solver)</td><td>30.0</td><td>39.4</td><td>48.8</td><td>57.4</td></tr><tr><td>Greedy</td><td>24.6</td><td>33.0</td><td>42.2</td><td>48.0</td></tr><tr><td>GMC-A</td><td>21.2</td><td>27.7</td><td>40.2</td><td>45.2</td></tr><tr><td>GMC-B</td><td>20.8</td><td>28.1</td><td>40.6</td><td>46.0</td></tr><tr><td>RUN-CSP</td><td>25.8</td><td>33.6</td><td>42.2</td><td>49.4</td></tr><tr><td>PIGNN</td><td>26.5</td><td>35.1</td><td>44.4</td><td>51.8</td></tr><tr><td>MAX-SAT</td><td>27.9</td><td>36.6</td><td>44.4</td><td>51.8</td></tr><tr><td>Ours</td><td>27.8</td><td>36.8</td><td>44.8</td><td>52.2</td></tr></table> ### 4.2 Performance of Solving COs In this section, we evaluated the effectiveness of the proposed framework and aimed to answer Q1. Without loss of generality, Max- Cut, MIS, and MDS are selected for evaluation. Our framework is also adaptive to other COs that can be transformed as Max- SAT. For Max- Cut, we first reported the mean \\(p\\) - values across 1,000 regular graphs with 100, 200, 500, and 1000 nodes for three levels of node degrees in Table 1. Then, the experiments on the GSET benchmark were carried out and the number of cuts was reported in Table 2 for comparison. When encountering large- scale datasets (G22 and G55), traditional methods, e.g., EO and BLS, were out of memory and could not obtain the results. Based on the two tables, we have the following observations: (1) Our method obtained the best results compared with learning- based methods, which demonstrated that the proposed framework exhibited the powerful ability to learn useful information from CO problems. The results also showed that Max- SAT can boost the performance of solving COs; (2) Our method was superior in most cases compared with all baselines. The advantages of our method were mainly in large- scale Max- Cut problems (e.g., \\(n = 800\\) , G22, and G55) where the performance surpassed traditional methods;\n\nTable 4 Results of solving MDS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Minimum Dominated Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Greedy</td><td>191.1</td><td>255.0</td><td>296.5</td><td>284.8</td></tr><tr><td>HTS-DS</td><td>191.0</td><td>255.2</td><td>297.6</td><td>286.4</td></tr><tr><td>MWDS-CRO</td><td>191.0</td><td>255.1</td><td>298.4</td><td>285.5</td></tr><tr><td>GMC-A</td><td>201.2</td><td>268.1</td><td>301.3</td><td>292.2</td></tr><tr><td>GMC-B</td><td>192.1</td><td>261.4</td><td>299.1</td><td>291.6</td></tr><tr><td>RUN-CSP</td><td>190.8</td><td>255.4</td><td>299.3</td><td>278.6</td></tr><tr><td>PIGNN</td><td>191.1</td><td>255.3</td><td>299.1</td><td>280.2</td></tr><tr><td>MAX-SAT</td><td>191.0</td><td>255.3</td><td>298.8</td><td>278.8</td></tr><tr><td>Ours</td><td>190.8</td><td>255.1</td><td>298.5</td><td>278.1</td></tr></table> (3) Traditional or heuristic methods are very competitive in Max-Cut. However, the gaps between our method and these methods were small. Besides, the time of training and inference for our method was much less than these methods. For MIS and MDS, we reported the number of sets for MIS and the number of dominated nodes for MDS in Tables 3 and 4. \\(V\\) and \\(E\\) are the sets for nodes and edges. Greedy denotes the classical greedy algorithm. Larger and smaller values denote better results for MIS and MDS, respectively. The experiments were carried out on four frb datasets. Similar results can be observed in these two CO problems. Particularly, In MIS, our results were relatively closer to the results obtained by the solver, which showed the effectiveness of our framework. Based on the results obtained on the above three CO problems, we can answer Q1: Max- SAT offered common knowledge and the pre- training and adaptation pipeline built based on Max- SAT can learn transferable and generalizable features to improve the ability to solve different CO problems on graphs. <center>Figure 3 Results of ablation study with different variants of our model on Max-Cut. </center>\n\n<center>Figure 4 Heat maps of knowledge transferability from three different CO problems to the maximum independent set problem on two scales of nodes. </center> ### 4.3 Ablation Study In this part, we conducted ablation studies on the Max- Cut problem to verify the effectiveness of each module in our framework. We gradually removed each single module and kept the other modules unchanged. The variants of our frameworks are listed as follows. \\(\\mathbf{w} / \\mathbf{o}\\) pre- T denotes the model that removes the pre- training stage but keeps the domain adaptation module. \\(\\mathbf{w} / \\mathbf{o}\\) DA denotes the model that removes the domain adaptation module but keeps the pre- training stage. \\(\\mathbf{w} / \\mathbf{o}\\) Bi- G is the model that directly uses GNNs to extract the features in the original graphs represented from CO problems without converting them to MAX- SAT and bipartite graphs. \\(\\mathbf{w} / \\mathbf{o}\\) Att is the model that keeps bipartite GNNs without the attention mechanism. \\(\\mathbf{w} / \\mathbf{o}\\) Mult- Att is the model that keeps bipartite GNNs without the multi- head mechanism. We reported the \\(p\\) - values for all variants in Figure 3 and had the following observations. (1) All modules boosted the performance and played important roles in our framework. The overall model obtained the best results, which demonstrated that these modules were nicely incorporated into our framework. (2) Removing the problem transfer step experienced the largest performance degradation, which proved the effectiveness of Max- SAT and also answered Q1. (3) Pre- training and adaptation were both useful in improving the performance. Since both steps utilized Max- SAT to assist the learning of transferable features, we can answer Q2 and conclude that the combination of different strategies to use Max- SAT can largely benefit solving CO problems. (4) Attention and multi- head mechanisms also affected the results, which showed that better feature extraction backbones indeed helped the learning methods to solve CO problems. ### 4.4 Effects of Different Knowledge Transfer Strategies In this subsection, we verify the knowledge transfer ability of our model. we fixed the target domain as the MIS problem and used Max- SAT, Max- Cut, and MDS as source domains to check different degrees of transferability. To be concrete, we fed the data from different source domains into our framework for pre- training and domain adaptation and observed how the performance was affected by the selection of domains. We selected two scales of nodes, i.e., 450 variables and 760 variables, for demonstration. The results were shown in Figure 4 where darker colors indicated better performance and transferability.\n\n<center>Figure 5 Parameter analysis for feature dimension \\(d\\) , number of attention heads \\(h\\) , weight of loss \\(\\lambda\\) , and number of layers \\(L\\) . </center> From the heat maps, we can observe that Max- SAT \\(\\rightarrow\\) MIS always obtained the best results compared with the other two types of knowledge transfer, which showed that Max- SAT was more general and had the strongest transferability across different CO problems. Moreover, MDS is a more relevant CO problem to MIS compared with Max- Cut. Therefore, we observed better results from MDS \\(\\rightarrow\\) MIS than Max- Cut \\(\\rightarrow\\) MIS. This observation answered Q3 where different source domains do affect the performance of the target domain. From a practical viewpoint, selecting an appropriate source domain was always hard, and irrelevant tasks would generate negative transfers and eventually affect the performance of the target domain. ### 4.5 Parameter Analysis In this subsection, we analyzed the influence of several significant hyper- parameters on the model performance: the dimension of features \\(d\\) , the number of attention heads \\(h\\) , the weight of losses \\(\\lambda\\) , and the number of pertaining layers \\(L\\) . We recorded the performance of different \\(d\\) , \\(h\\) , \\(\\lambda\\) , \\(L\\) values on Max- CUT. The results were reported in Figure 5. It can be observed that with different dimensions of features, our model exhibited relatively stable performance when \\(d\\) was greater than or equal to 16. Our\n\nmodel achieved optimal performance when \\(d\\) was 128. Our model was not significantly affected by the number of attention heads. It achieved optimal performance when \\(k\\) was 10. The optimal value of weight \\(\\lambda\\) was obtained in 0.2. Pre- training led to better results, especially when the number of layers exceeded three. ## 5 Conclusion In this study, we aimed to discover the transferability of deep learning architectures for solving different CO problems on graphs. To achieve this goal, we leveraged the Max- SAT problem as a general formulation of original CO problems. The key advantages were two- fold. First, Max- SAT served as a bridge for various CO problems and provided a tool for discovering common properties in CO problems. Second, Max- SAT problems were represented as clauses that included logical information in addition to the simple correlations in graphs. By further constructing a pre- training and adaptation framework, we can extract transferable features that can be used for various CO problems. Experiments showed that incorporating Max- SAT indeed improved the ability to solve various CO problems on graphs. Our current work can be extended from the following directions. The pre- training stage is built on supervised tasks where labeled data samples are required. As a future direction, it is necessary to develop unsupervised pre- training paradigms that can fully leverage information from the data itself. Moreover, advanced GNNs that are more suitable for the properties of CO problems should be designed in the future. Acknowledgements This work was supported by the National Natural Science Foundation of China (Grant No. 11991021, No. 11991020, No. 12271503). ## References 1 Petrica C Pop, Ovidiu Cosma, Cosmin Sabo, and Corina Pop Sitar. A comprehensive survey on the generalized traveling salesman problem. European Journal of Operational Research, 2023. 2 Nathan Krislock, J\u00e9r\u00f4me Malick, and Fr\u00e9d\u00e9ric Roupin. Improved semidefinite bounding procedure for solving max- cut problems to optimality. Mathematical Programming, 143:61- 86, 2014. 3 Yiyuan Wang, Shaowei Cai, Shiwei Pan, Ximing Li, and Monghao Yin. Reduction and local search for weighted graph coloring problem. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 2433- 2441, 2020. 4 Marc Los and Christian Lardinois. Combinatorial programming, statistical optimization and the optimal transportation network problem. Transportation Research Part B: Methodological, 16(2):89- 124, 1982. 5 Simone L Martins and Celso C Ribeiro. Metaheuristics and applications to optimization problems in telecommunications. Handbook of Optimization in Telecommunications, pages 103- 128, 2006. 6 Eugene L Lawler and David E Wood. Branch- and- bound methods: A survey. Operations Research, 14(4):699- 719, 1966. 7 Steven J Benson, Yinyu Ye, and Xiong Zhang. Solving large- scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000. 8 Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263- 1272. PMLR, 2017. 9 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. 10 Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016. 11 Quentin Cappart, Didier Ch\u00e9telat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 12 Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in Neural Information Processing Systems, 30, 2017. 13 Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in Neural Information Processing Systems, 31, 2018.\n\n14 Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019. 15 Tianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang, Howard Heaton, Jialin Liu, and Wotao Yin. Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research, 23(1):8562- 8620, 2022. 16 Matteo Fischetti, Paolo Toth, and Daniele Vigo. A branch- and- bound algorithm for the capacitated vehicle routing problem on directed graphs. Operations Research, 42(5):846- 859, 1994. 17 Panos M Pardalos and Gregory P Rodgers. A branch and bound algorithm for the maximum clique problem. Computers & Operations Research, 19(5):363- 375, 1992. 18 Richard Bellman. Dynamic programming. Science, 153(3731):34- 37, 1966. 19 Michael Held and Richard M Karp. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1):196- 210, 1962. 20 Silvano Martello, David Pisinger, and Paolo Toth. Dynamic programming and strong bounds for the 0- 1 knapsack problem. Management Science, 45(3):414- 424, 1999. 21 Laurence A Wolsey and George L Nemhauser. Integer and combinatorial optimization, volume 55. John Wiley & Sons, 1999. 22 Dimitris Bertsimas and Ramazan Demir. An approximate dynamic programming approach to multidimensional knapsack problems. Management Science, 48(4):550- 565, 2002. 23 Paul Bouman, Niels Agatz, and Marie Schmidt. Dynamic programming approaches for the traveling salesman problem with drone. Networks, 72(4):528- 542, 2018. 24 Mutsunori Yagiura and Toshihide Ibaraki. On metaheuristic algorithms for combinatorial optimization problems. Systems and Computers in Japan, 32(3):33- 55, 2001. 25 James B Orlin, Abraham P Punnen, and Andreas S Schulz. Approximate local search in combinatorial optimization. SIAM Journal on Computing, 33(5):1201- 1214, 2004. 26 Emile HL Aarts, Jan HM Korst, and Peter JM van Laarhoven. A quantitative analysis of the simulated annealing algorithm: A case study for the traveling salesman problem. Journal of Statistical Physics, 50:187- 206, 1988. 27 Jean Berger and Mohamed Barkaoui. A hybrid genetic algorithm for the capacitated vehicle routing problem. In Genetic and Evolutionary Computation Conference, pages 646- 656. Springer, 2003. 28 Andr\u00e9 Chassein, Marc Goerigk, Adam Kasperski, and Pawel Zieli\u0144ski. Approximating combinatorial optimization problems with the ordered weighted averaging criterion. European Journal of Operational Research, 286(3):828- 838, 2020. 29 Henrique Lemos, Marcelo Prates, Pedro Avelar, and Luis Lamb. Graph coloring meets deep learning: Effective graph neural network models for combinatorial problems. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, pages 879- 885. IEEE, 2019. 30 Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym- nco: Leveraging symmetricity for neural combinatorial optimization. Advances in Neural Information Processing Systems, 35:1936- 1949, 2022. 31 Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2018. 32 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in Neural Information Processing Systems, 30, 2017. 33 Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. 34 Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. Gppt: Graph pre- training and prompt tuning to generalize graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1717- 1727, 2022. 35 Zixuan Yi, Iadh Ounis, and Craig Macdonald. Contrastive graph prompt- tuning for cross- domain recommendation. ACM Transactions on Information Systems, 2023. 36 Cristian Bodnar, Francesco Di Giovanni, Benjamin Chamberlain, Pietro Li\u00f2, and Michael Bronstein. Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns. Advances in Neural Information Processing Systems, 35:18527- 18541, 2022. 37 Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein. On over- squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865- 7885. PMLR, 2023. 38 Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 39 Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. In The Eleventh International Conference on Learning Representations, 2022. 40 Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, and Marinka Zitnik. Evaluating explainability for graph neural\n\nnetworks. Scientific Data, 10(1):144, 2023. 41 Jin Yuan, Feng Hou, Yangzhou Du, Zhongchao Shi, Xin Geng, Jianping Fan, and Yong Rui. Self- supervised graph neural network for multi- source domain adaptation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3907- 3916, 2022. 42 Quanyu Dai, Xiao- Ming Wu, Jiarren Xiao, Xiao Shen, and Dan Wang. Graph transfer learning via adversarial domain adaptation with graph convolution. IEEE Transactions on Knowledge and Data Engineering, 35(5):4908- 4922, 2022. 43 Ronghang Zhu, Xiaodong Jiang, Jiasen Lu, and Sheng Li. Cross- domain graph convolutions for adversarial unsupervised domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 34(8):3847- 3858, 2023. 44 Xinhong Ma, Tianzhu Zhang, and Changsheng Xu. Gcan: Graph convolutional adversarial network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8266- 8276, 2019. 45 Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and Pan Li. Structural re- weighting improves graph domain adaptation. In International Conference on Machine Learning, pages 21778- 21793, 2023. 46 Mehmet Pilanci and Elif Vural. Domain adaptation on graphs by learning aligned graph bases. IEEE Transactions on Knowledge and Data Engineering, 34(2):587- 600, 2022. 47 Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Graph domain adaptation via theory- grounded spectral regularization. In International Conference on Learning Representations, 2023. 48 Carlos Ans\u00f3tegui, Maria Luisa Bonet, and Jordi Levy. Towards industrial- like random sat instances. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, page 387- 392, 2009. 49 Diogo V Andrade, Mauricio GC Resende, and Renato F Werneck. Fast local search for the maximum independent set problem. Journal of Heuristics, 18:525- 547, 2012. 50 Thomas A Feo, Mauricio GC Resende, and Stuart H Smith. A greedy randomized adaptive search procedure for maximum independent set. Operations Research, 42(5):860- 878, 1994. 51 Amir Dembo, Andrea Montanari, and Subhabrata Sen. Extremal cuts of sparse random graphs. The Annals of Probability, pages 1190- 1217, 2017. 52 Changhui Choi and Yinyu Ye. Solving sparse semidefinite programs using the dual scaling algorithm with an iterative solver. Department of Management Sciences, University of Iowa, Iowa City, IA, 52242, 2000. 53 Stefan Boettcher and Allon G Percus. Extremal optimization for graph partitioning. Physical Review E, 64(2):026114, 2001. 54 Una Benlic and Jin- Kao Hao. Breakout local search for the max- cutproblem. Engineering Applications of Artificial Intelligence, 26(3):1162- 1173, 2013. 55 Thomas Barrett, William Clements, Jakob Foerster, and Alex Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3243- 3250, 2020. 56 Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max- cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242- 251. SPIE, 2019. 57 Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence, 3:580607, 2021. 58 Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics- inspired graph neural networks. Nature Machine Intelligence, 4(4):367- 377, 2022. 59 Minghao Liu, Pei Huang, Fuqi Jia, Fan Zhang, Yuchen Sun, Shaowei Cai, Feifei Ma, and Jian Zhang. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 16264- 16265, 2023.\n\n## 1 Introduction Combinatorial optimization (CO) is a multidisciplinary field ranging from optimization to artificial intelligence. The goal of CO is to find optimal solutions from finite possible options where exhaustive enumeration cannot be accomplished. Graphs are the most common research objects in CO. Typical CO problems on graphs include traveling salesman problems (TSP) [1], Max- Cut problems [2], and graph coloring problems [3]. Solving CO on graphs can benefit many real- world applications, such as transportation logistics [4] and telecommunications [5]. Solving COs on graphs is extremely difficult due to their discrete property and high computational costs. For many decades, researchers have made many efforts to develop various algorithms to solve COs on graphs. For example, branch and bound [6] is a traditional exact algorithm that gradually divides the solution space into subspaces and uses bounds to prune possible solutions that exceed the bounds. Unfortunately, these traditional exact algorithms encounter excessive computational burdens as the scales of problems increase [7]. Recently, deep learning has developed as an effective tool for COs, inspiring the emergence of the category of learning- based methods for solving COs. Particularly, graph neural networks (GNNs) [8- 10] are advanced deep learning models designed for graph- structure data, which have been successfully applied to COs on graphs [11- 13]. In general, the graphs in CO problems are first fed into GNNs so that the high- level features can be extracted. Then, the predictions generated from features serve as feasible solutions or can be further used to guide the searching process of the heuristic learning process [14, 15]. Learning- based methods extract more useful knowledge to reduce the search space, which can be used to effectively and efficiently solve COs on graphs. Besides, the data- driven manner is more adaptive to real- world problems when real data is available. However, current GNN models for CO problems still face several challenges. First, directly modeling CO with graphs is intuitive but not necessarily accurate. Although graphs can capture relationships between objects, they ignore the mathematical logicality and properties inside COs. Therefore, constructing learning methods over original graphs often results in insufficient problem- solving abilities. Second, current methods are usually designed to solve a single CO problem on graphs. Nevertheless, different COs on graphs may share the same graphs and leverage similar GNN backbones for feature extraction. It is quite beneficial to explore whether the transferability of knowledge exists among different problems. Moreover, for real- world problems on graphs, the data collected for training GNNs is often limited, it is essential to leverage data from other problems that may have common knowledge to improve the generalization ability of GNNs. To address the above issues, we propose in this paper a unified pre- training and adaptation framework for COs on graphs. The core idea is to develop a generalizable and transferable GNN framework from which all COs can be benefited. To achieve this goal, it is necessary to find out the common knowledge and properties of all CO problems on graphs that can be extracted by GNNs and used for different problems. We notice that using graphs to formulate CO omits logical information that is usually more general and common for all problems. Inspired by this observation, we propose to leverage the maximum satisfiability problems (Max- SAT) to bridge different CO problems on graphs. Max- SAT is a classic problem containing propositional logic formulas that can be used to describe additional logic information beyond simple relations on graphs. With this unified representation, we further adopt a pre- training process and a fine- tuning process based on domain adaptation to extract general features and utilize the transferability between different problems. Our framework can be adaptive to all CO problems that can be transferred to Max- SAT problems and is also suitable for various GNN architectures, which exhibit superior flexibility and versatility. To be concrete, we first use the Max- SAT problem as an intermediate problem by which we can transform graphs from different COs to clauses and formulas. We can construct bipartite graphs from these clauses in Max- SAT that not only capture logical information but also can be adaptive to different COs on graphs. In the training stage, we can generate abundant clauses from certain Max- SAT problems without considering the original CO and transform them into bipartite graphs for pre- training. Due to the generic nature of Max- SAT, the pre- trained model is equipped with better initialization. Then, we\n\nuse the original CO and Max- SAT to construct an adaptation framework with adversarial training for fine- tuning, which can further utilize the transferable information in Max- SAT. In both the pre- training and fine- tuning stages, the graph attention networks are leveraged as backbones to extract information in bipartite graphs transformed from COs on graphs and Max- SAT. The bipartite graph attention networks separately consider the aggregation of clauses and variables, which can better capture the dependencies between variables and clauses with different neighbor aggregation processes. To summarize, the contributions of this work are as follows: (1) We propose a unified pre-training and adaptation framework based on Max-SAT that can learn generalizable and transferable information to benefit the solving of COs on graphs. (2) We propose to use Max-SAT to bridge various COs on graphs, by which all problems can be represented as a unified form. The Max-SAT instances also carry additional logical relationships that can be further utilized by GNNs. (3) We design a pre-training and domain adaptation architecture to extract generalizable and transferable features based on instances generated from the Max-SAT and COs on graphs. This framework is versatile to various COs and can be used to boost the ability to solve these problems. (4) We evaluate our method on both synthetic datasets and open benchmarks. The numerical results demonstrated the effectiveness of our framework in solving COs on graphs. The remainder of this paper is organized as follows. Section 2 presents the learning methods for COs and popular GNN architectures for feature extraction on graphs. Section 3 introduces the problem transfer, pre-training, and fine-tuning procedures in our framework and summarizes the overall algorithms for training. The experiments that aim to address our three main claims are listed in Section 4. We conclude our work in Section 5. ## 2 Related Work ### 2.1 Learning Methods for Solving CO Problems Traditional methods for solving CO mainly include branch and bound, dynamic programming, and heuristic algorithms. Branch and bound is a branch- and- prune search method used to reduce the search space [6, 16, 17]. It progressively decomposes the problem into smaller subproblems and applies pruning strategies to exclude branches that cannot produce the optimal solution. Dynamic programming breaks down a large problem into smaller subproblems and solves them by utilizing the relationships between the subproblems [18- 21]. Dynamic programming is suitable for problems with overlapping subproblem structures, such as the knapsack problem [22] and TSP [23]. Heuristic methods employ heuristic strategies or random operations to search the solution space and continuously improve the current solution until a satisfactory solution is found or a predetermined stopping condition is met. Common heuristic algorithms include greedy algorithms [24], local search [25], simulated annealing [26], and genetic algorithms [27]. Traditional methods for solving CO problems have limitations including high computational complexity, lack of scalability for large- size problems, and limited solution quality in terms of finding globally optimal solutions. In recent years, deep learning methods have emerged as promising approaches to tackle these problems by leveraging the power of data- driven modeling and computational intelligence. One category of methods approximates the process of solving CO problems through deep learning methods [28]. By learning from a dataset of problem instances, these deep learning models can capture intrinsic patterns and dependencies in the problem and the predictions are then utilized to guide the search process toward better solutions. These approaches offer advantages in terms of speed and scalability as they reduce the requirements of expensive evaluations for the objective function during the search. Another category of methods is based on learning- based heuristics and meta- heuristics [12- 14, 29]. Deep learning models can be employed to create intelligent decision rules or policies to guide the search process. Reinforcement learning techniques have demonstrated success in learning effective exploration- exploitation strategies for\n\nCO problems [30,31]. By incorporating learning- based heuristics into traditional optimization algorithms, better- quality solutions can be obtained with reduced computational effort. ### 2.2 Graph Neural Networks GNNs belong to a typical deep- learning framework for graphs that follow a message- passing mechanism [8]. Graph convolution networks (GCNs) [10] define graph convolutions on the spectral domain so that the structural information from neighbors can be aggregated. Graph SAGE [32] proposes an inductive representation learning method based on aggregation and sampling directly on the spatial domain. Graph attention networks (GATs) [33] utilize self- attention mechanisms to learn node features where the importance of neighbors can be considered during the aggregation of information. More recently, prompt tuning has been used to generalize GNNs [34,35]. GPPT [34] proposes a graph prompting function for GNNs so that the pre- trained models can predict labels without fine- tuning. Since GNNs have been successfully used in various downstream tasks, researchers pay attention to the theories behind GNNs. For example, over- smoothing [36] and over- squashing [37] are the two main issues for GNNs. BORF [38] proposes a rewiring algorithm based on Ollivier- Ricci curvature to relieve both over- smoothing and over- squashing problems for GNNs. The expressive abilities of GNNs have also been studied to demonstrate why GNNs perform well in many tasks [9,39,40]. Recently, GNNs have been used in graph domain adaptation to address the domain shift on graphs [41]. Domain adaptation on graphs aims at leveraging knowledge learned from the source domain to improve the performance of a GNN model on a different target domain. Adversarial training is often used to build the alignment framework [42,43]. For example, TFLG [43] utilizes instance and class levels of structures and leverages adversarial training to learn domain- invariant features. Except for the features that can be easily used for domain alignment, many researchers seek to learn invariant information from different levels. GCAN [44] is a graph convolutional adversarial network introduced for unsupervised domain adaptation by jointly modeling structures, domain labels, and class labels. Stru RW [45] presents a structural re- weighting method to address the conditional structure shift problem. Since spectral properties play an important role in graph structures, many works also attempt to explore the alignment of spectral information [46,47]. ## 3 Methodology ### 3.1 CO Problems on Graphs Given a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) , where \\(\\nu\\) is a set of nodes and \\(\\mathcal{E}\\) is a set of edges between nodes. The CO problems on graphs are to find a subset of nodes \\(\\mathcal{S}\\) under given constraint conditions so that the number of edges between the nodes in \\(\\mathcal{S}\\) satisfies a specific condition and the objective function can be maximized or minimized. Generally, the CO can be expressed as: \\[\\begin{array}{r}{\\max f(\\mathcal{S}),}\\\\ {g_{i}(\\mathcal{S})\\leqslant b_{i},b_{i}\\in \\Omega ,} \\end{array} \\quad (3.1b)\\] where \\(f(\\mathcal{S})\\) is the objective function, \\(g_{i}(\\mathcal{S})\\) is the constraint condition function, \\(b_{i}\\) is the boundary of the constraint condition, and \\(\\Omega\\) is the set of constraint indicators. In the following part, we give three concrete CO problems on graphs. Max- Cut Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) , the goal of the Max- Cut problem is to find a partition that divides \\(\\nu\\) into subset \\(\\mathcal{S}\\) and its complementary set \\(\\mathcal{S}^{\\prime} = \\mathcal{V} - \\mathcal{S}\\) where the number of cuts between \\(\\mathcal{S}\\) and \\(\\mathcal{S}^{\\prime}\\) is maximized. The cuts refer to the edges between \\(\\mathcal{S}\\) and \\(\\mathcal{S}^{\\prime}\\) . The objective function of Max- Cut can be denoted as: \\[f(\\mathcal{S}) = \\sum_{e_{i,j}\\in \\mathcal{E}}\\big(v_{i} + v_{j} - 2v_{i}v_{j}\\big), \\quad (3.2)\\] where \\(v_{i} = 1\\) when \\(v_{i}\\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise, and \\(e_{i,j}\\) is the edge between nodes \\(v_{i}\\) and \\(v_{j}\\)\n\nMaximum Independent Set (MIS) Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) , the maximum independent set problem aims to find a node set \\(\\mathcal{S}\\) with a maximal number of nodes where there are no edges between any two nodes in this set. The objective function and constraint conditions of the maximum independent set problem can be denoted as: \\[\\begin{array}{l}{f(\\mathcal{S}) = \\sum_{v_{i}\\in \\mathcal{V}}v_{i},}\\\\ {v_{i} + v_{j}\\leqslant 1|\\forall e_{i,j}\\in \\mathcal{E},} \\end{array} \\quad (3.3a)\\] where \\(v_{i} = 1\\) when \\(v_{i} \\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise. Minimum Dominated Set (MDS) Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) , the goal of the minimum dominated set problem is to find a dominated set \\(\\mathcal{S}\\) where each node \\(v_{i} \\in \\mathcal{V}\\) or at least one of its neighbor nodes belongs to the dominated set \\(\\mathcal{S}\\) . The number of nodes in \\(\\mathcal{S}\\) should also be minimized. The objective function and constraint conditions of the minimum dominated set problem can be denoted as: \\[\\begin{array}{l}{f(\\mathcal{S}) = -\\sum_{v_{i}\\in \\mathcal{V}}v_{i},}\\\\ {v_{i} + v_{j}\\geqslant 1|e_{i,j}\\in \\mathcal{E},\\forall v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.4a)\\] where \\(v_{i} = 1\\) when \\(v_{i} \\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise. ### 3.2 Problem Transfer via Max-SAT Problem In this subsection, we introduce the details of leveraging the Max- SAT problem to bridge different CO problems. In general, to fully utilize the common logical information in COs, we first convert the original graphs of the CO problems into several clauses of Max- SAT problems and then transform these clauses into bipartite graphs. #### 3.2.1 From Graph to Clauses Given a graph from any CO, we use clauses with variables to describe the objective function and the constraint conditions. Max- SAT is an optimization problem that contains a set of clauses and variables. Define \\(m\\) clauses \\(\\mathcal{C} = \\{c_{1},c_{2},\\dots ,c_{m}\\}\\) and \\(n\\) variables \\(\\mathcal{X} = \\{x_{1},x_{2},\\dots ,x_{n}\\}\\) , the Max- SAT problem is to find a truth assignment of variables so that the number of satisfied clauses is maximized. To generate clauses, we define two rules for the objective function and constraint conditions respectively. The soft rule is based on the objective function and the hard rule is based on the constraint conditions. The difference between those two rules mainly lies in whether the clauses should be strictly satisfied. For clarification, we let the variable \\(x_{i}\\) correspond to the node \\(v_{i}\\) in graph \\(\\mathcal{G}\\) . For demonstration purposes, we provide the details of transformations for Max- Cut, MIS, and MDS problems. For every \\(v_{i} \\in \\mathcal{V}\\) in MIS problems, the clauses are generated by: \\[\\begin{array}{l}{c_{h} = \\neg x_{i}\\vee \\neg x_{j}|e_{i,j}\\in \\mathcal{E},}\\\\ {c_{s} = x_{i}|v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.5a)\\] where \\(c_{h} \\in \\mathcal{C}_{h}\\) denotes the hard clause which describes that the set cannot contain the node and its neighbors at the same time, \\(c_{s} \\in \\mathcal{C}_{s}\\) denotes the soft clause which describes that the number of nodes in the MIS problem should be maximized, \\(\\neg\\) and \\(\\vee\\) are the negation and disjunction. For every \\(v_{i} \\in \\mathcal{V}\\) in MDS problems, the clauses are generated by: \\[\\begin{array}{l}{c_{h} = x_{i}\\vee x_{j}|e_{i,j}\\in \\mathcal{E},}\\\\ {c_{s} = \\neg x_{i}|v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.6a)\\] where \\(c_{h} \\in \\mathcal{C}_{h}\\) describes that the node or at least one of its neighbor nodes belongs to MDS, and \\(c_{s} \\in \\mathcal{C}_{s}\\) describes that the number of nodes in MDS should be minimized.\n\nFor every \\(e_{i,j} \\in \\mathcal{E}\\) in Max- Cut problems, the clauses are generated by: \\[\\begin{array}{l}{c_{s} = x_{i}\\vee x_{j},}\\\\ {c_{s}^{\\prime} = \\neg x_{i}\\vee \\neg x_{j},} \\end{array} \\quad (3.7a)\\] where \\(c_{s}\\) and \\(c_{s}^{\\prime}\\) are all soft clauses belonging to \\(\\mathcal{C}_{s}\\) that describe the number of cut edges that should be maximized. #### 3.2.2 From Clauses to Bipartite Graph The soft and hard clauses describe the relations between clauses and variables, which reveals the logical dependencies. Then, we can use bipartite graphs to further represent these relations in Max- SAT. In general, the bipartite graph is denoted as \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) where \\(\\tilde{\\mathcal{V}}_{x}\\) and \\(\\tilde{\\mathcal{V}}_{c}\\) are node sets, and \\(\\tilde{\\mathcal{E}}\\) is the edge set. The main difference between graphs and bipartite graphs is that the nodes in bipartite graphs belong to two different types. The variables \\(\\mathcal{X}\\) correspond to the set of nodes \\(\\tilde{\\mathcal{V}}_{x}\\) and the clauses \\(\\mathcal{C}\\) correspond to the set of nodes \\(\\tilde{\\mathcal{V}}_{c}\\) . To generate the bipartite graph, the core step is to construct the edges between nodes. For each variable \\(x_{i}\\) in each clause \\(c_{i}\\) , an edge \\(\\tilde{e}_{i,j} \\in \\tilde{\\mathcal{E}}\\) corresponds to the affiliation between the variable and the clause. In this transfer process, we transfer the origin graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) of the CO problem into the bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) . A concrete transfer process can be shown in Figure 1. We choose Max- Cut as an example for demonstration. The first step is to transfer the graph in Max- Cut into a set of clauses in Max- SAT. We use the hard and soft rules to generate these clauses. After generating these clauses, we then transfer them into a bipartite graph. The variables and clauses are set as nodes and the affiliations between variables and clauses constitute edges. The new bipartite graph can be then used for the next feature extraction process. <center>Figure 1 Problem transfer process of our framework via Max-SAT. The input graph of CO problems is first transferred as Max-SAT clauses. Then, a bipartite graph is generated from these clauses by the affiliations between variables and clauses. </center> Notice that transferring graphs in original COs to bipartite graphs has two main advantages: First, original graphs only contain simple dependencies between variables. In contrast, the bipartite graphs transferred from Max- SAT clauses are then equipped with logical information from COs; Second, the bipartite graphs are general forms that can represent various COs on graphs since all of these COs can be transformed into Max- SAT problems. Therefore, we can easily construct a knowledge transfer framework to leverage the generalizable and transferable information of these problems with the help of bipartite graphs.\n\n### 3.3 Learning with Bipartite Graph Attention Networks In this subsection, we first introduce the backbone network that is used to extract features from bipartite graphs. Traditional GNNs for feature extractions from graphs mainly follow a message- passing scheme: \\[\\begin{array}{r l} & {\\mathbf{a}_{v}^{(l)} = \\mathrm{AGG}(\\mathbf{x}_{u}\\mid u\\in \\mathcal{N}(v)),}\\\\ & {\\mathbf{x}_{v}^{(l + 1)} = \\mathrm{COM}(\\mathbf{a}_{v}^{(l)},\\mathbf{x}_{v}^{(l)}),} \\end{array} \\quad (3.8)\\] where \\(u\\) and \\(v\\) denote the nodes, \\(\\mathbf{a}_{v}\\) denotes the feature vector accumulated from neighbor nodes, \\(\\mathbf{x}_{v}^{(l)}\\) denotes the features at layer \\(l\\) and \\(\\mathbf{x}_{v}^{(0)}\\) is the initial attributes, \\(\\mathrm{AGG}(\\cdot)\\) and \\(\\mathrm{COM}(\\cdot)\\) are the aggregation and combination functions for message passing and feature updating, and \\(\\mathcal{N}(v)\\) denotes the set of neighbors for \\(v\\) . The above process is only suitable for graphs with one type of nodes. Recall that a bipartite graph contains two sets of nodes that represent variables and clauses and a set of edges that connect nodes from these two sets. Due to the different properties of nodes, the traditional message- passing scheme in GNNs cannot be directly used to extract features. We particularly design a bipartite GNN with attentive message- passing schemes. For an input bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\nu}_{x},\\tilde{\\nu}_{c},\\tilde{\\mathcal{E}})\\) , the attributed matrices of variables and clauses are denoted as \\(\\mathbf{C}\\in \\mathbb{R}^{m\\times d}\\) and \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times d}\\) where \\(m\\) and \\(n\\) denote the numbers of clauses and variables, and \\(d\\) is the dimension of attributes. For symbolic convenience, we use \\(v_{x}(i)\\) and \\(v_{c}(i)\\) to denote nodes of variables and clauses in the bipartite graph. The variable and clause of these nodes are denoted as \\(x_{i}\\) and \\(c_{i}\\) and their attributed vectors are denoted as \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{c}_{i}\\) accordingly. Since the initial bipartite graph does not contain attributed information, the attributed matrices require initialization. There are different strategies for initialization, e.g., uniform distribution, normal distribution, and all- one matrices. Since no significant influences have been observed in our experiments, we simply initialize attributed matrices as all- one matrices \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) . The message- passing process of bipartite GNNs consists of two steps in each iteration, the clause- wise aggregation step and the variable- wise aggregation step. First, clause- wise aggregation updates the feature of the clause node by aggregating the features from the variable nodes. To further discriminate the importance of different neighbors, the attention mechanism is introduced to learn features during the aggregation. Given a clause \\(c_{i}\\) and its neighbor variable \\(x_{j}\\) , the layer- wise aggregation from \\(x_{j}\\) to \\(c_{i}\\) through attention is represented as: \\[\\begin{array}{r l} & {\\alpha_{x_{j}\\to c_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{x}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{x}(k)\\in\\mathcal{N}(v_{c}(i))}\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{X}_{k}^{(l)})\\rangle\\right)}}\\\\ & {\\mathbf{c}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{c}_{i}^{(l)},\\sum_{v_{x}(j)\\in\\mathcal{N}(v_{c}(i))}\\alpha_{x_{j}\\to c_{i}}^{(l)}(\\mathbf{w}_{V}^{(l)}\\mathbf{x}_{j}^{(l)})\\right),} \\end{array} \\quad (3.11)\\] where \\(\\alpha_{x_{j}\\to c_{i}}\\) denotes the attention score between clause \\(c_{i}\\) and variable \\(x_{j}\\) , \\(\\mathbf{w}_{Q}^{(l)}\\in \\mathbb{R}^{d}\\) , \\(\\mathbf{w}_{K}^{(l)}\\in \\mathbb{R}^{d}\\) , and \\(\\mathbf{w}_{V}^{(l)}\\in \\mathbb{R}^{d}\\) are learnable parameters of weights, \\(\\mathbf{c}_{i}^{(l)}\\) and \\(\\mathbf{x}_{j}^{(l)}\\) are features of \\(i\\) - th clause and \\(j\\) - th variable at layer \\(l\\) , \\(\\mathcal{N}(v_{c}(i))\\) is the set of neighbor variable nodes for clause \\(c_{i}\\) , \\(\\langle \\cdot ,\\cdot \\rangle\\) is the dot product operation, \\(\\mathbf{c}_{i}^{(l + 1)}\\) is the updated features for clause \\(c_{i}\\) , and MLP denotes the multi- layer perceptron. Second, the variable feature can be updated through variable- wise aggregation from the features of clauses that contain the variable. Similarly, for variable \\(x_{i}\\) , the layer- wise aggregation from \\(c_{j}\\) to \\(x_{i}\\) can be denoted as: \\[\\begin{array}{r l} & {\\alpha_{c_{j}\\to x_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{c}(k)\\in\\mathcal{N}(v_{x}(i))}\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{k}^{(l)})\\rangle\\right)},}\\\\ & {\\mathbf{x}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{x}_{i}^{(l)},\\sum_{v_{c}(j)\\in\\mathcal{N}(v_{x}(i))}\\alpha_{c_{j}\\to x_{i}}^{(l)}(\\tilde{\\mathbf{w}}_{V}^{(l)}\\mathbf{c}_{j}^{(l)})\\right),} \\end{array} \\quad (3.12)\\]\n\n<center>Figure 2 Illustration of the proposed pre-training and fine-tuning architecture. In the pre-training stage, the bipartite graphs generated from Max-SAT clauses are used to train the MLP, bipartite GNN backbone (Bip-GNN(\u00b7)), and fully connected classification layers \\(\\mathrm{FC}(\\cdot)\\) . In the fine-tuning stage, the bipartite graphs generated by Max-SAT and the CO are treated as source and target domains and are separately fed into the MLP and Bip-GNN to obtain features. A discriminator is to obtain domain labels for domain adaptation. The features are passed through the classification layer to predict the labels of variable nodes. </center> where \\(\\alpha_{c_{j}\\to x_{i}}\\) denotes the attention score between clause \\(c_{j}\\) and variable \\(x_{i}\\) , \\(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\) , \\(\\tilde{\\mathbf{w}}_{K}^{(l)}\\) , and \\(\\tilde{\\mathbf{w}}_{V}^{(l)}\\) are parameters, and \\(\\mathcal{N}(v_{x}(i))\\) is the neighbor set of clause nodes for variable \\(x_{i}\\) . After the two steps of aggregation, we can obtain the updated features of variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers for bipartite GNNs. The correlations between variables and clauses can be captured by the aggregation processes and the importance of neighbors can be learned by the attention mechanism. We can then build our framework based on the bipartite GNN backbone. ### 3.4 Pre-Training and Fine-Tuning with Domain Adaptation In this subsection, we introduce the framework of our work for training as depicted in Figure 2. Our framework consists of two stages. The first stage is to pre- train our model by using massive samples generated from Max- SAT and the second stage is to further fine- tune the model with domain adaptation that combines samples from Max- SAT and CO problems. #### 3.4.1 Pre-Training In the pre- training stage, the goal is to learn the general knowledge from Max- SAT and obtain better parameter initialization that can be used for solving different CO problems. We first generate massive clauses from Max- SAT and then convert them into bipartite graphs. To generate Max- SAT clauses, we follow [48] and select different distributions to simulate different scenarios of problems: (1) In uniform distribution, the variables and clauses appear with the same frequency. The sizes of clauses also appear with the same frequency. The clause size refers to the number of variables within a clause. For example, given a clause \\(x_{1} \\lor x_{2} \\lor \\neg x_{3}\\) , the clause size is three. (2) The single power- low distribution is a non- uniform distribution where most variables appear in low frequencies while only a few variables appear in high frequencies. The sizes of clauses also appear with the same frequency. (3) Double power- low distribution is also a non- uniform distribution that aims to simulate extremely uneven samples. Small clause size appears in high frequencies and the frequency decreases as the clause size increases.\n\nAfter obtaining the bipartite graphs, we can build our pre- training process. To be concrete, the model structure for pre- training contains three parts, an MLP that is used to map the initial node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) into low- dimensional features \\(\\mathbf{X}^{(0)}\\) and \\(\\mathbf{C}^{(0)}\\) ; the bipartite GNN backbone \\(\\mathrm{Bip - GNN}(\\cdot)\\) from section 3.3 extracts high- level node features for variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers; and fully connected prediction network \\(\\mathrm{FC}(\\cdot)\\) maps node features to their labels. To pre- train the three parts, we build a supervised loss function to update the parameters. We leverage the binary cross entropy (BCE) as the objective of classification: \\[\\mathcal{L}_{c} = \\sum_{i = 1}^{n}\\mathrm{BCE}(p_{i},y_{i}), \\quad (3.14)\\] where \\(y_{i}\\in \\{0,1\\}\\) is the label that describes the truth assignment of the variable generated from the MAXHS solver, and \\(p_{i}\\in \\{0,1\\}\\) is the prediction of the pre- training model from the prediction network \\(\\mathrm{FC}(\\cdot)\\) . It is worth noting that we only consider the classification loss under \\(n\\) variable nodes. Since the truth assignments of clauses are easily affected by variables, it is difficult to determine the labels of clauses. Therefore, to avoid the disturbance brought by inaccurate labeling, we only consider the classification of variables. #### 3.4.2 Fine-Tuning with Domain Adaptation After the pre- training process, we further leverage Max- SAT to build a fine- tuning process based on domain adaptation. In general, domain adaptation aims to learn domain- invariant features that can be used for different tasks. For a target CO problem, the proposed adaptation network treats Max- SAT as the source domain and learns transferable features by combining the samples from the CO and Max- SAT problems. Before the feature extraction, we can construct the data from source and target domains via transformation operations in section 3.2. The data for the source domain is generated directly from Max- SAT with different distributions following the pre- training process while the data of the target domain is generated from Max- SAT that is converted from a certain CO problem. The network architecture of fine- tuning is based on the pre- trained network. The pre- training network learns generalizable features while the fine- tuning network focuses on learning task- specific features. With the help of pre- training, the model can converge faster and achieve better generalization performance. Concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on MLP and \\(\\mathrm{Bip - GNN}(\\cdot)\\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \\(\\mathrm{Dis}(\\cdot)\\) is to classify the domain labels for each sample. The source domain and target domain share the same feature extraction backbone but have different classification networks. The domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training. Our work can also be extended to the unsupervised setting where the labels for the target domain are unavailable. Based on the three parts, the overall loss for domain adaptation in the fine- tuning stage can be denoted as \\[\\mathcal{L}_{ft} = \\mathcal{L}_{c} + \\lambda \\mathcal{L}_{d}, \\quad (3.15)\\] where \\(\\mathcal{L}_{c}\\) and \\(\\mathcal{L}_{d}\\) are the losses for classification and discrimination, and \\(\\lambda\\) is a positive hyper- parameter that controls the weight of losses. The discriminative loss that classifies nodes based on their domain labels can be denoted as \\[\\mathcal{L}_{d} = (\\mathbf{X}_{S},\\mathbf{X}_{T}) = \\mathbb{E}_{\\mathbf{x}_{S}\\in \\mathcal{D}_{S}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{S}))\\right)] + \\mathbb{E}_{\\mathbf{x}_{T}\\in \\mathcal{D}_{T}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{T}))\\right)], \\quad (3.16)\\] where \\(\\mathbf{X}_{S}\\) and \\(\\mathbf{X}_{D}\\) are the node features from source and target domains, \\(\\mathcal{D}_{S}\\) and \\(\\mathcal{D}_{T}\\) represent the data distribution of source and target domains, \\(\\mathrm{Dis}(\\cdot)\\) is the domain classifier, and \\(\\mathrm{Bip - GNN}(\\cdot)\\) is the feature extractor.",
      "level": 1,
      "line_start": 1,
      "line_end": 41
    },
    {
      "heading": "Introduction",
      "content": "### 3.5 Inference with Local Search Method The last step of our model is to search the feasible solutions for CO problems with constraint conditions by using the predictions generated by our framework. We can also get close to optimal solutions with a search algorithm. For Max- Cut problems without hard clauses, our model can directly infer the truth assignment and obtain the target values. For problems with hard clauses, we introduce a heuristic local search method to discretely obtain feasible solutions that satisfy constraints and obtain the target values. Local search is a typical search method for discrete optimization. We adopt a 2- improvement local search algorithm [49,50] that iterates over all nodes in the graph and attempts to replace a 1- labeled node \\(v_{i}\\) with two 1- labeled nodes \\(v_{j}\\) and \\(v_{k}\\) . For the MIS problem, \\(v_{j}\\) and \\(v_{k}\\) must be the neighbors of \\(v_{i}\\) . These two nodes are 1- tight and are not connected. Here, a node is 1- tight if exactly one of its neighbors is 1- labeled. In other words, \\(v_{i}\\) is the only 1- labeled neighbor of \\(v_{j}\\) and \\(v_{k}\\) in the graph. Notice that this local search algorithm can find a valid 2- improvement in \\(O(E)\\) time if it exists. We introduce this method with fixed steps to find a feasible solution and evaluate the performance of our model. ### 3.6 Overall Algorithms Most of the previous works focus on modeling specific CO problems. In contrast, our method captures the common knowledge of COs and designs a unified learning method to utilize the knowledge to solve COs. To summarize, our framework has three essential processes. The problem transfer process leverages Max- SAT to bridge various COs. The COs are converted to Max- SAT with a general form that can capture logical information within CO problems. From the perspective of data transformation, the graphs in COs are firstly converted into clauses and then converted to bipartite graphs. The overall algorithm for problem transfer is listed in Algorithm 1. Algorithm 1 Problem Transfer via Max- SAT Input: A CO graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) Objective function: max \\(f(S)\\) Constraint condition: \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) Output: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) #From a graph to Max- SAT clauses# 1: Generate soft clauses \\(C_{s}\\) with objective function max \\(f(S)\\) 2: Generate hard clauses \\(C_{h}\\) with constraint condition \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) #From Max- SAT clauses to a bipartite graph# 3: Construct nodes \\(\\tilde{\\nu}_{x}\\) and \\(\\tilde{\\nu}_{c}\\) for each variable \\(v_{i}\\) and clause \\(c_{j}\\) 4: Construct edges \\(e_{(v_{i},c_{j})}^{\\prime}\\in \\tilde{\\mathcal{E}},\\forall v_{i}\\in c_{j}\\) The pre- training process uses samples from Max- SAT to learn generalizable features that can benefit all COs. The networks used for pre- training include the MLP, feature extraction backbones based on GNNs, and the classification network. The overall algorithm for pre- training is listed in Algorithm 2. The fine- tuning process uses samples from Max- SAT and target CO to build a domain adaptation architecture. Based on the pre- trained network, the fine- tuning network introduces an additional discriminator for domain classification, which can learn domain- invariant features to further improve the generalizability of features. The overall algorithm for fine- tuning is listed in Algorithm 3. ## 4 Experimental Results In this section, we evaluated the effectiveness of the proposed framework for three CO problems on graphs, i.e., Max- Cut, MIS, and MDS. Generally, we aimed to answer three essential questions by the experiments. Q1: Can we leverage Max- SAT to learn transferable and generalizable features that can improve the ability to solve different CO problems on graphs? Q2: How can we incorporate Max- SAT\n\nInput: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, True assignment \\(\\mathcal{V}\\) Number of pre- training layers \\(L\\) Number of iterations iters. Output: Pre- trained parameters \\(\\Theta_{p}\\) for MLP, Bip- GNN, and FC. 1: Initialize node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for nodes of variables and clauses; 4: for \\(l\\leqslant L\\) do 5: Extract features for clauses \\(\\mathbf{C}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 6: Extract features for variables \\(\\mathbf{X}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 7: end for 8: Map variable features to the predictions via \\(\\mathrm{FC}(\\mathbf{X}^{(L)})\\) 9: Compute the loss \\(\\mathcal{L}_{c}\\) in Eq. (3.14); 10: Backward and update parameters; 11: end for 12: Obtain the updated parameters \\(\\Theta_{p}\\) Algorithm 3 Fine- tuning Based on Domain Adaptation Input: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, Number of fine- tuning layers \\(L\\) Number of iterations iters, True assignment \\(\\mathcal{V}\\) Output: Predicted feasible solutions \\(\\mathcal{P} = \\{p_{i}\\}_{i = 1}^{n}\\) Fine- tuned parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, FC, and Dis. 1: Initialize node attributes of both domains \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for source variables and clauses; 4: Obtain features \\(\\mathbf{X}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for target variables and clauses; 5: for \\(l\\leqslant L\\) do 6: Extract features for clauses \\(\\mathbf{C}_{S}^{(l)}\\) and \\(\\mathbf{C}_{T}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 7: Extract features for variables \\(\\mathbf{X}_{S}^{(l)}\\) and \\(\\mathbf{X}_{T}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 8: end for 9: Map variable features to the assignment labels via \\(\\mathrm{FC}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{FC}(\\mathbf{X}_{T}^{(L)})\\) 10: Map variable features to the domain labels via \\(\\mathrm{Dis}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{Dis}(\\mathbf{X}_{T}^{(L)})\\) 11: Compute the loss \\(\\mathcal{L}_{ft}\\) in Eq. (3.15); 12: Updated parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, Dis, and FC; 13: end for 14: Obtain the optimal parameters \\(\\Theta_{ft}\\) 15: Predict feasible solutions \\(\\mathcal{P}\\) into the pipelines of GNN- based learning frameworks to fully utilize the information of Max- SAT? Q3: Do different pre- training and domain adaptation strategies matter for the ability of solving COs? ### 4.1 Experimental Setup Datasets We introduce the datasets that were used in our experiments in this part. In the pretraining stage, a large number of Max- SAT instances were required. We generated the clauses by running three representative generators with different distributions: the uniform distribution, the power- law distribution, and the double power- law distribution. A total number of 20,000 Max- SAT instances\n\nwere generated and the Max HS solver was used to obtain the solutions that served as labels for training. In the fine- tuning stage, according to the CO problems, different datasets are leveraged. For Max- Cut, we introduce the GSET benchmark \\(^{1)}\\) for evaluation. GSET is a set of 71 unweighted graphs that were commonly used for testing the algorithms to solve the Max- Cut problem. For MIS and MDS problems, we introduce the frb benchmark with four different instance scales for evaluation. Each frb dataset consists of five instances of hard CO problems and is regarded as the benchmark for CSP competition annually. Evaluation Metrics For different CO problems, we used the corresponding evaluation metrics based on their feasible solutions. For Max- Cut, \\(p\\) values [51] were used to evaluate the number of graph cuts. Concretely, \\(p\\) can be calculated by \\[p(z) = \\frac{z / n - \\gamma / 4}{\\sqrt{\\gamma / 4}}, \\quad (4.1)\\] where \\(z\\) is the predicted cut size for a \\(\\gamma\\) - regular graph with \\(n\\) nodes and \\(\\gamma\\) is the degree of nodes. For MIS and MDS, the number of cuts or sets is used for evaluation. Baseline methods To evaluate our method, we compare it with several baseline methods that have been widely used in solving COs. These methods can be roughly classified as traditional methods, heuristic methods, and learning- based methods. SDP [52] is a classical approach based on semi- definite programming. EO [53] is the extremal optimization method. BLS [54] is the breakout local search method. ECO- DQN [55] is an approach of exploratory CO with reinforcement learning. GMC- A and GMC- B [56] are two unsupervised GNN architectures for Max- CUT with different loss functions. RUN- CSP [57] is a recurrent unsupervised neural network for constraint satisfaction problems. PIGNN [58] is the physics- inspired GNNs for CO problems. MAXSAT [59] is a GNN- based framework designed for Max- SAT problems. Experimental Settings The proposed framework was implemented in Python with Py Torch. We summarized the hyper- parameters used in our paper as follows. The dimension of features \\(d\\) was set as 128. The number of GNN layers \\(L\\) for pre- training and fine- tuning was set as 5 and the first 2 layers were fixed during the fine- tuning. The number of layers for MLP was set as 2. The number of epochs for pre- training and fine- tuning were tuned and set as 400 and 100. The Adam optimizer was used for model training with a learning rate of \\(10^{- 5}\\) and a weight decay of \\(10^{- 10}\\) . The warm- up trick was also adopted. The number of FC layers for classification was set as 2 and the discriminator used a two- layer fully connected network to generate domain labels. The steps for local search for MIS and MDS were set as 120. All experiments were conducted on a workstation equipped with an Intel Xeon(R) Gold 6139M CPU 515 @ 2.30GHz and an NVIDIA RTX 3090 GPU with 24GB. Table 1 Results of solving Max-Cut with different approaches on random graphs. <table><tr><td rowspan=\"2\">Methods</td><td colspan=\"4\">n=100</td><td colspan=\"4\">n=200</td><td colspan=\"4\">n=500</td><td colspan=\"4\">n=800</td><td colspan=\"4\">n=1000</td></tr><tr><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td></td><td>\u03b3=10</td></tr><tr><td>SDP</td><td>0.709</td><td>0.697</td><td>0.689</td><td>0.709</td><td>0.702</td><td>0.692</td><td>0.702</td><td>0.690</td><td>0.682</td><td>0.701</td><td>0.688</td><td>0.679</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>0.712</td><td>0.708</td><td>0.703</td><td>0.721</td><td>0.723</td><td>0.724</td><td>0.727</td><td>0.737</td><td>0.735</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>0.712</td><td>0.707</td><td>0.704</td><td>0.720</td><td>0.721</td><td>0.719</td><td>0.722</td><td>0.725</td><td>0.721</td><td>0.720</td><td>0.726</td><td>0.717</td><td>0.721</td><td>0.725</td><td>0.720</td><td></td><td></td><td></td><td></td></tr><tr><td>ECO-DQN</td><td>0.713</td><td>0.705</td><td>0.707</td><td>0.718</td><td>0.723</td><td>0.720</td><td>0.725</td><td>0.727</td><td>0.725</td><td>0.722</td><td>0.721</td><td>0.722</td><td>0.726</td><td>0.726</td><td>0.721</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-A</td><td>0.691</td><td>0.655</td><td>0.637</td><td>0.701</td><td>0.683</td><td>0.660</td><td>0.693</td><td>0.668</td><td>0.599</td><td>0.691</td><td>0.666</td><td>0.602</td><td>0.688</td><td>0.662</td><td>0.601</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-B</td><td>0.698</td><td>0.660</td><td>0.630</td><td>0.708</td><td>0.667</td><td>0.646</td><td>0.707</td><td>0.701</td><td>0.670</td><td>0.699</td><td>0.696</td><td>0.653</td><td>0.702</td><td>0.694</td><td>0.651</td><td></td><td></td><td></td><td></td></tr><tr><td>RUN-CSP</td><td>0.702</td><td>0.706</td><td>0.704</td><td>0.711</td><td>0.715</td><td>0.714</td><td>0.714</td><td>0.726</td><td>0.710</td><td>0.704</td><td>0.713</td><td>0.705</td><td>0.705</td><td>0.711</td><td>0.702</td><td></td><td></td><td></td><td></td></tr><tr><td>PI-GNN</td><td>0.704</td><td>0.706</td><td>0.708</td><td>0.712</td><td>0.718</td><td>0.717</td><td>0.715</td><td>0.726</td><td>0.716</td><td>0.711</td><td>0.724</td><td>0.712</td><td>0.709</td><td>0.722</td><td>0.712</td><td></td><td></td><td></td><td></td></tr><tr><td>MAX-SAT</td><td>0.702</td><td>0.705</td><td>0.708</td><td>0.712</td><td>0.716</td><td>0.718</td><td>0.721</td><td>0.732</td><td>0.726</td><td>0.719</td><td>0.730</td><td>0.728</td><td>0.718</td><td>0.730</td><td>0.726</td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>0.702</td><td>0.708</td><td>0.707</td><td>0.722</td><td>0.721</td><td>0.725</td><td>0.732</td><td>0.738</td><td>0.733</td><td>0.734</td><td>0.739</td><td>0.733</td><td>0.732</td><td>0.738</td><td>0.733</td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 2 Results of solving Max-Cut with different approaches on the GSET benchmark. <table><tr><td colspan=\"5\">Max-Cut</td></tr><tr><td>Methods</td><td>G14</td><td>G15</td><td>G22</td><td>G55</td></tr><tr><td>V</td><td>800</td><td>800</td><td>2000</td><td>5000</td></tr><tr><td>E</td><td>4694</td><td>4661</td><td>19990</td><td>12468</td></tr><tr><td>SDP</td><td>2922</td><td>2938</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>2991</td><td>3047</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>3064</td><td>3050</td><td>-</td><td>-</td></tr><tr><td>ECO-DQN</td><td>3056</td><td>3028</td><td>13268</td><td>10198</td></tr><tr><td>GMC-A</td><td>2940</td><td>2948</td><td>12978</td><td>10094</td></tr><tr><td>GMC-B</td><td>2918</td><td>2906</td><td>13246</td><td>10104</td></tr><tr><td>RUN-CSP</td><td>2943</td><td>2928</td><td>13028</td><td>10116</td></tr><tr><td>PI-GNN</td><td>3026</td><td>2990</td><td>13181</td><td>10138</td></tr><tr><td>MAX-SAT</td><td>3046</td><td>3012</td><td>13250</td><td>10204</td></tr><tr><td>Ours</td><td>3052</td><td>3018</td><td>13269</td><td>10208</td></tr></table> Table 3 Results of solving MIS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Maximum Independent Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Redu MIS(Solver)</td><td>30.0</td><td>39.4</td><td>48.8</td><td>57.4</td></tr><tr><td>Greedy</td><td>24.6</td><td>33.0</td><td>42.2</td><td>48.0</td></tr><tr><td>GMC-A</td><td>21.2</td><td>27.7</td><td>40.2</td><td>45.2</td></tr><tr><td>GMC-B</td><td>20.8</td><td>28.1</td><td>40.6</td><td>46.0</td></tr><tr><td>RUN-CSP</td><td>25.8</td><td>33.6</td><td>42.2</td><td>49.4</td></tr><tr><td>PIGNN</td><td>26.5</td><td>35.1</td><td>44.4</td><td>51.8</td></tr><tr><td>MAX-SAT</td><td>27.9</td><td>36.6</td><td>44.4</td><td>51.8</td></tr><tr><td>Ours</td><td>27.8</td><td>36.8</td><td>44.8</td><td>52.2</td></tr></table> ### 4.2 Performance of Solving COs In this section, we evaluated the effectiveness of the proposed framework and aimed to answer Q1. Without loss of generality, Max- Cut, MIS, and MDS are selected for evaluation. Our framework is also adaptive to other COs that can be transformed as Max- SAT. For Max- Cut, we first reported the mean \\(p\\) - values across 1,000 regular graphs with 100, 200, 500, and 1000 nodes for three levels of node degrees in Table 1. Then, the experiments on the GSET benchmark were carried out and the number of cuts was reported in Table 2 for comparison. When encountering large- scale datasets (G22 and G55), traditional methods, e.g., EO and BLS, were out of memory and could not obtain the results. Based on the two tables, we have the following observations: (1) Our method obtained the best results compared with learning- based methods, which demonstrated that the proposed framework exhibited the powerful ability to learn useful information from CO problems. The results also showed that Max- SAT can boost the performance of solving COs; (2) Our method was superior in most cases compared with all baselines. The advantages of our method were mainly in large- scale Max- Cut problems (e.g., \\(n = 800\\) , G22, and G55) where the performance surpassed traditional methods;\n\nTable 4 Results of solving MDS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Minimum Dominated Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Greedy</td><td>191.1</td><td>255.0</td><td>296.5</td><td>284.8</td></tr><tr><td>HTS-DS</td><td>191.0</td><td>255.2</td><td>297.6</td><td>286.4</td></tr><tr><td>MWDS-CRO</td><td>191.0</td><td>255.1</td><td>298.4</td><td>285.5</td></tr><tr><td>GMC-A</td><td>201.2</td><td>268.1</td><td>301.3</td><td>292.2</td></tr><tr><td>GMC-B</td><td>192.1</td><td>261.4</td><td>299.1</td><td>291.6</td></tr><tr><td>RUN-CSP</td><td>190.8</td><td>255.4</td><td>299.3</td><td>278.6</td></tr><tr><td>PIGNN</td><td>191.1</td><td>255.3</td><td>299.1</td><td>280.2</td></tr><tr><td>MAX-SAT</td><td>191.0</td><td>255.3</td><td>298.8</td><td>278.8</td></tr><tr><td>Ours</td><td>190.8</td><td>255.1</td><td>298.5</td><td>278.1</td></tr></table> (3) Traditional or heuristic methods are very competitive in Max-Cut. However, the gaps between our method and these methods were small. Besides, the time of training and inference for our method was much less than these methods. For MIS and MDS, we reported the number of sets for MIS and the number of dominated nodes for MDS in Tables 3 and 4. \\(V\\) and \\(E\\) are the sets for nodes and edges. Greedy denotes the classical greedy algorithm. Larger and smaller values denote better results for MIS and MDS, respectively. The experiments were carried out on four frb datasets. Similar results can be observed in these two CO problems. Particularly, In MIS, our results were relatively closer to the results obtained by the solver, which showed the effectiveness of our framework. Based on the results obtained on the above three CO problems, we can answer Q1: Max- SAT offered common knowledge and the pre- training and adaptation pipeline built based on Max- SAT can learn transferable and generalizable features to improve the ability to solve different CO problems on graphs. <center>Figure 3 Results of ablation study with different variants of our model on Max-Cut. </center>\n\n<center>Figure 4 Heat maps of knowledge transferability from three different CO problems to the maximum independent set problem on two scales of nodes. </center> ### 4.3 Ablation Study In this part, we conducted ablation studies on the Max- Cut problem to verify the effectiveness of each module in our framework. We gradually removed each single module and kept the other modules unchanged. The variants of our frameworks are listed as follows. \\(\\mathbf{w} / \\mathbf{o}\\) pre- T denotes the model that removes the pre- training stage but keeps the domain adaptation module. \\(\\mathbf{w} / \\mathbf{o}\\) DA denotes the model that removes the domain adaptation module but keeps the pre- training stage. \\(\\mathbf{w} / \\mathbf{o}\\) Bi- G is the model that directly uses GNNs to extract the features in the original graphs represented from CO problems without converting them to MAX- SAT and bipartite graphs. \\(\\mathbf{w} / \\mathbf{o}\\) Att is the model that keeps bipartite GNNs without the attention mechanism. \\(\\mathbf{w} / \\mathbf{o}\\) Mult- Att is the model that keeps bipartite GNNs without the multi- head mechanism. We reported the \\(p\\) - values for all variants in Figure 3 and had the following observations. (1) All modules boosted the performance and played important roles in our framework. The overall model obtained the best results, which demonstrated that these modules were nicely incorporated into our framework. (2) Removing the problem transfer step experienced the largest performance degradation, which proved the effectiveness of Max- SAT and also answered Q1. (3) Pre- training and adaptation were both useful in improving the performance. Since both steps utilized Max- SAT to assist the learning of transferable features, we can answer Q2 and conclude that the combination of different strategies to use Max- SAT can largely benefit solving CO problems. (4) Attention and multi- head mechanisms also affected the results, which showed that better feature extraction backbones indeed helped the learning methods to solve CO problems. ### 4.4 Effects of Different Knowledge Transfer Strategies In this subsection, we verify the knowledge transfer ability of our model. we fixed the target domain as the MIS problem and used Max- SAT, Max- Cut, and MDS as source domains to check different degrees of transferability. To be concrete, we fed the data from different source domains into our framework for pre- training and domain adaptation and observed how the performance was affected by the selection of domains. We selected two scales of nodes, i.e., 450 variables and 760 variables, for demonstration. The results were shown in Figure 4 where darker colors indicated better performance and transferability.\n\n<center>Figure 5 Parameter analysis for feature dimension \\(d\\) , number of attention heads \\(h\\) , weight of loss \\(\\lambda\\) , and number of layers \\(L\\) . </center> From the heat maps, we can observe that Max- SAT \\(\\rightarrow\\) MIS always obtained the best results compared with the other two types of knowledge transfer, which showed that Max- SAT was more general and had the strongest transferability across different CO problems. Moreover, MDS is a more relevant CO problem to MIS compared with Max- Cut. Therefore, we observed better results from MDS \\(\\rightarrow\\) MIS than Max- Cut \\(\\rightarrow\\) MIS. This observation answered Q3 where different source domains do affect the performance of the target domain. From a practical viewpoint, selecting an appropriate source domain was always hard, and irrelevant tasks would generate negative transfers and eventually affect the performance of the target domain. ### 4.5 Parameter Analysis In this subsection, we analyzed the influence of several significant hyper- parameters on the model performance: the dimension of features \\(d\\) , the number of attention heads \\(h\\) , the weight of losses \\(\\lambda\\) , and the number of pertaining layers \\(L\\) . We recorded the performance of different \\(d\\) , \\(h\\) , \\(\\lambda\\) , \\(L\\) values on Max- CUT. The results were reported in Figure 5. It can be observed that with different dimensions of features, our model exhibited relatively stable performance when \\(d\\) was greater than or equal to 16. Our\n\nmodel achieved optimal performance when \\(d\\) was 128. Our model was not significantly affected by the number of attention heads. It achieved optimal performance when \\(k\\) was 10. The optimal value of weight \\(\\lambda\\) was obtained in 0.2. Pre- training led to better results, especially when the number of layers exceeded three. ## 5 Conclusion In this study, we aimed to discover the transferability of deep learning architectures for solving different CO problems on graphs. To achieve this goal, we leveraged the Max- SAT problem as a general formulation of original CO problems. The key advantages were two- fold. First, Max- SAT served as a bridge for various CO problems and provided a tool for discovering common properties in CO problems. Second, Max- SAT problems were represented as clauses that included logical information in addition to the simple correlations in graphs. By further constructing a pre- training and adaptation framework, we can extract transferable features that can be used for various CO problems. Experiments showed that incorporating Max- SAT indeed improved the ability to solve various CO problems on graphs. Our current work can be extended from the following directions. The pre- training stage is built on supervised tasks where labeled data samples are required. As a future direction, it is necessary to develop unsupervised pre- training paradigms that can fully leverage information from the data itself. Moreover, advanced GNNs that are more suitable for the properties of CO problems should be designed in the future. Acknowledgements This work was supported by the National Natural Science Foundation of China (Grant No. 11991021, No. 11991020, No. 12271503). ## References 1 Petrica C Pop, Ovidiu Cosma, Cosmin Sabo, and Corina Pop Sitar. A comprehensive survey on the generalized traveling salesman problem. European Journal of Operational Research, 2023. 2 Nathan Krislock, J\u00e9r\u00f4me Malick, and Fr\u00e9d\u00e9ric Roupin. Improved semidefinite bounding procedure for solving max- cut problems to optimality. Mathematical Programming, 143:61- 86, 2014. 3 Yiyuan Wang, Shaowei Cai, Shiwei Pan, Ximing Li, and Monghao Yin. Reduction and local search for weighted graph coloring problem. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 2433- 2441, 2020. 4 Marc Los and Christian Lardinois. Combinatorial programming, statistical optimization and the optimal transportation network problem. Transportation Research Part B: Methodological, 16(2):89- 124, 1982. 5 Simone L Martins and Celso C Ribeiro. Metaheuristics and applications to optimization problems in telecommunications. Handbook of Optimization in Telecommunications, pages 103- 128, 2006. 6 Eugene L Lawler and David E Wood. Branch- and- bound methods: A survey. Operations Research, 14(4):699- 719, 1966. 7 Steven J Benson, Yinyu Ye, and Xiong Zhang. Solving large- scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000. 8 Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263- 1272. PMLR, 2017. 9 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. 10 Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016. 11 Quentin Cappart, Didier Ch\u00e9telat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 12 Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in Neural Information Processing Systems, 30, 2017. 13 Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in Neural Information Processing Systems, 31, 2018.\n\n14 Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019. 15 Tianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang, Howard Heaton, Jialin Liu, and Wotao Yin. Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research, 23(1):8562- 8620, 2022. 16 Matteo Fischetti, Paolo Toth, and Daniele Vigo. A branch- and- bound algorithm for the capacitated vehicle routing problem on directed graphs. Operations Research, 42(5):846- 859, 1994. 17 Panos M Pardalos and Gregory P Rodgers. A branch and bound algorithm for the maximum clique problem. Computers & Operations Research, 19(5):363- 375, 1992. 18 Richard Bellman. Dynamic programming. Science, 153(3731):34- 37, 1966. 19 Michael Held and Richard M Karp. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1):196- 210, 1962. 20 Silvano Martello, David Pisinger, and Paolo Toth. Dynamic programming and strong bounds for the 0- 1 knapsack problem. Management Science, 45(3):414- 424, 1999. 21 Laurence A Wolsey and George L Nemhauser. Integer and combinatorial optimization, volume 55. John Wiley & Sons, 1999. 22 Dimitris Bertsimas and Ramazan Demir. An approximate dynamic programming approach to multidimensional knapsack problems. Management Science, 48(4):550- 565, 2002. 23 Paul Bouman, Niels Agatz, and Marie Schmidt. Dynamic programming approaches for the traveling salesman problem with drone. Networks, 72(4):528- 542, 2018. 24 Mutsunori Yagiura and Toshihide Ibaraki. On metaheuristic algorithms for combinatorial optimization problems. Systems and Computers in Japan, 32(3):33- 55, 2001. 25 James B Orlin, Abraham P Punnen, and Andreas S Schulz. Approximate local search in combinatorial optimization. SIAM Journal on Computing, 33(5):1201- 1214, 2004. 26 Emile HL Aarts, Jan HM Korst, and Peter JM van Laarhoven. A quantitative analysis of the simulated annealing algorithm: A case study for the traveling salesman problem. Journal of Statistical Physics, 50:187- 206, 1988. 27 Jean Berger and Mohamed Barkaoui. A hybrid genetic algorithm for the capacitated vehicle routing problem. In Genetic and Evolutionary Computation Conference, pages 646- 656. Springer, 2003. 28 Andr\u00e9 Chassein, Marc Goerigk, Adam Kasperski, and Pawel Zieli\u0144ski. Approximating combinatorial optimization problems with the ordered weighted averaging criterion. European Journal of Operational Research, 286(3):828- 838, 2020. 29 Henrique Lemos, Marcelo Prates, Pedro Avelar, and Luis Lamb. Graph coloring meets deep learning: Effective graph neural network models for combinatorial problems. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, pages 879- 885. IEEE, 2019. 30 Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym- nco: Leveraging symmetricity for neural combinatorial optimization. Advances in Neural Information Processing Systems, 35:1936- 1949, 2022. 31 Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2018. 32 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in Neural Information Processing Systems, 30, 2017. 33 Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. 34 Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. Gppt: Graph pre- training and prompt tuning to generalize graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1717- 1727, 2022. 35 Zixuan Yi, Iadh Ounis, and Craig Macdonald. Contrastive graph prompt- tuning for cross- domain recommendation. ACM Transactions on Information Systems, 2023. 36 Cristian Bodnar, Francesco Di Giovanni, Benjamin Chamberlain, Pietro Li\u00f2, and Michael Bronstein. Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns. Advances in Neural Information Processing Systems, 35:18527- 18541, 2022. 37 Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein. On over- squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865- 7885. PMLR, 2023. 38 Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 39 Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. In The Eleventh International Conference on Learning Representations, 2022. 40 Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, and Marinka Zitnik. Evaluating explainability for graph neural\n\nnetworks. Scientific Data, 10(1):144, 2023. 41 Jin Yuan, Feng Hou, Yangzhou Du, Zhongchao Shi, Xin Geng, Jianping Fan, and Yong Rui. Self- supervised graph neural network for multi- source domain adaptation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3907- 3916, 2022. 42 Quanyu Dai, Xiao- Ming Wu, Jiarren Xiao, Xiao Shen, and Dan Wang. Graph transfer learning via adversarial domain adaptation with graph convolution. IEEE Transactions on Knowledge and Data Engineering, 35(5):4908- 4922, 2022. 43 Ronghang Zhu, Xiaodong Jiang, Jiasen Lu, and Sheng Li. Cross- domain graph convolutions for adversarial unsupervised domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 34(8):3847- 3858, 2023. 44 Xinhong Ma, Tianzhu Zhang, and Changsheng Xu. Gcan: Graph convolutional adversarial network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8266- 8276, 2019. 45 Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and Pan Li. Structural re- weighting improves graph domain adaptation. In International Conference on Machine Learning, pages 21778- 21793, 2023. 46 Mehmet Pilanci and Elif Vural. Domain adaptation on graphs by learning aligned graph bases. IEEE Transactions on Knowledge and Data Engineering, 34(2):587- 600, 2022. 47 Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Graph domain adaptation via theory- grounded spectral regularization. In International Conference on Learning Representations, 2023. 48 Carlos Ans\u00f3tegui, Maria Luisa Bonet, and Jordi Levy. Towards industrial- like random sat instances. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, page 387- 392, 2009. 49 Diogo V Andrade, Mauricio GC Resende, and Renato F Werneck. Fast local search for the maximum independent set problem. Journal of Heuristics, 18:525- 547, 2012. 50 Thomas A Feo, Mauricio GC Resende, and Stuart H Smith. A greedy randomized adaptive search procedure for maximum independent set. Operations Research, 42(5):860- 878, 1994. 51 Amir Dembo, Andrea Montanari, and Subhabrata Sen. Extremal cuts of sparse random graphs. The Annals of Probability, pages 1190- 1217, 2017. 52 Changhui Choi and Yinyu Ye. Solving sparse semidefinite programs using the dual scaling algorithm with an iterative solver. Department of Management Sciences, University of Iowa, Iowa City, IA, 52242, 2000. 53 Stefan Boettcher and Allon G Percus. Extremal optimization for graph partitioning. Physical Review E, 64(2):026114, 2001. 54 Una Benlic and Jin- Kao Hao. Breakout local search for the max- cutproblem. Engineering Applications of Artificial Intelligence, 26(3):1162- 1173, 2013. 55 Thomas Barrett, William Clements, Jakob Foerster, and Alex Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3243- 3250, 2020. 56 Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max- cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242- 251. SPIE, 2019. 57 Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence, 3:580607, 2021. 58 Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics- inspired graph neural networks. Nature Machine Intelligence, 4(4):367- 377, 2022. 59 Minghao Liu, Pei Huang, Fuqi Jia, Fan Zhang, Yuchen Sun, Shaowei Cai, Feifei Ma, and Jian Zhang. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 16264- 16265, 2023.",
      "level": 2,
      "line_start": 4,
      "line_end": 26
    },
    {
      "heading": "3.5 Inference with Local Search Method The last step of our model is to search the feasible solutions for CO problems with constraint conditions by using the predictions generated by our framework. We can also get close to optimal solutions with a search algorithm. For Max- Cut problems without hard clauses, our model can directly infer the truth assignment and obtain the target values. For problems with hard clauses, we introduce a heuristic local search method to discretely obtain feasible solutions that satisfy constraints and obtain the target values. Local search is a typical search method for discrete optimization. We adopt a 2- improvement local search algorithm [49,50] that iterates over all nodes in the graph and attempts to replace a 1- labeled node \\(v_{i}\\) with two 1- labeled nodes \\(v_{j}\\) and \\(v_{k}\\) . For the MIS problem, \\(v_{j}\\) and \\(v_{k}\\) must be the neighbors of \\(v_{i}\\) . These two nodes are 1- tight and are not connected. Here, a node is 1- tight if exactly one of its neighbors is 1- labeled. In other words, \\(v_{i}\\) is the only 1- labeled neighbor of \\(v_{j}\\) and \\(v_{k}\\) in the graph. Notice that this local search algorithm can find a valid 2- improvement in \\(O(E)\\) time if it exists. We introduce this method with fixed steps to find a feasible solution and evaluate the performance of our model. ### 3.6 Overall Algorithms Most of the previous works focus on modeling specific CO problems. In contrast, our method captures the common knowledge of COs and designs a unified learning method to utilize the knowledge to solve COs. To summarize, our framework has three essential processes. The problem transfer process leverages Max- SAT to bridge various COs. The COs are converted to Max- SAT with a general form that can capture logical information within CO problems. From the perspective of data transformation, the graphs in COs are firstly converted into clauses and then converted to bipartite graphs. The overall algorithm for problem transfer is listed in Algorithm 1. Algorithm 1 Problem Transfer via Max- SAT Input: A CO graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) Objective function: max \\(f(S)\\) Constraint condition: \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) Output: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) #From a graph to Max- SAT clauses# 1: Generate soft clauses \\(C_{s}\\) with objective function max \\(f(S)\\) 2: Generate hard clauses \\(C_{h}\\) with constraint condition \\(g_{i}(S)\\leqslant b_{i},b_{i}\\in \\Omega\\) #From Max- SAT clauses to a bipartite graph# 3: Construct nodes \\(\\tilde{\\nu}_{x}\\) and \\(\\tilde{\\nu}_{c}\\) for each variable \\(v_{i}\\) and clause \\(c_{j}\\) 4: Construct edges \\(e_{(v_{i},c_{j})}^{\\prime}\\in \\tilde{\\mathcal{E}},\\forall v_{i}\\in c_{j}\\) The pre- training process uses samples from Max- SAT to learn generalizable features that can benefit all COs. The networks used for pre- training include the MLP, feature extraction backbones based on GNNs, and the classification network. The overall algorithm for pre- training is listed in Algorithm 2. The fine- tuning process uses samples from Max- SAT and target CO to build a domain adaptation architecture. Based on the pre- trained network, the fine- tuning network introduces an additional discriminator for domain classification, which can learn domain- invariant features to further improve the generalizability of features. The overall algorithm for fine- tuning is listed in Algorithm 3. ## 4 Experimental Results In this section, we evaluated the effectiveness of the proposed framework for three CO problems on graphs, i.e., Max- Cut, MIS, and MDS. Generally, we aimed to answer three essential questions by the experiments. Q1: Can we leverage Max- SAT to learn transferable and generalizable features that can improve the ability to solve different CO problems on graphs? Q2: How can we incorporate Max- SAT",
      "content": "Input: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, True assignment \\(\\mathcal{V}\\) Number of pre- training layers \\(L\\) Number of iterations iters. Output: Pre- trained parameters \\(\\Theta_{p}\\) for MLP, Bip- GNN, and FC. 1: Initialize node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for nodes of variables and clauses; 4: for \\(l\\leqslant L\\) do 5: Extract features for clauses \\(\\mathbf{C}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 6: Extract features for variables \\(\\mathbf{X}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 7: end for 8: Map variable features to the predictions via \\(\\mathrm{FC}(\\mathbf{X}^{(L)})\\) 9: Compute the loss \\(\\mathcal{L}_{c}\\) in Eq. (3.14); 10: Backward and update parameters; 11: end for 12: Obtain the updated parameters \\(\\Theta_{p}\\) Algorithm 3 Fine- tuning Based on Domain Adaptation Input: A bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) from Max- SAT, Number of fine- tuning layers \\(L\\) Number of iterations iters, True assignment \\(\\mathcal{V}\\) Output: Predicted feasible solutions \\(\\mathcal{P} = \\{p_{i}\\}_{i = 1}^{n}\\) Fine- tuned parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, FC, and Dis. 1: Initialize node attributes of both domains \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) 2: for iter \\(\\leqslant\\) iters do 3: Obtain features \\(\\mathbf{X}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{S}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for source variables and clauses; 4: Obtain features \\(\\mathbf{X}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{X}_{ini})\\) and \\(\\mathbf{C}_{T}^{(0)} = \\mathrm{MLP}(\\mathbf{C}_{ini})\\) for target variables and clauses; 5: for \\(l\\leqslant L\\) do 6: Extract features for clauses \\(\\mathbf{C}_{S}^{(l)}\\) and \\(\\mathbf{C}_{T}^{(l)}\\) via clause- wise aggregation Eqs. (3.10) and (3.11); 7: Extract features for variables \\(\\mathbf{X}_{S}^{(l)}\\) and \\(\\mathbf{X}_{T}^{(l)}\\) via variable- wise aggregation Eqs. (3.12) and (3.13); 8: end for 9: Map variable features to the assignment labels via \\(\\mathrm{FC}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{FC}(\\mathbf{X}_{T}^{(L)})\\) 10: Map variable features to the domain labels via \\(\\mathrm{Dis}(\\mathbf{X}_{S}^{(L)})\\) and \\(\\mathrm{Dis}(\\mathbf{X}_{T}^{(L)})\\) 11: Compute the loss \\(\\mathcal{L}_{ft}\\) in Eq. (3.15); 12: Updated parameters \\(\\Theta_{ft}\\) for MLP, Bip- GNN, Dis, and FC; 13: end for 14: Obtain the optimal parameters \\(\\Theta_{ft}\\) 15: Predict feasible solutions \\(\\mathcal{P}\\) into the pipelines of GNN- based learning frameworks to fully utilize the information of Max- SAT? Q3: Do different pre- training and domain adaptation strategies matter for the ability of solving COs? ### 4.1 Experimental Setup Datasets We introduce the datasets that were used in our experiments in this part. In the pretraining stage, a large number of Max- SAT instances were required. We generated the clauses by running three representative generators with different distributions: the uniform distribution, the power- law distribution, and the double power- law distribution. A total number of 20,000 Max- SAT instances\n\nwere generated and the Max HS solver was used to obtain the solutions that served as labels for training. In the fine- tuning stage, according to the CO problems, different datasets are leveraged. For Max- Cut, we introduce the GSET benchmark \\(^{1)}\\) for evaluation. GSET is a set of 71 unweighted graphs that were commonly used for testing the algorithms to solve the Max- Cut problem. For MIS and MDS problems, we introduce the frb benchmark with four different instance scales for evaluation. Each frb dataset consists of five instances of hard CO problems and is regarded as the benchmark for CSP competition annually. Evaluation Metrics For different CO problems, we used the corresponding evaluation metrics based on their feasible solutions. For Max- Cut, \\(p\\) values [51] were used to evaluate the number of graph cuts. Concretely, \\(p\\) can be calculated by \\[p(z) = \\frac{z / n - \\gamma / 4}{\\sqrt{\\gamma / 4}}, \\quad (4.1)\\] where \\(z\\) is the predicted cut size for a \\(\\gamma\\) - regular graph with \\(n\\) nodes and \\(\\gamma\\) is the degree of nodes. For MIS and MDS, the number of cuts or sets is used for evaluation. Baseline methods To evaluate our method, we compare it with several baseline methods that have been widely used in solving COs. These methods can be roughly classified as traditional methods, heuristic methods, and learning- based methods. SDP [52] is a classical approach based on semi- definite programming. EO [53] is the extremal optimization method. BLS [54] is the breakout local search method. ECO- DQN [55] is an approach of exploratory CO with reinforcement learning. GMC- A and GMC- B [56] are two unsupervised GNN architectures for Max- CUT with different loss functions. RUN- CSP [57] is a recurrent unsupervised neural network for constraint satisfaction problems. PIGNN [58] is the physics- inspired GNNs for CO problems. MAXSAT [59] is a GNN- based framework designed for Max- SAT problems. Experimental Settings The proposed framework was implemented in Python with Py Torch. We summarized the hyper- parameters used in our paper as follows. The dimension of features \\(d\\) was set as 128. The number of GNN layers \\(L\\) for pre- training and fine- tuning was set as 5 and the first 2 layers were fixed during the fine- tuning. The number of layers for MLP was set as 2. The number of epochs for pre- training and fine- tuning were tuned and set as 400 and 100. The Adam optimizer was used for model training with a learning rate of \\(10^{- 5}\\) and a weight decay of \\(10^{- 10}\\) . The warm- up trick was also adopted. The number of FC layers for classification was set as 2 and the discriminator used a two- layer fully connected network to generate domain labels. The steps for local search for MIS and MDS were set as 120. All experiments were conducted on a workstation equipped with an Intel Xeon(R) Gold 6139M CPU 515 @ 2.30GHz and an NVIDIA RTX 3090 GPU with 24GB. Table 1 Results of solving Max-Cut with different approaches on random graphs. <table><tr><td rowspan=\"2\">Methods</td><td colspan=\"4\">n=100</td><td colspan=\"4\">n=200</td><td colspan=\"4\">n=500</td><td colspan=\"4\">n=800</td><td colspan=\"4\">n=1000</td></tr><tr><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td>\u03b3=3</td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td>\u03b3=10</td><td></td><td>\u03b3=5</td><td></td><td>\u03b3=10</td></tr><tr><td>SDP</td><td>0.709</td><td>0.697</td><td>0.689</td><td>0.709</td><td>0.702</td><td>0.692</td><td>0.702</td><td>0.690</td><td>0.682</td><td>0.701</td><td>0.688</td><td>0.679</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>0.712</td><td>0.708</td><td>0.703</td><td>0.721</td><td>0.723</td><td>0.724</td><td>0.727</td><td>0.737</td><td>0.735</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>0.712</td><td>0.707</td><td>0.704</td><td>0.720</td><td>0.721</td><td>0.719</td><td>0.722</td><td>0.725</td><td>0.721</td><td>0.720</td><td>0.726</td><td>0.717</td><td>0.721</td><td>0.725</td><td>0.720</td><td></td><td></td><td></td><td></td></tr><tr><td>ECO-DQN</td><td>0.713</td><td>0.705</td><td>0.707</td><td>0.718</td><td>0.723</td><td>0.720</td><td>0.725</td><td>0.727</td><td>0.725</td><td>0.722</td><td>0.721</td><td>0.722</td><td>0.726</td><td>0.726</td><td>0.721</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-A</td><td>0.691</td><td>0.655</td><td>0.637</td><td>0.701</td><td>0.683</td><td>0.660</td><td>0.693</td><td>0.668</td><td>0.599</td><td>0.691</td><td>0.666</td><td>0.602</td><td>0.688</td><td>0.662</td><td>0.601</td><td></td><td></td><td></td><td></td></tr><tr><td>GMC-B</td><td>0.698</td><td>0.660</td><td>0.630</td><td>0.708</td><td>0.667</td><td>0.646</td><td>0.707</td><td>0.701</td><td>0.670</td><td>0.699</td><td>0.696</td><td>0.653</td><td>0.702</td><td>0.694</td><td>0.651</td><td></td><td></td><td></td><td></td></tr><tr><td>RUN-CSP</td><td>0.702</td><td>0.706</td><td>0.704</td><td>0.711</td><td>0.715</td><td>0.714</td><td>0.714</td><td>0.726</td><td>0.710</td><td>0.704</td><td>0.713</td><td>0.705</td><td>0.705</td><td>0.711</td><td>0.702</td><td></td><td></td><td></td><td></td></tr><tr><td>PI-GNN</td><td>0.704</td><td>0.706</td><td>0.708</td><td>0.712</td><td>0.718</td><td>0.717</td><td>0.715</td><td>0.726</td><td>0.716</td><td>0.711</td><td>0.724</td><td>0.712</td><td>0.709</td><td>0.722</td><td>0.712</td><td></td><td></td><td></td><td></td></tr><tr><td>MAX-SAT</td><td>0.702</td><td>0.705</td><td>0.708</td><td>0.712</td><td>0.716</td><td>0.718</td><td>0.721</td><td>0.732</td><td>0.726</td><td>0.719</td><td>0.730</td><td>0.728</td><td>0.718</td><td>0.730</td><td>0.726</td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>0.702</td><td>0.708</td><td>0.707</td><td>0.722</td><td>0.721</td><td>0.725</td><td>0.732</td><td>0.738</td><td>0.733</td><td>0.734</td><td>0.739</td><td>0.733</td><td>0.732</td><td>0.738</td><td>0.733</td><td></td><td></td><td></td><td></td></tr></table>\n\nTable 2 Results of solving Max-Cut with different approaches on the GSET benchmark. <table><tr><td colspan=\"5\">Max-Cut</td></tr><tr><td>Methods</td><td>G14</td><td>G15</td><td>G22</td><td>G55</td></tr><tr><td>V</td><td>800</td><td>800</td><td>2000</td><td>5000</td></tr><tr><td>E</td><td>4694</td><td>4661</td><td>19990</td><td>12468</td></tr><tr><td>SDP</td><td>2922</td><td>2938</td><td>-</td><td>-</td></tr><tr><td>EO</td><td>2991</td><td>3047</td><td>-</td><td>-</td></tr><tr><td>BLS</td><td>3064</td><td>3050</td><td>-</td><td>-</td></tr><tr><td>ECO-DQN</td><td>3056</td><td>3028</td><td>13268</td><td>10198</td></tr><tr><td>GMC-A</td><td>2940</td><td>2948</td><td>12978</td><td>10094</td></tr><tr><td>GMC-B</td><td>2918</td><td>2906</td><td>13246</td><td>10104</td></tr><tr><td>RUN-CSP</td><td>2943</td><td>2928</td><td>13028</td><td>10116</td></tr><tr><td>PI-GNN</td><td>3026</td><td>2990</td><td>13181</td><td>10138</td></tr><tr><td>MAX-SAT</td><td>3046</td><td>3012</td><td>13250</td><td>10204</td></tr><tr><td>Ours</td><td>3052</td><td>3018</td><td>13269</td><td>10208</td></tr></table> Table 3 Results of solving MIS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Maximum Independent Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Redu MIS(Solver)</td><td>30.0</td><td>39.4</td><td>48.8</td><td>57.4</td></tr><tr><td>Greedy</td><td>24.6</td><td>33.0</td><td>42.2</td><td>48.0</td></tr><tr><td>GMC-A</td><td>21.2</td><td>27.7</td><td>40.2</td><td>45.2</td></tr><tr><td>GMC-B</td><td>20.8</td><td>28.1</td><td>40.6</td><td>46.0</td></tr><tr><td>RUN-CSP</td><td>25.8</td><td>33.6</td><td>42.2</td><td>49.4</td></tr><tr><td>PIGNN</td><td>26.5</td><td>35.1</td><td>44.4</td><td>51.8</td></tr><tr><td>MAX-SAT</td><td>27.9</td><td>36.6</td><td>44.4</td><td>51.8</td></tr><tr><td>Ours</td><td>27.8</td><td>36.8</td><td>44.8</td><td>52.2</td></tr></table> ### 4.2 Performance of Solving COs In this section, we evaluated the effectiveness of the proposed framework and aimed to answer Q1. Without loss of generality, Max- Cut, MIS, and MDS are selected for evaluation. Our framework is also adaptive to other COs that can be transformed as Max- SAT. For Max- Cut, we first reported the mean \\(p\\) - values across 1,000 regular graphs with 100, 200, 500, and 1000 nodes for three levels of node degrees in Table 1. Then, the experiments on the GSET benchmark were carried out and the number of cuts was reported in Table 2 for comparison. When encountering large- scale datasets (G22 and G55), traditional methods, e.g., EO and BLS, were out of memory and could not obtain the results. Based on the two tables, we have the following observations: (1) Our method obtained the best results compared with learning- based methods, which demonstrated that the proposed framework exhibited the powerful ability to learn useful information from CO problems. The results also showed that Max- SAT can boost the performance of solving COs; (2) Our method was superior in most cases compared with all baselines. The advantages of our method were mainly in large- scale Max- Cut problems (e.g., \\(n = 800\\) , G22, and G55) where the performance surpassed traditional methods;\n\nTable 4 Results of solving MDS with different approaches on frb datasets. <table><tr><td colspan=\"5\">Minimum Dominated Set</td></tr><tr><td>Methods</td><td>frb30-15</td><td>frb40-19</td><td>frb50-23</td><td>frb59-26</td></tr><tr><td>V</td><td>450</td><td>760</td><td>1150</td><td>1534</td></tr><tr><td>E</td><td>18k</td><td>41k</td><td>80k</td><td>126k</td></tr><tr><td>Greedy</td><td>191.1</td><td>255.0</td><td>296.5</td><td>284.8</td></tr><tr><td>HTS-DS</td><td>191.0</td><td>255.2</td><td>297.6</td><td>286.4</td></tr><tr><td>MWDS-CRO</td><td>191.0</td><td>255.1</td><td>298.4</td><td>285.5</td></tr><tr><td>GMC-A</td><td>201.2</td><td>268.1</td><td>301.3</td><td>292.2</td></tr><tr><td>GMC-B</td><td>192.1</td><td>261.4</td><td>299.1</td><td>291.6</td></tr><tr><td>RUN-CSP</td><td>190.8</td><td>255.4</td><td>299.3</td><td>278.6</td></tr><tr><td>PIGNN</td><td>191.1</td><td>255.3</td><td>299.1</td><td>280.2</td></tr><tr><td>MAX-SAT</td><td>191.0</td><td>255.3</td><td>298.8</td><td>278.8</td></tr><tr><td>Ours</td><td>190.8</td><td>255.1</td><td>298.5</td><td>278.1</td></tr></table> (3) Traditional or heuristic methods are very competitive in Max-Cut. However, the gaps between our method and these methods were small. Besides, the time of training and inference for our method was much less than these methods. For MIS and MDS, we reported the number of sets for MIS and the number of dominated nodes for MDS in Tables 3 and 4. \\(V\\) and \\(E\\) are the sets for nodes and edges. Greedy denotes the classical greedy algorithm. Larger and smaller values denote better results for MIS and MDS, respectively. The experiments were carried out on four frb datasets. Similar results can be observed in these two CO problems. Particularly, In MIS, our results were relatively closer to the results obtained by the solver, which showed the effectiveness of our framework. Based on the results obtained on the above three CO problems, we can answer Q1: Max- SAT offered common knowledge and the pre- training and adaptation pipeline built based on Max- SAT can learn transferable and generalizable features to improve the ability to solve different CO problems on graphs. <center>Figure 3 Results of ablation study with different variants of our model on Max-Cut. </center>\n\n<center>Figure 4 Heat maps of knowledge transferability from three different CO problems to the maximum independent set problem on two scales of nodes. </center> ### 4.3 Ablation Study In this part, we conducted ablation studies on the Max- Cut problem to verify the effectiveness of each module in our framework. We gradually removed each single module and kept the other modules unchanged. The variants of our frameworks are listed as follows. \\(\\mathbf{w} / \\mathbf{o}\\) pre- T denotes the model that removes the pre- training stage but keeps the domain adaptation module. \\(\\mathbf{w} / \\mathbf{o}\\) DA denotes the model that removes the domain adaptation module but keeps the pre- training stage. \\(\\mathbf{w} / \\mathbf{o}\\) Bi- G is the model that directly uses GNNs to extract the features in the original graphs represented from CO problems without converting them to MAX- SAT and bipartite graphs. \\(\\mathbf{w} / \\mathbf{o}\\) Att is the model that keeps bipartite GNNs without the attention mechanism. \\(\\mathbf{w} / \\mathbf{o}\\) Mult- Att is the model that keeps bipartite GNNs without the multi- head mechanism. We reported the \\(p\\) - values for all variants in Figure 3 and had the following observations. (1) All modules boosted the performance and played important roles in our framework. The overall model obtained the best results, which demonstrated that these modules were nicely incorporated into our framework. (2) Removing the problem transfer step experienced the largest performance degradation, which proved the effectiveness of Max- SAT and also answered Q1. (3) Pre- training and adaptation were both useful in improving the performance. Since both steps utilized Max- SAT to assist the learning of transferable features, we can answer Q2 and conclude that the combination of different strategies to use Max- SAT can largely benefit solving CO problems. (4) Attention and multi- head mechanisms also affected the results, which showed that better feature extraction backbones indeed helped the learning methods to solve CO problems. ### 4.4 Effects of Different Knowledge Transfer Strategies In this subsection, we verify the knowledge transfer ability of our model. we fixed the target domain as the MIS problem and used Max- SAT, Max- Cut, and MDS as source domains to check different degrees of transferability. To be concrete, we fed the data from different source domains into our framework for pre- training and domain adaptation and observed how the performance was affected by the selection of domains. We selected two scales of nodes, i.e., 450 variables and 760 variables, for demonstration. The results were shown in Figure 4 where darker colors indicated better performance and transferability.\n\n<center>Figure 5 Parameter analysis for feature dimension \\(d\\) , number of attention heads \\(h\\) , weight of loss \\(\\lambda\\) , and number of layers \\(L\\) . </center> From the heat maps, we can observe that Max- SAT \\(\\rightarrow\\) MIS always obtained the best results compared with the other two types of knowledge transfer, which showed that Max- SAT was more general and had the strongest transferability across different CO problems. Moreover, MDS is a more relevant CO problem to MIS compared with Max- Cut. Therefore, we observed better results from MDS \\(\\rightarrow\\) MIS than Max- Cut \\(\\rightarrow\\) MIS. This observation answered Q3 where different source domains do affect the performance of the target domain. From a practical viewpoint, selecting an appropriate source domain was always hard, and irrelevant tasks would generate negative transfers and eventually affect the performance of the target domain. ### 4.5 Parameter Analysis In this subsection, we analyzed the influence of several significant hyper- parameters on the model performance: the dimension of features \\(d\\) , the number of attention heads \\(h\\) , the weight of losses \\(\\lambda\\) , and the number of pertaining layers \\(L\\) . We recorded the performance of different \\(d\\) , \\(h\\) , \\(\\lambda\\) , \\(L\\) values on Max- CUT. The results were reported in Figure 5. It can be observed that with different dimensions of features, our model exhibited relatively stable performance when \\(d\\) was greater than or equal to 16. Our\n\nmodel achieved optimal performance when \\(d\\) was 128. Our model was not significantly affected by the number of attention heads. It achieved optimal performance when \\(k\\) was 10. The optimal value of weight \\(\\lambda\\) was obtained in 0.2. Pre- training led to better results, especially when the number of layers exceeded three. ## 5 Conclusion In this study, we aimed to discover the transferability of deep learning architectures for solving different CO problems on graphs. To achieve this goal, we leveraged the Max- SAT problem as a general formulation of original CO problems. The key advantages were two- fold. First, Max- SAT served as a bridge for various CO problems and provided a tool for discovering common properties in CO problems. Second, Max- SAT problems were represented as clauses that included logical information in addition to the simple correlations in graphs. By further constructing a pre- training and adaptation framework, we can extract transferable features that can be used for various CO problems. Experiments showed that incorporating Max- SAT indeed improved the ability to solve various CO problems on graphs. Our current work can be extended from the following directions. The pre- training stage is built on supervised tasks where labeled data samples are required. As a future direction, it is necessary to develop unsupervised pre- training paradigms that can fully leverage information from the data itself. Moreover, advanced GNNs that are more suitable for the properties of CO problems should be designed in the future. Acknowledgements This work was supported by the National Natural Science Foundation of China (Grant No. 11991021, No. 11991020, No. 12271503). ## References 1 Petrica C Pop, Ovidiu Cosma, Cosmin Sabo, and Corina Pop Sitar. A comprehensive survey on the generalized traveling salesman problem. European Journal of Operational Research, 2023. 2 Nathan Krislock, J\u00e9r\u00f4me Malick, and Fr\u00e9d\u00e9ric Roupin. Improved semidefinite bounding procedure for solving max- cut problems to optimality. Mathematical Programming, 143:61- 86, 2014. 3 Yiyuan Wang, Shaowei Cai, Shiwei Pan, Ximing Li, and Monghao Yin. Reduction and local search for weighted graph coloring problem. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 2433- 2441, 2020. 4 Marc Los and Christian Lardinois. Combinatorial programming, statistical optimization and the optimal transportation network problem. Transportation Research Part B: Methodological, 16(2):89- 124, 1982. 5 Simone L Martins and Celso C Ribeiro. Metaheuristics and applications to optimization problems in telecommunications. Handbook of Optimization in Telecommunications, pages 103- 128, 2006. 6 Eugene L Lawler and David E Wood. Branch- and- bound methods: A survey. Operations Research, 14(4):699- 719, 1966. 7 Steven J Benson, Yinyu Ye, and Xiong Zhang. Solving large- scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443- 461, 2000. 8 Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263- 1272. PMLR, 2017. 9 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. 10 Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2016. 11 Quentin Cappart, Didier Ch\u00e9telat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 12 Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in Neural Information Processing Systems, 30, 2017. 13 Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in Neural Information Processing Systems, 31, 2018.\n\n14 Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019. 15 Tianlong Chen, Xiaohan Chen, Wuyang Chen, Zhangyang Wang, Howard Heaton, Jialin Liu, and Wotao Yin. Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research, 23(1):8562- 8620, 2022. 16 Matteo Fischetti, Paolo Toth, and Daniele Vigo. A branch- and- bound algorithm for the capacitated vehicle routing problem on directed graphs. Operations Research, 42(5):846- 859, 1994. 17 Panos M Pardalos and Gregory P Rodgers. A branch and bound algorithm for the maximum clique problem. Computers & Operations Research, 19(5):363- 375, 1992. 18 Richard Bellman. Dynamic programming. Science, 153(3731):34- 37, 1966. 19 Michael Held and Richard M Karp. A dynamic programming approach to sequencing problems. Journal of the Society for Industrial and Applied Mathematics, 10(1):196- 210, 1962. 20 Silvano Martello, David Pisinger, and Paolo Toth. Dynamic programming and strong bounds for the 0- 1 knapsack problem. Management Science, 45(3):414- 424, 1999. 21 Laurence A Wolsey and George L Nemhauser. Integer and combinatorial optimization, volume 55. John Wiley & Sons, 1999. 22 Dimitris Bertsimas and Ramazan Demir. An approximate dynamic programming approach to multidimensional knapsack problems. Management Science, 48(4):550- 565, 2002. 23 Paul Bouman, Niels Agatz, and Marie Schmidt. Dynamic programming approaches for the traveling salesman problem with drone. Networks, 72(4):528- 542, 2018. 24 Mutsunori Yagiura and Toshihide Ibaraki. On metaheuristic algorithms for combinatorial optimization problems. Systems and Computers in Japan, 32(3):33- 55, 2001. 25 James B Orlin, Abraham P Punnen, and Andreas S Schulz. Approximate local search in combinatorial optimization. SIAM Journal on Computing, 33(5):1201- 1214, 2004. 26 Emile HL Aarts, Jan HM Korst, and Peter JM van Laarhoven. A quantitative analysis of the simulated annealing algorithm: A case study for the traveling salesman problem. Journal of Statistical Physics, 50:187- 206, 1988. 27 Jean Berger and Mohamed Barkaoui. A hybrid genetic algorithm for the capacitated vehicle routing problem. In Genetic and Evolutionary Computation Conference, pages 646- 656. Springer, 2003. 28 Andr\u00e9 Chassein, Marc Goerigk, Adam Kasperski, and Pawel Zieli\u0144ski. Approximating combinatorial optimization problems with the ordered weighted averaging criterion. European Journal of Operational Research, 286(3):828- 838, 2020. 29 Henrique Lemos, Marcelo Prates, Pedro Avelar, and Luis Lamb. Graph coloring meets deep learning: Effective graph neural network models for combinatorial problems. In 2019 IEEE 31st International Conference on Tools with Artificial Intelligence, pages 879- 885. IEEE, 2019. 30 Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym- nco: Leveraging symmetricity for neural combinatorial optimization. Advances in Neural Information Processing Systems, 35:1936- 1949, 2022. 31 Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2018. 32 Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in Neural Information Processing Systems, 30, 2017. 33 Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. 34 Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. Gppt: Graph pre- training and prompt tuning to generalize graph neural networks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1717- 1727, 2022. 35 Zixuan Yi, Iadh Ounis, and Craig Macdonald. Contrastive graph prompt- tuning for cross- domain recommendation. ACM Transactions on Information Systems, 2023. 36 Cristian Bodnar, Francesco Di Giovanni, Benjamin Chamberlain, Pietro Li\u00f2, and Michael Bronstein. Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns. Advances in Neural Information Processing Systems, 35:18527- 18541, 2022. 37 Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein. On over- squashing in message passing neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pages 7865- 7885. PMLR, 2023. 38 Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 39 Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. In The Eleventh International Conference on Learning Representations, 2022. 40 Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, and Marinka Zitnik. Evaluating explainability for graph neural\n\nnetworks. Scientific Data, 10(1):144, 2023. 41 Jin Yuan, Feng Hou, Yangzhou Du, Zhongchao Shi, Xin Geng, Jianping Fan, and Yong Rui. Self- supervised graph neural network for multi- source domain adaptation. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3907- 3916, 2022. 42 Quanyu Dai, Xiao- Ming Wu, Jiarren Xiao, Xiao Shen, and Dan Wang. Graph transfer learning via adversarial domain adaptation with graph convolution. IEEE Transactions on Knowledge and Data Engineering, 35(5):4908- 4922, 2022. 43 Ronghang Zhu, Xiaodong Jiang, Jiasen Lu, and Sheng Li. Cross- domain graph convolutions for adversarial unsupervised domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 34(8):3847- 3858, 2023. 44 Xinhong Ma, Tianzhu Zhang, and Changsheng Xu. Gcan: Graph convolutional adversarial network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8266- 8276, 2019. 45 Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and Pan Li. Structural re- weighting improves graph domain adaptation. In International Conference on Machine Learning, pages 21778- 21793, 2023. 46 Mehmet Pilanci and Elif Vural. Domain adaptation on graphs by learning aligned graph bases. IEEE Transactions on Knowledge and Data Engineering, 34(2):587- 600, 2022. 47 Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. Graph domain adaptation via theory- grounded spectral regularization. In International Conference on Learning Representations, 2023. 48 Carlos Ans\u00f3tegui, Maria Luisa Bonet, and Jordi Levy. Towards industrial- like random sat instances. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, page 387- 392, 2009. 49 Diogo V Andrade, Mauricio GC Resende, and Renato F Werneck. Fast local search for the maximum independent set problem. Journal of Heuristics, 18:525- 547, 2012. 50 Thomas A Feo, Mauricio GC Resende, and Stuart H Smith. A greedy randomized adaptive search procedure for maximum independent set. Operations Research, 42(5):860- 878, 1994. 51 Amir Dembo, Andrea Montanari, and Subhabrata Sen. Extremal cuts of sparse random graphs. The Annals of Probability, pages 1190- 1217, 2017. 52 Changhui Choi and Yinyu Ye. Solving sparse semidefinite programs using the dual scaling algorithm with an iterative solver. Department of Management Sciences, University of Iowa, Iowa City, IA, 52242, 2000. 53 Stefan Boettcher and Allon G Percus. Extremal optimization for graph partitioning. Physical Review E, 64(2):026114, 2001. 54 Una Benlic and Jin- Kao Hao. Breakout local search for the max- cutproblem. Engineering Applications of Artificial Intelligence, 26(3):1162- 1173, 2013. 55 Thomas Barrett, William Clements, Jakob Foerster, and Alex Lvovsky. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3243- 3250, 2020. 56 Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural networks on random instances of max- cut. In Wavelets and Sparsity XVIII, volume 11138, pages 242- 251. SPIE, 2019. 57 Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Graph neural networks for maximum constraint satisfaction. Frontiers in Artificial Intelligence, 3:580607, 2021. 58 Martin JA Schuetz, J Kyle Brubaker, and Helmut G Katzgraber. Combinatorial optimization with physics- inspired graph neural networks. Nature Machine Intelligence, 4(4):367- 377, 2022. 59 Minghao Liu, Pei Huang, Fuqi Jia, Fan Zhang, Yuchen Sun, Shaowei Cai, Feifei Ma, and Jian Zhang. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 16264- 16265, 2023.",
      "level": 3,
      "line_start": 7,
      "line_end": 26
    },
    {
      "heading": "1 Introduction Combinatorial optimization (CO) is a multidisciplinary field ranging from optimization to artificial intelligence. The goal of CO is to find optimal solutions from finite possible options where exhaustive enumeration cannot be accomplished. Graphs are the most common research objects in CO. Typical CO problems on graphs include traveling salesman problems (TSP) [1], Max- Cut problems [2], and graph coloring problems [3]. Solving CO on graphs can benefit many real- world applications, such as transportation logistics [4] and telecommunications [5]. Solving COs on graphs is extremely difficult due to their discrete property and high computational costs. For many decades, researchers have made many efforts to develop various algorithms to solve COs on graphs. For example, branch and bound [6] is a traditional exact algorithm that gradually divides the solution space into subspaces and uses bounds to prune possible solutions that exceed the bounds. Unfortunately, these traditional exact algorithms encounter excessive computational burdens as the scales of problems increase [7]. Recently, deep learning has developed as an effective tool for COs, inspiring the emergence of the category of learning- based methods for solving COs. Particularly, graph neural networks (GNNs) [8- 10] are advanced deep learning models designed for graph- structure data, which have been successfully applied to COs on graphs [11- 13]. In general, the graphs in CO problems are first fed into GNNs so that the high- level features can be extracted. Then, the predictions generated from features serve as feasible solutions or can be further used to guide the searching process of the heuristic learning process [14, 15]. Learning- based methods extract more useful knowledge to reduce the search space, which can be used to effectively and efficiently solve COs on graphs. Besides, the data- driven manner is more adaptive to real- world problems when real data is available. However, current GNN models for CO problems still face several challenges. First, directly modeling CO with graphs is intuitive but not necessarily accurate. Although graphs can capture relationships between objects, they ignore the mathematical logicality and properties inside COs. Therefore, constructing learning methods over original graphs often results in insufficient problem- solving abilities. Second, current methods are usually designed to solve a single CO problem on graphs. Nevertheless, different COs on graphs may share the same graphs and leverage similar GNN backbones for feature extraction. It is quite beneficial to explore whether the transferability of knowledge exists among different problems. Moreover, for real- world problems on graphs, the data collected for training GNNs is often limited, it is essential to leverage data from other problems that may have common knowledge to improve the generalization ability of GNNs. To address the above issues, we propose in this paper a unified pre- training and adaptation framework for COs on graphs. The core idea is to develop a generalizable and transferable GNN framework from which all COs can be benefited. To achieve this goal, it is necessary to find out the common knowledge and properties of all CO problems on graphs that can be extracted by GNNs and used for different problems. We notice that using graphs to formulate CO omits logical information that is usually more general and common for all problems. Inspired by this observation, we propose to leverage the maximum satisfiability problems (Max- SAT) to bridge different CO problems on graphs. Max- SAT is a classic problem containing propositional logic formulas that can be used to describe additional logic information beyond simple relations on graphs. With this unified representation, we further adopt a pre- training process and a fine- tuning process based on domain adaptation to extract general features and utilize the transferability between different problems. Our framework can be adaptive to all CO problems that can be transferred to Max- SAT problems and is also suitable for various GNN architectures, which exhibit superior flexibility and versatility. To be concrete, we first use the Max- SAT problem as an intermediate problem by which we can transform graphs from different COs to clauses and formulas. We can construct bipartite graphs from these clauses in Max- SAT that not only capture logical information but also can be adaptive to different COs on graphs. In the training stage, we can generate abundant clauses from certain Max- SAT problems without considering the original CO and transform them into bipartite graphs for pre- training. Due to the generic nature of Max- SAT, the pre- trained model is equipped with better initialization. Then, we",
      "content": "use the original CO and Max- SAT to construct an adaptation framework with adversarial training for fine- tuning, which can further utilize the transferable information in Max- SAT. In both the pre- training and fine- tuning stages, the graph attention networks are leveraged as backbones to extract information in bipartite graphs transformed from COs on graphs and Max- SAT. The bipartite graph attention networks separately consider the aggregation of clauses and variables, which can better capture the dependencies between variables and clauses with different neighbor aggregation processes. To summarize, the contributions of this work are as follows: (1) We propose a unified pre-training and adaptation framework based on Max-SAT that can learn generalizable and transferable information to benefit the solving of COs on graphs. (2) We propose to use Max-SAT to bridge various COs on graphs, by which all problems can be represented as a unified form. The Max-SAT instances also carry additional logical relationships that can be further utilized by GNNs. (3) We design a pre-training and domain adaptation architecture to extract generalizable and transferable features based on instances generated from the Max-SAT and COs on graphs. This framework is versatile to various COs and can be used to boost the ability to solve these problems. (4) We evaluate our method on both synthetic datasets and open benchmarks. The numerical results demonstrated the effectiveness of our framework in solving COs on graphs. The remainder of this paper is organized as follows. Section 2 presents the learning methods for COs and popular GNN architectures for feature extraction on graphs. Section 3 introduces the problem transfer, pre-training, and fine-tuning procedures in our framework and summarizes the overall algorithms for training. The experiments that aim to address our three main claims are listed in Section 4. We conclude our work in Section 5. ## 2 Related Work ### 2.1 Learning Methods for Solving CO Problems Traditional methods for solving CO mainly include branch and bound, dynamic programming, and heuristic algorithms. Branch and bound is a branch- and- prune search method used to reduce the search space [6, 16, 17]. It progressively decomposes the problem into smaller subproblems and applies pruning strategies to exclude branches that cannot produce the optimal solution. Dynamic programming breaks down a large problem into smaller subproblems and solves them by utilizing the relationships between the subproblems [18- 21]. Dynamic programming is suitable for problems with overlapping subproblem structures, such as the knapsack problem [22] and TSP [23]. Heuristic methods employ heuristic strategies or random operations to search the solution space and continuously improve the current solution until a satisfactory solution is found or a predetermined stopping condition is met. Common heuristic algorithms include greedy algorithms [24], local search [25], simulated annealing [26], and genetic algorithms [27]. Traditional methods for solving CO problems have limitations including high computational complexity, lack of scalability for large- size problems, and limited solution quality in terms of finding globally optimal solutions. In recent years, deep learning methods have emerged as promising approaches to tackle these problems by leveraging the power of data- driven modeling and computational intelligence. One category of methods approximates the process of solving CO problems through deep learning methods [28]. By learning from a dataset of problem instances, these deep learning models can capture intrinsic patterns and dependencies in the problem and the predictions are then utilized to guide the search process toward better solutions. These approaches offer advantages in terms of speed and scalability as they reduce the requirements of expensive evaluations for the objective function during the search. Another category of methods is based on learning- based heuristics and meta- heuristics [12- 14, 29]. Deep learning models can be employed to create intelligent decision rules or policies to guide the search process. Reinforcement learning techniques have demonstrated success in learning effective exploration- exploitation strategies for\n\nCO problems [30,31]. By incorporating learning- based heuristics into traditional optimization algorithms, better- quality solutions can be obtained with reduced computational effort. ### 2.2 Graph Neural Networks GNNs belong to a typical deep- learning framework for graphs that follow a message- passing mechanism [8]. Graph convolution networks (GCNs) [10] define graph convolutions on the spectral domain so that the structural information from neighbors can be aggregated. Graph SAGE [32] proposes an inductive representation learning method based on aggregation and sampling directly on the spatial domain. Graph attention networks (GATs) [33] utilize self- attention mechanisms to learn node features where the importance of neighbors can be considered during the aggregation of information. More recently, prompt tuning has been used to generalize GNNs [34,35]. GPPT [34] proposes a graph prompting function for GNNs so that the pre- trained models can predict labels without fine- tuning. Since GNNs have been successfully used in various downstream tasks, researchers pay attention to the theories behind GNNs. For example, over- smoothing [36] and over- squashing [37] are the two main issues for GNNs. BORF [38] proposes a rewiring algorithm based on Ollivier- Ricci curvature to relieve both over- smoothing and over- squashing problems for GNNs. The expressive abilities of GNNs have also been studied to demonstrate why GNNs perform well in many tasks [9,39,40]. Recently, GNNs have been used in graph domain adaptation to address the domain shift on graphs [41]. Domain adaptation on graphs aims at leveraging knowledge learned from the source domain to improve the performance of a GNN model on a different target domain. Adversarial training is often used to build the alignment framework [42,43]. For example, TFLG [43] utilizes instance and class levels of structures and leverages adversarial training to learn domain- invariant features. Except for the features that can be easily used for domain alignment, many researchers seek to learn invariant information from different levels. GCAN [44] is a graph convolutional adversarial network introduced for unsupervised domain adaptation by jointly modeling structures, domain labels, and class labels. Stru RW [45] presents a structural re- weighting method to address the conditional structure shift problem. Since spectral properties play an important role in graph structures, many works also attempt to explore the alignment of spectral information [46,47]. ## 3 Methodology ### 3.1 CO Problems on Graphs Given a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) , where \\(\\nu\\) is a set of nodes and \\(\\mathcal{E}\\) is a set of edges between nodes. The CO problems on graphs are to find a subset of nodes \\(\\mathcal{S}\\) under given constraint conditions so that the number of edges between the nodes in \\(\\mathcal{S}\\) satisfies a specific condition and the objective function can be maximized or minimized. Generally, the CO can be expressed as: \\[\\begin{array}{r}{\\max f(\\mathcal{S}),}\\\\ {g_{i}(\\mathcal{S})\\leqslant b_{i},b_{i}\\in \\Omega ,} \\end{array} \\quad (3.1b)\\] where \\(f(\\mathcal{S})\\) is the objective function, \\(g_{i}(\\mathcal{S})\\) is the constraint condition function, \\(b_{i}\\) is the boundary of the constraint condition, and \\(\\Omega\\) is the set of constraint indicators. In the following part, we give three concrete CO problems on graphs. Max- Cut Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) , the goal of the Max- Cut problem is to find a partition that divides \\(\\nu\\) into subset \\(\\mathcal{S}\\) and its complementary set \\(\\mathcal{S}^{\\prime} = \\mathcal{V} - \\mathcal{S}\\) where the number of cuts between \\(\\mathcal{S}\\) and \\(\\mathcal{S}^{\\prime}\\) is maximized. The cuts refer to the edges between \\(\\mathcal{S}\\) and \\(\\mathcal{S}^{\\prime}\\) . The objective function of Max- Cut can be denoted as: \\[f(\\mathcal{S}) = \\sum_{e_{i,j}\\in \\mathcal{E}}\\big(v_{i} + v_{j} - 2v_{i}v_{j}\\big), \\quad (3.2)\\] where \\(v_{i} = 1\\) when \\(v_{i}\\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise, and \\(e_{i,j}\\) is the edge between nodes \\(v_{i}\\) and \\(v_{j}\\)\n\nMaximum Independent Set (MIS) Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) , the maximum independent set problem aims to find a node set \\(\\mathcal{S}\\) with a maximal number of nodes where there are no edges between any two nodes in this set. The objective function and constraint conditions of the maximum independent set problem can be denoted as: \\[\\begin{array}{l}{f(\\mathcal{S}) = \\sum_{v_{i}\\in \\mathcal{V}}v_{i},}\\\\ {v_{i} + v_{j}\\leqslant 1|\\forall e_{i,j}\\in \\mathcal{E},} \\end{array} \\quad (3.3a)\\] where \\(v_{i} = 1\\) when \\(v_{i} \\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise. Minimum Dominated Set (MDS) Problem Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) , the goal of the minimum dominated set problem is to find a dominated set \\(\\mathcal{S}\\) where each node \\(v_{i} \\in \\mathcal{V}\\) or at least one of its neighbor nodes belongs to the dominated set \\(\\mathcal{S}\\) . The number of nodes in \\(\\mathcal{S}\\) should also be minimized. The objective function and constraint conditions of the minimum dominated set problem can be denoted as: \\[\\begin{array}{l}{f(\\mathcal{S}) = -\\sum_{v_{i}\\in \\mathcal{V}}v_{i},}\\\\ {v_{i} + v_{j}\\geqslant 1|e_{i,j}\\in \\mathcal{E},\\forall v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.4a)\\] where \\(v_{i} = 1\\) when \\(v_{i} \\in \\mathcal{S}\\) and \\(v_{i} = 0\\) otherwise. ### 3.2 Problem Transfer via Max-SAT Problem In this subsection, we introduce the details of leveraging the Max- SAT problem to bridge different CO problems. In general, to fully utilize the common logical information in COs, we first convert the original graphs of the CO problems into several clauses of Max- SAT problems and then transform these clauses into bipartite graphs. #### 3.2.1 From Graph to Clauses Given a graph from any CO, we use clauses with variables to describe the objective function and the constraint conditions. Max- SAT is an optimization problem that contains a set of clauses and variables. Define \\(m\\) clauses \\(\\mathcal{C} = \\{c_{1},c_{2},\\dots ,c_{m}\\}\\) and \\(n\\) variables \\(\\mathcal{X} = \\{x_{1},x_{2},\\dots ,x_{n}\\}\\) , the Max- SAT problem is to find a truth assignment of variables so that the number of satisfied clauses is maximized. To generate clauses, we define two rules for the objective function and constraint conditions respectively. The soft rule is based on the objective function and the hard rule is based on the constraint conditions. The difference between those two rules mainly lies in whether the clauses should be strictly satisfied. For clarification, we let the variable \\(x_{i}\\) correspond to the node \\(v_{i}\\) in graph \\(\\mathcal{G}\\) . For demonstration purposes, we provide the details of transformations for Max- Cut, MIS, and MDS problems. For every \\(v_{i} \\in \\mathcal{V}\\) in MIS problems, the clauses are generated by: \\[\\begin{array}{l}{c_{h} = \\neg x_{i}\\vee \\neg x_{j}|e_{i,j}\\in \\mathcal{E},}\\\\ {c_{s} = x_{i}|v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.5a)\\] where \\(c_{h} \\in \\mathcal{C}_{h}\\) denotes the hard clause which describes that the set cannot contain the node and its neighbors at the same time, \\(c_{s} \\in \\mathcal{C}_{s}\\) denotes the soft clause which describes that the number of nodes in the MIS problem should be maximized, \\(\\neg\\) and \\(\\vee\\) are the negation and disjunction. For every \\(v_{i} \\in \\mathcal{V}\\) in MDS problems, the clauses are generated by: \\[\\begin{array}{l}{c_{h} = x_{i}\\vee x_{j}|e_{i,j}\\in \\mathcal{E},}\\\\ {c_{s} = \\neg x_{i}|v_{i}\\in \\mathcal{V},} \\end{array} \\quad (3.6a)\\] where \\(c_{h} \\in \\mathcal{C}_{h}\\) describes that the node or at least one of its neighbor nodes belongs to MDS, and \\(c_{s} \\in \\mathcal{C}_{s}\\) describes that the number of nodes in MDS should be minimized.\n\nFor every \\(e_{i,j} \\in \\mathcal{E}\\) in Max- Cut problems, the clauses are generated by: \\[\\begin{array}{l}{c_{s} = x_{i}\\vee x_{j},}\\\\ {c_{s}^{\\prime} = \\neg x_{i}\\vee \\neg x_{j},} \\end{array} \\quad (3.7a)\\] where \\(c_{s}\\) and \\(c_{s}^{\\prime}\\) are all soft clauses belonging to \\(\\mathcal{C}_{s}\\) that describe the number of cut edges that should be maximized. #### 3.2.2 From Clauses to Bipartite Graph The soft and hard clauses describe the relations between clauses and variables, which reveals the logical dependencies. Then, we can use bipartite graphs to further represent these relations in Max- SAT. In general, the bipartite graph is denoted as \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) where \\(\\tilde{\\mathcal{V}}_{x}\\) and \\(\\tilde{\\mathcal{V}}_{c}\\) are node sets, and \\(\\tilde{\\mathcal{E}}\\) is the edge set. The main difference between graphs and bipartite graphs is that the nodes in bipartite graphs belong to two different types. The variables \\(\\mathcal{X}\\) correspond to the set of nodes \\(\\tilde{\\mathcal{V}}_{x}\\) and the clauses \\(\\mathcal{C}\\) correspond to the set of nodes \\(\\tilde{\\mathcal{V}}_{c}\\) . To generate the bipartite graph, the core step is to construct the edges between nodes. For each variable \\(x_{i}\\) in each clause \\(c_{i}\\) , an edge \\(\\tilde{e}_{i,j} \\in \\tilde{\\mathcal{E}}\\) corresponds to the affiliation between the variable and the clause. In this transfer process, we transfer the origin graph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E})\\) of the CO problem into the bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\mathcal{V}}_{x},\\tilde{\\mathcal{V}}_{c},\\tilde{\\mathcal{E}})\\) . A concrete transfer process can be shown in Figure 1. We choose Max- Cut as an example for demonstration. The first step is to transfer the graph in Max- Cut into a set of clauses in Max- SAT. We use the hard and soft rules to generate these clauses. After generating these clauses, we then transfer them into a bipartite graph. The variables and clauses are set as nodes and the affiliations between variables and clauses constitute edges. The new bipartite graph can be then used for the next feature extraction process. <center>Figure 1 Problem transfer process of our framework via Max-SAT. The input graph of CO problems is first transferred as Max-SAT clauses. Then, a bipartite graph is generated from these clauses by the affiliations between variables and clauses. </center> Notice that transferring graphs in original COs to bipartite graphs has two main advantages: First, original graphs only contain simple dependencies between variables. In contrast, the bipartite graphs transferred from Max- SAT clauses are then equipped with logical information from COs; Second, the bipartite graphs are general forms that can represent various COs on graphs since all of these COs can be transformed into Max- SAT problems. Therefore, we can easily construct a knowledge transfer framework to leverage the generalizable and transferable information of these problems with the help of bipartite graphs.\n\n### 3.3 Learning with Bipartite Graph Attention Networks In this subsection, we first introduce the backbone network that is used to extract features from bipartite graphs. Traditional GNNs for feature extractions from graphs mainly follow a message- passing scheme: \\[\\begin{array}{r l} & {\\mathbf{a}_{v}^{(l)} = \\mathrm{AGG}(\\mathbf{x}_{u}\\mid u\\in \\mathcal{N}(v)),}\\\\ & {\\mathbf{x}_{v}^{(l + 1)} = \\mathrm{COM}(\\mathbf{a}_{v}^{(l)},\\mathbf{x}_{v}^{(l)}),} \\end{array} \\quad (3.8)\\] where \\(u\\) and \\(v\\) denote the nodes, \\(\\mathbf{a}_{v}\\) denotes the feature vector accumulated from neighbor nodes, \\(\\mathbf{x}_{v}^{(l)}\\) denotes the features at layer \\(l\\) and \\(\\mathbf{x}_{v}^{(0)}\\) is the initial attributes, \\(\\mathrm{AGG}(\\cdot)\\) and \\(\\mathrm{COM}(\\cdot)\\) are the aggregation and combination functions for message passing and feature updating, and \\(\\mathcal{N}(v)\\) denotes the set of neighbors for \\(v\\) . The above process is only suitable for graphs with one type of nodes. Recall that a bipartite graph contains two sets of nodes that represent variables and clauses and a set of edges that connect nodes from these two sets. Due to the different properties of nodes, the traditional message- passing scheme in GNNs cannot be directly used to extract features. We particularly design a bipartite GNN with attentive message- passing schemes. For an input bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\nu}_{x},\\tilde{\\nu}_{c},\\tilde{\\mathcal{E}})\\) , the attributed matrices of variables and clauses are denoted as \\(\\mathbf{C}\\in \\mathbb{R}^{m\\times d}\\) and \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times d}\\) where \\(m\\) and \\(n\\) denote the numbers of clauses and variables, and \\(d\\) is the dimension of attributes. For symbolic convenience, we use \\(v_{x}(i)\\) and \\(v_{c}(i)\\) to denote nodes of variables and clauses in the bipartite graph. The variable and clause of these nodes are denoted as \\(x_{i}\\) and \\(c_{i}\\) and their attributed vectors are denoted as \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{c}_{i}\\) accordingly. Since the initial bipartite graph does not contain attributed information, the attributed matrices require initialization. There are different strategies for initialization, e.g., uniform distribution, normal distribution, and all- one matrices. Since no significant influences have been observed in our experiments, we simply initialize attributed matrices as all- one matrices \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) . The message- passing process of bipartite GNNs consists of two steps in each iteration, the clause- wise aggregation step and the variable- wise aggregation step. First, clause- wise aggregation updates the feature of the clause node by aggregating the features from the variable nodes. To further discriminate the importance of different neighbors, the attention mechanism is introduced to learn features during the aggregation. Given a clause \\(c_{i}\\) and its neighbor variable \\(x_{j}\\) , the layer- wise aggregation from \\(x_{j}\\) to \\(c_{i}\\) through attention is represented as: \\[\\begin{array}{r l} & {\\alpha_{x_{j}\\to c_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{x}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{x}(k)\\in\\mathcal{N}(v_{c}(i))}\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{X}_{k}^{(l)})\\rangle\\right)}}\\\\ & {\\mathbf{c}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{c}_{i}^{(l)},\\sum_{v_{x}(j)\\in\\mathcal{N}(v_{c}(i))}\\alpha_{x_{j}\\to c_{i}}^{(l)}(\\mathbf{w}_{V}^{(l)}\\mathbf{x}_{j}^{(l)})\\right),} \\end{array} \\quad (3.11)\\] where \\(\\alpha_{x_{j}\\to c_{i}}\\) denotes the attention score between clause \\(c_{i}\\) and variable \\(x_{j}\\) , \\(\\mathbf{w}_{Q}^{(l)}\\in \\mathbb{R}^{d}\\) , \\(\\mathbf{w}_{K}^{(l)}\\in \\mathbb{R}^{d}\\) , and \\(\\mathbf{w}_{V}^{(l)}\\in \\mathbb{R}^{d}\\) are learnable parameters of weights, \\(\\mathbf{c}_{i}^{(l)}\\) and \\(\\mathbf{x}_{j}^{(l)}\\) are features of \\(i\\) - th clause and \\(j\\) - th variable at layer \\(l\\) , \\(\\mathcal{N}(v_{c}(i))\\) is the set of neighbor variable nodes for clause \\(c_{i}\\) , \\(\\langle \\cdot ,\\cdot \\rangle\\) is the dot product operation, \\(\\mathbf{c}_{i}^{(l + 1)}\\) is the updated features for clause \\(c_{i}\\) , and MLP denotes the multi- layer perceptron. Second, the variable feature can be updated through variable- wise aggregation from the features of clauses that contain the variable. Similarly, for variable \\(x_{i}\\) , the layer- wise aggregation from \\(c_{j}\\) to \\(x_{i}\\) can be denoted as: \\[\\begin{array}{r l} & {\\alpha_{c_{j}\\to x_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{c}(k)\\in\\mathcal{N}(v_{x}(i))}\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{k}^{(l)})\\rangle\\right)},}\\\\ & {\\mathbf{x}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{x}_{i}^{(l)},\\sum_{v_{c}(j)\\in\\mathcal{N}(v_{x}(i))}\\alpha_{c_{j}\\to x_{i}}^{(l)}(\\tilde{\\mathbf{w}}_{V}^{(l)}\\mathbf{c}_{j}^{(l)})\\right),} \\end{array} \\quad (3.12)\\]\n\n<center>Figure 2 Illustration of the proposed pre-training and fine-tuning architecture. In the pre-training stage, the bipartite graphs generated from Max-SAT clauses are used to train the MLP, bipartite GNN backbone (Bip-GNN(\u00b7)), and fully connected classification layers \\(\\mathrm{FC}(\\cdot)\\) . In the fine-tuning stage, the bipartite graphs generated by Max-SAT and the CO are treated as source and target domains and are separately fed into the MLP and Bip-GNN to obtain features. A discriminator is to obtain domain labels for domain adaptation. The features are passed through the classification layer to predict the labels of variable nodes. </center> where \\(\\alpha_{c_{j}\\to x_{i}}\\) denotes the attention score between clause \\(c_{j}\\) and variable \\(x_{i}\\) , \\(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\) , \\(\\tilde{\\mathbf{w}}_{K}^{(l)}\\) , and \\(\\tilde{\\mathbf{w}}_{V}^{(l)}\\) are parameters, and \\(\\mathcal{N}(v_{x}(i))\\) is the neighbor set of clause nodes for variable \\(x_{i}\\) . After the two steps of aggregation, we can obtain the updated features of variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers for bipartite GNNs. The correlations between variables and clauses can be captured by the aggregation processes and the importance of neighbors can be learned by the attention mechanism. We can then build our framework based on the bipartite GNN backbone. ### 3.4 Pre-Training and Fine-Tuning with Domain Adaptation In this subsection, we introduce the framework of our work for training as depicted in Figure 2. Our framework consists of two stages. The first stage is to pre- train our model by using massive samples generated from Max- SAT and the second stage is to further fine- tune the model with domain adaptation that combines samples from Max- SAT and CO problems. #### 3.4.1 Pre-Training In the pre- training stage, the goal is to learn the general knowledge from Max- SAT and obtain better parameter initialization that can be used for solving different CO problems. We first generate massive clauses from Max- SAT and then convert them into bipartite graphs. To generate Max- SAT clauses, we follow [48] and select different distributions to simulate different scenarios of problems: (1) In uniform distribution, the variables and clauses appear with the same frequency. The sizes of clauses also appear with the same frequency. The clause size refers to the number of variables within a clause. For example, given a clause \\(x_{1} \\lor x_{2} \\lor \\neg x_{3}\\) , the clause size is three. (2) The single power- low distribution is a non- uniform distribution where most variables appear in low frequencies while only a few variables appear in high frequencies. The sizes of clauses also appear with the same frequency. (3) Double power- low distribution is also a non- uniform distribution that aims to simulate extremely uneven samples. Small clause size appears in high frequencies and the frequency decreases as the clause size increases.\n\nAfter obtaining the bipartite graphs, we can build our pre- training process. To be concrete, the model structure for pre- training contains three parts, an MLP that is used to map the initial node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) into low- dimensional features \\(\\mathbf{X}^{(0)}\\) and \\(\\mathbf{C}^{(0)}\\) ; the bipartite GNN backbone \\(\\mathrm{Bip - GNN}(\\cdot)\\) from section 3.3 extracts high- level node features for variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers; and fully connected prediction network \\(\\mathrm{FC}(\\cdot)\\) maps node features to their labels. To pre- train the three parts, we build a supervised loss function to update the parameters. We leverage the binary cross entropy (BCE) as the objective of classification: \\[\\mathcal{L}_{c} = \\sum_{i = 1}^{n}\\mathrm{BCE}(p_{i},y_{i}), \\quad (3.14)\\] where \\(y_{i}\\in \\{0,1\\}\\) is the label that describes the truth assignment of the variable generated from the MAXHS solver, and \\(p_{i}\\in \\{0,1\\}\\) is the prediction of the pre- training model from the prediction network \\(\\mathrm{FC}(\\cdot)\\) . It is worth noting that we only consider the classification loss under \\(n\\) variable nodes. Since the truth assignments of clauses are easily affected by variables, it is difficult to determine the labels of clauses. Therefore, to avoid the disturbance brought by inaccurate labeling, we only consider the classification of variables. #### 3.4.2 Fine-Tuning with Domain Adaptation After the pre- training process, we further leverage Max- SAT to build a fine- tuning process based on domain adaptation. In general, domain adaptation aims to learn domain- invariant features that can be used for different tasks. For a target CO problem, the proposed adaptation network treats Max- SAT as the source domain and learns transferable features by combining the samples from the CO and Max- SAT problems. Before the feature extraction, we can construct the data from source and target domains via transformation operations in section 3.2. The data for the source domain is generated directly from Max- SAT with different distributions following the pre- training process while the data of the target domain is generated from Max- SAT that is converted from a certain CO problem. The network architecture of fine- tuning is based on the pre- trained network. The pre- training network learns generalizable features while the fine- tuning network focuses on learning task- specific features. With the help of pre- training, the model can converge faster and achieve better generalization performance. Concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on MLP and \\(\\mathrm{Bip - GNN}(\\cdot)\\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \\(\\mathrm{Dis}(\\cdot)\\) is to classify the domain labels for each sample. The source domain and target domain share the same feature extraction backbone but have different classification networks. The domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training. Our work can also be extended to the unsupervised setting where the labels for the target domain are unavailable. Based on the three parts, the overall loss for domain adaptation in the fine- tuning stage can be denoted as \\[\\mathcal{L}_{ft} = \\mathcal{L}_{c} + \\lambda \\mathcal{L}_{d}, \\quad (3.15)\\] where \\(\\mathcal{L}_{c}\\) and \\(\\mathcal{L}_{d}\\) are the losses for classification and discrimination, and \\(\\lambda\\) is a positive hyper- parameter that controls the weight of losses. The discriminative loss that classifies nodes based on their domain labels can be denoted as \\[\\mathcal{L}_{d} = (\\mathbf{X}_{S},\\mathbf{X}_{T}) = \\mathbb{E}_{\\mathbf{x}_{S}\\in \\mathcal{D}_{S}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{S}))\\right)] + \\mathbb{E}_{\\mathbf{x}_{T}\\in \\mathcal{D}_{T}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{T}))\\right)], \\quad (3.16)\\] where \\(\\mathbf{X}_{S}\\) and \\(\\mathbf{X}_{D}\\) are the node features from source and target domains, \\(\\mathcal{D}_{S}\\) and \\(\\mathcal{D}_{T}\\) represent the data distribution of source and target domains, \\(\\mathrm{Dis}(\\cdot)\\) is the domain classifier, and \\(\\mathrm{Bip - GNN}(\\cdot)\\) is the feature extractor.",
      "level": 2,
      "line_start": 27,
      "line_end": 41
    },
    {
      "heading": "3.3 Learning with Bipartite Graph Attention Networks In this subsection, we first introduce the backbone network that is used to extract features from bipartite graphs. Traditional GNNs for feature extractions from graphs mainly follow a message- passing scheme: \\[\\begin{array}{r l} & {\\mathbf{a}_{v}^{(l)} = \\mathrm{AGG}(\\mathbf{x}_{u}\\mid u\\in \\mathcal{N}(v)),}\\\\ & {\\mathbf{x}_{v}^{(l + 1)} = \\mathrm{COM}(\\mathbf{a}_{v}^{(l)},\\mathbf{x}_{v}^{(l)}),} \\end{array} \\quad (3.8)\\] where \\(u\\) and \\(v\\) denote the nodes, \\(\\mathbf{a}_{v}\\) denotes the feature vector accumulated from neighbor nodes, \\(\\mathbf{x}_{v}^{(l)}\\) denotes the features at layer \\(l\\) and \\(\\mathbf{x}_{v}^{(0)}\\) is the initial attributes, \\(\\mathrm{AGG}(\\cdot)\\) and \\(\\mathrm{COM}(\\cdot)\\) are the aggregation and combination functions for message passing and feature updating, and \\(\\mathcal{N}(v)\\) denotes the set of neighbors for \\(v\\) . The above process is only suitable for graphs with one type of nodes. Recall that a bipartite graph contains two sets of nodes that represent variables and clauses and a set of edges that connect nodes from these two sets. Due to the different properties of nodes, the traditional message- passing scheme in GNNs cannot be directly used to extract features. We particularly design a bipartite GNN with attentive message- passing schemes. For an input bipartite graph \\(\\tilde{\\mathcal{G}} = (\\tilde{\\nu}_{x},\\tilde{\\nu}_{c},\\tilde{\\mathcal{E}})\\) , the attributed matrices of variables and clauses are denoted as \\(\\mathbf{C}\\in \\mathbb{R}^{m\\times d}\\) and \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times d}\\) where \\(m\\) and \\(n\\) denote the numbers of clauses and variables, and \\(d\\) is the dimension of attributes. For symbolic convenience, we use \\(v_{x}(i)\\) and \\(v_{c}(i)\\) to denote nodes of variables and clauses in the bipartite graph. The variable and clause of these nodes are denoted as \\(x_{i}\\) and \\(c_{i}\\) and their attributed vectors are denoted as \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{c}_{i}\\) accordingly. Since the initial bipartite graph does not contain attributed information, the attributed matrices require initialization. There are different strategies for initialization, e.g., uniform distribution, normal distribution, and all- one matrices. Since no significant influences have been observed in our experiments, we simply initialize attributed matrices as all- one matrices \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) . The message- passing process of bipartite GNNs consists of two steps in each iteration, the clause- wise aggregation step and the variable- wise aggregation step. First, clause- wise aggregation updates the feature of the clause node by aggregating the features from the variable nodes. To further discriminate the importance of different neighbors, the attention mechanism is introduced to learn features during the aggregation. Given a clause \\(c_{i}\\) and its neighbor variable \\(x_{j}\\) , the layer- wise aggregation from \\(x_{j}\\) to \\(c_{i}\\) through attention is represented as: \\[\\begin{array}{r l} & {\\alpha_{x_{j}\\to c_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{x}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{x}(k)\\in\\mathcal{N}(v_{c}(i))}\\exp\\left(\\langle(\\mathbf{w}_{Q}^{(l)}\\mathbf{c}_{i}^{(l)}),(\\mathbf{w}_{K}^{(l)}\\mathbf{X}_{k}^{(l)})\\rangle\\right)}}\\\\ & {\\mathbf{c}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{c}_{i}^{(l)},\\sum_{v_{x}(j)\\in\\mathcal{N}(v_{c}(i))}\\alpha_{x_{j}\\to c_{i}}^{(l)}(\\mathbf{w}_{V}^{(l)}\\mathbf{x}_{j}^{(l)})\\right),} \\end{array} \\quad (3.11)\\] where \\(\\alpha_{x_{j}\\to c_{i}}\\) denotes the attention score between clause \\(c_{i}\\) and variable \\(x_{j}\\) , \\(\\mathbf{w}_{Q}^{(l)}\\in \\mathbb{R}^{d}\\) , \\(\\mathbf{w}_{K}^{(l)}\\in \\mathbb{R}^{d}\\) , and \\(\\mathbf{w}_{V}^{(l)}\\in \\mathbb{R}^{d}\\) are learnable parameters of weights, \\(\\mathbf{c}_{i}^{(l)}\\) and \\(\\mathbf{x}_{j}^{(l)}\\) are features of \\(i\\) - th clause and \\(j\\) - th variable at layer \\(l\\) , \\(\\mathcal{N}(v_{c}(i))\\) is the set of neighbor variable nodes for clause \\(c_{i}\\) , \\(\\langle \\cdot ,\\cdot \\rangle\\) is the dot product operation, \\(\\mathbf{c}_{i}^{(l + 1)}\\) is the updated features for clause \\(c_{i}\\) , and MLP denotes the multi- layer perceptron. Second, the variable feature can be updated through variable- wise aggregation from the features of clauses that contain the variable. Similarly, for variable \\(x_{i}\\) , the layer- wise aggregation from \\(c_{j}\\) to \\(x_{i}\\) can be denoted as: \\[\\begin{array}{r l} & {\\alpha_{c_{j}\\to x_{i}}^{(l)} = \\frac{\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{j}^{(l)})\\rangle\\right)}{\\sum_{v_{c}(k)\\in\\mathcal{N}(v_{x}(i))}\\exp\\left(\\langle(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\mathbf{x}_{i}^{(l)}),(\\tilde{\\mathbf{w}}_{K}^{(l)}\\mathbf{c}_{k}^{(l)})\\rangle\\right)},}\\\\ & {\\mathbf{x}_{i}^{(l + 1)} = \\mathrm{MLP}\\left(\\mathbf{x}_{i}^{(l)},\\sum_{v_{c}(j)\\in\\mathcal{N}(v_{x}(i))}\\alpha_{c_{j}\\to x_{i}}^{(l)}(\\tilde{\\mathbf{w}}_{V}^{(l)}\\mathbf{c}_{j}^{(l)})\\right),} \\end{array} \\quad (3.12)\\]",
      "content": "<center>Figure 2 Illustration of the proposed pre-training and fine-tuning architecture. In the pre-training stage, the bipartite graphs generated from Max-SAT clauses are used to train the MLP, bipartite GNN backbone (Bip-GNN(\u00b7)), and fully connected classification layers \\(\\mathrm{FC}(\\cdot)\\) . In the fine-tuning stage, the bipartite graphs generated by Max-SAT and the CO are treated as source and target domains and are separately fed into the MLP and Bip-GNN to obtain features. A discriminator is to obtain domain labels for domain adaptation. The features are passed through the classification layer to predict the labels of variable nodes. </center> where \\(\\alpha_{c_{j}\\to x_{i}}\\) denotes the attention score between clause \\(c_{j}\\) and variable \\(x_{i}\\) , \\(\\tilde{\\mathbf{w}}_{Q}^{(l)}\\) , \\(\\tilde{\\mathbf{w}}_{K}^{(l)}\\) , and \\(\\tilde{\\mathbf{w}}_{V}^{(l)}\\) are parameters, and \\(\\mathcal{N}(v_{x}(i))\\) is the neighbor set of clause nodes for variable \\(x_{i}\\) . After the two steps of aggregation, we can obtain the updated features of variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers for bipartite GNNs. The correlations between variables and clauses can be captured by the aggregation processes and the importance of neighbors can be learned by the attention mechanism. We can then build our framework based on the bipartite GNN backbone. ### 3.4 Pre-Training and Fine-Tuning with Domain Adaptation In this subsection, we introduce the framework of our work for training as depicted in Figure 2. Our framework consists of two stages. The first stage is to pre- train our model by using massive samples generated from Max- SAT and the second stage is to further fine- tune the model with domain adaptation that combines samples from Max- SAT and CO problems. #### 3.4.1 Pre-Training In the pre- training stage, the goal is to learn the general knowledge from Max- SAT and obtain better parameter initialization that can be used for solving different CO problems. We first generate massive clauses from Max- SAT and then convert them into bipartite graphs. To generate Max- SAT clauses, we follow [48] and select different distributions to simulate different scenarios of problems: (1) In uniform distribution, the variables and clauses appear with the same frequency. The sizes of clauses also appear with the same frequency. The clause size refers to the number of variables within a clause. For example, given a clause \\(x_{1} \\lor x_{2} \\lor \\neg x_{3}\\) , the clause size is three. (2) The single power- low distribution is a non- uniform distribution where most variables appear in low frequencies while only a few variables appear in high frequencies. The sizes of clauses also appear with the same frequency. (3) Double power- low distribution is also a non- uniform distribution that aims to simulate extremely uneven samples. Small clause size appears in high frequencies and the frequency decreases as the clause size increases.\n\nAfter obtaining the bipartite graphs, we can build our pre- training process. To be concrete, the model structure for pre- training contains three parts, an MLP that is used to map the initial node attributes \\(\\mathbf{X}_{ini}\\) and \\(\\mathbf{C}_{ini}\\) into low- dimensional features \\(\\mathbf{X}^{(0)}\\) and \\(\\mathbf{C}^{(0)}\\) ; the bipartite GNN backbone \\(\\mathrm{Bip - GNN}(\\cdot)\\) from section 3.3 extracts high- level node features for variables \\(\\mathbf{X}^{(L)}\\) and clauses \\(\\mathbf{C}^{(L)}\\) where \\(L\\) is the number of layers; and fully connected prediction network \\(\\mathrm{FC}(\\cdot)\\) maps node features to their labels. To pre- train the three parts, we build a supervised loss function to update the parameters. We leverage the binary cross entropy (BCE) as the objective of classification: \\[\\mathcal{L}_{c} = \\sum_{i = 1}^{n}\\mathrm{BCE}(p_{i},y_{i}), \\quad (3.14)\\] where \\(y_{i}\\in \\{0,1\\}\\) is the label that describes the truth assignment of the variable generated from the MAXHS solver, and \\(p_{i}\\in \\{0,1\\}\\) is the prediction of the pre- training model from the prediction network \\(\\mathrm{FC}(\\cdot)\\) . It is worth noting that we only consider the classification loss under \\(n\\) variable nodes. Since the truth assignments of clauses are easily affected by variables, it is difficult to determine the labels of clauses. Therefore, to avoid the disturbance brought by inaccurate labeling, we only consider the classification of variables. #### 3.4.2 Fine-Tuning with Domain Adaptation After the pre- training process, we further leverage Max- SAT to build a fine- tuning process based on domain adaptation. In general, domain adaptation aims to learn domain- invariant features that can be used for different tasks. For a target CO problem, the proposed adaptation network treats Max- SAT as the source domain and learns transferable features by combining the samples from the CO and Max- SAT problems. Before the feature extraction, we can construct the data from source and target domains via transformation operations in section 3.2. The data for the source domain is generated directly from Max- SAT with different distributions following the pre- training process while the data of the target domain is generated from Max- SAT that is converted from a certain CO problem. The network architecture of fine- tuning is based on the pre- trained network. The pre- training network learns generalizable features while the fine- tuning network focuses on learning task- specific features. With the help of pre- training, the model can converge faster and achieve better generalization performance. Concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on MLP and \\(\\mathrm{Bip - GNN}(\\cdot)\\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \\(\\mathrm{Dis}(\\cdot)\\) is to classify the domain labels for each sample. The source domain and target domain share the same feature extraction backbone but have different classification networks. The domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training. Our work can also be extended to the unsupervised setting where the labels for the target domain are unavailable. Based on the three parts, the overall loss for domain adaptation in the fine- tuning stage can be denoted as \\[\\mathcal{L}_{ft} = \\mathcal{L}_{c} + \\lambda \\mathcal{L}_{d}, \\quad (3.15)\\] where \\(\\mathcal{L}_{c}\\) and \\(\\mathcal{L}_{d}\\) are the losses for classification and discrimination, and \\(\\lambda\\) is a positive hyper- parameter that controls the weight of losses. The discriminative loss that classifies nodes based on their domain labels can be denoted as \\[\\mathcal{L}_{d} = (\\mathbf{X}_{S},\\mathbf{X}_{T}) = \\mathbb{E}_{\\mathbf{x}_{S}\\in \\mathcal{D}_{S}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{S}))\\right)] + \\mathbb{E}_{\\mathbf{x}_{T}\\in \\mathcal{D}_{T}}[\\log \\left(1 - \\mathrm{Dis}(\\mathrm{Bip - GNN}(\\mathbf{x}_{T}))\\right)], \\quad (3.16)\\] where \\(\\mathbf{X}_{S}\\) and \\(\\mathbf{X}_{D}\\) are the node features from source and target domains, \\(\\mathcal{D}_{S}\\) and \\(\\mathcal{D}_{T}\\) represent the data distribution of source and target domains, \\(\\mathrm{Dis}(\\cdot)\\) is the domain classifier, and \\(\\mathrm{Bip - GNN}(\\cdot)\\) is the feature extractor.",
      "level": 3,
      "line_start": 37,
      "line_end": 41
    }
  ]
}