{
  "sections": {
    "circuit_aware_sat_solving_guiding_cdcl_via_conditi": "## Introduction\n\n\n<table><tr><td>i</td><td colspan=\"2\">P(C)</td><td colspan=\"2\">P(Ai\u2229C)</td><td colspan=\"2\">P(Ai|C)</td></tr><tr><td></td><td>ground truth</td><td>prediction</td><td>ground truth</td><td>prediction</td><td>ground truth</td><td>prediction</td></tr><tr><td>1</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00017</td><td>1</td><td>0.05277834</td></tr><tr><td>2</td><td>0.01</td><td>0.00323</td><td>0</td><td>0.00014</td><td>0</td><td>0.04442138</td></tr><tr><td>3</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00014</td><td>1</td><td>0.05133383</td></tr><tr><td>4</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00016</td><td>1</td><td>0.07029346</td></tr><tr><td>5</td><td>0.01</td><td>0.00323</td><td>0</td><td>0.00009</td><td>0</td><td>0.04350258</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Overall Error</td><td colspan=\"2\">0.00677</td><td colspan=\"2\">0.00786</td><td colspan=\"2\">0.72396</td></tr></table> Table A1: Performance of the division operation with extremely small \\(P(C)\\) \\(P(A_{i}\\mid C)\\) is calculated by \\(\\frac{P(A_{i}\\wedge C)}{P(C)}\\) <table><tr><td>i</td><td colspan=\"2\">P(C)</td><td colspan=\"2\">P(Ai\u2229C)</td><td colspan=\"2\">P(Ai|C)</td></tr><tr><td></td><td>gt</td><td>pred</td><td>gt</td><td>pred</td><td>gt</td><td>pred</td></tr><tr><td>1</td><td>0.42</td><td>0.4210119</td><td>0.24</td><td>0.24765863</td><td>0.5714286</td><td>0.58824617</td></tr><tr><td>2</td><td>0.42</td><td>0.4210119</td><td>0.33</td><td>0.34514248</td><td>0.7857143</td><td>0.8197927</td></tr><tr><td>3</td><td>0.42</td><td>0.4210119</td><td>0.31</td><td>0.3523217</td><td>0.7380953</td><td>0.83684504</td></tr><tr><td>4</td><td>0.42</td><td>0.4210119</td><td>0.36</td><td>0.3949966</td><td>0.8571429</td><td>0.93820775</td></tr><tr><td>5</td><td>0.42</td><td>0.4210119</td><td>0.38</td><td>0.3954696</td><td>0.9047619</td><td>0.6914532</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Overall Error</td><td colspan=\"2\">0.0010119</td><td colspan=\"2\">0.025311</td><td colspan=\"2\">0.101676</td></tr></table> Table A2: Performance of the division operation with moderate \\(P(C)\\) \\(P(A_{i}\\mid C)\\) is calculated by \\(\\frac{P(A_{i}\\wedge C)}{P(C)}\\) <table><tr><td rowspan=\"2\"></td><td colspan=\"3\">P&amp;lt;0.8</td><td colspan=\"3\">P\u22650.8</td></tr><tr><td>LBD</td><td>Num.</td><td>Time.Red.</td><td>Dec.Red.</td><td>Num.</td><td>Time.Red.</td></tr><tr><td>1</td><td>1515</td><td>47.1%</td><td>55.2%</td><td>2468</td><td>5.3%</td><td>18.9%</td></tr><tr><td>2</td><td>1819</td><td>21.2%</td><td>26.6%</td><td>7142</td><td>-71.0%</td><td>-0.2%</td></tr><tr><td>3</td><td>300</td><td>6.7%</td><td>6.5%</td><td>2728</td><td>-15.6%</td><td>4.9%</td></tr></table> <center>Figure A1: Toy circuit \\(c = a\\wedge b\\) </center> <table><tr><td>P(a)</td><td>P(b)</td><td>P(c=a\u2227b)</td></tr><tr><td>0.500</td><td>0.500</td><td>0.249</td></tr><tr><td>0.500</td><td>0.500</td><td>0.253</td></tr><tr><td>0.100</td><td>0.400</td><td>0.040</td></tr><tr><td>0.300</td><td>0.600</td><td>0.182</td></tr><tr><td>0.700</td><td>0.900</td><td>0.630</td></tr></table> Table A4: Node probabilities of the toy circuit (Fig. A1) under different PI workload levels \\(\\rho\\) . To illustrate this diversity, we simulate two contrasting scenarios (see Table A4). In the first case, all primary inputs (PIs) are assigned a fixed workload of \\(\\rho = 0.5\\) , under which each internal node converges to a stable logic probability; a representative slice is shown in the first two rows of Table A4. In the second case, each PI's \\(\\rho\\) is independently sampled from the set \\(0.1, 0.2, \\ldots , 0.9\\) , resulting in a broader range of logic probabilities across internal nodes. This variation exposes the network to a richer set of behaviors and Table A5: Statistics of clauses, solving time reduction, and decision count reduction under different LBD values and predicted probabilities. better reflects diverse operational conditions. ## A5 Analysis of Probability-Based Metric In our probability- based clause filtering method, the solving process is paused once the number of conflicts reaches 50,000, at which point all learnt clauses are extracted. For better analysis, these clauses are grouped according to their LBD values (1, 2, or 3) and further partitioned based on their predicted probabilities. Each group is then re- inserted into the solver independently, allowing us to evaluate their respective impacts on the remainder of the solving process. Table A5 presents statistics for the learnt clauses under different LBD levels, further divided by probability thresholds ( \\(< 0.8\\) and \\(\\geq 0.8\\) ). The table reports the number of clauses in each group, along with the corresponding reductions in solving time and decision count. From Table A5, we draw two key conclusions. First, for clauses with the same LBD, retaining those with lower\n\nprobability is more beneficial for subsequent solving, even though they are fewer in number. In fact, keeping only high- probability clauses can negatively impact solver performance. Second, we observe that the probability distribution of clauses closely matches the distribution of LBD: among clauses with lower LBD, a larger proportion have low probability. We further find that clause probability is a more informative metric than LBD alone. As a baseline, Ca Di Ca L simply discards all learnt clauses with LBD greater than 2. However, by filtering \\(LBD = 3\\) clauses based on probability and retaining only those with low probability, we can still achieve a positive effect on solving performance. ## A6 Applying Probability-Based Phase Selection for UNSAT Case The proposed phase selection method prioritizes variable assignments that are more likely to satisfy the circuit, effectively accelerating SAT solving, reducing solving time by up to \\(10\\times\\) on SAT instances. Leveraging this property, we regard the method as a lightweight classifier: for a given instance for solving, if the solver fails to find a solution within a predefined time budget under our phase selection heuristic, the instance is likely to be UNSAT. In such cases, we dynamically switch the solver to a configuration tuned specifically for UNSAT problems. We implement this strategy on top of Ca Di Ca L, using a simple timeout- based switching mechanism. We set the time budget as 5 seconds. Figure A2 summarizes the performance on the LECUNSAT dataset. On this dataset, the average solving time is reduced from 166.26 seconds (raw Ca Di Ca L) to 80.36 seconds using our adaptive strategy. As shown in Figure A2, the adaptive strategy consistently reduces solving time across all selected UNSAT- prone cases. This demonstrates that early- stage solver behavior under the SAT- oriented phase heuristic provides useful signals for unsatisfiability prediction, enabling effective and low- overhead adaptation to more suitable solving strategies. <center>Figure A2: Number of solved instances as a function of solving time using probability-based phase selection. A total of 150 UNSAT case in LECSAT dataset. TIMEOUT = 300s. </center>\n\n<center>Figure 1: Framework of CASCADE. We convert the original circuit into a DAG representation, augmented with virtual and and div gates. In the training pipeline, we employ pattern-based pre-training followed by workload-aware fine-tuning to train the model for conditional probability prediction. During inference, the predicted conditional probabilities are used to guide SAT solving, specifically in the inprocessing stages of phase selection and clause filtering. </center> achieves an additional \\(23.5\\%\\) reduction in total solving time on benchmarks from logic equivalence checking scenarios. In summary, our primary contributions include: - Introducing the first learning-based in-processing heuristics specifically designed for CSAT solving, effectively bridging static circuit representations and the dynamic CDCL solving process.- Developing a highly accurate GNN-based probabilistic model that predicts gate-level conditional probabilities, achieving validation accuracy of \\(96.69\\%\\) .- Demonstrating substantial empirical improvements by integrating the GNN-based conditional probability guidance into state-of-the-art SAT solvers, resulting in significant efficiency gains in real-world EDA applications. Our approach not only advances the theoretical understanding of leveraging structural information in SAT solving but also provides a robust framework for future solver enhancements and EDA tool designs. ## 2 Related Work ### 2.1 Circuit Representation Learning Function- aware representation learning has become a critical subfield in Electronic Design Automation (EDA), reflecting the broader trend in AI toward learning unified embeddings that can serve multiple downstream tasks. This area can be categorized into two main approaches: predictive models and contrastive models. In predictive models, Deep Gate (Shi et al. 2023, 2024; Zheng et al. 2025) and Deep Cell (Shi et al. 2025b) leverage asynchronous message passing neural network with representation disentanglement techniques to generate separate embeddings for functionality and structure, with pretraining across various EDA benchmarks. Polar Gate (Liu et al. 2024) further refines functional embeddings by incorporating ambipolar device principles. In contrastive models, FGNN (Wang et al. 2022, 2024) employs contrastive learning to align circuit embeddings based on functional similarity. Additionally, MGVGA (Wu et al. 2025), Circuit Fusion (Fang et al. 2025b), and Net TAG (Fang et al. 2025a) apply contrastive learning across multi- modal circuits to obtain function- invariant embeddings. In this paper, we focus on predicting exact conditional probabilities, for which we choose the state- of- the- art predictive model, Deep Gate4, as backbone. ### 2.2 Circuit Satisfiability The Boolean Satisfiability Problem (SAT) is a canonical NP- complete problem that determines whether a given propositional formula is satisfiable. As a variant of SAT, the Circuit Satisfiability Problem (CSAT, Circuit SAT) asks whether there exists an input assignment that makes the output of a given Boolean circuit evaluate to true. Several circuit- based solvers have been developed to address the Circuit SAT problem, aiming to operate directly on circuit format and leverage the structural information. For instance, NIMO (Lu et al. 2003) employs advanced circuit- level Boolean constraint propagation techniques to enhance conflict detection and decision- making. Similarly, Qute- SAT (Wu et al. 2007) incorporates conflict- driven learning strategies optimized for solving complex circuit topologies. Despite these innovations, such solvers generally offer only basic functionality and still fall short of the performance achieved by state- of- the- art CNF- based solvers on large- scale benchmarks.\n\nModern SAT solvers, such as Kissat (Armin et al. 2024), and Ca Di Cal (Fleury and Heisinger 2020), have demonstrated remarkable success in handling CNF- based instances. The de- facto standard workflow for solving CSAT problems is to translate circuit into the CNF formulas to be processed by highly- optimized SAT solvers. However, their performance on circuit- based problems is severely bottlenecked by the circuit- to- CNF transformation, which disrupts the structural properties of circuits and produces solver- unfriendly representations. Previous efforts, such as applying EDA- driven circuit optimization techniques (E\u00e9n, Mishchenko, and S\u00f6rensson 2007; Shi et al. 2025c) and extracting XOR logic on raw circuit (Qian et al. 2025) to reformat the problem into solver- friendly representations before solving, have shown attractive improvements. However, these approaches remain static\u2014they have no impact on solving heuristics and are inactive during the inprocessing phase, a critical period where search decisions and analysis routines are dynamically adjusted. As a result, the rich structural intelligence of circuits remains untapped precisely when it could be most beneficial. ## 3 Methodology ### 3.1 Problem Definition During CDCL solving, the CNF formula evolves dynamically through decision making, conflict analysis, and clause management routines. Two core heuristics\u2014phase selection and clause filtering\u2014play a central role in guiding the search and maintaining solver efficiency. Phase selection determines polarity (true/false) of variable assignments during the search process, aiming to minimize conflicts and accelerate convergence. Meanwhile, clause filtering focuses on evaluating and managing learned clauses derived from conflict analysis, retaining high- quality ones that improve propagation efficiency while discarding low- value clauses to reduce memory overhead. We reformulate both heuristics as conditional probability prediction tasks over circuit structures. Phase Selection Given a Boolean circuit \\(C\\) with primary output (PO), we formulate phase selection as a probability inference problem over the circuit graph. Let \\(s\\in V\\) denote a variable corresponding to a gate in the circuit. At each decision step \\(t\\) , the solver selects a variable \\(s\\in V\\) and estimates the conditional probability: \\(P(s = 1\\mid \\mathrm{PO} = 1)\\) which measures the likelihood that setting \\(s = 1\\) helps satisfy the output. By leveraging these probabilities to guide phase assignments, the solver implicitly favors value choices that are more aligned with satisfying assignments\u2014often leading to stronger propagation and fewer conflicts. A threshold \\(\\tau \\in (0,0.5)\\) governs the phase assignment: \\[v_{s} = \\left\\{ \\begin{array}{ll}0, & \\mathrm{if~}P(s = 1\\mid \\mathrm{PO} = 1)< \\tau ,\\\\ 1, & \\mathrm{if~}P(s = 1\\mid \\mathrm{PO} = 1) > 1 - \\tau ,\\\\ \\mathrm{default,} & \\mathrm{otherwise.} \\end{array} \\right. \\quad (1)\\] This probability inference strategy biases phase decisions toward values more likely to satisfy downstream logic, improving propagation and reducing conflicts. <center>Figure 2: Augmented graph with virtual and and div gates. </center> Clause Management Existing metrics for clause quality like LBD and clause length rely solely on CNF- level features, ignoring structural insights from the original circuit, thus hindering performance on circuit- derived problems. We propose a new metric, clause probability, to evaluate the quality of learned clauses by estimating their likelihood of being satisfied under structural semantics. Consider a learned clause \\(C = l_{1}\\lor l_{2}\\lor \\dots \\lor l_{k}\\) , which is semantically equivalent to a \\(k\\) - input OR gate over its literals. The probability that this clause is satisfied can be expressed as: \\[P(C) = P(l_{1}\\lor l_{2}\\lor \\dots \\lor l_{k}). \\quad (2)\\] where \\(P(l_{i})\\) is the estimated probability of literal \\(l_{i}\\) being true, derived from circuit- level signal estimation. This probabilistic view provides an intuitive interpretation: a clause with high probability \\(P(C)\\) is likely to be satisfied easily, implying that it encodes a weak constraint and has limited impact on restricting the search space. In contrast, a clause with low \\(P(C)\\) represents a strong constraint that is harder to satisfy, hence has higher potential to prune large infeasible regions in the search space. Therefore, we regard clauses with lower \\(P(C)\\) as more informative and prioritize them during inprocessing. Concretely, during periodic clause database elimination, the solver evaluates \\(P(C)\\) for all learned clauses and retains those with the lowest probabilities. This filtering mechanism allows the solver to preserve high- quality constraints that are more aligned with the underlying circuit semantics. We adopt the DEEPGATE4 framework (Zheng et al. 2025) as our encoder \\(E\\) , which generates structure and function embeddings for each gate in the circuit graph \\(G\\) . We use these embeddings to estimate conditional probabilities that guide inprocessing heuristics, enabling more efficient, probability- aware Circuit SAT solving. ### 3.2 Graph Construction To enable probability- aware learning on circuit graphs, we construct directed acyclic graphs (DAGs) where each node represents a logical gate, and further augment with virtual gates for better probability modeling, as shown in Figure 2. DAG Construction with and and not Gates Following prior work (Shi et al. 2023, 2024; Zheng et al. 2025; Liu et al. 2024), we begin by constructing a standard DAG representation of the input circuit. Each logic gate is represented as a node, and edges represent signal propagation between gates. Specifically, for a given circuit graph \\(G\\) , we construct\n\na DAG comprising two basic gate types: and gates for binary conjunctions and not gates for logic inversion. We adopt the aggregator \\(Aggr_{and}\\) and \\(Aggr_{not}\\) for and gates and not gates respectively, following Deep Gate4 framework(Zheng et al. 2025). Virtual and Gates for Joint Probabilities To expose joint probabilities directly in the graph, we insert virtual and gates in \\(\\mathcal{G}\\) : for any node \\(A\\) conditioned on \\(C\\) , we add \\[A_{joint} = A\\wedge C, \\quad (3)\\] so the model can observe the joint probability \\(P(A\\wedge C)\\) directly, denoted as \\(P(A_{joint})\\) , without intermediate arithmetic computations. When encode the virtual and gate, we directly adopt the \\(Aggr_{and}\\) to get the embedding and predict the joint probability as: \\[h^{A_{joint}} = Aggr(h^{A},h^{C}),\\hat{P} (A_{joint}) = \\phi (h^{A_{joint}}), \\quad (4)\\] where \\(\\phi\\) is a 3- layer MLPs. Virtual div Gates for Conditional Probabilities As joint probabilities can be modeled via virtual and gates, computing conditional probabilities such as \\[P(A\\mid C) = \\frac{P(A\\wedge C)}{P(C)} = \\frac{P(A_{joint})}{P(C)} \\quad (5)\\] can lead to amplified errors, especially when \\(P(C)\\) is small. Details are shown in Appendix A2. Nodes with extremely low probabilities- - referred to as polar nodes (i.e., those with truth- table probabilities below 0.1)- are empirically harder to learn and tend to amplify prediction errors. These nodes require special attention to ensure accurate and reliable modeling. To address this, we introduce virtual div gates to represent conditional probabilities directly within the graph. Each such gate takes two inputs: the numerator, which is a virtual and gate representing the joint event \\(A_{\\mathrm{joint}} = A\\wedge C\\) , and the denominator, which corresponds to the condition node \\(C\\) . Then with aggregator \\(Aggr_{div}\\) designed for div gate, we first get the embedding with: \\[h^{A_{cond}} = Aggr_{div}(h^{A_{joint}},h^{C}). \\quad (6)\\] Then predict the probability with task head \\(\\phi\\) : \\[\\hat{P} (A_{cond}) = \\phi (h^{A_{cond}}) \\quad (7)\\] This design enables the model to predict \\(P(A\\mid C)\\) directly from graph context, reducing error sensitivity to small denominators. ### 3.3 Two-Stage Training Strategy We train the network with a two- stage training strategy: 1. Pattern-Based Pre-training. Previous works (Shi et al. 2023, 2024; Zheng et al. 2025; Liu et al. 2024) initialize PI embeddings using input probabilities, however, it will lead to information distortion. As shown in Table 1, distinct input subsets (0-3 and 4-7) may share the same empirical PI probability (0.5), despite differing in actual assignments. This Table 1: Example of simulation patterns and probability. <table><tr><td>Pattern ID</td><td>P1</td><td>P2</td><td>P3</td><td>P4</td><td>Node x</td><td>Node y</td><td>Node z</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>4</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>5</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>6</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td></tr><tr><td>7</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr></table> illustrates a key limitation: probabilities are coarse- grained and may fail to capture fine- grained circuit behavior. To address this, we use 100 random patterns per minibatch to compute fine- grained probability features, which are fed into the model. This supervision enables our model to capture both statistical trends and functional semantics. Suppose the circuit contains \\(n\\) nodes in total, where the first \\(m\\) nodes are designated as primary inputs (PIs). For each primary input (PI) node, a 100- bit simulation trace is encoded into an initial embedding using an autoencoder. Embeddings for all other nodes are then propagated level- by- level through the circuit. The stage- 1 training loss is defined as: \\[\\begin{array}{l}{\\mathcal{L}_{\\mathrm{stage1}} = w_{1}\\cdot \\frac{1}{n - m}\\sum_{i = m + 1}^{n}L1(P_{i} - \\hat{P}_{i})}\\\\ {+\\ w_{2}\\cdot \\frac{1}{m}\\sum_{j = 1}^{m}L1(P_{j} - \\hat{P}_{j})} \\end{array} \\quad (8)\\] Here, the L1 Loss function is defined as: \\[\\mathrm{L1}(a,b) = |a - b| \\quad (9)\\] where \\(P_{i}\\) denotes the ground- truth probability of the \\(i\\) - th node, and \\(\\hat{P}_{i}\\) is the predicted value. The second term focuses on PI nodes, encouraging accurate reconstruction of their probabilities, since their embeddings are initialized from actual simulation data. 2. Workload-Aware Fine-tuning. Although training on batches of 100 patterns helps capture fine-grained circuit behaviors, our ultimate goal is to generalize to the statistical properties of large-scale pattern distributions. To bridge this gap and enhance data diversity, we perform 200 simulations, each using 100 random patterns with designated PI workloads drawn from \\(\\{0.1,0.2,\\ldots ,0.9\\}\\) (detailed in Appendix A4). For each PI, the 100- bit trace in a simulation is averaged into a single probability, resulting in a 200- dimensional vector that captures its behavior under diverse input distributions. This vector is then passed through an MLP to produce the PIs' initial embedding. For internal nodes, ground- truth probabilities are computed by aggregating predictions over all \\(200\\times 100 = 20,000\\) input patterns, serving as training targets in stage- 2.\n\n<center>Figure 3: Example of conditional probability calculation for three conditions </center> The overall loss for stage- 2 is defined as: \\[\\begin{array}{rl} & {\\mathcal{L}_{\\mathrm{stage2}} = w_1\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{all}}|}\\sum_{i\\in \\mathcal{S}_{\\mathrm{all}}}L1(P_i - \\hat{P}_i)}\\\\ & {\\qquad +w_2\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{div}}|}\\sum_{j\\in \\mathcal{S}_{\\mathrm{div}}}L1(P_j - \\hat{P}_j)}\\\\ & {\\qquad +w_3\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{polar}}|}\\sum_{k\\in \\mathcal{S}_{\\mathrm{polar}}}L1(P_k - \\hat{P}_k)} \\end{array} \\quad (10)\\] Here, Sall denotes all internal nodes in the circuit, Sdiv refers to div nodes, and Spolar includes polarized nodes with low probability \\((P< 0.1)\\) Higher weights \\(w_{2}\\) and \\(w_{3}\\) prioritize structurally and semantically critical nodes: div nodes reflect key conditional probabilities, while polarized nodes are harder to predict but crucial for tasks like Circuit SAT. ### 3.4 Multiple Condition Probability Through Equation 5, our model can estimate the probability of a target node conditioned on a single other node. However, in practical Circuit SAT scenarios, it is often more useful to infer the most likely value of a node given that multiple other nodes have already been assigned. As illustrated in Figure 3, when multiple condition nodes (denoted as \\(C_1, C_2, \\ldots , C_k\\) ) are involved, we combine them using a chain of two- input and gates to construct a single aggregated condition node \\(C\\) . This transformation enables the model to capture the joint condition \\(C = C_1 \\wedge C_2 \\wedge \\ldots \\wedge C_k\\) through logical conjunction. Once the final condition node \\(C\\) is obtained, we apply Equation 5 in the same manner as with a single condition node, treating \\(P(A | C)\\) as the target. This allows the model to generalize single- condition inference to multi- condition scenarios in a structurally consistent way. ## 4 Experiments ### 4.1 Dataset Generation In most cases, conditioning on a specific node \\(C\\) primarily affects a local subregion of the circuit rather than its entirety. We define this subregion as the influence area of \\(C\\) , constructed in three steps. First, we identify the fanin cone of \\(C\\) . Within this cone, we then locate an ancestor node \\(P\\) that satisfies two conditions: it has multiple fanouts, and it differs from \\(C\\) by no more than 10 logic levels. Finally, starting from \\(P\\) , we collect all nodes within its fanout cone, bounded to a depth of 10 logic levels downstream. We use a dataset of 10,824 AIGs from ITC99 (Davidson 1999), EPFL (Amaru et al. 2019), and Open Core (Open Core 1999), with circuit sizes ranging from 36 to 3,214 gates. To make conditional information explicitly available to the model, we insert and and div gates within each node's influence area. For supervision, we conduct 20,000 random simulations per circuit to record full truth assignments. ### 4.2 Experiment Setting In the one- round GNN model configuration, both the structural embedding \\(h^s\\) and the functional embedding \\(h^f\\) are set to a dimension of 128. The task head, \\(\\phi\\) , has 3 hidden layers with 32 neurons and utilizes the Si LU activation function. The model is trained for 60 epochs to ensure convergence with a batch size of 512 using a single NVIDIA RTX 4090 GPU. The Adam optimizer (Kingma and Ba 2014) is used with a learning rate of \\(10^{- 4}\\) . ### 4.3 Performance on Conditional Probability Prediction Table 2: Conditional probability prediction performance (MAE) under varying condition counts and circuit sizes. <table><tr><td>#Condition Nodes</td><td>#Nodes=928</td><td>#Nodes=6428</td><td>#Nodes=17796</td></tr><tr><td>1</td><td>0.0243</td><td>0.0372</td><td>0.0457</td></tr><tr><td>2</td><td>0.0267</td><td>0.0401</td><td>0.0496</td></tr><tr><td>5</td><td>0.0314</td><td>0.0479</td><td>0.0532</td></tr></table> We train the model on small circuits and evaluate its conditional probability prediction accuracy on both small and large- scale benchmarks. In the single- condition setting, the average L1 loss on the validation set reaches as low as 0.0331. To illustrate generalizability, we select representative circuits of varying sizes. As shown in Table 2, the model maintains low prediction error across all scales, with L1 loss as low as 0.0243 for small circuits and remaining below 0.05 even for circuits with over 17K nodes. As the number of conditions increases from 1 to 5, prediction error rises slightly yet remains within acceptable bounds, demonstrating strong robustness. Importantly, inference overhead under multi- condition settings is negligible, thanks to the lightweight design of the aggregator functions of and gates. ### 4.4 Ablation Study Effectiveness of div Gate Table 3 compares our div gate- based model with direct division for conditional probability prediction. Across all settings, div gates yield consistently lower MAE, e.g., reducing error from 0.1173 to 0.0312 with one moderate condition. This highlights their advantage in numerical stability and learning robustness, while direct division suffers from unstable gradients and sensitivity to small denominators.\n\nTable 3: Ablation study on the effectiveness of using div gates for computing conditional probability (MAE). Here, \"#Cond.\" indicates the number of condition nodes involved. <table><tr><td rowspan=\"2\"># Cond.</td><td colspan=\"2\">Moderate Condition</td><td colspan=\"2\">Polar Condition</td></tr><tr><td>div gate</td><td>direct division</td><td>div gate</td><td>direct division</td></tr><tr><td>1</td><td>0.0312</td><td>0.1173</td><td>0.0395</td><td>0.7129</td></tr><tr><td>2</td><td>0.0339</td><td>0.1495</td><td>0.0423</td><td>0.7940</td></tr><tr><td>5</td><td>0.0368</td><td>0.2030</td><td>0.0486</td><td>0.8546</td></tr></table> Table 4: Ablation study on the effectiveness of our two-stage training strategy (MAE). <table><tr><td># Condition Nodes</td><td>Two-stage</td><td>Only Stage-1</td><td>Only Stage-2</td></tr><tr><td>1</td><td>0.0331</td><td>0.0422</td><td>0.0435</td></tr><tr><td>2</td><td>0.0352</td><td>0.0583</td><td>0.0493</td></tr><tr><td>5</td><td>0.0387</td><td>0.0738</td><td>0.0588</td></tr></table> Effectiveness of Training Strategy We evaluate the effectiveness of our two- stage training strategy, where stage1 learns fine- grained behavior from a 100- pattern simulation and stage- 2 captures broader statistical distributions using 20,000- pattern data. As shown in Table 4, the two- stage model consistently achieves the lowest L1 loss, outperforming both stage- 1 and stage- 2 models individually. For instance, with one condition node, L1 loss improves from 0.0422 (stage- 1) and 0.0435 (stage- 2) to 0.0331 (two- stage). Additionally, the second- stage model accelerates inference, reducing the time by a factor of 200 compared to stage- 1, which needs 200 invocations to simulate the 20,000- pattern workload. ## 5 Application to Circuit SAT ### 5.1 Experiment Setting We evaluate the proposed methods (see Section 3.1) in the context of the Logic Equivalence Checking (LEC) task, one of the most critical CSAT problem determining whether two given circuit designs are logically equivalent. All the experiments, including model inference and SAT solving are conducted on an Intel(R) Platinum 8474C. Since CDCL SAT solvers exhibit fundamentally different behaviors when solving satisfiable and unsatisfiable instances (Chanseok 2015), we conduct separate evaluations of our proposed probability- based phase selection and clause filtering techniques. For example, solvers favor variable branching heuristics to quickly find satisfying assignments, but rely on advanced clause learning and prune strategies to prove unsatisfiability. To assess the phase selection method, we construct 150 hard satisfiable instances as LECSAT set by applying logic synthesis and minor revisions of different circuits from Forge EDA dataset (Shi et al. 2025a). Circuit pairs are then combined using miter construction via the ABC tool (Brayton and Mishchenko 2010). These instances have an average solving time of 17.39 seconds using Ca Di Ca L, ensuring that they are sufficiently challenging. Besides, to evaluate the proposed clause filtering tech nique, we construct another set of unsatisfiable instances LECUNSAT by generating miters from pairs of datapath circuits. In these cases, the original circuits are logically equivalent by design, ensuring that the resulting miter circuits are unsatisfiable, which cannot be solved within 20 seconds by Ca Di Cal. ### 5.2 Node Probability for Phase Selection <center>Figure 4: Solving Time on LECSAT (150 SAT cases) with probability-based phase selection(unit: seconds) </center> We use CASCAD to predict condition probability and evaluate our selection method(defined in Section 3.1) on 150 SAT instances from the LECSAT benchmark under different thresholds \\(\\tau\\) . Figure 4 compares solving times of our method (x- axis) under various threshold with Ca Di Ca L (y- axis), both in log scale. We also include the recent circuit representation model Deep Gate4 (Zheng et al. 2025) (black dots) as a comparison, which predicts node- level similarity and encourages assigning opposite values to functional similar nodes to trigger conflicts earlier and prune the search. Most points lie above the diagonal \\((y = x)\\) , indicating that our method outperforms the mainstream phase selection heuristic widely used in modern SAT solvers (has integrated into Ca Di Ca L). Compared to Deep Gate4, our method consistently achieves lower solving times, demonstrating more effective phase guidance. At \\(\\tau = 0.005\\) , our method achieves up to \\(10 \\times\\) speedup, and nearly \\(5 \\times\\) at \\(\\tau = 0.01\\) . Points further left imply lower average solving times, highlighting the overall efficiency of our probability- guided strategy across different \\(\\tau\\) values. In addition, the phase selection method serves as a lightweight classifier for UNSAT instances: since it greatly accelerates SAT solving\u2014if a case remains unsolved within a fixed time budget, it is likely to be UNSAT. This insight allows us to adapt solver parameters dynamically to better handle UNSAT- dominated scenarios(see Appendix A6), achieving \\(2 \\times\\) average speedup on LECUNSAT dataset.\n\n<center>Figure 5: Number of solved instances as a function of solving time using probability-based clause filtering. A total of 130 small case in LECUNSAT dataset. TIMEOUT = 400s. </center> Table 5: Average Par2 of 20 large cases in LECUNSAT using probability-based clause filtering. TIMEOUT \\(= 1000\\mathrm{s}\\) <table><tr><td>Threshold</td><td>Baseline</td><td>Ours.Solving</td><td>Ours.Overall</td></tr><tr><td>0.85</td><td rowspan=\"3\">1828.83</td><td>1560.72</td><td>1563.45</td></tr><tr><td>0.9</td><td>1406.19</td><td>1408.03</td></tr><tr><td>0.95</td><td>1396.13</td><td>1399.34</td></tr><tr><td></td><td></td><td>1429.60</td><td>1432.48</td></tr></table> ### 5.3 Clause Probability for Clause Management We validate our clause probability metric using a simple protocol. The solver runs until 50,000 conflicts are reached, after which all learnt clauses are extracted. Our CASCADE- model computes each clause's probability (as defined in Section 3.1) and retains only those below a predefined threshold. We apply this filtering in Ca Di Ca L with varying thresholds \\(\\tau\\) , and evaluate performance using the PAR- 2 (Par2) score, which penalizes timeouts with twice the cutoff time to reflect both efficiency and robustness. We evaluate our probability- guided clause filtering metric on LECUNSAT dataset (150 instances, split into 130 small and 20 hardest cases). Two configurations are compared: (1) the Ca Di Ca L baseline with mainstream LBD- based metric, and (2) our method with probability- based filtering. Total time includes both solving and model inference time. We test thresholds, 0.8, 0.85, 0.9, and 0.95, as shown in Figure 5 and Table 5. On 130 small cases, our methods consistently outperform the baseline by solving more instances within the same time limit. For the 20 challenging cases, our methods significantly outperform the baseline even when accounting for inference time. These results demonstrate that our filtering method identifies and preserves more useful clauses than the default LBD heuristic (see Appendix A5). The optimal threshold of 0.9 reflects a trade- off: low thresholds yield high- quality but sparse clauses, weakening propagation; high thresholds increase quantity but introduce noise. Table 6: Average Par2 of 150 SAT cases in LECSAT using probability-based phase selection with preprocessing. <table><tr><td>Method</td><td>Threshold</td><td>Solving</td><td>Overall</td></tr><tr><td>Preprocessing Only</td><td>-</td><td>13.39</td><td>13.39</td></tr><tr><td>Preprocessing</td><td>0.01</td><td>3.26</td><td>5.01</td></tr><tr><td>+</td><td>0.005</td><td>1.04</td><td>2.89</td></tr><tr><td>CASCADE</td><td>0.003</td><td>6.29</td><td>8.35</td></tr></table> Table 7: Average Par2 of 150 UNSAT cases using probability-based clause filtering with preprocessing. <table><tr><td>Setting</td><td>Threshold</td><td>Solving</td><td>Overall</td></tr><tr><td>Preprocessing Only</td><td>-</td><td>304.42</td><td>304.42</td></tr><tr><td rowspan=\"3\">Preprocessing + CASCADE</td><td>0.8</td><td>245.61</td><td>248.82</td></tr><tr><td>0.85</td><td>228.98</td><td>231.76</td></tr><tr><td>0.9</td><td>221.50</td><td>224.95</td></tr><tr><td></td><td>0.95</td><td>223.91</td><td>226.34</td></tr></table> ### 5.4 Effectiveness of Inprocessing Beyond Preprocessing To further validate the distinct effectiveness of our proposed inprocessing framework beyond preprocessing techniques, we compare the two strategies: - Preprocessing-only: We adopt the preprocessing techniques proposed in prior work (Shi et al. 2025c), to transform the circuit into a solver-friendly format.- Preprocessing + CASCADE: Building on the same preprocessing technique, we integrate it into the inprocessing stage via our CASCADE framework. As shown in Table 6 and Table 7, even when strong preprocessing technique has been applied, integrating our inprocessing framework leads to substantial additional speedups. For SAT cases, the best setting reduces the average PAR2 from 13.39 to 2.89 (a \\(4.6 \\times\\) improvement), while for UNSAT cases, our clause filtering achieves a reduction from 304.42 to 224.95. This highlights the complementary nature of the two techniques, and demonstrates that inprocessing contributes dynamic guidance during solving\u2014providing essential capabilities that static preprocessing alone cannot achieve. ## 6 Conclusion In this work, we propose CASCADE, a novel framework incorporating the inherent circuit information from CSAT instance into dynamic CDCL reasoning. By using a GNN- based model to estimate conditional probabilities of unassigned variable, CASCADE dynamically guides two key CDCL heuristics in the modern SAT solvers: variable phase selection and clause management. Extensive experiments on industry- standard benchmarks demonstrate that CASCADE achieves significant performance gains, reducing solving time by up to \\(90\\%\\) through probability- guided phase selection and achieving an additional \\(23.5\\%\\) reduction via clause filtering in logic equivalence scenarios.\n\n## References Amaru, L.; Gaillardon, P.- E.; Testa, E.; and Micheli, G. D. 2019. The EPFL Combinational Benchmark Suite. In 24th International Workshop on Logic & Synthesis (IWLS).Amizadeh, S.; Matusevych, S.; and Weimer, M. 2018. Learning to solve circuit- sat: An unsupervised differentiable approach. In International conference on learning representations.Armin, B.; Tobias, F.; Katalin, F.; Mathias, F.; Nils, F.; and Florian, P. 2024. Ca Di Ca L, Gimsatul, Isa SAT and Kissat entering the SAT Competition 2024. Proc. of SAT Competition, 8- 10. Audemard, G.; and Simon, L. 2018. On the Glucose SAT Solver. International Journal on Artificial Intelligence Tools, 27(01): 1840001. Brayton, R. K.; and Mishchenko, A. 2010. ABC: An Academic Industrial- Strength Verification Tool. In International Conference on Computer Aided Verification.Chanseok, O. 2015. Between SAT and UNSAT: the fundamental difference in CDCL SAT. In International Conference on Theory and Applications of Satisfiability Testing, 307- 323. Springer.Davidson, S. 1999. ITC'99 Benchmark Circuits - Preliminary Results. In International Test Conference 1999. Proceedings (IEEE Cat. No.99CH37034), 1125- 1125. E\u00e9n, N.; Mishchenko, A.; and S\u00f6rensson, N. 2007. Applying logic synthesis for speeding up SAT. In Theory and Applications of Satisfiability Testing - SAT 2007, 272- 286. Springer.Fang, W.; Li, W.; Liu, S.; Lu, Y.; Zhang, H.; and Xie, Z. 2025a. Net TAG: A Multimodal RTL- and- Layout- Aligned Netlist Foundation Model via Text- Attributed Graph. ar Xiv preprint ar Xiv:2504.09260. Fang, W.; Liu, S.; Wang, J.; and Xie, Z. 2025b. Circuitfusion: multimodal circuit representation learning for agile chip design. ar Xiv preprint ar Xiv:2505.02168. Fleury, A.; and Heisinger, M. 2020. Cadical, kissat, paracooba, plungeling and treengeling entering the sat competition 2020. Sat Competition, 2020: 50. Goldberg, E.; Prasad, M.; and Brayton, R. 2001. Using SAT for combinational equivalence checking. Design, Automation, and Test in Europe. Design, Automation, and Test in Europe.Kingma, D.; and Ba, J. 2014. Adam: A Method for Stochastic Optimization. ar Xiv: Learning, ar Xiv: Learning.Liu, J.; Zhai, J.; Zhao, M.; Lin, Z.; Yu, B.; and Shi, C. 2024. Polar Gate: Breaking the Functionality Representation Bottleneck of And- Inverter Graph Neural Network. In 2024 IEEE/ACM International Conference on Computer- Aided Design (ICCAD).Lu, F.; Wang, L.- C.; Cheng, K.- T.; and Huang, R.- Y. 2003. A circuit SAT solver with signal correlation guided learning. In 2003 Design, Automation and Test in Europe Conference and Exhibition, 892- 897. Marques- Silva, J.; Lynce, I.; and Malik, S. 2021. Conflict- driven clause learning SAT solvers. In Handbook of satisfiability, 133- 182. ios Press. Open Core, T. 1999. Open Core. https://opencores.org/.Qian, Y.; Chen, Z.; Zhang, X.; and Cai, S. 2025. X- SAT: An Efficient Circuit- Based SAT Solver. In 2025 62nd ACM/IEEE Design Automation Conference (DAC). IEEE.Shi, Z.; Li, Z.; Ma, C.; Zhou, Y.; Zheng, Z.; Liu, J.; Pan, H.; Zhou, L.; Li, K.; Zhu, J.; Yan, L.; He, Z.; Xue, C.; Jiang, W.; Yang, F.; Sun, G.; Yang, X.; Chen, G.; Shi, C.; Chu, Z.; Yang, J.; and Xu, Q. 2025a. Forge EDA: Towards Verifiable and Customizable Circuit Benchmarks. ar Xiv preprint ar Xiv:2505.02016. Shi, Z.; Ma, C.; Zheng, Z.; Zhou, L.; Pan, H.; Jiang, W.; Yang, F.; Yang, X.; Chu, Z.; and Xu, Q. 2025b. Deep Cell: Multiview Representation Learning for Post- Mapping Netlists. ar Xiv preprint ar Xiv:2502.06816. Shi, Z.; Pan, H.; Khan, S.; Li, M.; Liu, Y.; Huang, J.; Zhen, H.- L.; Yuan, M.; Chu, Z.; and Xu, Q. 2023. Deepgate2: Functionality- aware circuit representation learning. In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), 1- 9. IEEE.Shi, Z.; Tang, T.; Zhu, J.; Sadaf, K.; Zhen, H.- L.; Yuan, M.; Chu, Z.; and Xu, Q. 2025c. Logic Optimization Meets SAT: A Novel Framework for Circuit- SAT Solving. In 2025 62nd ACM/IEEE Design Automation Conference (DAC). IEEE.Shi, Z.; Zheng, Z.; Khan, S.; Zhong, J.; Li, M.; and Xu, Q. 2024. Deep Gate3: Towards Scalable Circuit Representation Learning. ar Xiv preprint ar Xiv:2407.11095. Stephan, P.; Brayton, R.; and Sangiovanni- Vincentelli, A. 1996. Combinational test generation using satisfiability. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, 1167- 1176. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is All you Need. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.Wang, Z.; Bai, C.; He, Z.; Zhang, G.; Xu, Q.; Ho, T.- Y.; Huang, Y.; and Yu, B. 2024. Fgnn2: A powerful pre- training framework for learning the logic functionality of circuits. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems.Wang, Z.; Bai, C.; He, Z.; Zhang, G.; Xu, Q.; Ho, T.- Y.; Yu, B.; and Huang, Y. 2022. Functionality matters in netlist representation learning. In Proceedings of the 59th ACM/IEEE Design Automation Conference, 61- 66. Wu, C.- A.; Lin, T.- H.; Lee, C.- C.; and Huang, C.- Y. 2007. Qute SAT: A Robust Circuit- based SAT Solver for Complex Circuit Structure. In 2007 Design, Automation & Test in Europe Conference & Exhibition, 1- 6. Wu, H.; Zheng, H.; Pu, Y.; and Yu, B. 2025. Circuit Representation Learning with Masked Gate Modeling and Verilog- AIG Alignment. ar Xiv preprint ar Xiv:2502.12732. Zheng, Z.; Huang, S.; Zhong, J.; Shi, Z.; Dai, G.; Xu, N.; and Xu, Q. 2025. Deep Gate4: Efficient and Effective Representation Learning for Circuit Design at Scale. ar Xiv preprint ar Xiv:2502.01681.\n\n## A1 Model Details Model Architecture. We adopt the DEEPGATE4 framework (Zheng et al. 2025), where each gate is represented by structural and functional embeddings and updated via selfattention (Vaswani et al. 2017). For each gate type \\(g\\in \\{\\mathrm{and},\\mathrm{not},\\mathrm{div}\\}\\) , we implement type- specific aggregation and update functions: not . The aggregator directly propagates the single input and applies a logical negation. and / virtual and. The aggregation function captures conjunction semantics, where attention weights reflect the relative importance of inputs- for example, giving higher weight to controlling inputs that can determine the output value. virtual div. In addition to a two- input aggregator, we account for the asymmetric roles of the inputs (numerator \\(A_{joint}\\) vs. denominator \\(C\\) ) by incorporating positional encodings to distinguish them during message passing. This unified framework enables the model to reason jointly over logical and probabilistic structures. Level- by- Level Propagation. Let \\(\\mathcal{L}(v)\\) denote the topological level of gate \\(v\\) . For every level \\(\\ell\\) (from primary inputs PI to primary outputs PO) the model performs: \\[\\begin{array}{r l} & {h_{v}^{s}\\leftarrow \\mathrm{Agg}_{g}^{s}\\big(\\{h_{u}^{s}\\mid u\\in P(v),\\mathcal{L}(u) = \\ell -1\\} \\big)}\\\\ & {h_{v}^{f}\\leftarrow \\mathrm{Agg}_{g}^{f}\\big(\\{h_{u}^{f},h_{u}^{s}\\mid u\\in P(v),\\mathcal{L}(u) = \\ell -1\\} \\big)} \\end{array} \\quad (A1)\\] where \\(P(v)\\) denotes the set of predecessor nodes of \\(v\\) ## A2 Effectiveness of div Gate The div gate explicitly models conditional probability in the circuit graph via the following equation: \\[P(A\\mid C) = \\frac{P(A\\wedge C)}{P(C)}. \\quad (A3)\\] While this formulation is convenient, the division operation can amplify errors significantly when \\(P(C)\\) is very small (always smaller than 1, as it represents the logic probability of node \\(C\\) ). To illustrate this behavior, we analyze two representative cases using ac97_ctrl.aig as a case study. Case 1: Extremely Small \\(P(C)\\) . In this scenario, when the conditional node rarely evaluates to 1 ( \\(P(C) = 0.01\\) ), even minor absolute errors in \\(P(A\\wedge C)\\) translate into large relative errors in \\(P(A\\mid C)\\) . Table A1 presents the ground- truth and predicted probabilities for \\(P(C)\\) and a subset of 20 target nodes \\(A\\) . The prediction error for \\(P(C)\\) is moderate ( \\(\\hat{P} (C) = 0.00323\\) ), yet the resulting conditional probabilities are significantly distorted due to error amplification. Case 2: Moderate \\(P(C)\\) . When \\(P(C)\\) is in a mid- range (e.g., \\(P(C) > 0.3\\) ), the division operation becomes relatively numerically stable. However, it still amplifies errors in \\(P(A\\mid C)\\) to an extent that is practically unacceptable. Table A2 compares aggregate losses, showing that the overall absolute error remains greater than 0.1. In contrast, our method incorporates the div gate during the dataset preparation stage. The probability labels for the DIV gate are computed as the division of the probabilities of its two input nodes, which are tagged with distinct positional markers. By learning this behavior as a standard gate type, the model directly internalizes the division operation, eliminating the need to explicitly compute the quotient of joint and marginal probabilities. ## A3 Pattern-Based Dataset To bootstrap training, we hope to generate pattern- based traces that approximate full truth tables. Given a circuit with \\(m\\) primary inputs (PIs), the complete truth table has \\(2^{m}\\) rows\u2014impractical beyond \\(m\\approx 20\\) . Instead, we conduct simulation with 20,000 patterns and uniformly sample 100 random patterns per circuit per epoch (see Table A3) and record the resulting logic probability values for every node. <table><tr><td>Pattern ID</td><td>PI1</td><td>PI2</td><td>PI3</td><td>...</td><td>PIm</td><td>Node u</td><td>Node v</td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>...</td><td>1</td><td>0</td><td>0</td></tr><tr><td>2</td><td>1</td><td>0</td><td>0</td><td>...</td><td>0</td><td>1</td><td>0</td></tr><tr><td>3</td><td>0</td><td>0</td><td>1</td><td>...</td><td>1</td><td>0</td><td>1</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>19997</td><td>0</td><td>1</td><td>1</td><td>...</td><td>0</td><td>1</td><td>1</td></tr><tr><td>19998</td><td>0</td><td>1</td><td>1</td><td>...</td><td>0</td><td>0</td><td>1</td></tr><tr><td>19999</td><td>1</td><td>0</td><td>1</td><td>...</td><td>1</td><td>1</td><td>0</td></tr><tr><td>20000</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>1</td></tr></table> Table A3: Excerpt of a truth table. Each training epoch picks a fresh random subset of 100 rows, ensuring the model learns to interpolate between unseen patterns. For each node \\(v\\) , we compute \\[\\hat{P}_{\\mathrm{rand}}(v) = \\frac{1}{100}\\sum_{i = 1}^{100}\\mathbf{1}\\big(v = 1\\mathrm{~in~row~}i\\big), \\quad (A4)\\] which serves as a supervisory signal encouraging the GNN to capture fine- grained functional behavior. Because the 100- row subset changes every epoch, the network is exposed to thousands of distinct local views, yielding better generalization than a single large simulation. ## A4 Workload-Based Dataset While random patterns provide breadth, they fail to capture realistic activity biases. We therefore introduce workload- based simulations, where each primary input (PI) is independently assigned a probability value \\(\\rho\\) sampled from the set \\(0.1, 0.2, \\ldots , 0.9\\) . A toy circuit ( \\(c = a \\wedge b\\) , Fig. A1) is used to illustrate the necessity.",
    "introduction": "<table><tr><td>i</td><td colspan=\"2\">P(C)</td><td colspan=\"2\">P(Ai\u2229C)</td><td colspan=\"2\">P(Ai|C)</td></tr><tr><td></td><td>ground truth</td><td>prediction</td><td>ground truth</td><td>prediction</td><td>ground truth</td><td>prediction</td></tr><tr><td>1</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00017</td><td>1</td><td>0.05277834</td></tr><tr><td>2</td><td>0.01</td><td>0.00323</td><td>0</td><td>0.00014</td><td>0</td><td>0.04442138</td></tr><tr><td>3</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00014</td><td>1</td><td>0.05133383</td></tr><tr><td>4</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00016</td><td>1</td><td>0.07029346</td></tr><tr><td>5</td><td>0.01</td><td>0.00323</td><td>0</td><td>0.00009</td><td>0</td><td>0.04350258</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Overall Error</td><td colspan=\"2\">0.00677</td><td colspan=\"2\">0.00786</td><td colspan=\"2\">0.72396</td></tr></table> Table A1: Performance of the division operation with extremely small \\(P(C)\\) \\(P(A_{i}\\mid C)\\) is calculated by \\(\\frac{P(A_{i}\\wedge C)}{P(C)}\\) <table><tr><td>i</td><td colspan=\"2\">P(C)</td><td colspan=\"2\">P(Ai\u2229C)</td><td colspan=\"2\">P(Ai|C)</td></tr><tr><td></td><td>gt</td><td>pred</td><td>gt</td><td>pred</td><td>gt</td><td>pred</td></tr><tr><td>1</td><td>0.42</td><td>0.4210119</td><td>0.24</td><td>0.24765863</td><td>0.5714286</td><td>0.58824617</td></tr><tr><td>2</td><td>0.42</td><td>0.4210119</td><td>0.33</td><td>0.34514248</td><td>0.7857143</td><td>0.8197927</td></tr><tr><td>3</td><td>0.42</td><td>0.4210119</td><td>0.31</td><td>0.3523217</td><td>0.7380953</td><td>0.83684504</td></tr><tr><td>4</td><td>0.42</td><td>0.4210119</td><td>0.36</td><td>0.3949966</td><td>0.8571429</td><td>0.93820775</td></tr><tr><td>5</td><td>0.42</td><td>0.4210119</td><td>0.38</td><td>0.3954696</td><td>0.9047619</td><td>0.6914532</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Overall Error</td><td colspan=\"2\">0.0010119</td><td colspan=\"2\">0.025311</td><td colspan=\"2\">0.101676</td></tr></table> Table A2: Performance of the division operation with moderate \\(P(C)\\) \\(P(A_{i}\\mid C)\\) is calculated by \\(\\frac{P(A_{i}\\wedge C)}{P(C)}\\) <table><tr><td rowspan=\"2\"></td><td colspan=\"3\">P&amp;lt;0.8</td><td colspan=\"3\">P\u22650.8</td></tr><tr><td>LBD</td><td>Num.</td><td>Time.Red.</td><td>Dec.Red.</td><td>Num.</td><td>Time.Red.</td></tr><tr><td>1</td><td>1515</td><td>47.1%</td><td>55.2%</td><td>2468</td><td>5.3%</td><td>18.9%</td></tr><tr><td>2</td><td>1819</td><td>21.2%</td><td>26.6%</td><td>7142</td><td>-71.0%</td><td>-0.2%</td></tr><tr><td>3</td><td>300</td><td>6.7%</td><td>6.5%</td><td>2728</td><td>-15.6%</td><td>4.9%</td></tr></table> <center>Figure A1: Toy circuit \\(c = a\\wedge b\\) </center> <table><tr><td>P(a)</td><td>P(b)</td><td>P(c=a\u2227b)</td></tr><tr><td>0.500</td><td>0.500</td><td>0.249</td></tr><tr><td>0.500</td><td>0.500</td><td>0.253</td></tr><tr><td>0.100</td><td>0.400</td><td>0.040</td></tr><tr><td>0.300</td><td>0.600</td><td>0.182</td></tr><tr><td>0.700</td><td>0.900</td><td>0.630</td></tr></table> Table A4: Node probabilities of the toy circuit (Fig. A1) under different PI workload levels \\(\\rho\\) . To illustrate this diversity, we simulate two contrasting scenarios (see Table A4). In the first case, all primary inputs (PIs) are assigned a fixed workload of \\(\\rho = 0.5\\) , under which each internal node converges to a stable logic probability; a representative slice is shown in the first two rows of Table A4. In the second case, each PI's \\(\\rho\\) is independently sampled from the set \\(0.1, 0.2, \\ldots , 0.9\\) , resulting in a broader range of logic probabilities across internal nodes. This variation exposes the network to a richer set of behaviors and Table A5: Statistics of clauses, solving time reduction, and decision count reduction under different LBD values and predicted probabilities. better reflects diverse operational conditions. ## A5 Analysis of Probability-Based Metric In our probability- based clause filtering method, the solving process is paused once the number of conflicts reaches 50,000, at which point all learnt clauses are extracted. For better analysis, these clauses are grouped according to their LBD values (1, 2, or 3) and further partitioned based on their predicted probabilities. Each group is then re- inserted into the solver independently, allowing us to evaluate their respective impacts on the remainder of the solving process. Table A5 presents statistics for the learnt clauses under different LBD levels, further divided by probability thresholds ( \\(< 0.8\\) and \\(\\geq 0.8\\) ). The table reports the number of clauses in each group, along with the corresponding reductions in solving time and decision count. From Table A5, we draw two key conclusions. First, for clauses with the same LBD, retaining those with lower\n\nprobability is more beneficial for subsequent solving, even though they are fewer in number. In fact, keeping only high- probability clauses can negatively impact solver performance. Second, we observe that the probability distribution of clauses closely matches the distribution of LBD: among clauses with lower LBD, a larger proportion have low probability. We further find that clause probability is a more informative metric than LBD alone. As a baseline, Ca Di Ca L simply discards all learnt clauses with LBD greater than 2. However, by filtering \\(LBD = 3\\) clauses based on probability and retaining only those with low probability, we can still achieve a positive effect on solving performance. ## A6 Applying Probability-Based Phase Selection for UNSAT Case The proposed phase selection method prioritizes variable assignments that are more likely to satisfy the circuit, effectively accelerating SAT solving, reducing solving time by up to \\(10\\times\\) on SAT instances. Leveraging this property, we regard the method as a lightweight classifier: for a given instance for solving, if the solver fails to find a solution within a predefined time budget under our phase selection heuristic, the instance is likely to be UNSAT. In such cases, we dynamically switch the solver to a configuration tuned specifically for UNSAT problems. We implement this strategy on top of Ca Di Ca L, using a simple timeout- based switching mechanism. We set the time budget as 5 seconds. Figure A2 summarizes the performance on the LECUNSAT dataset. On this dataset, the average solving time is reduced from 166.26 seconds (raw Ca Di Ca L) to 80.36 seconds using our adaptive strategy. As shown in Figure A2, the adaptive strategy consistently reduces solving time across all selected UNSAT- prone cases. This demonstrates that early- stage solver behavior under the SAT- oriented phase heuristic provides useful signals for unsatisfiability prediction, enabling effective and low- overhead adaptation to more suitable solving strategies. <center>Figure A2: Number of solved instances as a function of solving time using probability-based phase selection. A total of 150 UNSAT case in LECSAT dataset. TIMEOUT = 300s. </center>\n\n<center>Figure 1: Framework of CASCADE. We convert the original circuit into a DAG representation, augmented with virtual and and div gates. In the training pipeline, we employ pattern-based pre-training followed by workload-aware fine-tuning to train the model for conditional probability prediction. During inference, the predicted conditional probabilities are used to guide SAT solving, specifically in the inprocessing stages of phase selection and clause filtering. </center> achieves an additional \\(23.5\\%\\) reduction in total solving time on benchmarks from logic equivalence checking scenarios. In summary, our primary contributions include: - Introducing the first learning-based in-processing heuristics specifically designed for CSAT solving, effectively bridging static circuit representations and the dynamic CDCL solving process.- Developing a highly accurate GNN-based probabilistic model that predicts gate-level conditional probabilities, achieving validation accuracy of \\(96.69\\%\\) .- Demonstrating substantial empirical improvements by integrating the GNN-based conditional probability guidance into state-of-the-art SAT solvers, resulting in significant efficiency gains in real-world EDA applications. Our approach not only advances the theoretical understanding of leveraging structural information in SAT solving but also provides a robust framework for future solver enhancements and EDA tool designs. ## 2 Related Work ### 2.1 Circuit Representation Learning Function- aware representation learning has become a critical subfield in Electronic Design Automation (EDA), reflecting the broader trend in AI toward learning unified embeddings that can serve multiple downstream tasks. This area can be categorized into two main approaches: predictive models and contrastive models. In predictive models, Deep Gate (Shi et al. 2023, 2024; Zheng et al. 2025) and Deep Cell (Shi et al. 2025b) leverage asynchronous message passing neural network with representation disentanglement techniques to generate separate embeddings for functionality and structure, with pretraining across various EDA benchmarks. Polar Gate (Liu et al. 2024) further refines functional embeddings by incorporating ambipolar device principles. In contrastive models, FGNN (Wang et al. 2022, 2024) employs contrastive learning to align circuit embeddings based on functional similarity. Additionally, MGVGA (Wu et al. 2025), Circuit Fusion (Fang et al. 2025b), and Net TAG (Fang et al. 2025a) apply contrastive learning across multi- modal circuits to obtain function- invariant embeddings. In this paper, we focus on predicting exact conditional probabilities, for which we choose the state- of- the- art predictive model, Deep Gate4, as backbone. ### 2.2 Circuit Satisfiability The Boolean Satisfiability Problem (SAT) is a canonical NP- complete problem that determines whether a given propositional formula is satisfiable. As a variant of SAT, the Circuit Satisfiability Problem (CSAT, Circuit SAT) asks whether there exists an input assignment that makes the output of a given Boolean circuit evaluate to true. Several circuit- based solvers have been developed to address the Circuit SAT problem, aiming to operate directly on circuit format and leverage the structural information. For instance, NIMO (Lu et al. 2003) employs advanced circuit- level Boolean constraint propagation techniques to enhance conflict detection and decision- making. Similarly, Qute- SAT (Wu et al. 2007) incorporates conflict- driven learning strategies optimized for solving complex circuit topologies. Despite these innovations, such solvers generally offer only basic functionality and still fall short of the performance achieved by state- of- the- art CNF- based solvers on large- scale benchmarks.\n\nModern SAT solvers, such as Kissat (Armin et al. 2024), and Ca Di Cal (Fleury and Heisinger 2020), have demonstrated remarkable success in handling CNF- based instances. The de- facto standard workflow for solving CSAT problems is to translate circuit into the CNF formulas to be processed by highly- optimized SAT solvers. However, their performance on circuit- based problems is severely bottlenecked by the circuit- to- CNF transformation, which disrupts the structural properties of circuits and produces solver- unfriendly representations. Previous efforts, such as applying EDA- driven circuit optimization techniques (E\u00e9n, Mishchenko, and S\u00f6rensson 2007; Shi et al. 2025c) and extracting XOR logic on raw circuit (Qian et al. 2025) to reformat the problem into solver- friendly representations before solving, have shown attractive improvements. However, these approaches remain static\u2014they have no impact on solving heuristics and are inactive during the inprocessing phase, a critical period where search decisions and analysis routines are dynamically adjusted. As a result, the rich structural intelligence of circuits remains untapped precisely when it could be most beneficial. ## 3 Methodology ### 3.1 Problem Definition During CDCL solving, the CNF formula evolves dynamically through decision making, conflict analysis, and clause management routines. Two core heuristics\u2014phase selection and clause filtering\u2014play a central role in guiding the search and maintaining solver efficiency. Phase selection determines polarity (true/false) of variable assignments during the search process, aiming to minimize conflicts and accelerate convergence. Meanwhile, clause filtering focuses on evaluating and managing learned clauses derived from conflict analysis, retaining high- quality ones that improve propagation efficiency while discarding low- value clauses to reduce memory overhead. We reformulate both heuristics as conditional probability prediction tasks over circuit structures. Phase Selection Given a Boolean circuit \\(C\\) with primary output (PO), we formulate phase selection as a probability inference problem over the circuit graph. Let \\(s\\in V\\) denote a variable corresponding to a gate in the circuit. At each decision step \\(t\\) , the solver selects a variable \\(s\\in V\\) and estimates the conditional probability: \\(P(s = 1\\mid \\mathrm{PO} = 1)\\) which measures the likelihood that setting \\(s = 1\\) helps satisfy the output. By leveraging these probabilities to guide phase assignments, the solver implicitly favors value choices that are more aligned with satisfying assignments\u2014often leading to stronger propagation and fewer conflicts. A threshold \\(\\tau \\in (0,0.5)\\) governs the phase assignment: \\[v_{s} = \\left\\{ \\begin{array}{ll}0, & \\mathrm{if~}P(s = 1\\mid \\mathrm{PO} = 1)< \\tau ,\\\\ 1, & \\mathrm{if~}P(s = 1\\mid \\mathrm{PO} = 1) > 1 - \\tau ,\\\\ \\mathrm{default,} & \\mathrm{otherwise.} \\end{array} \\right. \\quad (1)\\] This probability inference strategy biases phase decisions toward values more likely to satisfy downstream logic, improving propagation and reducing conflicts. <center>Figure 2: Augmented graph with virtual and and div gates. </center> Clause Management Existing metrics for clause quality like LBD and clause length rely solely on CNF- level features, ignoring structural insights from the original circuit, thus hindering performance on circuit- derived problems. We propose a new metric, clause probability, to evaluate the quality of learned clauses by estimating their likelihood of being satisfied under structural semantics. Consider a learned clause \\(C = l_{1}\\lor l_{2}\\lor \\dots \\lor l_{k}\\) , which is semantically equivalent to a \\(k\\) - input OR gate over its literals. The probability that this clause is satisfied can be expressed as: \\[P(C) = P(l_{1}\\lor l_{2}\\lor \\dots \\lor l_{k}). \\quad (2)\\] where \\(P(l_{i})\\) is the estimated probability of literal \\(l_{i}\\) being true, derived from circuit- level signal estimation. This probabilistic view provides an intuitive interpretation: a clause with high probability \\(P(C)\\) is likely to be satisfied easily, implying that it encodes a weak constraint and has limited impact on restricting the search space. In contrast, a clause with low \\(P(C)\\) represents a strong constraint that is harder to satisfy, hence has higher potential to prune large infeasible regions in the search space. Therefore, we regard clauses with lower \\(P(C)\\) as more informative and prioritize them during inprocessing. Concretely, during periodic clause database elimination, the solver evaluates \\(P(C)\\) for all learned clauses and retains those with the lowest probabilities. This filtering mechanism allows the solver to preserve high- quality constraints that are more aligned with the underlying circuit semantics. We adopt the DEEPGATE4 framework (Zheng et al. 2025) as our encoder \\(E\\) , which generates structure and function embeddings for each gate in the circuit graph \\(G\\) . We use these embeddings to estimate conditional probabilities that guide inprocessing heuristics, enabling more efficient, probability- aware Circuit SAT solving. ### 3.2 Graph Construction To enable probability- aware learning on circuit graphs, we construct directed acyclic graphs (DAGs) where each node represents a logical gate, and further augment with virtual gates for better probability modeling, as shown in Figure 2. DAG Construction with and and not Gates Following prior work (Shi et al. 2023, 2024; Zheng et al. 2025; Liu et al. 2024), we begin by constructing a standard DAG representation of the input circuit. Each logic gate is represented as a node, and edges represent signal propagation between gates. Specifically, for a given circuit graph \\(G\\) , we construct\n\na DAG comprising two basic gate types: and gates for binary conjunctions and not gates for logic inversion. We adopt the aggregator \\(Aggr_{and}\\) and \\(Aggr_{not}\\) for and gates and not gates respectively, following Deep Gate4 framework(Zheng et al. 2025). Virtual and Gates for Joint Probabilities To expose joint probabilities directly in the graph, we insert virtual and gates in \\(\\mathcal{G}\\) : for any node \\(A\\) conditioned on \\(C\\) , we add \\[A_{joint} = A\\wedge C, \\quad (3)\\] so the model can observe the joint probability \\(P(A\\wedge C)\\) directly, denoted as \\(P(A_{joint})\\) , without intermediate arithmetic computations. When encode the virtual and gate, we directly adopt the \\(Aggr_{and}\\) to get the embedding and predict the joint probability as: \\[h^{A_{joint}} = Aggr(h^{A},h^{C}),\\hat{P} (A_{joint}) = \\phi (h^{A_{joint}}), \\quad (4)\\] where \\(\\phi\\) is a 3- layer MLPs. Virtual div Gates for Conditional Probabilities As joint probabilities can be modeled via virtual and gates, computing conditional probabilities such as \\[P(A\\mid C) = \\frac{P(A\\wedge C)}{P(C)} = \\frac{P(A_{joint})}{P(C)} \\quad (5)\\] can lead to amplified errors, especially when \\(P(C)\\) is small. Details are shown in Appendix A2. Nodes with extremely low probabilities- - referred to as polar nodes (i.e., those with truth- table probabilities below 0.1)- are empirically harder to learn and tend to amplify prediction errors. These nodes require special attention to ensure accurate and reliable modeling. To address this, we introduce virtual div gates to represent conditional probabilities directly within the graph. Each such gate takes two inputs: the numerator, which is a virtual and gate representing the joint event \\(A_{\\mathrm{joint}} = A\\wedge C\\) , and the denominator, which corresponds to the condition node \\(C\\) . Then with aggregator \\(Aggr_{div}\\) designed for div gate, we first get the embedding with: \\[h^{A_{cond}} = Aggr_{div}(h^{A_{joint}},h^{C}). \\quad (6)\\] Then predict the probability with task head \\(\\phi\\) : \\[\\hat{P} (A_{cond}) = \\phi (h^{A_{cond}}) \\quad (7)\\] This design enables the model to predict \\(P(A\\mid C)\\) directly from graph context, reducing error sensitivity to small denominators. ### 3.3 Two-Stage Training Strategy We train the network with a two- stage training strategy: 1. Pattern-Based Pre-training. Previous works (Shi et al. 2023, 2024; Zheng et al. 2025; Liu et al. 2024) initialize PI embeddings using input probabilities, however, it will lead to information distortion. As shown in Table 1, distinct input subsets (0-3 and 4-7) may share the same empirical PI probability (0.5), despite differing in actual assignments. This Table 1: Example of simulation patterns and probability. <table><tr><td>Pattern ID</td><td>P1</td><td>P2</td><td>P3</td><td>P4</td><td>Node x</td><td>Node y</td><td>Node z</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>4</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>5</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>6</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td></tr><tr><td>7</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr></table> illustrates a key limitation: probabilities are coarse- grained and may fail to capture fine- grained circuit behavior. To address this, we use 100 random patterns per minibatch to compute fine- grained probability features, which are fed into the model. This supervision enables our model to capture both statistical trends and functional semantics. Suppose the circuit contains \\(n\\) nodes in total, where the first \\(m\\) nodes are designated as primary inputs (PIs). For each primary input (PI) node, a 100- bit simulation trace is encoded into an initial embedding using an autoencoder. Embeddings for all other nodes are then propagated level- by- level through the circuit. The stage- 1 training loss is defined as: \\[\\begin{array}{l}{\\mathcal{L}_{\\mathrm{stage1}} = w_{1}\\cdot \\frac{1}{n - m}\\sum_{i = m + 1}^{n}L1(P_{i} - \\hat{P}_{i})}\\\\ {+\\ w_{2}\\cdot \\frac{1}{m}\\sum_{j = 1}^{m}L1(P_{j} - \\hat{P}_{j})} \\end{array} \\quad (8)\\] Here, the L1 Loss function is defined as: \\[\\mathrm{L1}(a,b) = |a - b| \\quad (9)\\] where \\(P_{i}\\) denotes the ground- truth probability of the \\(i\\) - th node, and \\(\\hat{P}_{i}\\) is the predicted value. The second term focuses on PI nodes, encouraging accurate reconstruction of their probabilities, since their embeddings are initialized from actual simulation data. 2. Workload-Aware Fine-tuning. Although training on batches of 100 patterns helps capture fine-grained circuit behaviors, our ultimate goal is to generalize to the statistical properties of large-scale pattern distributions. To bridge this gap and enhance data diversity, we perform 200 simulations, each using 100 random patterns with designated PI workloads drawn from \\(\\{0.1,0.2,\\ldots ,0.9\\}\\) (detailed in Appendix A4). For each PI, the 100- bit trace in a simulation is averaged into a single probability, resulting in a 200- dimensional vector that captures its behavior under diverse input distributions. This vector is then passed through an MLP to produce the PIs' initial embedding. For internal nodes, ground- truth probabilities are computed by aggregating predictions over all \\(200\\times 100 = 20,000\\) input patterns, serving as training targets in stage- 2.\n\n<center>Figure 3: Example of conditional probability calculation for three conditions </center> The overall loss for stage- 2 is defined as: \\[\\begin{array}{rl} & {\\mathcal{L}_{\\mathrm{stage2}} = w_1\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{all}}|}\\sum_{i\\in \\mathcal{S}_{\\mathrm{all}}}L1(P_i - \\hat{P}_i)}\\\\ & {\\qquad +w_2\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{div}}|}\\sum_{j\\in \\mathcal{S}_{\\mathrm{div}}}L1(P_j - \\hat{P}_j)}\\\\ & {\\qquad +w_3\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{polar}}|}\\sum_{k\\in \\mathcal{S}_{\\mathrm{polar}}}L1(P_k - \\hat{P}_k)} \\end{array} \\quad (10)\\] Here, Sall denotes all internal nodes in the circuit, Sdiv refers to div nodes, and Spolar includes polarized nodes with low probability \\((P< 0.1)\\) Higher weights \\(w_{2}\\) and \\(w_{3}\\) prioritize structurally and semantically critical nodes: div nodes reflect key conditional probabilities, while polarized nodes are harder to predict but crucial for tasks like Circuit SAT. ### 3.4 Multiple Condition Probability Through Equation 5, our model can estimate the probability of a target node conditioned on a single other node. However, in practical Circuit SAT scenarios, it is often more useful to infer the most likely value of a node given that multiple other nodes have already been assigned. As illustrated in Figure 3, when multiple condition nodes (denoted as \\(C_1, C_2, \\ldots , C_k\\) ) are involved, we combine them using a chain of two- input and gates to construct a single aggregated condition node \\(C\\) . This transformation enables the model to capture the joint condition \\(C = C_1 \\wedge C_2 \\wedge \\ldots \\wedge C_k\\) through logical conjunction. Once the final condition node \\(C\\) is obtained, we apply Equation 5 in the same manner as with a single condition node, treating \\(P(A | C)\\) as the target. This allows the model to generalize single- condition inference to multi- condition scenarios in a structurally consistent way. ## 4 Experiments ### 4.1 Dataset Generation In most cases, conditioning on a specific node \\(C\\) primarily affects a local subregion of the circuit rather than its entirety. We define this subregion as the influence area of \\(C\\) , constructed in three steps. First, we identify the fanin cone of \\(C\\) . Within this cone, we then locate an ancestor node \\(P\\) that satisfies two conditions: it has multiple fanouts, and it differs from \\(C\\) by no more than 10 logic levels. Finally, starting from \\(P\\) , we collect all nodes within its fanout cone, bounded to a depth of 10 logic levels downstream. We use a dataset of 10,824 AIGs from ITC99 (Davidson 1999), EPFL (Amaru et al. 2019), and Open Core (Open Core 1999), with circuit sizes ranging from 36 to 3,214 gates. To make conditional information explicitly available to the model, we insert and and div gates within each node's influence area. For supervision, we conduct 20,000 random simulations per circuit to record full truth assignments. ### 4.2 Experiment Setting In the one- round GNN model configuration, both the structural embedding \\(h^s\\) and the functional embedding \\(h^f\\) are set to a dimension of 128. The task head, \\(\\phi\\) , has 3 hidden layers with 32 neurons and utilizes the Si LU activation function. The model is trained for 60 epochs to ensure convergence with a batch size of 512 using a single NVIDIA RTX 4090 GPU. The Adam optimizer (Kingma and Ba 2014) is used with a learning rate of \\(10^{- 4}\\) . ### 4.3 Performance on Conditional Probability Prediction Table 2: Conditional probability prediction performance (MAE) under varying condition counts and circuit sizes. <table><tr><td>#Condition Nodes</td><td>#Nodes=928</td><td>#Nodes=6428</td><td>#Nodes=17796</td></tr><tr><td>1</td><td>0.0243</td><td>0.0372</td><td>0.0457</td></tr><tr><td>2</td><td>0.0267</td><td>0.0401</td><td>0.0496</td></tr><tr><td>5</td><td>0.0314</td><td>0.0479</td><td>0.0532</td></tr></table> We train the model on small circuits and evaluate its conditional probability prediction accuracy on both small and large- scale benchmarks. In the single- condition setting, the average L1 loss on the validation set reaches as low as 0.0331. To illustrate generalizability, we select representative circuits of varying sizes. As shown in Table 2, the model maintains low prediction error across all scales, with L1 loss as low as 0.0243 for small circuits and remaining below 0.05 even for circuits with over 17K nodes. As the number of conditions increases from 1 to 5, prediction error rises slightly yet remains within acceptable bounds, demonstrating strong robustness. Importantly, inference overhead under multi- condition settings is negligible, thanks to the lightweight design of the aggregator functions of and gates. ### 4.4 Ablation Study Effectiveness of div Gate Table 3 compares our div gate- based model with direct division for conditional probability prediction. Across all settings, div gates yield consistently lower MAE, e.g., reducing error from 0.1173 to 0.0312 with one moderate condition. This highlights their advantage in numerical stability and learning robustness, while direct division suffers from unstable gradients and sensitivity to small denominators.\n\nTable 3: Ablation study on the effectiveness of using div gates for computing conditional probability (MAE). Here, \"#Cond.\" indicates the number of condition nodes involved. <table><tr><td rowspan=\"2\"># Cond.</td><td colspan=\"2\">Moderate Condition</td><td colspan=\"2\">Polar Condition</td></tr><tr><td>div gate</td><td>direct division</td><td>div gate</td><td>direct division</td></tr><tr><td>1</td><td>0.0312</td><td>0.1173</td><td>0.0395</td><td>0.7129</td></tr><tr><td>2</td><td>0.0339</td><td>0.1495</td><td>0.0423</td><td>0.7940</td></tr><tr><td>5</td><td>0.0368</td><td>0.2030</td><td>0.0486</td><td>0.8546</td></tr></table> Table 4: Ablation study on the effectiveness of our two-stage training strategy (MAE). <table><tr><td># Condition Nodes</td><td>Two-stage</td><td>Only Stage-1</td><td>Only Stage-2</td></tr><tr><td>1</td><td>0.0331</td><td>0.0422</td><td>0.0435</td></tr><tr><td>2</td><td>0.0352</td><td>0.0583</td><td>0.0493</td></tr><tr><td>5</td><td>0.0387</td><td>0.0738</td><td>0.0588</td></tr></table> Effectiveness of Training Strategy We evaluate the effectiveness of our two- stage training strategy, where stage1 learns fine- grained behavior from a 100- pattern simulation and stage- 2 captures broader statistical distributions using 20,000- pattern data. As shown in Table 4, the two- stage model consistently achieves the lowest L1 loss, outperforming both stage- 1 and stage- 2 models individually. For instance, with one condition node, L1 loss improves from 0.0422 (stage- 1) and 0.0435 (stage- 2) to 0.0331 (two- stage). Additionally, the second- stage model accelerates inference, reducing the time by a factor of 200 compared to stage- 1, which needs 200 invocations to simulate the 20,000- pattern workload. ## 5 Application to Circuit SAT ### 5.1 Experiment Setting We evaluate the proposed methods (see Section 3.1) in the context of the Logic Equivalence Checking (LEC) task, one of the most critical CSAT problem determining whether two given circuit designs are logically equivalent. All the experiments, including model inference and SAT solving are conducted on an Intel(R) Platinum 8474C. Since CDCL SAT solvers exhibit fundamentally different behaviors when solving satisfiable and unsatisfiable instances (Chanseok 2015), we conduct separate evaluations of our proposed probability- based phase selection and clause filtering techniques. For example, solvers favor variable branching heuristics to quickly find satisfying assignments, but rely on advanced clause learning and prune strategies to prove unsatisfiability. To assess the phase selection method, we construct 150 hard satisfiable instances as LECSAT set by applying logic synthesis and minor revisions of different circuits from Forge EDA dataset (Shi et al. 2025a). Circuit pairs are then combined using miter construction via the ABC tool (Brayton and Mishchenko 2010). These instances have an average solving time of 17.39 seconds using Ca Di Ca L, ensuring that they are sufficiently challenging. Besides, to evaluate the proposed clause filtering tech nique, we construct another set of unsatisfiable instances LECUNSAT by generating miters from pairs of datapath circuits. In these cases, the original circuits are logically equivalent by design, ensuring that the resulting miter circuits are unsatisfiable, which cannot be solved within 20 seconds by Ca Di Cal. ### 5.2 Node Probability for Phase Selection <center>Figure 4: Solving Time on LECSAT (150 SAT cases) with probability-based phase selection(unit: seconds) </center> We use CASCAD to predict condition probability and evaluate our selection method(defined in Section 3.1) on 150 SAT instances from the LECSAT benchmark under different thresholds \\(\\tau\\) . Figure 4 compares solving times of our method (x- axis) under various threshold with Ca Di Ca L (y- axis), both in log scale. We also include the recent circuit representation model Deep Gate4 (Zheng et al. 2025) (black dots) as a comparison, which predicts node- level similarity and encourages assigning opposite values to functional similar nodes to trigger conflicts earlier and prune the search. Most points lie above the diagonal \\((y = x)\\) , indicating that our method outperforms the mainstream phase selection heuristic widely used in modern SAT solvers (has integrated into Ca Di Ca L). Compared to Deep Gate4, our method consistently achieves lower solving times, demonstrating more effective phase guidance. At \\(\\tau = 0.005\\) , our method achieves up to \\(10 \\times\\) speedup, and nearly \\(5 \\times\\) at \\(\\tau = 0.01\\) . Points further left imply lower average solving times, highlighting the overall efficiency of our probability- guided strategy across different \\(\\tau\\) values. In addition, the phase selection method serves as a lightweight classifier for UNSAT instances: since it greatly accelerates SAT solving\u2014if a case remains unsolved within a fixed time budget, it is likely to be UNSAT. This insight allows us to adapt solver parameters dynamically to better handle UNSAT- dominated scenarios(see Appendix A6), achieving \\(2 \\times\\) average speedup on LECUNSAT dataset.\n\n<center>Figure 5: Number of solved instances as a function of solving time using probability-based clause filtering. A total of 130 small case in LECUNSAT dataset. TIMEOUT = 400s. </center> Table 5: Average Par2 of 20 large cases in LECUNSAT using probability-based clause filtering. TIMEOUT \\(= 1000\\mathrm{s}\\) <table><tr><td>Threshold</td><td>Baseline</td><td>Ours.Solving</td><td>Ours.Overall</td></tr><tr><td>0.85</td><td rowspan=\"3\">1828.83</td><td>1560.72</td><td>1563.45</td></tr><tr><td>0.9</td><td>1406.19</td><td>1408.03</td></tr><tr><td>0.95</td><td>1396.13</td><td>1399.34</td></tr><tr><td></td><td></td><td>1429.60</td><td>1432.48</td></tr></table> ### 5.3 Clause Probability for Clause Management We validate our clause probability metric using a simple protocol. The solver runs until 50,000 conflicts are reached, after which all learnt clauses are extracted. Our CASCADE- model computes each clause's probability (as defined in Section 3.1) and retains only those below a predefined threshold. We apply this filtering in Ca Di Ca L with varying thresholds \\(\\tau\\) , and evaluate performance using the PAR- 2 (Par2) score, which penalizes timeouts with twice the cutoff time to reflect both efficiency and robustness. We evaluate our probability- guided clause filtering metric on LECUNSAT dataset (150 instances, split into 130 small and 20 hardest cases). Two configurations are compared: (1) the Ca Di Ca L baseline with mainstream LBD- based metric, and (2) our method with probability- based filtering. Total time includes both solving and model inference time. We test thresholds, 0.8, 0.85, 0.9, and 0.95, as shown in Figure 5 and Table 5. On 130 small cases, our methods consistently outperform the baseline by solving more instances within the same time limit. For the 20 challenging cases, our methods significantly outperform the baseline even when accounting for inference time. These results demonstrate that our filtering method identifies and preserves more useful clauses than the default LBD heuristic (see Appendix A5). The optimal threshold of 0.9 reflects a trade- off: low thresholds yield high- quality but sparse clauses, weakening propagation; high thresholds increase quantity but introduce noise. Table 6: Average Par2 of 150 SAT cases in LECSAT using probability-based phase selection with preprocessing. <table><tr><td>Method</td><td>Threshold</td><td>Solving</td><td>Overall</td></tr><tr><td>Preprocessing Only</td><td>-</td><td>13.39</td><td>13.39</td></tr><tr><td>Preprocessing</td><td>0.01</td><td>3.26</td><td>5.01</td></tr><tr><td>+</td><td>0.005</td><td>1.04</td><td>2.89</td></tr><tr><td>CASCADE</td><td>0.003</td><td>6.29</td><td>8.35</td></tr></table> Table 7: Average Par2 of 150 UNSAT cases using probability-based clause filtering with preprocessing. <table><tr><td>Setting</td><td>Threshold</td><td>Solving</td><td>Overall</td></tr><tr><td>Preprocessing Only</td><td>-</td><td>304.42</td><td>304.42</td></tr><tr><td rowspan=\"3\">Preprocessing + CASCADE</td><td>0.8</td><td>245.61</td><td>248.82</td></tr><tr><td>0.85</td><td>228.98</td><td>231.76</td></tr><tr><td>0.9</td><td>221.50</td><td>224.95</td></tr><tr><td></td><td>0.95</td><td>223.91</td><td>226.34</td></tr></table> ### 5.4 Effectiveness of Inprocessing Beyond Preprocessing To further validate the distinct effectiveness of our proposed inprocessing framework beyond preprocessing techniques, we compare the two strategies: - Preprocessing-only: We adopt the preprocessing techniques proposed in prior work (Shi et al. 2025c), to transform the circuit into a solver-friendly format.- Preprocessing + CASCADE: Building on the same preprocessing technique, we integrate it into the inprocessing stage via our CASCADE framework. As shown in Table 6 and Table 7, even when strong preprocessing technique has been applied, integrating our inprocessing framework leads to substantial additional speedups. For SAT cases, the best setting reduces the average PAR2 from 13.39 to 2.89 (a \\(4.6 \\times\\) improvement), while for UNSAT cases, our clause filtering achieves a reduction from 304.42 to 224.95. This highlights the complementary nature of the two techniques, and demonstrates that inprocessing contributes dynamic guidance during solving\u2014providing essential capabilities that static preprocessing alone cannot achieve. ## 6 Conclusion In this work, we propose CASCADE, a novel framework incorporating the inherent circuit information from CSAT instance into dynamic CDCL reasoning. By using a GNN- based model to estimate conditional probabilities of unassigned variable, CASCADE dynamically guides two key CDCL heuristics in the modern SAT solvers: variable phase selection and clause management. Extensive experiments on industry- standard benchmarks demonstrate that CASCADE achieves significant performance gains, reducing solving time by up to \\(90\\%\\) through probability- guided phase selection and achieving an additional \\(23.5\\%\\) reduction via clause filtering in logic equivalence scenarios.",
    "references_amaru_l_gaillardon_p-_e_testa_e_and_micheli_g_d_2019_the_epfl_combinational_benchmark_suite_in_24th_international_workshop_on_logic_synthesis_iwlsamizadeh_s_matusevych_s_and_weimer_m_2018_learning_to_solve_circuit-_sat_an_unsupervised_differentiable_approach_in_international_conference_on_learning_representationsarmin_b_tobias_f_katalin_f_mathias_f_nils_f_and_florian_p_2024_ca_di_ca_l_gimsatul_isa_sat_and_kissat_entering_the_sat_competition_2024_proc_of_sat_competition_8-_10_audemard_g_and_simon_l_2018_on_the_glucose_sat_solver_international_journal_on_artificial_intelligence_tools_2701_1840001_brayton_r_k_and_mishchenko_a_2010_abc_an_academic_industrial-_strength_verification_tool_in_international_conference_on_computer_aided_verificationchanseok_o_2015_between_sat_and_unsat_the_fundamental_difference_in_cdcl_sat_in_international_conference_on_theory_and_applications_of_satisfiability_testing_307-_323_springerdavidson_s_1999_itc99_benchmark_circuits_-_preliminary_results_in_international_test_conference_1999_proceedings_ieee_cat_no99ch37034_1125-_1125_e\u00e9n_n_mishchenko_a_and_s\u00f6rensson_n_2007_applying_logic_synthesis_for_speeding_up_sat_in_theory_and_applications_of_satisfiability_testing_-_sat_2007_272-_286_springerfang_w_li_w_liu_s_lu_y_zhang_h_and_xie_z_2025a_net_tag_a_multimodal_rtl-_and-_layout-_aligned_netlist_foundation_model_via_text-_attributed_graph_ar_xiv_preprint_ar_xiv250409260_fang_w_liu_s_wang_j_and_xie_z_2025b_circuitfusion_multimodal_circuit_representation_learning_for_agile_chip_design_ar_xiv_preprint_ar_xiv250502168_fleury_a_and_heisinger_m_2020_cadical_kissat_paracooba_plungeling_and_treengeling_entering_the_sat_competition_2020_sat_competition_2020_50_goldberg_e_prasad_m_and_brayton_r_2001_using_sat_for_combinational_equivalence_checking_design_automation_and_test_in_europe_design_automation_and_test_in_europekingma_d_and_ba_j_2014_adam_a_method_for_stochastic_optimization_ar_xiv_learning_ar_xiv_learningliu_j_zhai_j_zhao_m_lin_z_yu_b_and_shi_c_2024_polar_gate_breaking_the_functionality_representation_bottleneck_of_and-_inverter_graph_neural_network_in_2024_ieeeacm_international_conference_on_computer-_aided_design_iccadlu_f_wang_l-_c_cheng_k-_t_and_huang_r-_y_2003_a_circuit_sat_solver_with_signal_correlation_guided_learning_in_2003_design_automation_and_test_in_europe_conference_and_exhibition_892-_897_marques-_silva_j_lynce_i_and_malik_s_2021_conflict-_driven_clause_learning_sat_solvers_in_handbook_of_satisfiability_133-_182_ios_press_open_core_t_1999_open_core_httpsopencoresorgqian_y_chen_z_zhang_x_and_cai_s_2025_x-_sat_an_efficient_circuit-_based_sat_solver_in_2025_62nd_acmieee_design_automation_conference_dac_ieeeshi_z_li_z_ma_c_zhou_y_zheng_z_liu_j_pan_h_zhou_l_li_k_zhu_j_yan_l_he_z_xue_c_jiang_w_yang_f_sun_g_yang_x_chen_g_shi_c_chu_z_yang_j_and_xu_q_2025a_forge_eda_towards_verifiable_and_customizable_circuit_benchmarks_ar_xiv_preprint_ar_xiv250502016_shi_z_ma_c_zheng_z_zhou_l_pan_h_jiang_w_yang_f_yang_x_chu_z_and_xu_q_2025b_deep_cell_multiview_representation_learning_for_post-_mapping_netlists_ar_xiv_preprint_ar_xiv250206816_shi_z_pan_h_khan_s_li_m_liu_y_huang_j_zhen_h-_l_yuan_m_chu_z_and_xu_q_2023_deepgate2_functionality-_aware_circuit_representation_learning_in_2023_ieeeacm_international_conference_on_computer_aided_design_iccad_1-_9_ieeeshi_z_tang_t_zhu_j_sadaf_k_zhen_h-_l_yuan_m_chu_z_and_xu_q_2025c_logic_optimization_meets_sat_a_novel_framework_for_circuit-_sat_solving_in_2025_62nd_acmieee_design_automation_conference_dac_ieeeshi_z_zheng_z_khan_s_zhong_j_li_m_and_xu_q_2024_deep_gate3_towards_scalable_circuit_representation_learning_ar_xiv_preprint_ar_xiv240711095_stephan_p_brayton_r_and_sangiovanni-_vincentelli_a_1996_combinational_test_generation_using_satisfiability_ieee_transactions_on_computer-_aided_design_of_integrated_circuits_and_systems_1167-_1176_vaswani_a_shazeer_n_parmar_n_uszkoreit_j_jones_l_gomez_a_n_kaiser_l_u_and_polosukhin_i_2017_attention_is_all_you_need_in_guyon_i_luxburg_u_v_bengio_s_wallach_h_fergus_r_vishwanathan_s_and_garnett_r_eds_advances_in_neural_information_processing_systems_volume_30_curran_associates_incwang_z_bai_c_he_z_zhang_g_xu_q_ho_t-_y_huang_y_and_yu_b_2024_fgnn2_a_powerful_pre-_training_framework_for_learning_the_logic_functionality_of_circuits_ieee_transactions_on_computer-_aided_design_of_integrated_circuits_and_systemswang_z_bai_c_he_z_zhang_g_xu_q_ho_t-_y_yu_b_and_huang_y_2022_functionality_matters_in_netlist_representation_learning_in_proceedings_of_the_59th_acmieee_design_automation_conference_61-_66_wu_c-_a_lin_t-_h_lee_c-_c_and_huang_c-_y_2007_qute_sat_a_robust_circuit-_based_sat_solver_for_complex_circuit_structure_in_2007_design_automation_test_in_europe_conference_exhibition_1-_6_wu_h_zheng_h_pu_y_and_yu_b_2025_circuit_representation_learning_with_masked_gate_modeling_and_verilog-_aig_alignment_ar_xiv_preprint_ar_xiv250212732_zheng_z_huang_s_zhong_j_shi_z_dai_g_xu_n_and_xu_q_2025_deep_gate4_efficient_and_effective_representation_learning_for_circuit_design_at_scale_ar_xiv_preprint_ar_xiv250201681": "",
    "a1_model_details_model_architecture_we_adopt_the_deepgate4_framework_zheng_et_al_2025_where_each_gate_is_represented_by_structural_and_functional_embeddings_and_updated_via_selfattention_vaswani_et_al_2017_for_each_gate_type_gin_mathrmandmathrmnotmathrmdiv_we_implement_type-_specific_aggregation_and_update_functions_not_the_aggregator_directly_propagates_the_single_input_and_applies_a_logical_negation_and_virtual_and_the_aggregation_function_captures_conjunction_semantics_where_attention_weights_reflect_the_relative_importance_of_inputs-_for_example_giving_higher_weight_to_controlling_inputs_that_can_determine_the_output_value_virtual_div_in_addition_to_a_two-_input_aggregator_we_account_for_the_asymmetric_roles_of_the_inputs_numerator_a_joint_vs_denominator_c_by_incorporating_positional_encodings_to_distinguish_them_during_message_passing_this_unified_framework_enables_the_model_to_reason_jointly_over_logical_and_probabilistic_structures_level-_by-_level_propagation_let_mathcallv_denote_the_topological_level_of_gate_v_for_every_level_ell_from_primary_inputs_pi_to_primary_outputs_po_the_model_performs_beginarrayr_l_h_vsleftarrow_mathrmagg_gsbigh_usmid_uin_pvmathcallu_ell_-1_big_h_vfleftarrow_mathrmagg_gfbigh_ufh_usmid_uin_pvmathcallu_ell_-1_big_endarray_quad_a1_where_pv_denotes_the_set_of_predecessor_nodes_of_v_a2_effectiveness_of_div_gate_the_div_gate_explicitly_models_conditional_probability_in_the_circuit_graph_via_the_following_equation_pamid_c_fracpawedge_cpc_quad_a3_while_this_formulation_is_convenient_the_division_operation_can_amplify_errors_significantly_when_pc_is_very_small_always_smaller_than_1_as_it_represents_the_logic_probability_of_node_c_to_illustrate_this_behavior_we_analyze_two_representative_cases_using_ac97_ctrlaig_as_a_case_study_case_1_extremely_small_pc_in_this_scenario_when_the_conditional_node_rarely_evaluates_to_1_pc_001_even_minor_absolute_errors_in_pawedge_c_translate_into_large_relative_errors_in_pamid_c_table_a1_presents_the_ground-_truth_and_predicted_probabilities_for_pc_and_a_subset_of_20_target_nodes_a_the_prediction_error_for_pc_is_moderate_hatp_c_000323_yet_the_resulting_conditional_probabilities_are_significantly_distorted_due_to_error_amplification_case_2_moderate_pc_when_pc_is_in_a_mid-_range_eg_pc_03_the_division_operation_becomes_relatively_numerically_stable_however_it_still_amplifies_errors_in_pamid_c_to_an_extent_that_is_practically_unacceptable_table_a2_compares_aggregate_losses_showing_that_the_overall_absolute_error_remains_greater_than_01_in_contrast_our_method_incorporates_the_div_gate_during_the_dataset_preparation_stage_the_probability_labels_for_the_div_gate_are_computed_as_the_division_of_the_probabilities_of_its_two_input_nodes_which_are_tagged_with_distinct_positional_markers_by_learning_this_behavior_as_a_standard_gate_type_the_model_directly_internalizes_the_division_operation_eliminating_the_need_to_explicitly_compute_the_quotient_of_joint_and_marginal_probabilities_a3_pattern-based_dataset_to_bootstrap_training_we_hope_to_generate_pattern-_based_traces_that_approximate_full_truth_tables_given_a_circuit_with_m_primary_inputs_pis_the_complete_truth_table_has_2m_rowsimpractical_beyond_mapprox_20_instead_we_conduct_simulation_with_20000_patterns_and_uniformly_sample_100_random_patterns_per_circuit_per_epoch_see_table_a3_and_record_the_resulting_logic_probability_values_for_every_node_tabletrtdpattern_idtdtdpi1tdtdpi2tdtdpi3tdtdtdtdpimtdtdnode_utdtdnode_vtdtrtrtd1tdtd0tdtd1tdtd0tdtdtdtd1tdtd0tdtd0tdtrtrtd2tdtd1tdtd0tdtd0tdtdtdtd0tdtd1tdtd0tdtrtrtd3tdtd0tdtd0tdtd1tdtdtdtd1tdtd0tdtd1tdtrtrtdtdtdtdtdtdtdtdtdtdtdtdtdtdtdtdtrtrtd19997tdtd0tdtd1tdtd1tdtdtdtd0tdtd1tdtd1tdtrtrtd19998tdtd0tdtd1tdtd1tdtdtdtd0tdtd0tdtd1tdtrtrtd19999tdtd1tdtd0tdtd1tdtdtdtd1tdtd1tdtd0tdtrtrtd20000tdtd0tdtd0tdtd0tdtdtdtd0tdtd0tdtd1tdtrtable_table_a3_excerpt_of_a_truth_table_each_training_epoch_picks_a_fresh_random_subset_of_100_rows_ensuring_the_model_learns_to_interpolate_between_unseen_patterns_for_each_node_v_we_compute_hatp_mathrmrandv_frac1100sum_i_1100mathbf1bigv_1mathrminrowibig_quad_a4_which_serves_as_a_supervisory_signal_encouraging_the_gnn_to_capture_fine-_grained_functional_behavior_because_the_100-_row_subset_changes_every_epoch_the_network_is_exposed_to_thousands_of_distinct_local_views_yielding_better_generalization_than_a_single_large_simulation_a4_workload-based_dataset_while_random_patterns_provide_breadth_they_fail_to_capture_realistic_activity_biases_we_therefore_introduce_workload-_based_simulations_where_each_primary_input_pi_is_independently_assigned_a_probability_value_rho_sampled_from_the_set_01_02_ldots_09_a_toy_circuit_c_a_wedge_b_fig_a1_is_used_to_illustrate_the_necessity": ""
  },
  "section_objects": [
    {
      "heading": "Circuit Aware SAT Solving Guiding CDCL via Conditi",
      "content": "## Introduction\n\n\n<table><tr><td>i</td><td colspan=\"2\">P(C)</td><td colspan=\"2\">P(Ai\u2229C)</td><td colspan=\"2\">P(Ai|C)</td></tr><tr><td></td><td>ground truth</td><td>prediction</td><td>ground truth</td><td>prediction</td><td>ground truth</td><td>prediction</td></tr><tr><td>1</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00017</td><td>1</td><td>0.05277834</td></tr><tr><td>2</td><td>0.01</td><td>0.00323</td><td>0</td><td>0.00014</td><td>0</td><td>0.04442138</td></tr><tr><td>3</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00014</td><td>1</td><td>0.05133383</td></tr><tr><td>4</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00016</td><td>1</td><td>0.07029346</td></tr><tr><td>5</td><td>0.01</td><td>0.00323</td><td>0</td><td>0.00009</td><td>0</td><td>0.04350258</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Overall Error</td><td colspan=\"2\">0.00677</td><td colspan=\"2\">0.00786</td><td colspan=\"2\">0.72396</td></tr></table> Table A1: Performance of the division operation with extremely small \\(P(C)\\) \\(P(A_{i}\\mid C)\\) is calculated by \\(\\frac{P(A_{i}\\wedge C)}{P(C)}\\) <table><tr><td>i</td><td colspan=\"2\">P(C)</td><td colspan=\"2\">P(Ai\u2229C)</td><td colspan=\"2\">P(Ai|C)</td></tr><tr><td></td><td>gt</td><td>pred</td><td>gt</td><td>pred</td><td>gt</td><td>pred</td></tr><tr><td>1</td><td>0.42</td><td>0.4210119</td><td>0.24</td><td>0.24765863</td><td>0.5714286</td><td>0.58824617</td></tr><tr><td>2</td><td>0.42</td><td>0.4210119</td><td>0.33</td><td>0.34514248</td><td>0.7857143</td><td>0.8197927</td></tr><tr><td>3</td><td>0.42</td><td>0.4210119</td><td>0.31</td><td>0.3523217</td><td>0.7380953</td><td>0.83684504</td></tr><tr><td>4</td><td>0.42</td><td>0.4210119</td><td>0.36</td><td>0.3949966</td><td>0.8571429</td><td>0.93820775</td></tr><tr><td>5</td><td>0.42</td><td>0.4210119</td><td>0.38</td><td>0.3954696</td><td>0.9047619</td><td>0.6914532</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Overall Error</td><td colspan=\"2\">0.0010119</td><td colspan=\"2\">0.025311</td><td colspan=\"2\">0.101676</td></tr></table> Table A2: Performance of the division operation with moderate \\(P(C)\\) \\(P(A_{i}\\mid C)\\) is calculated by \\(\\frac{P(A_{i}\\wedge C)}{P(C)}\\) <table><tr><td rowspan=\"2\"></td><td colspan=\"3\">P&amp;lt;0.8</td><td colspan=\"3\">P\u22650.8</td></tr><tr><td>LBD</td><td>Num.</td><td>Time.Red.</td><td>Dec.Red.</td><td>Num.</td><td>Time.Red.</td></tr><tr><td>1</td><td>1515</td><td>47.1%</td><td>55.2%</td><td>2468</td><td>5.3%</td><td>18.9%</td></tr><tr><td>2</td><td>1819</td><td>21.2%</td><td>26.6%</td><td>7142</td><td>-71.0%</td><td>-0.2%</td></tr><tr><td>3</td><td>300</td><td>6.7%</td><td>6.5%</td><td>2728</td><td>-15.6%</td><td>4.9%</td></tr></table> <center>Figure A1: Toy circuit \\(c = a\\wedge b\\) </center> <table><tr><td>P(a)</td><td>P(b)</td><td>P(c=a\u2227b)</td></tr><tr><td>0.500</td><td>0.500</td><td>0.249</td></tr><tr><td>0.500</td><td>0.500</td><td>0.253</td></tr><tr><td>0.100</td><td>0.400</td><td>0.040</td></tr><tr><td>0.300</td><td>0.600</td><td>0.182</td></tr><tr><td>0.700</td><td>0.900</td><td>0.630</td></tr></table> Table A4: Node probabilities of the toy circuit (Fig. A1) under different PI workload levels \\(\\rho\\) . To illustrate this diversity, we simulate two contrasting scenarios (see Table A4). In the first case, all primary inputs (PIs) are assigned a fixed workload of \\(\\rho = 0.5\\) , under which each internal node converges to a stable logic probability; a representative slice is shown in the first two rows of Table A4. In the second case, each PI's \\(\\rho\\) is independently sampled from the set \\(0.1, 0.2, \\ldots , 0.9\\) , resulting in a broader range of logic probabilities across internal nodes. This variation exposes the network to a richer set of behaviors and Table A5: Statistics of clauses, solving time reduction, and decision count reduction under different LBD values and predicted probabilities. better reflects diverse operational conditions. ## A5 Analysis of Probability-Based Metric In our probability- based clause filtering method, the solving process is paused once the number of conflicts reaches 50,000, at which point all learnt clauses are extracted. For better analysis, these clauses are grouped according to their LBD values (1, 2, or 3) and further partitioned based on their predicted probabilities. Each group is then re- inserted into the solver independently, allowing us to evaluate their respective impacts on the remainder of the solving process. Table A5 presents statistics for the learnt clauses under different LBD levels, further divided by probability thresholds ( \\(< 0.8\\) and \\(\\geq 0.8\\) ). The table reports the number of clauses in each group, along with the corresponding reductions in solving time and decision count. From Table A5, we draw two key conclusions. First, for clauses with the same LBD, retaining those with lower\n\nprobability is more beneficial for subsequent solving, even though they are fewer in number. In fact, keeping only high- probability clauses can negatively impact solver performance. Second, we observe that the probability distribution of clauses closely matches the distribution of LBD: among clauses with lower LBD, a larger proportion have low probability. We further find that clause probability is a more informative metric than LBD alone. As a baseline, Ca Di Ca L simply discards all learnt clauses with LBD greater than 2. However, by filtering \\(LBD = 3\\) clauses based on probability and retaining only those with low probability, we can still achieve a positive effect on solving performance. ## A6 Applying Probability-Based Phase Selection for UNSAT Case The proposed phase selection method prioritizes variable assignments that are more likely to satisfy the circuit, effectively accelerating SAT solving, reducing solving time by up to \\(10\\times\\) on SAT instances. Leveraging this property, we regard the method as a lightweight classifier: for a given instance for solving, if the solver fails to find a solution within a predefined time budget under our phase selection heuristic, the instance is likely to be UNSAT. In such cases, we dynamically switch the solver to a configuration tuned specifically for UNSAT problems. We implement this strategy on top of Ca Di Ca L, using a simple timeout- based switching mechanism. We set the time budget as 5 seconds. Figure A2 summarizes the performance on the LECUNSAT dataset. On this dataset, the average solving time is reduced from 166.26 seconds (raw Ca Di Ca L) to 80.36 seconds using our adaptive strategy. As shown in Figure A2, the adaptive strategy consistently reduces solving time across all selected UNSAT- prone cases. This demonstrates that early- stage solver behavior under the SAT- oriented phase heuristic provides useful signals for unsatisfiability prediction, enabling effective and low- overhead adaptation to more suitable solving strategies. <center>Figure A2: Number of solved instances as a function of solving time using probability-based phase selection. A total of 150 UNSAT case in LECSAT dataset. TIMEOUT = 300s. </center>\n\n<center>Figure 1: Framework of CASCADE. We convert the original circuit into a DAG representation, augmented with virtual and and div gates. In the training pipeline, we employ pattern-based pre-training followed by workload-aware fine-tuning to train the model for conditional probability prediction. During inference, the predicted conditional probabilities are used to guide SAT solving, specifically in the inprocessing stages of phase selection and clause filtering. </center> achieves an additional \\(23.5\\%\\) reduction in total solving time on benchmarks from logic equivalence checking scenarios. In summary, our primary contributions include: - Introducing the first learning-based in-processing heuristics specifically designed for CSAT solving, effectively bridging static circuit representations and the dynamic CDCL solving process.- Developing a highly accurate GNN-based probabilistic model that predicts gate-level conditional probabilities, achieving validation accuracy of \\(96.69\\%\\) .- Demonstrating substantial empirical improvements by integrating the GNN-based conditional probability guidance into state-of-the-art SAT solvers, resulting in significant efficiency gains in real-world EDA applications. Our approach not only advances the theoretical understanding of leveraging structural information in SAT solving but also provides a robust framework for future solver enhancements and EDA tool designs. ## 2 Related Work ### 2.1 Circuit Representation Learning Function- aware representation learning has become a critical subfield in Electronic Design Automation (EDA), reflecting the broader trend in AI toward learning unified embeddings that can serve multiple downstream tasks. This area can be categorized into two main approaches: predictive models and contrastive models. In predictive models, Deep Gate (Shi et al. 2023, 2024; Zheng et al. 2025) and Deep Cell (Shi et al. 2025b) leverage asynchronous message passing neural network with representation disentanglement techniques to generate separate embeddings for functionality and structure, with pretraining across various EDA benchmarks. Polar Gate (Liu et al. 2024) further refines functional embeddings by incorporating ambipolar device principles. In contrastive models, FGNN (Wang et al. 2022, 2024) employs contrastive learning to align circuit embeddings based on functional similarity. Additionally, MGVGA (Wu et al. 2025), Circuit Fusion (Fang et al. 2025b), and Net TAG (Fang et al. 2025a) apply contrastive learning across multi- modal circuits to obtain function- invariant embeddings. In this paper, we focus on predicting exact conditional probabilities, for which we choose the state- of- the- art predictive model, Deep Gate4, as backbone. ### 2.2 Circuit Satisfiability The Boolean Satisfiability Problem (SAT) is a canonical NP- complete problem that determines whether a given propositional formula is satisfiable. As a variant of SAT, the Circuit Satisfiability Problem (CSAT, Circuit SAT) asks whether there exists an input assignment that makes the output of a given Boolean circuit evaluate to true. Several circuit- based solvers have been developed to address the Circuit SAT problem, aiming to operate directly on circuit format and leverage the structural information. For instance, NIMO (Lu et al. 2003) employs advanced circuit- level Boolean constraint propagation techniques to enhance conflict detection and decision- making. Similarly, Qute- SAT (Wu et al. 2007) incorporates conflict- driven learning strategies optimized for solving complex circuit topologies. Despite these innovations, such solvers generally offer only basic functionality and still fall short of the performance achieved by state- of- the- art CNF- based solvers on large- scale benchmarks.\n\nModern SAT solvers, such as Kissat (Armin et al. 2024), and Ca Di Cal (Fleury and Heisinger 2020), have demonstrated remarkable success in handling CNF- based instances. The de- facto standard workflow for solving CSAT problems is to translate circuit into the CNF formulas to be processed by highly- optimized SAT solvers. However, their performance on circuit- based problems is severely bottlenecked by the circuit- to- CNF transformation, which disrupts the structural properties of circuits and produces solver- unfriendly representations. Previous efforts, such as applying EDA- driven circuit optimization techniques (E\u00e9n, Mishchenko, and S\u00f6rensson 2007; Shi et al. 2025c) and extracting XOR logic on raw circuit (Qian et al. 2025) to reformat the problem into solver- friendly representations before solving, have shown attractive improvements. However, these approaches remain static\u2014they have no impact on solving heuristics and are inactive during the inprocessing phase, a critical period where search decisions and analysis routines are dynamically adjusted. As a result, the rich structural intelligence of circuits remains untapped precisely when it could be most beneficial. ## 3 Methodology ### 3.1 Problem Definition During CDCL solving, the CNF formula evolves dynamically through decision making, conflict analysis, and clause management routines. Two core heuristics\u2014phase selection and clause filtering\u2014play a central role in guiding the search and maintaining solver efficiency. Phase selection determines polarity (true/false) of variable assignments during the search process, aiming to minimize conflicts and accelerate convergence. Meanwhile, clause filtering focuses on evaluating and managing learned clauses derived from conflict analysis, retaining high- quality ones that improve propagation efficiency while discarding low- value clauses to reduce memory overhead. We reformulate both heuristics as conditional probability prediction tasks over circuit structures. Phase Selection Given a Boolean circuit \\(C\\) with primary output (PO), we formulate phase selection as a probability inference problem over the circuit graph. Let \\(s\\in V\\) denote a variable corresponding to a gate in the circuit. At each decision step \\(t\\) , the solver selects a variable \\(s\\in V\\) and estimates the conditional probability: \\(P(s = 1\\mid \\mathrm{PO} = 1)\\) which measures the likelihood that setting \\(s = 1\\) helps satisfy the output. By leveraging these probabilities to guide phase assignments, the solver implicitly favors value choices that are more aligned with satisfying assignments\u2014often leading to stronger propagation and fewer conflicts. A threshold \\(\\tau \\in (0,0.5)\\) governs the phase assignment: \\[v_{s} = \\left\\{ \\begin{array}{ll}0, & \\mathrm{if~}P(s = 1\\mid \\mathrm{PO} = 1)< \\tau ,\\\\ 1, & \\mathrm{if~}P(s = 1\\mid \\mathrm{PO} = 1) > 1 - \\tau ,\\\\ \\mathrm{default,} & \\mathrm{otherwise.} \\end{array} \\right. \\quad (1)\\] This probability inference strategy biases phase decisions toward values more likely to satisfy downstream logic, improving propagation and reducing conflicts. <center>Figure 2: Augmented graph with virtual and and div gates. </center> Clause Management Existing metrics for clause quality like LBD and clause length rely solely on CNF- level features, ignoring structural insights from the original circuit, thus hindering performance on circuit- derived problems. We propose a new metric, clause probability, to evaluate the quality of learned clauses by estimating their likelihood of being satisfied under structural semantics. Consider a learned clause \\(C = l_{1}\\lor l_{2}\\lor \\dots \\lor l_{k}\\) , which is semantically equivalent to a \\(k\\) - input OR gate over its literals. The probability that this clause is satisfied can be expressed as: \\[P(C) = P(l_{1}\\lor l_{2}\\lor \\dots \\lor l_{k}). \\quad (2)\\] where \\(P(l_{i})\\) is the estimated probability of literal \\(l_{i}\\) being true, derived from circuit- level signal estimation. This probabilistic view provides an intuitive interpretation: a clause with high probability \\(P(C)\\) is likely to be satisfied easily, implying that it encodes a weak constraint and has limited impact on restricting the search space. In contrast, a clause with low \\(P(C)\\) represents a strong constraint that is harder to satisfy, hence has higher potential to prune large infeasible regions in the search space. Therefore, we regard clauses with lower \\(P(C)\\) as more informative and prioritize them during inprocessing. Concretely, during periodic clause database elimination, the solver evaluates \\(P(C)\\) for all learned clauses and retains those with the lowest probabilities. This filtering mechanism allows the solver to preserve high- quality constraints that are more aligned with the underlying circuit semantics. We adopt the DEEPGATE4 framework (Zheng et al. 2025) as our encoder \\(E\\) , which generates structure and function embeddings for each gate in the circuit graph \\(G\\) . We use these embeddings to estimate conditional probabilities that guide inprocessing heuristics, enabling more efficient, probability- aware Circuit SAT solving. ### 3.2 Graph Construction To enable probability- aware learning on circuit graphs, we construct directed acyclic graphs (DAGs) where each node represents a logical gate, and further augment with virtual gates for better probability modeling, as shown in Figure 2. DAG Construction with and and not Gates Following prior work (Shi et al. 2023, 2024; Zheng et al. 2025; Liu et al. 2024), we begin by constructing a standard DAG representation of the input circuit. Each logic gate is represented as a node, and edges represent signal propagation between gates. Specifically, for a given circuit graph \\(G\\) , we construct\n\na DAG comprising two basic gate types: and gates for binary conjunctions and not gates for logic inversion. We adopt the aggregator \\(Aggr_{and}\\) and \\(Aggr_{not}\\) for and gates and not gates respectively, following Deep Gate4 framework(Zheng et al. 2025). Virtual and Gates for Joint Probabilities To expose joint probabilities directly in the graph, we insert virtual and gates in \\(\\mathcal{G}\\) : for any node \\(A\\) conditioned on \\(C\\) , we add \\[A_{joint} = A\\wedge C, \\quad (3)\\] so the model can observe the joint probability \\(P(A\\wedge C)\\) directly, denoted as \\(P(A_{joint})\\) , without intermediate arithmetic computations. When encode the virtual and gate, we directly adopt the \\(Aggr_{and}\\) to get the embedding and predict the joint probability as: \\[h^{A_{joint}} = Aggr(h^{A},h^{C}),\\hat{P} (A_{joint}) = \\phi (h^{A_{joint}}), \\quad (4)\\] where \\(\\phi\\) is a 3- layer MLPs. Virtual div Gates for Conditional Probabilities As joint probabilities can be modeled via virtual and gates, computing conditional probabilities such as \\[P(A\\mid C) = \\frac{P(A\\wedge C)}{P(C)} = \\frac{P(A_{joint})}{P(C)} \\quad (5)\\] can lead to amplified errors, especially when \\(P(C)\\) is small. Details are shown in Appendix A2. Nodes with extremely low probabilities- - referred to as polar nodes (i.e., those with truth- table probabilities below 0.1)- are empirically harder to learn and tend to amplify prediction errors. These nodes require special attention to ensure accurate and reliable modeling. To address this, we introduce virtual div gates to represent conditional probabilities directly within the graph. Each such gate takes two inputs: the numerator, which is a virtual and gate representing the joint event \\(A_{\\mathrm{joint}} = A\\wedge C\\) , and the denominator, which corresponds to the condition node \\(C\\) . Then with aggregator \\(Aggr_{div}\\) designed for div gate, we first get the embedding with: \\[h^{A_{cond}} = Aggr_{div}(h^{A_{joint}},h^{C}). \\quad (6)\\] Then predict the probability with task head \\(\\phi\\) : \\[\\hat{P} (A_{cond}) = \\phi (h^{A_{cond}}) \\quad (7)\\] This design enables the model to predict \\(P(A\\mid C)\\) directly from graph context, reducing error sensitivity to small denominators. ### 3.3 Two-Stage Training Strategy We train the network with a two- stage training strategy: 1. Pattern-Based Pre-training. Previous works (Shi et al. 2023, 2024; Zheng et al. 2025; Liu et al. 2024) initialize PI embeddings using input probabilities, however, it will lead to information distortion. As shown in Table 1, distinct input subsets (0-3 and 4-7) may share the same empirical PI probability (0.5), despite differing in actual assignments. This Table 1: Example of simulation patterns and probability. <table><tr><td>Pattern ID</td><td>P1</td><td>P2</td><td>P3</td><td>P4</td><td>Node x</td><td>Node y</td><td>Node z</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>4</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>5</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>6</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td></tr><tr><td>7</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr></table> illustrates a key limitation: probabilities are coarse- grained and may fail to capture fine- grained circuit behavior. To address this, we use 100 random patterns per minibatch to compute fine- grained probability features, which are fed into the model. This supervision enables our model to capture both statistical trends and functional semantics. Suppose the circuit contains \\(n\\) nodes in total, where the first \\(m\\) nodes are designated as primary inputs (PIs). For each primary input (PI) node, a 100- bit simulation trace is encoded into an initial embedding using an autoencoder. Embeddings for all other nodes are then propagated level- by- level through the circuit. The stage- 1 training loss is defined as: \\[\\begin{array}{l}{\\mathcal{L}_{\\mathrm{stage1}} = w_{1}\\cdot \\frac{1}{n - m}\\sum_{i = m + 1}^{n}L1(P_{i} - \\hat{P}_{i})}\\\\ {+\\ w_{2}\\cdot \\frac{1}{m}\\sum_{j = 1}^{m}L1(P_{j} - \\hat{P}_{j})} \\end{array} \\quad (8)\\] Here, the L1 Loss function is defined as: \\[\\mathrm{L1}(a,b) = |a - b| \\quad (9)\\] where \\(P_{i}\\) denotes the ground- truth probability of the \\(i\\) - th node, and \\(\\hat{P}_{i}\\) is the predicted value. The second term focuses on PI nodes, encouraging accurate reconstruction of their probabilities, since their embeddings are initialized from actual simulation data. 2. Workload-Aware Fine-tuning. Although training on batches of 100 patterns helps capture fine-grained circuit behaviors, our ultimate goal is to generalize to the statistical properties of large-scale pattern distributions. To bridge this gap and enhance data diversity, we perform 200 simulations, each using 100 random patterns with designated PI workloads drawn from \\(\\{0.1,0.2,\\ldots ,0.9\\}\\) (detailed in Appendix A4). For each PI, the 100- bit trace in a simulation is averaged into a single probability, resulting in a 200- dimensional vector that captures its behavior under diverse input distributions. This vector is then passed through an MLP to produce the PIs' initial embedding. For internal nodes, ground- truth probabilities are computed by aggregating predictions over all \\(200\\times 100 = 20,000\\) input patterns, serving as training targets in stage- 2.\n\n<center>Figure 3: Example of conditional probability calculation for three conditions </center> The overall loss for stage- 2 is defined as: \\[\\begin{array}{rl} & {\\mathcal{L}_{\\mathrm{stage2}} = w_1\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{all}}|}\\sum_{i\\in \\mathcal{S}_{\\mathrm{all}}}L1(P_i - \\hat{P}_i)}\\\\ & {\\qquad +w_2\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{div}}|}\\sum_{j\\in \\mathcal{S}_{\\mathrm{div}}}L1(P_j - \\hat{P}_j)}\\\\ & {\\qquad +w_3\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{polar}}|}\\sum_{k\\in \\mathcal{S}_{\\mathrm{polar}}}L1(P_k - \\hat{P}_k)} \\end{array} \\quad (10)\\] Here, Sall denotes all internal nodes in the circuit, Sdiv refers to div nodes, and Spolar includes polarized nodes with low probability \\((P< 0.1)\\) Higher weights \\(w_{2}\\) and \\(w_{3}\\) prioritize structurally and semantically critical nodes: div nodes reflect key conditional probabilities, while polarized nodes are harder to predict but crucial for tasks like Circuit SAT. ### 3.4 Multiple Condition Probability Through Equation 5, our model can estimate the probability of a target node conditioned on a single other node. However, in practical Circuit SAT scenarios, it is often more useful to infer the most likely value of a node given that multiple other nodes have already been assigned. As illustrated in Figure 3, when multiple condition nodes (denoted as \\(C_1, C_2, \\ldots , C_k\\) ) are involved, we combine them using a chain of two- input and gates to construct a single aggregated condition node \\(C\\) . This transformation enables the model to capture the joint condition \\(C = C_1 \\wedge C_2 \\wedge \\ldots \\wedge C_k\\) through logical conjunction. Once the final condition node \\(C\\) is obtained, we apply Equation 5 in the same manner as with a single condition node, treating \\(P(A | C)\\) as the target. This allows the model to generalize single- condition inference to multi- condition scenarios in a structurally consistent way. ## 4 Experiments ### 4.1 Dataset Generation In most cases, conditioning on a specific node \\(C\\) primarily affects a local subregion of the circuit rather than its entirety. We define this subregion as the influence area of \\(C\\) , constructed in three steps. First, we identify the fanin cone of \\(C\\) . Within this cone, we then locate an ancestor node \\(P\\) that satisfies two conditions: it has multiple fanouts, and it differs from \\(C\\) by no more than 10 logic levels. Finally, starting from \\(P\\) , we collect all nodes within its fanout cone, bounded to a depth of 10 logic levels downstream. We use a dataset of 10,824 AIGs from ITC99 (Davidson 1999), EPFL (Amaru et al. 2019), and Open Core (Open Core 1999), with circuit sizes ranging from 36 to 3,214 gates. To make conditional information explicitly available to the model, we insert and and div gates within each node's influence area. For supervision, we conduct 20,000 random simulations per circuit to record full truth assignments. ### 4.2 Experiment Setting In the one- round GNN model configuration, both the structural embedding \\(h^s\\) and the functional embedding \\(h^f\\) are set to a dimension of 128. The task head, \\(\\phi\\) , has 3 hidden layers with 32 neurons and utilizes the Si LU activation function. The model is trained for 60 epochs to ensure convergence with a batch size of 512 using a single NVIDIA RTX 4090 GPU. The Adam optimizer (Kingma and Ba 2014) is used with a learning rate of \\(10^{- 4}\\) . ### 4.3 Performance on Conditional Probability Prediction Table 2: Conditional probability prediction performance (MAE) under varying condition counts and circuit sizes. <table><tr><td>#Condition Nodes</td><td>#Nodes=928</td><td>#Nodes=6428</td><td>#Nodes=17796</td></tr><tr><td>1</td><td>0.0243</td><td>0.0372</td><td>0.0457</td></tr><tr><td>2</td><td>0.0267</td><td>0.0401</td><td>0.0496</td></tr><tr><td>5</td><td>0.0314</td><td>0.0479</td><td>0.0532</td></tr></table> We train the model on small circuits and evaluate its conditional probability prediction accuracy on both small and large- scale benchmarks. In the single- condition setting, the average L1 loss on the validation set reaches as low as 0.0331. To illustrate generalizability, we select representative circuits of varying sizes. As shown in Table 2, the model maintains low prediction error across all scales, with L1 loss as low as 0.0243 for small circuits and remaining below 0.05 even for circuits with over 17K nodes. As the number of conditions increases from 1 to 5, prediction error rises slightly yet remains within acceptable bounds, demonstrating strong robustness. Importantly, inference overhead under multi- condition settings is negligible, thanks to the lightweight design of the aggregator functions of and gates. ### 4.4 Ablation Study Effectiveness of div Gate Table 3 compares our div gate- based model with direct division for conditional probability prediction. Across all settings, div gates yield consistently lower MAE, e.g., reducing error from 0.1173 to 0.0312 with one moderate condition. This highlights their advantage in numerical stability and learning robustness, while direct division suffers from unstable gradients and sensitivity to small denominators.\n\nTable 3: Ablation study on the effectiveness of using div gates for computing conditional probability (MAE). Here, \"#Cond.\" indicates the number of condition nodes involved. <table><tr><td rowspan=\"2\"># Cond.</td><td colspan=\"2\">Moderate Condition</td><td colspan=\"2\">Polar Condition</td></tr><tr><td>div gate</td><td>direct division</td><td>div gate</td><td>direct division</td></tr><tr><td>1</td><td>0.0312</td><td>0.1173</td><td>0.0395</td><td>0.7129</td></tr><tr><td>2</td><td>0.0339</td><td>0.1495</td><td>0.0423</td><td>0.7940</td></tr><tr><td>5</td><td>0.0368</td><td>0.2030</td><td>0.0486</td><td>0.8546</td></tr></table> Table 4: Ablation study on the effectiveness of our two-stage training strategy (MAE). <table><tr><td># Condition Nodes</td><td>Two-stage</td><td>Only Stage-1</td><td>Only Stage-2</td></tr><tr><td>1</td><td>0.0331</td><td>0.0422</td><td>0.0435</td></tr><tr><td>2</td><td>0.0352</td><td>0.0583</td><td>0.0493</td></tr><tr><td>5</td><td>0.0387</td><td>0.0738</td><td>0.0588</td></tr></table> Effectiveness of Training Strategy We evaluate the effectiveness of our two- stage training strategy, where stage1 learns fine- grained behavior from a 100- pattern simulation and stage- 2 captures broader statistical distributions using 20,000- pattern data. As shown in Table 4, the two- stage model consistently achieves the lowest L1 loss, outperforming both stage- 1 and stage- 2 models individually. For instance, with one condition node, L1 loss improves from 0.0422 (stage- 1) and 0.0435 (stage- 2) to 0.0331 (two- stage). Additionally, the second- stage model accelerates inference, reducing the time by a factor of 200 compared to stage- 1, which needs 200 invocations to simulate the 20,000- pattern workload. ## 5 Application to Circuit SAT ### 5.1 Experiment Setting We evaluate the proposed methods (see Section 3.1) in the context of the Logic Equivalence Checking (LEC) task, one of the most critical CSAT problem determining whether two given circuit designs are logically equivalent. All the experiments, including model inference and SAT solving are conducted on an Intel(R) Platinum 8474C. Since CDCL SAT solvers exhibit fundamentally different behaviors when solving satisfiable and unsatisfiable instances (Chanseok 2015), we conduct separate evaluations of our proposed probability- based phase selection and clause filtering techniques. For example, solvers favor variable branching heuristics to quickly find satisfying assignments, but rely on advanced clause learning and prune strategies to prove unsatisfiability. To assess the phase selection method, we construct 150 hard satisfiable instances as LECSAT set by applying logic synthesis and minor revisions of different circuits from Forge EDA dataset (Shi et al. 2025a). Circuit pairs are then combined using miter construction via the ABC tool (Brayton and Mishchenko 2010). These instances have an average solving time of 17.39 seconds using Ca Di Ca L, ensuring that they are sufficiently challenging. Besides, to evaluate the proposed clause filtering tech nique, we construct another set of unsatisfiable instances LECUNSAT by generating miters from pairs of datapath circuits. In these cases, the original circuits are logically equivalent by design, ensuring that the resulting miter circuits are unsatisfiable, which cannot be solved within 20 seconds by Ca Di Cal. ### 5.2 Node Probability for Phase Selection <center>Figure 4: Solving Time on LECSAT (150 SAT cases) with probability-based phase selection(unit: seconds) </center> We use CASCAD to predict condition probability and evaluate our selection method(defined in Section 3.1) on 150 SAT instances from the LECSAT benchmark under different thresholds \\(\\tau\\) . Figure 4 compares solving times of our method (x- axis) under various threshold with Ca Di Ca L (y- axis), both in log scale. We also include the recent circuit representation model Deep Gate4 (Zheng et al. 2025) (black dots) as a comparison, which predicts node- level similarity and encourages assigning opposite values to functional similar nodes to trigger conflicts earlier and prune the search. Most points lie above the diagonal \\((y = x)\\) , indicating that our method outperforms the mainstream phase selection heuristic widely used in modern SAT solvers (has integrated into Ca Di Ca L). Compared to Deep Gate4, our method consistently achieves lower solving times, demonstrating more effective phase guidance. At \\(\\tau = 0.005\\) , our method achieves up to \\(10 \\times\\) speedup, and nearly \\(5 \\times\\) at \\(\\tau = 0.01\\) . Points further left imply lower average solving times, highlighting the overall efficiency of our probability- guided strategy across different \\(\\tau\\) values. In addition, the phase selection method serves as a lightweight classifier for UNSAT instances: since it greatly accelerates SAT solving\u2014if a case remains unsolved within a fixed time budget, it is likely to be UNSAT. This insight allows us to adapt solver parameters dynamically to better handle UNSAT- dominated scenarios(see Appendix A6), achieving \\(2 \\times\\) average speedup on LECUNSAT dataset.\n\n<center>Figure 5: Number of solved instances as a function of solving time using probability-based clause filtering. A total of 130 small case in LECUNSAT dataset. TIMEOUT = 400s. </center> Table 5: Average Par2 of 20 large cases in LECUNSAT using probability-based clause filtering. TIMEOUT \\(= 1000\\mathrm{s}\\) <table><tr><td>Threshold</td><td>Baseline</td><td>Ours.Solving</td><td>Ours.Overall</td></tr><tr><td>0.85</td><td rowspan=\"3\">1828.83</td><td>1560.72</td><td>1563.45</td></tr><tr><td>0.9</td><td>1406.19</td><td>1408.03</td></tr><tr><td>0.95</td><td>1396.13</td><td>1399.34</td></tr><tr><td></td><td></td><td>1429.60</td><td>1432.48</td></tr></table> ### 5.3 Clause Probability for Clause Management We validate our clause probability metric using a simple protocol. The solver runs until 50,000 conflicts are reached, after which all learnt clauses are extracted. Our CASCADE- model computes each clause's probability (as defined in Section 3.1) and retains only those below a predefined threshold. We apply this filtering in Ca Di Ca L with varying thresholds \\(\\tau\\) , and evaluate performance using the PAR- 2 (Par2) score, which penalizes timeouts with twice the cutoff time to reflect both efficiency and robustness. We evaluate our probability- guided clause filtering metric on LECUNSAT dataset (150 instances, split into 130 small and 20 hardest cases). Two configurations are compared: (1) the Ca Di Ca L baseline with mainstream LBD- based metric, and (2) our method with probability- based filtering. Total time includes both solving and model inference time. We test thresholds, 0.8, 0.85, 0.9, and 0.95, as shown in Figure 5 and Table 5. On 130 small cases, our methods consistently outperform the baseline by solving more instances within the same time limit. For the 20 challenging cases, our methods significantly outperform the baseline even when accounting for inference time. These results demonstrate that our filtering method identifies and preserves more useful clauses than the default LBD heuristic (see Appendix A5). The optimal threshold of 0.9 reflects a trade- off: low thresholds yield high- quality but sparse clauses, weakening propagation; high thresholds increase quantity but introduce noise. Table 6: Average Par2 of 150 SAT cases in LECSAT using probability-based phase selection with preprocessing. <table><tr><td>Method</td><td>Threshold</td><td>Solving</td><td>Overall</td></tr><tr><td>Preprocessing Only</td><td>-</td><td>13.39</td><td>13.39</td></tr><tr><td>Preprocessing</td><td>0.01</td><td>3.26</td><td>5.01</td></tr><tr><td>+</td><td>0.005</td><td>1.04</td><td>2.89</td></tr><tr><td>CASCADE</td><td>0.003</td><td>6.29</td><td>8.35</td></tr></table> Table 7: Average Par2 of 150 UNSAT cases using probability-based clause filtering with preprocessing. <table><tr><td>Setting</td><td>Threshold</td><td>Solving</td><td>Overall</td></tr><tr><td>Preprocessing Only</td><td>-</td><td>304.42</td><td>304.42</td></tr><tr><td rowspan=\"3\">Preprocessing + CASCADE</td><td>0.8</td><td>245.61</td><td>248.82</td></tr><tr><td>0.85</td><td>228.98</td><td>231.76</td></tr><tr><td>0.9</td><td>221.50</td><td>224.95</td></tr><tr><td></td><td>0.95</td><td>223.91</td><td>226.34</td></tr></table> ### 5.4 Effectiveness of Inprocessing Beyond Preprocessing To further validate the distinct effectiveness of our proposed inprocessing framework beyond preprocessing techniques, we compare the two strategies: - Preprocessing-only: We adopt the preprocessing techniques proposed in prior work (Shi et al. 2025c), to transform the circuit into a solver-friendly format.- Preprocessing + CASCADE: Building on the same preprocessing technique, we integrate it into the inprocessing stage via our CASCADE framework. As shown in Table 6 and Table 7, even when strong preprocessing technique has been applied, integrating our inprocessing framework leads to substantial additional speedups. For SAT cases, the best setting reduces the average PAR2 from 13.39 to 2.89 (a \\(4.6 \\times\\) improvement), while for UNSAT cases, our clause filtering achieves a reduction from 304.42 to 224.95. This highlights the complementary nature of the two techniques, and demonstrates that inprocessing contributes dynamic guidance during solving\u2014providing essential capabilities that static preprocessing alone cannot achieve. ## 6 Conclusion In this work, we propose CASCADE, a novel framework incorporating the inherent circuit information from CSAT instance into dynamic CDCL reasoning. By using a GNN- based model to estimate conditional probabilities of unassigned variable, CASCADE dynamically guides two key CDCL heuristics in the modern SAT solvers: variable phase selection and clause management. Extensive experiments on industry- standard benchmarks demonstrate that CASCADE achieves significant performance gains, reducing solving time by up to \\(90\\%\\) through probability- guided phase selection and achieving an additional \\(23.5\\%\\) reduction via clause filtering in logic equivalence scenarios.\n\n## References Amaru, L.; Gaillardon, P.- E.; Testa, E.; and Micheli, G. D. 2019. The EPFL Combinational Benchmark Suite. In 24th International Workshop on Logic & Synthesis (IWLS).Amizadeh, S.; Matusevych, S.; and Weimer, M. 2018. Learning to solve circuit- sat: An unsupervised differentiable approach. In International conference on learning representations.Armin, B.; Tobias, F.; Katalin, F.; Mathias, F.; Nils, F.; and Florian, P. 2024. Ca Di Ca L, Gimsatul, Isa SAT and Kissat entering the SAT Competition 2024. Proc. of SAT Competition, 8- 10. Audemard, G.; and Simon, L. 2018. On the Glucose SAT Solver. International Journal on Artificial Intelligence Tools, 27(01): 1840001. Brayton, R. K.; and Mishchenko, A. 2010. ABC: An Academic Industrial- Strength Verification Tool. In International Conference on Computer Aided Verification.Chanseok, O. 2015. Between SAT and UNSAT: the fundamental difference in CDCL SAT. In International Conference on Theory and Applications of Satisfiability Testing, 307- 323. Springer.Davidson, S. 1999. ITC'99 Benchmark Circuits - Preliminary Results. In International Test Conference 1999. Proceedings (IEEE Cat. No.99CH37034), 1125- 1125. E\u00e9n, N.; Mishchenko, A.; and S\u00f6rensson, N. 2007. Applying logic synthesis for speeding up SAT. In Theory and Applications of Satisfiability Testing - SAT 2007, 272- 286. Springer.Fang, W.; Li, W.; Liu, S.; Lu, Y.; Zhang, H.; and Xie, Z. 2025a. Net TAG: A Multimodal RTL- and- Layout- Aligned Netlist Foundation Model via Text- Attributed Graph. ar Xiv preprint ar Xiv:2504.09260. Fang, W.; Liu, S.; Wang, J.; and Xie, Z. 2025b. Circuitfusion: multimodal circuit representation learning for agile chip design. ar Xiv preprint ar Xiv:2505.02168. Fleury, A.; and Heisinger, M. 2020. Cadical, kissat, paracooba, plungeling and treengeling entering the sat competition 2020. Sat Competition, 2020: 50. Goldberg, E.; Prasad, M.; and Brayton, R. 2001. Using SAT for combinational equivalence checking. Design, Automation, and Test in Europe. Design, Automation, and Test in Europe.Kingma, D.; and Ba, J. 2014. Adam: A Method for Stochastic Optimization. ar Xiv: Learning, ar Xiv: Learning.Liu, J.; Zhai, J.; Zhao, M.; Lin, Z.; Yu, B.; and Shi, C. 2024. Polar Gate: Breaking the Functionality Representation Bottleneck of And- Inverter Graph Neural Network. In 2024 IEEE/ACM International Conference on Computer- Aided Design (ICCAD).Lu, F.; Wang, L.- C.; Cheng, K.- T.; and Huang, R.- Y. 2003. A circuit SAT solver with signal correlation guided learning. In 2003 Design, Automation and Test in Europe Conference and Exhibition, 892- 897. Marques- Silva, J.; Lynce, I.; and Malik, S. 2021. Conflict- driven clause learning SAT solvers. In Handbook of satisfiability, 133- 182. ios Press. Open Core, T. 1999. Open Core. https://opencores.org/.Qian, Y.; Chen, Z.; Zhang, X.; and Cai, S. 2025. X- SAT: An Efficient Circuit- Based SAT Solver. In 2025 62nd ACM/IEEE Design Automation Conference (DAC). IEEE.Shi, Z.; Li, Z.; Ma, C.; Zhou, Y.; Zheng, Z.; Liu, J.; Pan, H.; Zhou, L.; Li, K.; Zhu, J.; Yan, L.; He, Z.; Xue, C.; Jiang, W.; Yang, F.; Sun, G.; Yang, X.; Chen, G.; Shi, C.; Chu, Z.; Yang, J.; and Xu, Q. 2025a. Forge EDA: Towards Verifiable and Customizable Circuit Benchmarks. ar Xiv preprint ar Xiv:2505.02016. Shi, Z.; Ma, C.; Zheng, Z.; Zhou, L.; Pan, H.; Jiang, W.; Yang, F.; Yang, X.; Chu, Z.; and Xu, Q. 2025b. Deep Cell: Multiview Representation Learning for Post- Mapping Netlists. ar Xiv preprint ar Xiv:2502.06816. Shi, Z.; Pan, H.; Khan, S.; Li, M.; Liu, Y.; Huang, J.; Zhen, H.- L.; Yuan, M.; Chu, Z.; and Xu, Q. 2023. Deepgate2: Functionality- aware circuit representation learning. In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), 1- 9. IEEE.Shi, Z.; Tang, T.; Zhu, J.; Sadaf, K.; Zhen, H.- L.; Yuan, M.; Chu, Z.; and Xu, Q. 2025c. Logic Optimization Meets SAT: A Novel Framework for Circuit- SAT Solving. In 2025 62nd ACM/IEEE Design Automation Conference (DAC). IEEE.Shi, Z.; Zheng, Z.; Khan, S.; Zhong, J.; Li, M.; and Xu, Q. 2024. Deep Gate3: Towards Scalable Circuit Representation Learning. ar Xiv preprint ar Xiv:2407.11095. Stephan, P.; Brayton, R.; and Sangiovanni- Vincentelli, A. 1996. Combinational test generation using satisfiability. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, 1167- 1176. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is All you Need. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.Wang, Z.; Bai, C.; He, Z.; Zhang, G.; Xu, Q.; Ho, T.- Y.; Huang, Y.; and Yu, B. 2024. Fgnn2: A powerful pre- training framework for learning the logic functionality of circuits. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems.Wang, Z.; Bai, C.; He, Z.; Zhang, G.; Xu, Q.; Ho, T.- Y.; Yu, B.; and Huang, Y. 2022. Functionality matters in netlist representation learning. In Proceedings of the 59th ACM/IEEE Design Automation Conference, 61- 66. Wu, C.- A.; Lin, T.- H.; Lee, C.- C.; and Huang, C.- Y. 2007. Qute SAT: A Robust Circuit- based SAT Solver for Complex Circuit Structure. In 2007 Design, Automation & Test in Europe Conference & Exhibition, 1- 6. Wu, H.; Zheng, H.; Pu, Y.; and Yu, B. 2025. Circuit Representation Learning with Masked Gate Modeling and Verilog- AIG Alignment. ar Xiv preprint ar Xiv:2502.12732. Zheng, Z.; Huang, S.; Zhong, J.; Shi, Z.; Dai, G.; Xu, N.; and Xu, Q. 2025. Deep Gate4: Efficient and Effective Representation Learning for Circuit Design at Scale. ar Xiv preprint ar Xiv:2502.01681.\n\n## A1 Model Details Model Architecture. We adopt the DEEPGATE4 framework (Zheng et al. 2025), where each gate is represented by structural and functional embeddings and updated via selfattention (Vaswani et al. 2017). For each gate type \\(g\\in \\{\\mathrm{and},\\mathrm{not},\\mathrm{div}\\}\\) , we implement type- specific aggregation and update functions: not . The aggregator directly propagates the single input and applies a logical negation. and / virtual and. The aggregation function captures conjunction semantics, where attention weights reflect the relative importance of inputs- for example, giving higher weight to controlling inputs that can determine the output value. virtual div. In addition to a two- input aggregator, we account for the asymmetric roles of the inputs (numerator \\(A_{joint}\\) vs. denominator \\(C\\) ) by incorporating positional encodings to distinguish them during message passing. This unified framework enables the model to reason jointly over logical and probabilistic structures. Level- by- Level Propagation. Let \\(\\mathcal{L}(v)\\) denote the topological level of gate \\(v\\) . For every level \\(\\ell\\) (from primary inputs PI to primary outputs PO) the model performs: \\[\\begin{array}{r l} & {h_{v}^{s}\\leftarrow \\mathrm{Agg}_{g}^{s}\\big(\\{h_{u}^{s}\\mid u\\in P(v),\\mathcal{L}(u) = \\ell -1\\} \\big)}\\\\ & {h_{v}^{f}\\leftarrow \\mathrm{Agg}_{g}^{f}\\big(\\{h_{u}^{f},h_{u}^{s}\\mid u\\in P(v),\\mathcal{L}(u) = \\ell -1\\} \\big)} \\end{array} \\quad (A1)\\] where \\(P(v)\\) denotes the set of predecessor nodes of \\(v\\) ## A2 Effectiveness of div Gate The div gate explicitly models conditional probability in the circuit graph via the following equation: \\[P(A\\mid C) = \\frac{P(A\\wedge C)}{P(C)}. \\quad (A3)\\] While this formulation is convenient, the division operation can amplify errors significantly when \\(P(C)\\) is very small (always smaller than 1, as it represents the logic probability of node \\(C\\) ). To illustrate this behavior, we analyze two representative cases using ac97_ctrl.aig as a case study. Case 1: Extremely Small \\(P(C)\\) . In this scenario, when the conditional node rarely evaluates to 1 ( \\(P(C) = 0.01\\) ), even minor absolute errors in \\(P(A\\wedge C)\\) translate into large relative errors in \\(P(A\\mid C)\\) . Table A1 presents the ground- truth and predicted probabilities for \\(P(C)\\) and a subset of 20 target nodes \\(A\\) . The prediction error for \\(P(C)\\) is moderate ( \\(\\hat{P} (C) = 0.00323\\) ), yet the resulting conditional probabilities are significantly distorted due to error amplification. Case 2: Moderate \\(P(C)\\) . When \\(P(C)\\) is in a mid- range (e.g., \\(P(C) > 0.3\\) ), the division operation becomes relatively numerically stable. However, it still amplifies errors in \\(P(A\\mid C)\\) to an extent that is practically unacceptable. Table A2 compares aggregate losses, showing that the overall absolute error remains greater than 0.1. In contrast, our method incorporates the div gate during the dataset preparation stage. The probability labels for the DIV gate are computed as the division of the probabilities of its two input nodes, which are tagged with distinct positional markers. By learning this behavior as a standard gate type, the model directly internalizes the division operation, eliminating the need to explicitly compute the quotient of joint and marginal probabilities. ## A3 Pattern-Based Dataset To bootstrap training, we hope to generate pattern- based traces that approximate full truth tables. Given a circuit with \\(m\\) primary inputs (PIs), the complete truth table has \\(2^{m}\\) rows\u2014impractical beyond \\(m\\approx 20\\) . Instead, we conduct simulation with 20,000 patterns and uniformly sample 100 random patterns per circuit per epoch (see Table A3) and record the resulting logic probability values for every node. <table><tr><td>Pattern ID</td><td>PI1</td><td>PI2</td><td>PI3</td><td>...</td><td>PIm</td><td>Node u</td><td>Node v</td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>...</td><td>1</td><td>0</td><td>0</td></tr><tr><td>2</td><td>1</td><td>0</td><td>0</td><td>...</td><td>0</td><td>1</td><td>0</td></tr><tr><td>3</td><td>0</td><td>0</td><td>1</td><td>...</td><td>1</td><td>0</td><td>1</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>19997</td><td>0</td><td>1</td><td>1</td><td>...</td><td>0</td><td>1</td><td>1</td></tr><tr><td>19998</td><td>0</td><td>1</td><td>1</td><td>...</td><td>0</td><td>0</td><td>1</td></tr><tr><td>19999</td><td>1</td><td>0</td><td>1</td><td>...</td><td>1</td><td>1</td><td>0</td></tr><tr><td>20000</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>1</td></tr></table> Table A3: Excerpt of a truth table. Each training epoch picks a fresh random subset of 100 rows, ensuring the model learns to interpolate between unseen patterns. For each node \\(v\\) , we compute \\[\\hat{P}_{\\mathrm{rand}}(v) = \\frac{1}{100}\\sum_{i = 1}^{100}\\mathbf{1}\\big(v = 1\\mathrm{~in~row~}i\\big), \\quad (A4)\\] which serves as a supervisory signal encouraging the GNN to capture fine- grained functional behavior. Because the 100- row subset changes every epoch, the network is exposed to thousands of distinct local views, yielding better generalization than a single large simulation. ## A4 Workload-Based Dataset While random patterns provide breadth, they fail to capture realistic activity biases. We therefore introduce workload- based simulations, where each primary input (PI) is independently assigned a probability value \\(\\rho\\) sampled from the set \\(0.1, 0.2, \\ldots , 0.9\\) . A toy circuit ( \\(c = a \\wedge b\\) , Fig. A1) is used to illustrate the necessity.",
      "level": 1,
      "line_start": 1,
      "line_end": 25
    },
    {
      "heading": "Introduction",
      "content": "<table><tr><td>i</td><td colspan=\"2\">P(C)</td><td colspan=\"2\">P(Ai\u2229C)</td><td colspan=\"2\">P(Ai|C)</td></tr><tr><td></td><td>ground truth</td><td>prediction</td><td>ground truth</td><td>prediction</td><td>ground truth</td><td>prediction</td></tr><tr><td>1</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00017</td><td>1</td><td>0.05277834</td></tr><tr><td>2</td><td>0.01</td><td>0.00323</td><td>0</td><td>0.00014</td><td>0</td><td>0.04442138</td></tr><tr><td>3</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00014</td><td>1</td><td>0.05133383</td></tr><tr><td>4</td><td>0.01</td><td>0.00323</td><td>0.01</td><td>0.00016</td><td>1</td><td>0.07029346</td></tr><tr><td>5</td><td>0.01</td><td>0.00323</td><td>0</td><td>0.00009</td><td>0</td><td>0.04350258</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Overall Error</td><td colspan=\"2\">0.00677</td><td colspan=\"2\">0.00786</td><td colspan=\"2\">0.72396</td></tr></table> Table A1: Performance of the division operation with extremely small \\(P(C)\\) \\(P(A_{i}\\mid C)\\) is calculated by \\(\\frac{P(A_{i}\\wedge C)}{P(C)}\\) <table><tr><td>i</td><td colspan=\"2\">P(C)</td><td colspan=\"2\">P(Ai\u2229C)</td><td colspan=\"2\">P(Ai|C)</td></tr><tr><td></td><td>gt</td><td>pred</td><td>gt</td><td>pred</td><td>gt</td><td>pred</td></tr><tr><td>1</td><td>0.42</td><td>0.4210119</td><td>0.24</td><td>0.24765863</td><td>0.5714286</td><td>0.58824617</td></tr><tr><td>2</td><td>0.42</td><td>0.4210119</td><td>0.33</td><td>0.34514248</td><td>0.7857143</td><td>0.8197927</td></tr><tr><td>3</td><td>0.42</td><td>0.4210119</td><td>0.31</td><td>0.3523217</td><td>0.7380953</td><td>0.83684504</td></tr><tr><td>4</td><td>0.42</td><td>0.4210119</td><td>0.36</td><td>0.3949966</td><td>0.8571429</td><td>0.93820775</td></tr><tr><td>5</td><td>0.42</td><td>0.4210119</td><td>0.38</td><td>0.3954696</td><td>0.9047619</td><td>0.6914532</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Overall Error</td><td colspan=\"2\">0.0010119</td><td colspan=\"2\">0.025311</td><td colspan=\"2\">0.101676</td></tr></table> Table A2: Performance of the division operation with moderate \\(P(C)\\) \\(P(A_{i}\\mid C)\\) is calculated by \\(\\frac{P(A_{i}\\wedge C)}{P(C)}\\) <table><tr><td rowspan=\"2\"></td><td colspan=\"3\">P&amp;lt;0.8</td><td colspan=\"3\">P\u22650.8</td></tr><tr><td>LBD</td><td>Num.</td><td>Time.Red.</td><td>Dec.Red.</td><td>Num.</td><td>Time.Red.</td></tr><tr><td>1</td><td>1515</td><td>47.1%</td><td>55.2%</td><td>2468</td><td>5.3%</td><td>18.9%</td></tr><tr><td>2</td><td>1819</td><td>21.2%</td><td>26.6%</td><td>7142</td><td>-71.0%</td><td>-0.2%</td></tr><tr><td>3</td><td>300</td><td>6.7%</td><td>6.5%</td><td>2728</td><td>-15.6%</td><td>4.9%</td></tr></table> <center>Figure A1: Toy circuit \\(c = a\\wedge b\\) </center> <table><tr><td>P(a)</td><td>P(b)</td><td>P(c=a\u2227b)</td></tr><tr><td>0.500</td><td>0.500</td><td>0.249</td></tr><tr><td>0.500</td><td>0.500</td><td>0.253</td></tr><tr><td>0.100</td><td>0.400</td><td>0.040</td></tr><tr><td>0.300</td><td>0.600</td><td>0.182</td></tr><tr><td>0.700</td><td>0.900</td><td>0.630</td></tr></table> Table A4: Node probabilities of the toy circuit (Fig. A1) under different PI workload levels \\(\\rho\\) . To illustrate this diversity, we simulate two contrasting scenarios (see Table A4). In the first case, all primary inputs (PIs) are assigned a fixed workload of \\(\\rho = 0.5\\) , under which each internal node converges to a stable logic probability; a representative slice is shown in the first two rows of Table A4. In the second case, each PI's \\(\\rho\\) is independently sampled from the set \\(0.1, 0.2, \\ldots , 0.9\\) , resulting in a broader range of logic probabilities across internal nodes. This variation exposes the network to a richer set of behaviors and Table A5: Statistics of clauses, solving time reduction, and decision count reduction under different LBD values and predicted probabilities. better reflects diverse operational conditions. ## A5 Analysis of Probability-Based Metric In our probability- based clause filtering method, the solving process is paused once the number of conflicts reaches 50,000, at which point all learnt clauses are extracted. For better analysis, these clauses are grouped according to their LBD values (1, 2, or 3) and further partitioned based on their predicted probabilities. Each group is then re- inserted into the solver independently, allowing us to evaluate their respective impacts on the remainder of the solving process. Table A5 presents statistics for the learnt clauses under different LBD levels, further divided by probability thresholds ( \\(< 0.8\\) and \\(\\geq 0.8\\) ). The table reports the number of clauses in each group, along with the corresponding reductions in solving time and decision count. From Table A5, we draw two key conclusions. First, for clauses with the same LBD, retaining those with lower\n\nprobability is more beneficial for subsequent solving, even though they are fewer in number. In fact, keeping only high- probability clauses can negatively impact solver performance. Second, we observe that the probability distribution of clauses closely matches the distribution of LBD: among clauses with lower LBD, a larger proportion have low probability. We further find that clause probability is a more informative metric than LBD alone. As a baseline, Ca Di Ca L simply discards all learnt clauses with LBD greater than 2. However, by filtering \\(LBD = 3\\) clauses based on probability and retaining only those with low probability, we can still achieve a positive effect on solving performance. ## A6 Applying Probability-Based Phase Selection for UNSAT Case The proposed phase selection method prioritizes variable assignments that are more likely to satisfy the circuit, effectively accelerating SAT solving, reducing solving time by up to \\(10\\times\\) on SAT instances. Leveraging this property, we regard the method as a lightweight classifier: for a given instance for solving, if the solver fails to find a solution within a predefined time budget under our phase selection heuristic, the instance is likely to be UNSAT. In such cases, we dynamically switch the solver to a configuration tuned specifically for UNSAT problems. We implement this strategy on top of Ca Di Ca L, using a simple timeout- based switching mechanism. We set the time budget as 5 seconds. Figure A2 summarizes the performance on the LECUNSAT dataset. On this dataset, the average solving time is reduced from 166.26 seconds (raw Ca Di Ca L) to 80.36 seconds using our adaptive strategy. As shown in Figure A2, the adaptive strategy consistently reduces solving time across all selected UNSAT- prone cases. This demonstrates that early- stage solver behavior under the SAT- oriented phase heuristic provides useful signals for unsatisfiability prediction, enabling effective and low- overhead adaptation to more suitable solving strategies. <center>Figure A2: Number of solved instances as a function of solving time using probability-based phase selection. A total of 150 UNSAT case in LECSAT dataset. TIMEOUT = 300s. </center>\n\n<center>Figure 1: Framework of CASCADE. We convert the original circuit into a DAG representation, augmented with virtual and and div gates. In the training pipeline, we employ pattern-based pre-training followed by workload-aware fine-tuning to train the model for conditional probability prediction. During inference, the predicted conditional probabilities are used to guide SAT solving, specifically in the inprocessing stages of phase selection and clause filtering. </center> achieves an additional \\(23.5\\%\\) reduction in total solving time on benchmarks from logic equivalence checking scenarios. In summary, our primary contributions include: - Introducing the first learning-based in-processing heuristics specifically designed for CSAT solving, effectively bridging static circuit representations and the dynamic CDCL solving process.- Developing a highly accurate GNN-based probabilistic model that predicts gate-level conditional probabilities, achieving validation accuracy of \\(96.69\\%\\) .- Demonstrating substantial empirical improvements by integrating the GNN-based conditional probability guidance into state-of-the-art SAT solvers, resulting in significant efficiency gains in real-world EDA applications. Our approach not only advances the theoretical understanding of leveraging structural information in SAT solving but also provides a robust framework for future solver enhancements and EDA tool designs. ## 2 Related Work ### 2.1 Circuit Representation Learning Function- aware representation learning has become a critical subfield in Electronic Design Automation (EDA), reflecting the broader trend in AI toward learning unified embeddings that can serve multiple downstream tasks. This area can be categorized into two main approaches: predictive models and contrastive models. In predictive models, Deep Gate (Shi et al. 2023, 2024; Zheng et al. 2025) and Deep Cell (Shi et al. 2025b) leverage asynchronous message passing neural network with representation disentanglement techniques to generate separate embeddings for functionality and structure, with pretraining across various EDA benchmarks. Polar Gate (Liu et al. 2024) further refines functional embeddings by incorporating ambipolar device principles. In contrastive models, FGNN (Wang et al. 2022, 2024) employs contrastive learning to align circuit embeddings based on functional similarity. Additionally, MGVGA (Wu et al. 2025), Circuit Fusion (Fang et al. 2025b), and Net TAG (Fang et al. 2025a) apply contrastive learning across multi- modal circuits to obtain function- invariant embeddings. In this paper, we focus on predicting exact conditional probabilities, for which we choose the state- of- the- art predictive model, Deep Gate4, as backbone. ### 2.2 Circuit Satisfiability The Boolean Satisfiability Problem (SAT) is a canonical NP- complete problem that determines whether a given propositional formula is satisfiable. As a variant of SAT, the Circuit Satisfiability Problem (CSAT, Circuit SAT) asks whether there exists an input assignment that makes the output of a given Boolean circuit evaluate to true. Several circuit- based solvers have been developed to address the Circuit SAT problem, aiming to operate directly on circuit format and leverage the structural information. For instance, NIMO (Lu et al. 2003) employs advanced circuit- level Boolean constraint propagation techniques to enhance conflict detection and decision- making. Similarly, Qute- SAT (Wu et al. 2007) incorporates conflict- driven learning strategies optimized for solving complex circuit topologies. Despite these innovations, such solvers generally offer only basic functionality and still fall short of the performance achieved by state- of- the- art CNF- based solvers on large- scale benchmarks.\n\nModern SAT solvers, such as Kissat (Armin et al. 2024), and Ca Di Cal (Fleury and Heisinger 2020), have demonstrated remarkable success in handling CNF- based instances. The de- facto standard workflow for solving CSAT problems is to translate circuit into the CNF formulas to be processed by highly- optimized SAT solvers. However, their performance on circuit- based problems is severely bottlenecked by the circuit- to- CNF transformation, which disrupts the structural properties of circuits and produces solver- unfriendly representations. Previous efforts, such as applying EDA- driven circuit optimization techniques (E\u00e9n, Mishchenko, and S\u00f6rensson 2007; Shi et al. 2025c) and extracting XOR logic on raw circuit (Qian et al. 2025) to reformat the problem into solver- friendly representations before solving, have shown attractive improvements. However, these approaches remain static\u2014they have no impact on solving heuristics and are inactive during the inprocessing phase, a critical period where search decisions and analysis routines are dynamically adjusted. As a result, the rich structural intelligence of circuits remains untapped precisely when it could be most beneficial. ## 3 Methodology ### 3.1 Problem Definition During CDCL solving, the CNF formula evolves dynamically through decision making, conflict analysis, and clause management routines. Two core heuristics\u2014phase selection and clause filtering\u2014play a central role in guiding the search and maintaining solver efficiency. Phase selection determines polarity (true/false) of variable assignments during the search process, aiming to minimize conflicts and accelerate convergence. Meanwhile, clause filtering focuses on evaluating and managing learned clauses derived from conflict analysis, retaining high- quality ones that improve propagation efficiency while discarding low- value clauses to reduce memory overhead. We reformulate both heuristics as conditional probability prediction tasks over circuit structures. Phase Selection Given a Boolean circuit \\(C\\) with primary output (PO), we formulate phase selection as a probability inference problem over the circuit graph. Let \\(s\\in V\\) denote a variable corresponding to a gate in the circuit. At each decision step \\(t\\) , the solver selects a variable \\(s\\in V\\) and estimates the conditional probability: \\(P(s = 1\\mid \\mathrm{PO} = 1)\\) which measures the likelihood that setting \\(s = 1\\) helps satisfy the output. By leveraging these probabilities to guide phase assignments, the solver implicitly favors value choices that are more aligned with satisfying assignments\u2014often leading to stronger propagation and fewer conflicts. A threshold \\(\\tau \\in (0,0.5)\\) governs the phase assignment: \\[v_{s} = \\left\\{ \\begin{array}{ll}0, & \\mathrm{if~}P(s = 1\\mid \\mathrm{PO} = 1)< \\tau ,\\\\ 1, & \\mathrm{if~}P(s = 1\\mid \\mathrm{PO} = 1) > 1 - \\tau ,\\\\ \\mathrm{default,} & \\mathrm{otherwise.} \\end{array} \\right. \\quad (1)\\] This probability inference strategy biases phase decisions toward values more likely to satisfy downstream logic, improving propagation and reducing conflicts. <center>Figure 2: Augmented graph with virtual and and div gates. </center> Clause Management Existing metrics for clause quality like LBD and clause length rely solely on CNF- level features, ignoring structural insights from the original circuit, thus hindering performance on circuit- derived problems. We propose a new metric, clause probability, to evaluate the quality of learned clauses by estimating their likelihood of being satisfied under structural semantics. Consider a learned clause \\(C = l_{1}\\lor l_{2}\\lor \\dots \\lor l_{k}\\) , which is semantically equivalent to a \\(k\\) - input OR gate over its literals. The probability that this clause is satisfied can be expressed as: \\[P(C) = P(l_{1}\\lor l_{2}\\lor \\dots \\lor l_{k}). \\quad (2)\\] where \\(P(l_{i})\\) is the estimated probability of literal \\(l_{i}\\) being true, derived from circuit- level signal estimation. This probabilistic view provides an intuitive interpretation: a clause with high probability \\(P(C)\\) is likely to be satisfied easily, implying that it encodes a weak constraint and has limited impact on restricting the search space. In contrast, a clause with low \\(P(C)\\) represents a strong constraint that is harder to satisfy, hence has higher potential to prune large infeasible regions in the search space. Therefore, we regard clauses with lower \\(P(C)\\) as more informative and prioritize them during inprocessing. Concretely, during periodic clause database elimination, the solver evaluates \\(P(C)\\) for all learned clauses and retains those with the lowest probabilities. This filtering mechanism allows the solver to preserve high- quality constraints that are more aligned with the underlying circuit semantics. We adopt the DEEPGATE4 framework (Zheng et al. 2025) as our encoder \\(E\\) , which generates structure and function embeddings for each gate in the circuit graph \\(G\\) . We use these embeddings to estimate conditional probabilities that guide inprocessing heuristics, enabling more efficient, probability- aware Circuit SAT solving. ### 3.2 Graph Construction To enable probability- aware learning on circuit graphs, we construct directed acyclic graphs (DAGs) where each node represents a logical gate, and further augment with virtual gates for better probability modeling, as shown in Figure 2. DAG Construction with and and not Gates Following prior work (Shi et al. 2023, 2024; Zheng et al. 2025; Liu et al. 2024), we begin by constructing a standard DAG representation of the input circuit. Each logic gate is represented as a node, and edges represent signal propagation between gates. Specifically, for a given circuit graph \\(G\\) , we construct\n\na DAG comprising two basic gate types: and gates for binary conjunctions and not gates for logic inversion. We adopt the aggregator \\(Aggr_{and}\\) and \\(Aggr_{not}\\) for and gates and not gates respectively, following Deep Gate4 framework(Zheng et al. 2025). Virtual and Gates for Joint Probabilities To expose joint probabilities directly in the graph, we insert virtual and gates in \\(\\mathcal{G}\\) : for any node \\(A\\) conditioned on \\(C\\) , we add \\[A_{joint} = A\\wedge C, \\quad (3)\\] so the model can observe the joint probability \\(P(A\\wedge C)\\) directly, denoted as \\(P(A_{joint})\\) , without intermediate arithmetic computations. When encode the virtual and gate, we directly adopt the \\(Aggr_{and}\\) to get the embedding and predict the joint probability as: \\[h^{A_{joint}} = Aggr(h^{A},h^{C}),\\hat{P} (A_{joint}) = \\phi (h^{A_{joint}}), \\quad (4)\\] where \\(\\phi\\) is a 3- layer MLPs. Virtual div Gates for Conditional Probabilities As joint probabilities can be modeled via virtual and gates, computing conditional probabilities such as \\[P(A\\mid C) = \\frac{P(A\\wedge C)}{P(C)} = \\frac{P(A_{joint})}{P(C)} \\quad (5)\\] can lead to amplified errors, especially when \\(P(C)\\) is small. Details are shown in Appendix A2. Nodes with extremely low probabilities- - referred to as polar nodes (i.e., those with truth- table probabilities below 0.1)- are empirically harder to learn and tend to amplify prediction errors. These nodes require special attention to ensure accurate and reliable modeling. To address this, we introduce virtual div gates to represent conditional probabilities directly within the graph. Each such gate takes two inputs: the numerator, which is a virtual and gate representing the joint event \\(A_{\\mathrm{joint}} = A\\wedge C\\) , and the denominator, which corresponds to the condition node \\(C\\) . Then with aggregator \\(Aggr_{div}\\) designed for div gate, we first get the embedding with: \\[h^{A_{cond}} = Aggr_{div}(h^{A_{joint}},h^{C}). \\quad (6)\\] Then predict the probability with task head \\(\\phi\\) : \\[\\hat{P} (A_{cond}) = \\phi (h^{A_{cond}}) \\quad (7)\\] This design enables the model to predict \\(P(A\\mid C)\\) directly from graph context, reducing error sensitivity to small denominators. ### 3.3 Two-Stage Training Strategy We train the network with a two- stage training strategy: 1. Pattern-Based Pre-training. Previous works (Shi et al. 2023, 2024; Zheng et al. 2025; Liu et al. 2024) initialize PI embeddings using input probabilities, however, it will lead to information distortion. As shown in Table 1, distinct input subsets (0-3 and 4-7) may share the same empirical PI probability (0.5), despite differing in actual assignments. This Table 1: Example of simulation patterns and probability. <table><tr><td>Pattern ID</td><td>P1</td><td>P2</td><td>P3</td><td>P4</td><td>Node x</td><td>Node y</td><td>Node z</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>4</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>5</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td></tr><tr><td>6</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td></tr><tr><td>7</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr></table> illustrates a key limitation: probabilities are coarse- grained and may fail to capture fine- grained circuit behavior. To address this, we use 100 random patterns per minibatch to compute fine- grained probability features, which are fed into the model. This supervision enables our model to capture both statistical trends and functional semantics. Suppose the circuit contains \\(n\\) nodes in total, where the first \\(m\\) nodes are designated as primary inputs (PIs). For each primary input (PI) node, a 100- bit simulation trace is encoded into an initial embedding using an autoencoder. Embeddings for all other nodes are then propagated level- by- level through the circuit. The stage- 1 training loss is defined as: \\[\\begin{array}{l}{\\mathcal{L}_{\\mathrm{stage1}} = w_{1}\\cdot \\frac{1}{n - m}\\sum_{i = m + 1}^{n}L1(P_{i} - \\hat{P}_{i})}\\\\ {+\\ w_{2}\\cdot \\frac{1}{m}\\sum_{j = 1}^{m}L1(P_{j} - \\hat{P}_{j})} \\end{array} \\quad (8)\\] Here, the L1 Loss function is defined as: \\[\\mathrm{L1}(a,b) = |a - b| \\quad (9)\\] where \\(P_{i}\\) denotes the ground- truth probability of the \\(i\\) - th node, and \\(\\hat{P}_{i}\\) is the predicted value. The second term focuses on PI nodes, encouraging accurate reconstruction of their probabilities, since their embeddings are initialized from actual simulation data. 2. Workload-Aware Fine-tuning. Although training on batches of 100 patterns helps capture fine-grained circuit behaviors, our ultimate goal is to generalize to the statistical properties of large-scale pattern distributions. To bridge this gap and enhance data diversity, we perform 200 simulations, each using 100 random patterns with designated PI workloads drawn from \\(\\{0.1,0.2,\\ldots ,0.9\\}\\) (detailed in Appendix A4). For each PI, the 100- bit trace in a simulation is averaged into a single probability, resulting in a 200- dimensional vector that captures its behavior under diverse input distributions. This vector is then passed through an MLP to produce the PIs' initial embedding. For internal nodes, ground- truth probabilities are computed by aggregating predictions over all \\(200\\times 100 = 20,000\\) input patterns, serving as training targets in stage- 2.\n\n<center>Figure 3: Example of conditional probability calculation for three conditions </center> The overall loss for stage- 2 is defined as: \\[\\begin{array}{rl} & {\\mathcal{L}_{\\mathrm{stage2}} = w_1\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{all}}|}\\sum_{i\\in \\mathcal{S}_{\\mathrm{all}}}L1(P_i - \\hat{P}_i)}\\\\ & {\\qquad +w_2\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{div}}|}\\sum_{j\\in \\mathcal{S}_{\\mathrm{div}}}L1(P_j - \\hat{P}_j)}\\\\ & {\\qquad +w_3\\cdot \\frac{1}{|\\mathcal{S}_{\\mathrm{polar}}|}\\sum_{k\\in \\mathcal{S}_{\\mathrm{polar}}}L1(P_k - \\hat{P}_k)} \\end{array} \\quad (10)\\] Here, Sall denotes all internal nodes in the circuit, Sdiv refers to div nodes, and Spolar includes polarized nodes with low probability \\((P< 0.1)\\) Higher weights \\(w_{2}\\) and \\(w_{3}\\) prioritize structurally and semantically critical nodes: div nodes reflect key conditional probabilities, while polarized nodes are harder to predict but crucial for tasks like Circuit SAT. ### 3.4 Multiple Condition Probability Through Equation 5, our model can estimate the probability of a target node conditioned on a single other node. However, in practical Circuit SAT scenarios, it is often more useful to infer the most likely value of a node given that multiple other nodes have already been assigned. As illustrated in Figure 3, when multiple condition nodes (denoted as \\(C_1, C_2, \\ldots , C_k\\) ) are involved, we combine them using a chain of two- input and gates to construct a single aggregated condition node \\(C\\) . This transformation enables the model to capture the joint condition \\(C = C_1 \\wedge C_2 \\wedge \\ldots \\wedge C_k\\) through logical conjunction. Once the final condition node \\(C\\) is obtained, we apply Equation 5 in the same manner as with a single condition node, treating \\(P(A | C)\\) as the target. This allows the model to generalize single- condition inference to multi- condition scenarios in a structurally consistent way. ## 4 Experiments ### 4.1 Dataset Generation In most cases, conditioning on a specific node \\(C\\) primarily affects a local subregion of the circuit rather than its entirety. We define this subregion as the influence area of \\(C\\) , constructed in three steps. First, we identify the fanin cone of \\(C\\) . Within this cone, we then locate an ancestor node \\(P\\) that satisfies two conditions: it has multiple fanouts, and it differs from \\(C\\) by no more than 10 logic levels. Finally, starting from \\(P\\) , we collect all nodes within its fanout cone, bounded to a depth of 10 logic levels downstream. We use a dataset of 10,824 AIGs from ITC99 (Davidson 1999), EPFL (Amaru et al. 2019), and Open Core (Open Core 1999), with circuit sizes ranging from 36 to 3,214 gates. To make conditional information explicitly available to the model, we insert and and div gates within each node's influence area. For supervision, we conduct 20,000 random simulations per circuit to record full truth assignments. ### 4.2 Experiment Setting In the one- round GNN model configuration, both the structural embedding \\(h^s\\) and the functional embedding \\(h^f\\) are set to a dimension of 128. The task head, \\(\\phi\\) , has 3 hidden layers with 32 neurons and utilizes the Si LU activation function. The model is trained for 60 epochs to ensure convergence with a batch size of 512 using a single NVIDIA RTX 4090 GPU. The Adam optimizer (Kingma and Ba 2014) is used with a learning rate of \\(10^{- 4}\\) . ### 4.3 Performance on Conditional Probability Prediction Table 2: Conditional probability prediction performance (MAE) under varying condition counts and circuit sizes. <table><tr><td>#Condition Nodes</td><td>#Nodes=928</td><td>#Nodes=6428</td><td>#Nodes=17796</td></tr><tr><td>1</td><td>0.0243</td><td>0.0372</td><td>0.0457</td></tr><tr><td>2</td><td>0.0267</td><td>0.0401</td><td>0.0496</td></tr><tr><td>5</td><td>0.0314</td><td>0.0479</td><td>0.0532</td></tr></table> We train the model on small circuits and evaluate its conditional probability prediction accuracy on both small and large- scale benchmarks. In the single- condition setting, the average L1 loss on the validation set reaches as low as 0.0331. To illustrate generalizability, we select representative circuits of varying sizes. As shown in Table 2, the model maintains low prediction error across all scales, with L1 loss as low as 0.0243 for small circuits and remaining below 0.05 even for circuits with over 17K nodes. As the number of conditions increases from 1 to 5, prediction error rises slightly yet remains within acceptable bounds, demonstrating strong robustness. Importantly, inference overhead under multi- condition settings is negligible, thanks to the lightweight design of the aggregator functions of and gates. ### 4.4 Ablation Study Effectiveness of div Gate Table 3 compares our div gate- based model with direct division for conditional probability prediction. Across all settings, div gates yield consistently lower MAE, e.g., reducing error from 0.1173 to 0.0312 with one moderate condition. This highlights their advantage in numerical stability and learning robustness, while direct division suffers from unstable gradients and sensitivity to small denominators.\n\nTable 3: Ablation study on the effectiveness of using div gates for computing conditional probability (MAE). Here, \"#Cond.\" indicates the number of condition nodes involved. <table><tr><td rowspan=\"2\"># Cond.</td><td colspan=\"2\">Moderate Condition</td><td colspan=\"2\">Polar Condition</td></tr><tr><td>div gate</td><td>direct division</td><td>div gate</td><td>direct division</td></tr><tr><td>1</td><td>0.0312</td><td>0.1173</td><td>0.0395</td><td>0.7129</td></tr><tr><td>2</td><td>0.0339</td><td>0.1495</td><td>0.0423</td><td>0.7940</td></tr><tr><td>5</td><td>0.0368</td><td>0.2030</td><td>0.0486</td><td>0.8546</td></tr></table> Table 4: Ablation study on the effectiveness of our two-stage training strategy (MAE). <table><tr><td># Condition Nodes</td><td>Two-stage</td><td>Only Stage-1</td><td>Only Stage-2</td></tr><tr><td>1</td><td>0.0331</td><td>0.0422</td><td>0.0435</td></tr><tr><td>2</td><td>0.0352</td><td>0.0583</td><td>0.0493</td></tr><tr><td>5</td><td>0.0387</td><td>0.0738</td><td>0.0588</td></tr></table> Effectiveness of Training Strategy We evaluate the effectiveness of our two- stage training strategy, where stage1 learns fine- grained behavior from a 100- pattern simulation and stage- 2 captures broader statistical distributions using 20,000- pattern data. As shown in Table 4, the two- stage model consistently achieves the lowest L1 loss, outperforming both stage- 1 and stage- 2 models individually. For instance, with one condition node, L1 loss improves from 0.0422 (stage- 1) and 0.0435 (stage- 2) to 0.0331 (two- stage). Additionally, the second- stage model accelerates inference, reducing the time by a factor of 200 compared to stage- 1, which needs 200 invocations to simulate the 20,000- pattern workload. ## 5 Application to Circuit SAT ### 5.1 Experiment Setting We evaluate the proposed methods (see Section 3.1) in the context of the Logic Equivalence Checking (LEC) task, one of the most critical CSAT problem determining whether two given circuit designs are logically equivalent. All the experiments, including model inference and SAT solving are conducted on an Intel(R) Platinum 8474C. Since CDCL SAT solvers exhibit fundamentally different behaviors when solving satisfiable and unsatisfiable instances (Chanseok 2015), we conduct separate evaluations of our proposed probability- based phase selection and clause filtering techniques. For example, solvers favor variable branching heuristics to quickly find satisfying assignments, but rely on advanced clause learning and prune strategies to prove unsatisfiability. To assess the phase selection method, we construct 150 hard satisfiable instances as LECSAT set by applying logic synthesis and minor revisions of different circuits from Forge EDA dataset (Shi et al. 2025a). Circuit pairs are then combined using miter construction via the ABC tool (Brayton and Mishchenko 2010). These instances have an average solving time of 17.39 seconds using Ca Di Ca L, ensuring that they are sufficiently challenging. Besides, to evaluate the proposed clause filtering tech nique, we construct another set of unsatisfiable instances LECUNSAT by generating miters from pairs of datapath circuits. In these cases, the original circuits are logically equivalent by design, ensuring that the resulting miter circuits are unsatisfiable, which cannot be solved within 20 seconds by Ca Di Cal. ### 5.2 Node Probability for Phase Selection <center>Figure 4: Solving Time on LECSAT (150 SAT cases) with probability-based phase selection(unit: seconds) </center> We use CASCAD to predict condition probability and evaluate our selection method(defined in Section 3.1) on 150 SAT instances from the LECSAT benchmark under different thresholds \\(\\tau\\) . Figure 4 compares solving times of our method (x- axis) under various threshold with Ca Di Ca L (y- axis), both in log scale. We also include the recent circuit representation model Deep Gate4 (Zheng et al. 2025) (black dots) as a comparison, which predicts node- level similarity and encourages assigning opposite values to functional similar nodes to trigger conflicts earlier and prune the search. Most points lie above the diagonal \\((y = x)\\) , indicating that our method outperforms the mainstream phase selection heuristic widely used in modern SAT solvers (has integrated into Ca Di Ca L). Compared to Deep Gate4, our method consistently achieves lower solving times, demonstrating more effective phase guidance. At \\(\\tau = 0.005\\) , our method achieves up to \\(10 \\times\\) speedup, and nearly \\(5 \\times\\) at \\(\\tau = 0.01\\) . Points further left imply lower average solving times, highlighting the overall efficiency of our probability- guided strategy across different \\(\\tau\\) values. In addition, the phase selection method serves as a lightweight classifier for UNSAT instances: since it greatly accelerates SAT solving\u2014if a case remains unsolved within a fixed time budget, it is likely to be UNSAT. This insight allows us to adapt solver parameters dynamically to better handle UNSAT- dominated scenarios(see Appendix A6), achieving \\(2 \\times\\) average speedup on LECUNSAT dataset.\n\n<center>Figure 5: Number of solved instances as a function of solving time using probability-based clause filtering. A total of 130 small case in LECUNSAT dataset. TIMEOUT = 400s. </center> Table 5: Average Par2 of 20 large cases in LECUNSAT using probability-based clause filtering. TIMEOUT \\(= 1000\\mathrm{s}\\) <table><tr><td>Threshold</td><td>Baseline</td><td>Ours.Solving</td><td>Ours.Overall</td></tr><tr><td>0.85</td><td rowspan=\"3\">1828.83</td><td>1560.72</td><td>1563.45</td></tr><tr><td>0.9</td><td>1406.19</td><td>1408.03</td></tr><tr><td>0.95</td><td>1396.13</td><td>1399.34</td></tr><tr><td></td><td></td><td>1429.60</td><td>1432.48</td></tr></table> ### 5.3 Clause Probability for Clause Management We validate our clause probability metric using a simple protocol. The solver runs until 50,000 conflicts are reached, after which all learnt clauses are extracted. Our CASCADE- model computes each clause's probability (as defined in Section 3.1) and retains only those below a predefined threshold. We apply this filtering in Ca Di Ca L with varying thresholds \\(\\tau\\) , and evaluate performance using the PAR- 2 (Par2) score, which penalizes timeouts with twice the cutoff time to reflect both efficiency and robustness. We evaluate our probability- guided clause filtering metric on LECUNSAT dataset (150 instances, split into 130 small and 20 hardest cases). Two configurations are compared: (1) the Ca Di Ca L baseline with mainstream LBD- based metric, and (2) our method with probability- based filtering. Total time includes both solving and model inference time. We test thresholds, 0.8, 0.85, 0.9, and 0.95, as shown in Figure 5 and Table 5. On 130 small cases, our methods consistently outperform the baseline by solving more instances within the same time limit. For the 20 challenging cases, our methods significantly outperform the baseline even when accounting for inference time. These results demonstrate that our filtering method identifies and preserves more useful clauses than the default LBD heuristic (see Appendix A5). The optimal threshold of 0.9 reflects a trade- off: low thresholds yield high- quality but sparse clauses, weakening propagation; high thresholds increase quantity but introduce noise. Table 6: Average Par2 of 150 SAT cases in LECSAT using probability-based phase selection with preprocessing. <table><tr><td>Method</td><td>Threshold</td><td>Solving</td><td>Overall</td></tr><tr><td>Preprocessing Only</td><td>-</td><td>13.39</td><td>13.39</td></tr><tr><td>Preprocessing</td><td>0.01</td><td>3.26</td><td>5.01</td></tr><tr><td>+</td><td>0.005</td><td>1.04</td><td>2.89</td></tr><tr><td>CASCADE</td><td>0.003</td><td>6.29</td><td>8.35</td></tr></table> Table 7: Average Par2 of 150 UNSAT cases using probability-based clause filtering with preprocessing. <table><tr><td>Setting</td><td>Threshold</td><td>Solving</td><td>Overall</td></tr><tr><td>Preprocessing Only</td><td>-</td><td>304.42</td><td>304.42</td></tr><tr><td rowspan=\"3\">Preprocessing + CASCADE</td><td>0.8</td><td>245.61</td><td>248.82</td></tr><tr><td>0.85</td><td>228.98</td><td>231.76</td></tr><tr><td>0.9</td><td>221.50</td><td>224.95</td></tr><tr><td></td><td>0.95</td><td>223.91</td><td>226.34</td></tr></table> ### 5.4 Effectiveness of Inprocessing Beyond Preprocessing To further validate the distinct effectiveness of our proposed inprocessing framework beyond preprocessing techniques, we compare the two strategies: - Preprocessing-only: We adopt the preprocessing techniques proposed in prior work (Shi et al. 2025c), to transform the circuit into a solver-friendly format.- Preprocessing + CASCADE: Building on the same preprocessing technique, we integrate it into the inprocessing stage via our CASCADE framework. As shown in Table 6 and Table 7, even when strong preprocessing technique has been applied, integrating our inprocessing framework leads to substantial additional speedups. For SAT cases, the best setting reduces the average PAR2 from 13.39 to 2.89 (a \\(4.6 \\times\\) improvement), while for UNSAT cases, our clause filtering achieves a reduction from 304.42 to 224.95. This highlights the complementary nature of the two techniques, and demonstrates that inprocessing contributes dynamic guidance during solving\u2014providing essential capabilities that static preprocessing alone cannot achieve. ## 6 Conclusion In this work, we propose CASCADE, a novel framework incorporating the inherent circuit information from CSAT instance into dynamic CDCL reasoning. By using a GNN- based model to estimate conditional probabilities of unassigned variable, CASCADE dynamically guides two key CDCL heuristics in the modern SAT solvers: variable phase selection and clause management. Extensive experiments on industry- standard benchmarks demonstrate that CASCADE achieves significant performance gains, reducing solving time by up to \\(90\\%\\) through probability- guided phase selection and achieving an additional \\(23.5\\%\\) reduction via clause filtering in logic equivalence scenarios.",
      "level": 2,
      "line_start": 4,
      "line_end": 22
    },
    {
      "heading": "References Amaru, L.; Gaillardon, P.- E.; Testa, E.; and Micheli, G. D. 2019. The EPFL Combinational Benchmark Suite. In 24th International Workshop on Logic & Synthesis (IWLS).Amizadeh, S.; Matusevych, S.; and Weimer, M. 2018. Learning to solve circuit- sat: An unsupervised differentiable approach. In International conference on learning representations.Armin, B.; Tobias, F.; Katalin, F.; Mathias, F.; Nils, F.; and Florian, P. 2024. Ca Di Ca L, Gimsatul, Isa SAT and Kissat entering the SAT Competition 2024. Proc. of SAT Competition, 8- 10. Audemard, G.; and Simon, L. 2018. On the Glucose SAT Solver. International Journal on Artificial Intelligence Tools, 27(01): 1840001. Brayton, R. K.; and Mishchenko, A. 2010. ABC: An Academic Industrial- Strength Verification Tool. In International Conference on Computer Aided Verification.Chanseok, O. 2015. Between SAT and UNSAT: the fundamental difference in CDCL SAT. In International Conference on Theory and Applications of Satisfiability Testing, 307- 323. Springer.Davidson, S. 1999. ITC'99 Benchmark Circuits - Preliminary Results. In International Test Conference 1999. Proceedings (IEEE Cat. No.99CH37034), 1125- 1125. E\u00e9n, N.; Mishchenko, A.; and S\u00f6rensson, N. 2007. Applying logic synthesis for speeding up SAT. In Theory and Applications of Satisfiability Testing - SAT 2007, 272- 286. Springer.Fang, W.; Li, W.; Liu, S.; Lu, Y.; Zhang, H.; and Xie, Z. 2025a. Net TAG: A Multimodal RTL- and- Layout- Aligned Netlist Foundation Model via Text- Attributed Graph. ar Xiv preprint ar Xiv:2504.09260. Fang, W.; Liu, S.; Wang, J.; and Xie, Z. 2025b. Circuitfusion: multimodal circuit representation learning for agile chip design. ar Xiv preprint ar Xiv:2505.02168. Fleury, A.; and Heisinger, M. 2020. Cadical, kissat, paracooba, plungeling and treengeling entering the sat competition 2020. Sat Competition, 2020: 50. Goldberg, E.; Prasad, M.; and Brayton, R. 2001. Using SAT for combinational equivalence checking. Design, Automation, and Test in Europe. Design, Automation, and Test in Europe.Kingma, D.; and Ba, J. 2014. Adam: A Method for Stochastic Optimization. ar Xiv: Learning, ar Xiv: Learning.Liu, J.; Zhai, J.; Zhao, M.; Lin, Z.; Yu, B.; and Shi, C. 2024. Polar Gate: Breaking the Functionality Representation Bottleneck of And- Inverter Graph Neural Network. In 2024 IEEE/ACM International Conference on Computer- Aided Design (ICCAD).Lu, F.; Wang, L.- C.; Cheng, K.- T.; and Huang, R.- Y. 2003. A circuit SAT solver with signal correlation guided learning. In 2003 Design, Automation and Test in Europe Conference and Exhibition, 892- 897. Marques- Silva, J.; Lynce, I.; and Malik, S. 2021. Conflict- driven clause learning SAT solvers. In Handbook of satisfiability, 133- 182. ios Press. Open Core, T. 1999. Open Core. https://opencores.org/.Qian, Y.; Chen, Z.; Zhang, X.; and Cai, S. 2025. X- SAT: An Efficient Circuit- Based SAT Solver. In 2025 62nd ACM/IEEE Design Automation Conference (DAC). IEEE.Shi, Z.; Li, Z.; Ma, C.; Zhou, Y.; Zheng, Z.; Liu, J.; Pan, H.; Zhou, L.; Li, K.; Zhu, J.; Yan, L.; He, Z.; Xue, C.; Jiang, W.; Yang, F.; Sun, G.; Yang, X.; Chen, G.; Shi, C.; Chu, Z.; Yang, J.; and Xu, Q. 2025a. Forge EDA: Towards Verifiable and Customizable Circuit Benchmarks. ar Xiv preprint ar Xiv:2505.02016. Shi, Z.; Ma, C.; Zheng, Z.; Zhou, L.; Pan, H.; Jiang, W.; Yang, F.; Yang, X.; Chu, Z.; and Xu, Q. 2025b. Deep Cell: Multiview Representation Learning for Post- Mapping Netlists. ar Xiv preprint ar Xiv:2502.06816. Shi, Z.; Pan, H.; Khan, S.; Li, M.; Liu, Y.; Huang, J.; Zhen, H.- L.; Yuan, M.; Chu, Z.; and Xu, Q. 2023. Deepgate2: Functionality- aware circuit representation learning. In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), 1- 9. IEEE.Shi, Z.; Tang, T.; Zhu, J.; Sadaf, K.; Zhen, H.- L.; Yuan, M.; Chu, Z.; and Xu, Q. 2025c. Logic Optimization Meets SAT: A Novel Framework for Circuit- SAT Solving. In 2025 62nd ACM/IEEE Design Automation Conference (DAC). IEEE.Shi, Z.; Zheng, Z.; Khan, S.; Zhong, J.; Li, M.; and Xu, Q. 2024. Deep Gate3: Towards Scalable Circuit Representation Learning. ar Xiv preprint ar Xiv:2407.11095. Stephan, P.; Brayton, R.; and Sangiovanni- Vincentelli, A. 1996. Combinational test generation using satisfiability. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, 1167- 1176. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is All you Need. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.Wang, Z.; Bai, C.; He, Z.; Zhang, G.; Xu, Q.; Ho, T.- Y.; Huang, Y.; and Yu, B. 2024. Fgnn2: A powerful pre- training framework for learning the logic functionality of circuits. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems.Wang, Z.; Bai, C.; He, Z.; Zhang, G.; Xu, Q.; Ho, T.- Y.; Yu, B.; and Huang, Y. 2022. Functionality matters in netlist representation learning. In Proceedings of the 59th ACM/IEEE Design Automation Conference, 61- 66. Wu, C.- A.; Lin, T.- H.; Lee, C.- C.; and Huang, C.- Y. 2007. Qute SAT: A Robust Circuit- based SAT Solver for Complex Circuit Structure. In 2007 Design, Automation & Test in Europe Conference & Exhibition, 1- 6. Wu, H.; Zheng, H.; Pu, Y.; and Yu, B. 2025. Circuit Representation Learning with Masked Gate Modeling and Verilog- AIG Alignment. ar Xiv preprint ar Xiv:2502.12732. Zheng, Z.; Huang, S.; Zhong, J.; Shi, Z.; Dai, G.; Xu, N.; and Xu, Q. 2025. Deep Gate4: Efficient and Effective Representation Learning for Circuit Design at Scale. ar Xiv preprint ar Xiv:2502.01681.",
      "content": "",
      "level": 2,
      "line_start": 23,
      "line_end": 24
    },
    {
      "heading": "A1 Model Details Model Architecture. We adopt the DEEPGATE4 framework (Zheng et al. 2025), where each gate is represented by structural and functional embeddings and updated via selfattention (Vaswani et al. 2017). For each gate type \\(g\\in \\{\\mathrm{and},\\mathrm{not},\\mathrm{div}\\}\\) , we implement type- specific aggregation and update functions: not . The aggregator directly propagates the single input and applies a logical negation. and / virtual and. The aggregation function captures conjunction semantics, where attention weights reflect the relative importance of inputs- for example, giving higher weight to controlling inputs that can determine the output value. virtual div. In addition to a two- input aggregator, we account for the asymmetric roles of the inputs (numerator \\(A_{joint}\\) vs. denominator \\(C\\) ) by incorporating positional encodings to distinguish them during message passing. This unified framework enables the model to reason jointly over logical and probabilistic structures. Level- by- Level Propagation. Let \\(\\mathcal{L}(v)\\) denote the topological level of gate \\(v\\) . For every level \\(\\ell\\) (from primary inputs PI to primary outputs PO) the model performs: \\[\\begin{array}{r l} & {h_{v}^{s}\\leftarrow \\mathrm{Agg}_{g}^{s}\\big(\\{h_{u}^{s}\\mid u\\in P(v),\\mathcal{L}(u) = \\ell -1\\} \\big)}\\\\ & {h_{v}^{f}\\leftarrow \\mathrm{Agg}_{g}^{f}\\big(\\{h_{u}^{f},h_{u}^{s}\\mid u\\in P(v),\\mathcal{L}(u) = \\ell -1\\} \\big)} \\end{array} \\quad (A1)\\] where \\(P(v)\\) denotes the set of predecessor nodes of \\(v\\) ## A2 Effectiveness of div Gate The div gate explicitly models conditional probability in the circuit graph via the following equation: \\[P(A\\mid C) = \\frac{P(A\\wedge C)}{P(C)}. \\quad (A3)\\] While this formulation is convenient, the division operation can amplify errors significantly when \\(P(C)\\) is very small (always smaller than 1, as it represents the logic probability of node \\(C\\) ). To illustrate this behavior, we analyze two representative cases using ac97_ctrl.aig as a case study. Case 1: Extremely Small \\(P(C)\\) . In this scenario, when the conditional node rarely evaluates to 1 ( \\(P(C) = 0.01\\) ), even minor absolute errors in \\(P(A\\wedge C)\\) translate into large relative errors in \\(P(A\\mid C)\\) . Table A1 presents the ground- truth and predicted probabilities for \\(P(C)\\) and a subset of 20 target nodes \\(A\\) . The prediction error for \\(P(C)\\) is moderate ( \\(\\hat{P} (C) = 0.00323\\) ), yet the resulting conditional probabilities are significantly distorted due to error amplification. Case 2: Moderate \\(P(C)\\) . When \\(P(C)\\) is in a mid- range (e.g., \\(P(C) > 0.3\\) ), the division operation becomes relatively numerically stable. However, it still amplifies errors in \\(P(A\\mid C)\\) to an extent that is practically unacceptable. Table A2 compares aggregate losses, showing that the overall absolute error remains greater than 0.1. In contrast, our method incorporates the div gate during the dataset preparation stage. The probability labels for the DIV gate are computed as the division of the probabilities of its two input nodes, which are tagged with distinct positional markers. By learning this behavior as a standard gate type, the model directly internalizes the division operation, eliminating the need to explicitly compute the quotient of joint and marginal probabilities. ## A3 Pattern-Based Dataset To bootstrap training, we hope to generate pattern- based traces that approximate full truth tables. Given a circuit with \\(m\\) primary inputs (PIs), the complete truth table has \\(2^{m}\\) rows\u2014impractical beyond \\(m\\approx 20\\) . Instead, we conduct simulation with 20,000 patterns and uniformly sample 100 random patterns per circuit per epoch (see Table A3) and record the resulting logic probability values for every node. <table><tr><td>Pattern ID</td><td>PI1</td><td>PI2</td><td>PI3</td><td>...</td><td>PIm</td><td>Node u</td><td>Node v</td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>...</td><td>1</td><td>0</td><td>0</td></tr><tr><td>2</td><td>1</td><td>0</td><td>0</td><td>...</td><td>0</td><td>1</td><td>0</td></tr><tr><td>3</td><td>0</td><td>0</td><td>1</td><td>...</td><td>1</td><td>0</td><td>1</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>19997</td><td>0</td><td>1</td><td>1</td><td>...</td><td>0</td><td>1</td><td>1</td></tr><tr><td>19998</td><td>0</td><td>1</td><td>1</td><td>...</td><td>0</td><td>0</td><td>1</td></tr><tr><td>19999</td><td>1</td><td>0</td><td>1</td><td>...</td><td>1</td><td>1</td><td>0</td></tr><tr><td>20000</td><td>0</td><td>0</td><td>0</td><td>...</td><td>0</td><td>0</td><td>1</td></tr></table> Table A3: Excerpt of a truth table. Each training epoch picks a fresh random subset of 100 rows, ensuring the model learns to interpolate between unseen patterns. For each node \\(v\\) , we compute \\[\\hat{P}_{\\mathrm{rand}}(v) = \\frac{1}{100}\\sum_{i = 1}^{100}\\mathbf{1}\\big(v = 1\\mathrm{~in~row~}i\\big), \\quad (A4)\\] which serves as a supervisory signal encouraging the GNN to capture fine- grained functional behavior. Because the 100- row subset changes every epoch, the network is exposed to thousands of distinct local views, yielding better generalization than a single large simulation. ## A4 Workload-Based Dataset While random patterns provide breadth, they fail to capture realistic activity biases. We therefore introduce workload- based simulations, where each primary input (PI) is independently assigned a probability value \\(\\rho\\) sampled from the set \\(0.1, 0.2, \\ldots , 0.9\\) . A toy circuit ( \\(c = a \\wedge b\\) , Fig. A1) is used to illustrate the necessity.",
      "content": "",
      "level": 2,
      "line_start": 25,
      "line_end": 25
    }
  ]
}