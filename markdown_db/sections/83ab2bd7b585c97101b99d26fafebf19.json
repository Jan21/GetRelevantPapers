{
  "sections": {
    "neural_heuristics_for_sat_solving": "## Introduction\n\n\ngeneral problem of determining equivalence of Boolean (alternatively: arithmetic) expressions (satisfiability can be seen as equivalence to any unsatisfiable formula e.g. \\(a \\wedge \\neg a\\) ). However, the formulas solved by Eq Net have up to 10 variables and 13 symbols, while we tackle formulas beyond one hundred variables and thousands of symbols. Learned Restart Policy [LOM\\(^+\\) 18] presents a different approach to improve a SAT solver with machine learning, where the network decides at each step whether the algorithm should be restarted to follow another random path in the search tree. [VLV\\(^+\\) 20] uses reinforcement learning to train clause deletion heuristics in DPLL based solvers. ## 3 ARCHITECTURE We use a message- passing graph- based neural network architecture similar to Neuro SAT introduced in [SLB\\(^+\\) 18]. The general idea is to represent a formula as a graph with two node types (literal and clause) and two edge types (literal- literal edges represent the negation relation, and clause- literal edges represent relation between each clause and literals it contains). Example formula represented as a graph is shown in Figure 2 Left. Each node has its own state, represented by an embedding vector. Thanks to this representation, we have the following properties: 1. Invariance to variable renaming. 2. Invariance to negation of all occurrences of a variable. 3. Invariance to permutation of literals in a clause. 4. Invariance to permutation of clauses in a formula. <center>Figure 2: Left: A graph representation of formula \\((A \\vee \\neg C \\vee B) \\wedge (\\neg B \\vee C)\\) used in our work. In the model nodes are unlabeled (labels are included only for the reader's convenience). Different colors mark two distinct types of nodes (clause and literal) and two distinct types of edges (literal-literal and clause-literal). Right: Overview of message-passing architecture. In each iteration we take as the input: connection matrix between clauses and literals \\((CCL)\\) , connection between literals and their negations \\((CLL)\\) , literal embeddings from previous iteration \\((LE_{t-1})\\) , and clause embeddings from previous iteration \\((CE_{t-1})\\) . We use 5 separate MLPs, which share parameters across iterations. Aggregation method depends on a model, see the description below. </center> We initialize all embedding vectors with a trainable initial embedding, different for each type of node. Then we run a number of iterations (from 20 to 40 in our experiments), visualized in Figure 2 Right. Each iteration consists of three stages: Stage 1. Message: Each node generates a message vector \\(V\\) (and a vector \\(K\\) if needed) based on its embedding, to every connected node. \\(V\\) and \\(K\\) are generated with a three- layer MLP with Leaky Re LU [MHN13] activation after each hidden layer and linear activation after the last layer. Stage 2. Aggregate: all messages are delivered according to the connection matrix, then aggregated for each receiver with one of the aggregation functions (described in the next paragraph). Stage 3. Update: Each node updates its embedding based on its previous embedding and aggregated received messages. New embedding is computed by a three- layer MLP with Leaky Re LU activation after each hidden layer and sigmoid activation after the last layer. We explore two different aggregation methods. The first is the average of received \\(V\\) vectors. The second method is a modified attention mechanism. As a message, instead of just a single vector \\(V\\) , we send two vectors, \\(V\\) and \\(K\\) . Receiving node generates one vector \\(Q\\) based on its embedding, and the result of aggregation is \\(\\sum_{i} V_{i} \\text{ sigmoid } (K_{i} \\cdot Q)\\) . Thanks to this, each message may be selectively rejected or accepted by the receiver, depending on relation between \\(K\\) and \\(Q\\) . The intuitive difference between this mechanism and the standard attention is as follows: the standard attention as in [VCC\\(^+\\) 17] chooses one message to look at, while our mechanism rejects or accepts messages independently and looks at their sum.\n\nLike Neuro SAT, our architecture learns to predict satisfiability of the whole formula (which we name sat prediction). However, it also predicts, for each literal separately, the existence of a solution with this literal (which we name policy prediction). To get policy prediction we add a logistic regression on top of each literal's embedding in each iteration (with parameters shared across all literals and iterations). To get sat prediction we add a linear regression on top of each literal's embedding in each iteration, and then apply a sigmoid on sum of their outputs. We define sat loss as cross-entropy loss between the sat prediction and the ground truth. We define policy loss as zero if formula is unsatisfiable and as the average of cross-entropy losses between policy predictions and ground truths if formula is satisfiable. To get a loss of the model we sum together both losses for every iteration. ## 4 EXPERIMENTAL RESULTS Dataset and training details. To train and evaluate the models we use a class of SAT problems \\(SR(n)\\) introduced and described in detail in [SLB \\(^{+18}\\) ]. It is parametrized only by \\(n\\) - the number of variables used in a formula. Both the size and the number of clauses vary. The dataset is balanced in terms of number of satisfiable and unsatisfiable examples. Each of the \\(SR(n)\\) samples has two labels (see Section 3): sat indicating whether the formula \\(\\Phi\\) is satisfiable and policy indicating for each literal \\(l\\) whether \\(\\Phi \\wedge l\\) is satisfiable. We generate each of those numbers by running Mini Sat 2.2 [ES03]. Sample random \\(SR(30)\\) formulas are solved by Mini SAT 2.2 in 0.007 seconds, while \\(SR(110)\\) takes 0.137 second and \\(SR(150)\\) takes 3.406 seconds (for a Xeon E5- 2680v3@2,5 GHz computer). We have trained separate models on SR(30), SR(50), SR(70) and SR(100). Table 1 shows the details of the training procedure. Metrics sat error and policy error are defined as mean absolute error of sat or policy prediction versus labels. The presented models are message- passing neural networks with our modified attention mechanism. Table 1: Each of the models was trained on SAT samples drawn from the distribution marked in the first column. The metrics: loss, sat error and policy error are evaluated on an independently generated evaluation set. The values indicate mean and standard deviation over 3-5 trained models. Models were trained using single TPU v2. <table><tr><td>Problem</td><td>Loss</td><td>sat error</td><td>policy error</td><td>Batch size</td><td>Train. steps</td><td>Train. time</td></tr><tr><td>SR(30)</td><td>28.178\u00b10.672</td><td>0.084\u00b10.004</td><td>0.050\u00b10.002</td><td>128</td><td>1200K</td><td>20h</td></tr><tr><td>SR(50)</td><td>32.024\u00b10.555</td><td>0.233\u00b10.017</td><td>0.105\u00b10.006</td><td>64</td><td>600K</td><td>12h</td></tr><tr><td>SR(70)</td><td>33.010\u00b10.482</td><td>0.266\u00b10.033</td><td>0.110\u00b10.007</td><td>64</td><td>600K</td><td>22h</td></tr><tr><td>SR(100)</td><td>34.227\u00b10.127</td><td>0.319\u00b10.007</td><td>0.123\u00b10.002</td><td>32</td><td>1200K</td><td>28h</td></tr></table> Experiment 1: comparison of all models with DLIS and JW- OS heuristics. We evaluated the DPLL algorithm guided by our 4 kinds of models described above and compared to DPLL guided by JW- OS and DLIS. As a performance consideration we decided to stop DPLL after 1000 steps (see Experiment 2 below for a comparison without this restriction) and count the number of solved formulas out of 100 in each class. We present the results in Figure 3. For this and subsequent experiments we only consider satisfiable \\(SR(n)\\) samples. JW- OS proved to be the best on average classes of problems: \\(SR(50)\\) and \\(SR(70)\\) , whereas neural guidance- based algorithms proved to be the best on large problems: \\(SR(90)\\) and \\(SR(110)\\) . <center>Figure 3: Performance of DPLL with different guidance heuristics on specific problem sizes. The \\(x\\) axis indicates the class of the evaluation set: evaluation is performed on fresh randomly chosen one satisfiable hundred \\(SR(x)\\) formulas. The \\(y\\) axis indicates the percent of instances (out of 100) solved by DPLL within 1000 steps. </center> Experiment 2: detailed comparison with the JW- OS heuristic. We have selected the SR(50) model for a detailed comparison of the learned heuristics versus JW- OS and for the sake of this comparison designed hybrid guidance algorithm that uses a model trained on \\(SR(50)\\) (a fixed one of the three similar replicas) and switches to JW- OS when the network predicts sat probability below a threshold of \\(0.3^{2}\\) . We then compared the new hybrid guidance with the heuristic JW- OS without the 1000 step restriction. JW- OS was selected on the basis of Experiment 1. The experiment shows that the hybrid approach is faster in terms of number of steps in a significant majority of cases, both when used with DPLL (Figure 4 Left) and with CDCL (Figure 4 Right).\n\n<center>Figure 4: Left: Comparison of Hybrid (ours) and JW-OS as heuristics in DPLL. We measure the performance of each method according to the number of steps required to find a solution for a given SAT instance. A method wins if it solves a given instance in a smaller number of steps. The blue bar reflects the percentage of formulas where the Hybrid (ours) method won, the green bar means that JW-OS won, and the orange bar means that there was a draw. Right: the same for CDCL. </center> Experiment 3: an ablation for the attention mechanism. From experiments presented in Figure 5 follows that in most cases attention improved evaluation metrics by a significant margin. Only in the case of \\(SR(30)\\) , level 20 attention degraded the model performance. For \\(SR(50)\\) , level 40 the metrics with and without attention stayed within the standard deviation of each other. Reproducibility. For each set of hyperparameters (e.g. \\(SR(30)\\) , level 40), we trained five models. We considered a model not correctly trained if adding it to the set of models raised standard deviation of the losses above 1, see Table 1. We excluded such models (up to 2 models out of 5 for a set of hyperparameters) from further comparisons and left the question of stability of training as a topic of further investigations. The code including hyperparameters is published at https://bit.ly/neurheur. Our code is based on Tensor Flow [AAB+15]. It uses a CDCL implementation by [Zho18]. We access Mini Sat through Py SAT interface [IMM18]. <center>Figure 5: Comparison of policy error with and without attention. The presented values are mean and standard deviation over 3-5 trained models calculated on the evaluation set. </center> 5 CONCLUSIONS AND FUTURE WORKIn this work we have shown three experiments confirming that SAT- solving can be augmented by neural networks. The message- passing architecture augmented by attention performs competitively comparing with standard heuristics when evaluated on relatively large propositional problems, including problems with more than a hundred variables (see Section 4). From the ablation presented in Experiment 3 follows that the message- passing architecture that uses the attention mechanism overall performs better than the same architecture without attention and we attribute it to a selective acceptance of incoming messages made possible by the attention mechanism. We believe that using an appropriately large computing infrastructure the learning process can be extended to more complex examples and that in the near future parallelization combined with a variant of the message- passing architecture can be used to train models which will tackle larger SR problems, and possibly SAT problem classes currently beyond the reach of SAT- solvers. As a future step we consider extending our improved heuristics so that a neural network would be able to control other aspects of the SAT solver behavior, like restarting and backtracking. Eventually, other prediction targets, including expected number of steps, may be beneficial. Once we exhaust the pool of available supervised data it would be interesting to apply reinforcement learning methods, including methods recently presented in [KUMO18]. In this work we focus on the number of steps of the algorithm rather than execution time. Moving the main loop of DPLL or CDCL to a tensor computation graph would be a step towards making the algorithms more competitive in terms of the execution time.\n\n## 6 ACKNOWLEDGEMENTS 6 ACKNOWLEDGEMENTSThis was work was supported by (1) the Polish National Science Center grant UMO- 2018/29/B/ST6/02959 (2) the Tensor Flow Research Cloud which granted 50 TPUs (3) the Academic Computer Center Cyfronet at the AGH University of Science and Technology in Krak\u00f3w, Poland. ## REFERENCES REFERENCES[AAB+15] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor Flow: Large- scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.[ACKS16] Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, and Charles A. Sutton. Learning continuous semantic representations of symbolic expressions. Co RR, abs/1611.01423, 2016. [BHB+18] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez- Gonzalez, Vin\u00edcius Flores Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, \u00c7aglar G\u00fcl\u00e7ehre, Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. Co RR, abs/1806.01261, 2018. [ES03] Niklas E\u00e9n and Niklas S\u00f6rensson. An extensible sat- solver. In Theory and Applications of Satisfiability Testing, 6th International Conference, SAT 2003. Santa Margherita Ligure, Italy, May 5- 8, 2003 Selected Revised Papers, pages 502- 518, 2003. [ESA+18] Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural networks understand logical entailment? Co RR, abs/1802.08535, 2018. [IMM18] Alexey Ignatiev, Antonio Morgado, and Joao Marques- Silva. Py SAT: A Python toolkit for prototyping with SAT oracles. In SAT, pages 428- 437, 2018. [Kar72] R. Karp. Reducibility among combinatorial problems. In R. Miller and J. Thatcher, editors, Complexity of Computer Computations, pages 85- 103. Plenum Press, 1972. [KUMO18] Cezary Kaliszyk, Josef Urban, Henryk Michalewski, and Mirek Ols\u00e1k. Reinforcement learning of theorem proving. Co RR, abs/1805.07563, 2018. [LOM+18] Jia Hui Liang, Chanseok Oh, Minu Mathew, Ciza Thomas, Chunxiao Li, and Vijay Ganesh. Machine learning- based restart policy for CDCL SAT solvers. In Theory and Applications of Satisfiability Testing - SAT 2018 - 21st International Conference, SAT 2018, Held as Part of the Federated Logic Conference, Flo C 2018, Oxford, UK, July 9- 12, 2018, Proceedings, pages 94- 110, 2018. [MHN13] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013. [MMZ+01] Matthew W. Moskewicz, Conor F. Madigan, Ying Zhao, Lintao Zhang, and Sharad Malik. Chaff: Engineering an efficient SAT solver. In Proceedings of the 38th Design Automation Conference, DAC 2001, Las Vegas, NV, USA, June 18- 22, 2001, pages 530- 535, 2001. [MS99] Joao Marques- Silva. The impact of branching heuristics in propositional satisfiability algorithms. In EPIA, 1999. [SLB+18] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single- bit supervision. Co RR, abs/1802.03685, 2018.\n\n[SS18] Taro Sekiyama and Kohei Suenaga. Automated proof synthesis for propositional logic with deep neural networks. Co RR, abs/1805.11799, 2018. [VCC+17] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. Co RR, abs/1710.10903, 2017. [VLW+20] Pashootan Vaezipoor, Gil Lederman, Yuhuai Wu, Roger Grosse, and Fahiem Bacchus. Learning clause deletion heuristics with reinforcement learning. In 5th Conference on Artificial Intelligence and Theorem Proving, 2020. [WTWD17] Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise selection for theorem proving by deep graph embedding. Co RR, abs/1709.09994, 2017. [Zho18] Zhang Zhongwei. Simple SAT solver with CDCL implemented in Python. https://github.com/zlii/pysat/, 2018.",
    "introduction": "general problem of determining equivalence of Boolean (alternatively: arithmetic) expressions (satisfiability can be seen as equivalence to any unsatisfiable formula e.g. \\(a \\wedge \\neg a\\) ). However, the formulas solved by Eq Net have up to 10 variables and 13 symbols, while we tackle formulas beyond one hundred variables and thousands of symbols. Learned Restart Policy [LOM\\(^+\\) 18] presents a different approach to improve a SAT solver with machine learning, where the network decides at each step whether the algorithm should be restarted to follow another random path in the search tree. [VLV\\(^+\\) 20] uses reinforcement learning to train clause deletion heuristics in DPLL based solvers. ## 3 ARCHITECTURE We use a message- passing graph- based neural network architecture similar to Neuro SAT introduced in [SLB\\(^+\\) 18]. The general idea is to represent a formula as a graph with two node types (literal and clause) and two edge types (literal- literal edges represent the negation relation, and clause- literal edges represent relation between each clause and literals it contains). Example formula represented as a graph is shown in Figure 2 Left. Each node has its own state, represented by an embedding vector. Thanks to this representation, we have the following properties: 1. Invariance to variable renaming. 2. Invariance to negation of all occurrences of a variable. 3. Invariance to permutation of literals in a clause. 4. Invariance to permutation of clauses in a formula. <center>Figure 2: Left: A graph representation of formula \\((A \\vee \\neg C \\vee B) \\wedge (\\neg B \\vee C)\\) used in our work. In the model nodes are unlabeled (labels are included only for the reader's convenience). Different colors mark two distinct types of nodes (clause and literal) and two distinct types of edges (literal-literal and clause-literal). Right: Overview of message-passing architecture. In each iteration we take as the input: connection matrix between clauses and literals \\((CCL)\\) , connection between literals and their negations \\((CLL)\\) , literal embeddings from previous iteration \\((LE_{t-1})\\) , and clause embeddings from previous iteration \\((CE_{t-1})\\) . We use 5 separate MLPs, which share parameters across iterations. Aggregation method depends on a model, see the description below. </center> We initialize all embedding vectors with a trainable initial embedding, different for each type of node. Then we run a number of iterations (from 20 to 40 in our experiments), visualized in Figure 2 Right. Each iteration consists of three stages: Stage 1. Message: Each node generates a message vector \\(V\\) (and a vector \\(K\\) if needed) based on its embedding, to every connected node. \\(V\\) and \\(K\\) are generated with a three- layer MLP with Leaky Re LU [MHN13] activation after each hidden layer and linear activation after the last layer. Stage 2. Aggregate: all messages are delivered according to the connection matrix, then aggregated for each receiver with one of the aggregation functions (described in the next paragraph). Stage 3. Update: Each node updates its embedding based on its previous embedding and aggregated received messages. New embedding is computed by a three- layer MLP with Leaky Re LU activation after each hidden layer and sigmoid activation after the last layer. We explore two different aggregation methods. The first is the average of received \\(V\\) vectors. The second method is a modified attention mechanism. As a message, instead of just a single vector \\(V\\) , we send two vectors, \\(V\\) and \\(K\\) . Receiving node generates one vector \\(Q\\) based on its embedding, and the result of aggregation is \\(\\sum_{i} V_{i} \\text{ sigmoid } (K_{i} \\cdot Q)\\) . Thanks to this, each message may be selectively rejected or accepted by the receiver, depending on relation between \\(K\\) and \\(Q\\) . The intuitive difference between this mechanism and the standard attention is as follows: the standard attention as in [VCC\\(^+\\) 17] chooses one message to look at, while our mechanism rejects or accepts messages independently and looks at their sum.\n\nLike Neuro SAT, our architecture learns to predict satisfiability of the whole formula (which we name sat prediction). However, it also predicts, for each literal separately, the existence of a solution with this literal (which we name policy prediction). To get policy prediction we add a logistic regression on top of each literal's embedding in each iteration (with parameters shared across all literals and iterations). To get sat prediction we add a linear regression on top of each literal's embedding in each iteration, and then apply a sigmoid on sum of their outputs. We define sat loss as cross-entropy loss between the sat prediction and the ground truth. We define policy loss as zero if formula is unsatisfiable and as the average of cross-entropy losses between policy predictions and ground truths if formula is satisfiable. To get a loss of the model we sum together both losses for every iteration. ## 4 EXPERIMENTAL RESULTS Dataset and training details. To train and evaluate the models we use a class of SAT problems \\(SR(n)\\) introduced and described in detail in [SLB \\(^{+18}\\) ]. It is parametrized only by \\(n\\) - the number of variables used in a formula. Both the size and the number of clauses vary. The dataset is balanced in terms of number of satisfiable and unsatisfiable examples. Each of the \\(SR(n)\\) samples has two labels (see Section 3): sat indicating whether the formula \\(\\Phi\\) is satisfiable and policy indicating for each literal \\(l\\) whether \\(\\Phi \\wedge l\\) is satisfiable. We generate each of those numbers by running Mini Sat 2.2 [ES03]. Sample random \\(SR(30)\\) formulas are solved by Mini SAT 2.2 in 0.007 seconds, while \\(SR(110)\\) takes 0.137 second and \\(SR(150)\\) takes 3.406 seconds (for a Xeon E5- 2680v3@2,5 GHz computer). We have trained separate models on SR(30), SR(50), SR(70) and SR(100). Table 1 shows the details of the training procedure. Metrics sat error and policy error are defined as mean absolute error of sat or policy prediction versus labels. The presented models are message- passing neural networks with our modified attention mechanism. Table 1: Each of the models was trained on SAT samples drawn from the distribution marked in the first column. The metrics: loss, sat error and policy error are evaluated on an independently generated evaluation set. The values indicate mean and standard deviation over 3-5 trained models. Models were trained using single TPU v2. <table><tr><td>Problem</td><td>Loss</td><td>sat error</td><td>policy error</td><td>Batch size</td><td>Train. steps</td><td>Train. time</td></tr><tr><td>SR(30)</td><td>28.178\u00b10.672</td><td>0.084\u00b10.004</td><td>0.050\u00b10.002</td><td>128</td><td>1200K</td><td>20h</td></tr><tr><td>SR(50)</td><td>32.024\u00b10.555</td><td>0.233\u00b10.017</td><td>0.105\u00b10.006</td><td>64</td><td>600K</td><td>12h</td></tr><tr><td>SR(70)</td><td>33.010\u00b10.482</td><td>0.266\u00b10.033</td><td>0.110\u00b10.007</td><td>64</td><td>600K</td><td>22h</td></tr><tr><td>SR(100)</td><td>34.227\u00b10.127</td><td>0.319\u00b10.007</td><td>0.123\u00b10.002</td><td>32</td><td>1200K</td><td>28h</td></tr></table> Experiment 1: comparison of all models with DLIS and JW- OS heuristics. We evaluated the DPLL algorithm guided by our 4 kinds of models described above and compared to DPLL guided by JW- OS and DLIS. As a performance consideration we decided to stop DPLL after 1000 steps (see Experiment 2 below for a comparison without this restriction) and count the number of solved formulas out of 100 in each class. We present the results in Figure 3. For this and subsequent experiments we only consider satisfiable \\(SR(n)\\) samples. JW- OS proved to be the best on average classes of problems: \\(SR(50)\\) and \\(SR(70)\\) , whereas neural guidance- based algorithms proved to be the best on large problems: \\(SR(90)\\) and \\(SR(110)\\) . <center>Figure 3: Performance of DPLL with different guidance heuristics on specific problem sizes. The \\(x\\) axis indicates the class of the evaluation set: evaluation is performed on fresh randomly chosen one satisfiable hundred \\(SR(x)\\) formulas. The \\(y\\) axis indicates the percent of instances (out of 100) solved by DPLL within 1000 steps. </center> Experiment 2: detailed comparison with the JW- OS heuristic. We have selected the SR(50) model for a detailed comparison of the learned heuristics versus JW- OS and for the sake of this comparison designed hybrid guidance algorithm that uses a model trained on \\(SR(50)\\) (a fixed one of the three similar replicas) and switches to JW- OS when the network predicts sat probability below a threshold of \\(0.3^{2}\\) . We then compared the new hybrid guidance with the heuristic JW- OS without the 1000 step restriction. JW- OS was selected on the basis of Experiment 1. The experiment shows that the hybrid approach is faster in terms of number of steps in a significant majority of cases, both when used with DPLL (Figure 4 Left) and with CDCL (Figure 4 Right).\n\n<center>Figure 4: Left: Comparison of Hybrid (ours) and JW-OS as heuristics in DPLL. We measure the performance of each method according to the number of steps required to find a solution for a given SAT instance. A method wins if it solves a given instance in a smaller number of steps. The blue bar reflects the percentage of formulas where the Hybrid (ours) method won, the green bar means that JW-OS won, and the orange bar means that there was a draw. Right: the same for CDCL. </center> Experiment 3: an ablation for the attention mechanism. From experiments presented in Figure 5 follows that in most cases attention improved evaluation metrics by a significant margin. Only in the case of \\(SR(30)\\) , level 20 attention degraded the model performance. For \\(SR(50)\\) , level 40 the metrics with and without attention stayed within the standard deviation of each other. Reproducibility. For each set of hyperparameters (e.g. \\(SR(30)\\) , level 40), we trained five models. We considered a model not correctly trained if adding it to the set of models raised standard deviation of the losses above 1, see Table 1. We excluded such models (up to 2 models out of 5 for a set of hyperparameters) from further comparisons and left the question of stability of training as a topic of further investigations. The code including hyperparameters is published at https://bit.ly/neurheur. Our code is based on Tensor Flow [AAB+15]. It uses a CDCL implementation by [Zho18]. We access Mini Sat through Py SAT interface [IMM18]. <center>Figure 5: Comparison of policy error with and without attention. The presented values are mean and standard deviation over 3-5 trained models calculated on the evaluation set. </center> 5 CONCLUSIONS AND FUTURE WORKIn this work we have shown three experiments confirming that SAT- solving can be augmented by neural networks. The message- passing architecture augmented by attention performs competitively comparing with standard heuristics when evaluated on relatively large propositional problems, including problems with more than a hundred variables (see Section 4). From the ablation presented in Experiment 3 follows that the message- passing architecture that uses the attention mechanism overall performs better than the same architecture without attention and we attribute it to a selective acceptance of incoming messages made possible by the attention mechanism. We believe that using an appropriately large computing infrastructure the learning process can be extended to more complex examples and that in the near future parallelization combined with a variant of the message- passing architecture can be used to train models which will tackle larger SR problems, and possibly SAT problem classes currently beyond the reach of SAT- solvers. As a future step we consider extending our improved heuristics so that a neural network would be able to control other aspects of the SAT solver behavior, like restarting and backtracking. Eventually, other prediction targets, including expected number of steps, may be beneficial. Once we exhaust the pool of available supervised data it would be interesting to apply reinforcement learning methods, including methods recently presented in [KUMO18]. In this work we focus on the number of steps of the algorithm rather than execution time. Moving the main loop of DPLL or CDCL to a tensor computation graph would be a step towards making the algorithms more competitive in terms of the execution time.",
    "6_acknowledgements_6_acknowledgementsthis_was_work_was_supported_by_1_the_polish_national_science_center_grant_umo-_201829bst602959_2_the_tensor_flow_research_cloud_which_granted_50_tpus_3_the_academic_computer_center_cyfronet_at_the_agh_university_of_science_and_technology_in_krak\u00f3w_poland_references_referencesaab15_mart\u00edn_abadi_ashish_agarwal_paul_barham_eugene_brevdo_zhifeng_chen_craig_citro_greg_s_corrado_andy_davis_jeffrey_dean_matthieu_devin_sanjay_ghemawat_ian_goodfellow_andrew_harp_geoffrey_irving_michael_isard_yangqing_jia_rafal_jozefowicz_lukasz_kaiser_manjunath_kudlur_josh_levenberg_dandelion_man\u00e9_rajat_monga_sherry_moore_derek_murray_chris_olah_mike_schuster_jonathon_shlens_benoit_steiner_ilya_sutskever_kunal_talwar_paul_tucker_vincent_vanhoucke_vijay_vasudevan_fernanda_vi\u00e9gas_oriol_vinyals_pete_warden_martin_wattenberg_martin_wicke_yuan_yu_and_xiaoqiang_zheng_tensor_flow_large-_scale_machine_learning_on_heterogeneous_systems_2015_software_available_from_tensorfloworgacks16_miltiadis_allamanis_pankajan_chanthirasegaran_pushmeet_kohli_and_charles_a_sutton_learning_continuous_semantic_representations_of_symbolic_expressions_co_rr_abs161101423_2016_bhb18_peter_w_battaglia_jessica_b_hamrick_victor_bapst_alvaro_sanchez-_gonzalez_vin\u00edcius_flores_zambaldi_mateusz_malinowski_andrea_tacchetti_david_raposo_adam_santoro_ryan_faulkner_\u00e7aglar_g\u00fcl\u00e7ehre_francis_song_andrew_j_ballard_justin_gilmer_george_e_dahl_ashish_vaswani_kelsey_allen_charles_nash_victoria_langston_chris_dyer_nicolas_heess_daan_wierstra_pushmeet_kohli_matthew_botvinick_oriol_vinyals_yujia_li_and_razvan_pascanu_relational_inductive_biases_deep_learning_and_graph_networks_co_rr_abs180601261_2018_es03_niklas_e\u00e9n_and_niklas_s\u00f6rensson_an_extensible_sat-_solver_in_theory_and_applications_of_satisfiability_testing_6th_international_conference_sat_2003_santa_margherita_ligure_italy_may_5-_8_2003_selected_revised_papers_pages_502-_518_2003_esa18_richard_evans_david_saxton_david_amos_pushmeet_kohli_and_edward_grefenstette_can_neural_networks_understand_logical_entailment_co_rr_abs180208535_2018_imm18_alexey_ignatiev_antonio_morgado_and_joao_marques-_silva_py_sat_a_python_toolkit_for_prototyping_with_sat_oracles_in_sat_pages_428-_437_2018_kar72_r_karp_reducibility_among_combinatorial_problems_in_r_miller_and_j_thatcher_editors_complexity_of_computer_computations_pages_85-_103_plenum_press_1972_kumo18_cezary_kaliszyk_josef_urban_henryk_michalewski_and_mirek_ols\u00e1k_reinforcement_learning_of_theorem_proving_co_rr_abs180507563_2018_lom18_jia_hui_liang_chanseok_oh_minu_mathew_ciza_thomas_chunxiao_li_and_vijay_ganesh_machine_learning-_based_restart_policy_for_cdcl_sat_solvers_in_theory_and_applications_of_satisfiability_testing_-_sat_2018_-_21st_international_conference_sat_2018_held_as_part_of_the_federated_logic_conference_flo_c_2018_oxford_uk_july_9-_12_2018_proceedings_pages_94-_110_2018_mhn13_andrew_l_maas_awni_y_hannun_and_andrew_y_ng_rectifier_nonlinearities_improve_neural_network_acoustic_models_in_in_icml_workshop_on_deep_learning_for_audio_speech_and_language_processing_2013_mmz01_matthew_w_moskewicz_conor_f_madigan_ying_zhao_lintao_zhang_and_sharad_malik_chaff_engineering_an_efficient_sat_solver_in_proceedings_of_the_38th_design_automation_conference_dac_2001_las_vegas_nv_usa_june_18-_22_2001_pages_530-_535_2001_ms99_joao_marques-_silva_the_impact_of_branching_heuristics_in_propositional_satisfiability_algorithms_in_epia_1999_slb18_daniel_selsam_matthew_lamm_benedikt_b\u00fcnz_percy_liang_leonardo_de_moura_and_david_l_dill_learning_a_sat_solver_from_single-_bit_supervision_co_rr_abs180203685_2018": "[SS18] Taro Sekiyama and Kohei Suenaga. Automated proof synthesis for propositional logic with deep neural networks. Co RR, abs/1805.11799, 2018. [VCC+17] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. Co RR, abs/1710.10903, 2017. [VLW+20] Pashootan Vaezipoor, Gil Lederman, Yuhuai Wu, Roger Grosse, and Fahiem Bacchus. Learning clause deletion heuristics with reinforcement learning. In 5th Conference on Artificial Intelligence and Theorem Proving, 2020. [WTWD17] Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise selection for theorem proving by deep graph embedding. Co RR, abs/1709.09994, 2017. [Zho18] Zhang Zhongwei. Simple SAT solver with CDCL implemented in Python. https://github.com/zlii/pysat/, 2018."
  },
  "section_objects": [
    {
      "heading": "Neural heuristics for SAT solving",
      "content": "## Introduction\n\n\ngeneral problem of determining equivalence of Boolean (alternatively: arithmetic) expressions (satisfiability can be seen as equivalence to any unsatisfiable formula e.g. \\(a \\wedge \\neg a\\) ). However, the formulas solved by Eq Net have up to 10 variables and 13 symbols, while we tackle formulas beyond one hundred variables and thousands of symbols. Learned Restart Policy [LOM\\(^+\\) 18] presents a different approach to improve a SAT solver with machine learning, where the network decides at each step whether the algorithm should be restarted to follow another random path in the search tree. [VLV\\(^+\\) 20] uses reinforcement learning to train clause deletion heuristics in DPLL based solvers. ## 3 ARCHITECTURE We use a message- passing graph- based neural network architecture similar to Neuro SAT introduced in [SLB\\(^+\\) 18]. The general idea is to represent a formula as a graph with two node types (literal and clause) and two edge types (literal- literal edges represent the negation relation, and clause- literal edges represent relation between each clause and literals it contains). Example formula represented as a graph is shown in Figure 2 Left. Each node has its own state, represented by an embedding vector. Thanks to this representation, we have the following properties: 1. Invariance to variable renaming. 2. Invariance to negation of all occurrences of a variable. 3. Invariance to permutation of literals in a clause. 4. Invariance to permutation of clauses in a formula. <center>Figure 2: Left: A graph representation of formula \\((A \\vee \\neg C \\vee B) \\wedge (\\neg B \\vee C)\\) used in our work. In the model nodes are unlabeled (labels are included only for the reader's convenience). Different colors mark two distinct types of nodes (clause and literal) and two distinct types of edges (literal-literal and clause-literal). Right: Overview of message-passing architecture. In each iteration we take as the input: connection matrix between clauses and literals \\((CCL)\\) , connection between literals and their negations \\((CLL)\\) , literal embeddings from previous iteration \\((LE_{t-1})\\) , and clause embeddings from previous iteration \\((CE_{t-1})\\) . We use 5 separate MLPs, which share parameters across iterations. Aggregation method depends on a model, see the description below. </center> We initialize all embedding vectors with a trainable initial embedding, different for each type of node. Then we run a number of iterations (from 20 to 40 in our experiments), visualized in Figure 2 Right. Each iteration consists of three stages: Stage 1. Message: Each node generates a message vector \\(V\\) (and a vector \\(K\\) if needed) based on its embedding, to every connected node. \\(V\\) and \\(K\\) are generated with a three- layer MLP with Leaky Re LU [MHN13] activation after each hidden layer and linear activation after the last layer. Stage 2. Aggregate: all messages are delivered according to the connection matrix, then aggregated for each receiver with one of the aggregation functions (described in the next paragraph). Stage 3. Update: Each node updates its embedding based on its previous embedding and aggregated received messages. New embedding is computed by a three- layer MLP with Leaky Re LU activation after each hidden layer and sigmoid activation after the last layer. We explore two different aggregation methods. The first is the average of received \\(V\\) vectors. The second method is a modified attention mechanism. As a message, instead of just a single vector \\(V\\) , we send two vectors, \\(V\\) and \\(K\\) . Receiving node generates one vector \\(Q\\) based on its embedding, and the result of aggregation is \\(\\sum_{i} V_{i} \\text{ sigmoid } (K_{i} \\cdot Q)\\) . Thanks to this, each message may be selectively rejected or accepted by the receiver, depending on relation between \\(K\\) and \\(Q\\) . The intuitive difference between this mechanism and the standard attention is as follows: the standard attention as in [VCC\\(^+\\) 17] chooses one message to look at, while our mechanism rejects or accepts messages independently and looks at their sum.\n\nLike Neuro SAT, our architecture learns to predict satisfiability of the whole formula (which we name sat prediction). However, it also predicts, for each literal separately, the existence of a solution with this literal (which we name policy prediction). To get policy prediction we add a logistic regression on top of each literal's embedding in each iteration (with parameters shared across all literals and iterations). To get sat prediction we add a linear regression on top of each literal's embedding in each iteration, and then apply a sigmoid on sum of their outputs. We define sat loss as cross-entropy loss between the sat prediction and the ground truth. We define policy loss as zero if formula is unsatisfiable and as the average of cross-entropy losses between policy predictions and ground truths if formula is satisfiable. To get a loss of the model we sum together both losses for every iteration. ## 4 EXPERIMENTAL RESULTS Dataset and training details. To train and evaluate the models we use a class of SAT problems \\(SR(n)\\) introduced and described in detail in [SLB \\(^{+18}\\) ]. It is parametrized only by \\(n\\) - the number of variables used in a formula. Both the size and the number of clauses vary. The dataset is balanced in terms of number of satisfiable and unsatisfiable examples. Each of the \\(SR(n)\\) samples has two labels (see Section 3): sat indicating whether the formula \\(\\Phi\\) is satisfiable and policy indicating for each literal \\(l\\) whether \\(\\Phi \\wedge l\\) is satisfiable. We generate each of those numbers by running Mini Sat 2.2 [ES03]. Sample random \\(SR(30)\\) formulas are solved by Mini SAT 2.2 in 0.007 seconds, while \\(SR(110)\\) takes 0.137 second and \\(SR(150)\\) takes 3.406 seconds (for a Xeon E5- 2680v3@2,5 GHz computer). We have trained separate models on SR(30), SR(50), SR(70) and SR(100). Table 1 shows the details of the training procedure. Metrics sat error and policy error are defined as mean absolute error of sat or policy prediction versus labels. The presented models are message- passing neural networks with our modified attention mechanism. Table 1: Each of the models was trained on SAT samples drawn from the distribution marked in the first column. The metrics: loss, sat error and policy error are evaluated on an independently generated evaluation set. The values indicate mean and standard deviation over 3-5 trained models. Models were trained using single TPU v2. <table><tr><td>Problem</td><td>Loss</td><td>sat error</td><td>policy error</td><td>Batch size</td><td>Train. steps</td><td>Train. time</td></tr><tr><td>SR(30)</td><td>28.178\u00b10.672</td><td>0.084\u00b10.004</td><td>0.050\u00b10.002</td><td>128</td><td>1200K</td><td>20h</td></tr><tr><td>SR(50)</td><td>32.024\u00b10.555</td><td>0.233\u00b10.017</td><td>0.105\u00b10.006</td><td>64</td><td>600K</td><td>12h</td></tr><tr><td>SR(70)</td><td>33.010\u00b10.482</td><td>0.266\u00b10.033</td><td>0.110\u00b10.007</td><td>64</td><td>600K</td><td>22h</td></tr><tr><td>SR(100)</td><td>34.227\u00b10.127</td><td>0.319\u00b10.007</td><td>0.123\u00b10.002</td><td>32</td><td>1200K</td><td>28h</td></tr></table> Experiment 1: comparison of all models with DLIS and JW- OS heuristics. We evaluated the DPLL algorithm guided by our 4 kinds of models described above and compared to DPLL guided by JW- OS and DLIS. As a performance consideration we decided to stop DPLL after 1000 steps (see Experiment 2 below for a comparison without this restriction) and count the number of solved formulas out of 100 in each class. We present the results in Figure 3. For this and subsequent experiments we only consider satisfiable \\(SR(n)\\) samples. JW- OS proved to be the best on average classes of problems: \\(SR(50)\\) and \\(SR(70)\\) , whereas neural guidance- based algorithms proved to be the best on large problems: \\(SR(90)\\) and \\(SR(110)\\) . <center>Figure 3: Performance of DPLL with different guidance heuristics on specific problem sizes. The \\(x\\) axis indicates the class of the evaluation set: evaluation is performed on fresh randomly chosen one satisfiable hundred \\(SR(x)\\) formulas. The \\(y\\) axis indicates the percent of instances (out of 100) solved by DPLL within 1000 steps. </center> Experiment 2: detailed comparison with the JW- OS heuristic. We have selected the SR(50) model for a detailed comparison of the learned heuristics versus JW- OS and for the sake of this comparison designed hybrid guidance algorithm that uses a model trained on \\(SR(50)\\) (a fixed one of the three similar replicas) and switches to JW- OS when the network predicts sat probability below a threshold of \\(0.3^{2}\\) . We then compared the new hybrid guidance with the heuristic JW- OS without the 1000 step restriction. JW- OS was selected on the basis of Experiment 1. The experiment shows that the hybrid approach is faster in terms of number of steps in a significant majority of cases, both when used with DPLL (Figure 4 Left) and with CDCL (Figure 4 Right).\n\n<center>Figure 4: Left: Comparison of Hybrid (ours) and JW-OS as heuristics in DPLL. We measure the performance of each method according to the number of steps required to find a solution for a given SAT instance. A method wins if it solves a given instance in a smaller number of steps. The blue bar reflects the percentage of formulas where the Hybrid (ours) method won, the green bar means that JW-OS won, and the orange bar means that there was a draw. Right: the same for CDCL. </center> Experiment 3: an ablation for the attention mechanism. From experiments presented in Figure 5 follows that in most cases attention improved evaluation metrics by a significant margin. Only in the case of \\(SR(30)\\) , level 20 attention degraded the model performance. For \\(SR(50)\\) , level 40 the metrics with and without attention stayed within the standard deviation of each other. Reproducibility. For each set of hyperparameters (e.g. \\(SR(30)\\) , level 40), we trained five models. We considered a model not correctly trained if adding it to the set of models raised standard deviation of the losses above 1, see Table 1. We excluded such models (up to 2 models out of 5 for a set of hyperparameters) from further comparisons and left the question of stability of training as a topic of further investigations. The code including hyperparameters is published at https://bit.ly/neurheur. Our code is based on Tensor Flow [AAB+15]. It uses a CDCL implementation by [Zho18]. We access Mini Sat through Py SAT interface [IMM18]. <center>Figure 5: Comparison of policy error with and without attention. The presented values are mean and standard deviation over 3-5 trained models calculated on the evaluation set. </center> 5 CONCLUSIONS AND FUTURE WORKIn this work we have shown three experiments confirming that SAT- solving can be augmented by neural networks. The message- passing architecture augmented by attention performs competitively comparing with standard heuristics when evaluated on relatively large propositional problems, including problems with more than a hundred variables (see Section 4). From the ablation presented in Experiment 3 follows that the message- passing architecture that uses the attention mechanism overall performs better than the same architecture without attention and we attribute it to a selective acceptance of incoming messages made possible by the attention mechanism. We believe that using an appropriately large computing infrastructure the learning process can be extended to more complex examples and that in the near future parallelization combined with a variant of the message- passing architecture can be used to train models which will tackle larger SR problems, and possibly SAT problem classes currently beyond the reach of SAT- solvers. As a future step we consider extending our improved heuristics so that a neural network would be able to control other aspects of the SAT solver behavior, like restarting and backtracking. Eventually, other prediction targets, including expected number of steps, may be beneficial. Once we exhaust the pool of available supervised data it would be interesting to apply reinforcement learning methods, including methods recently presented in [KUMO18]. In this work we focus on the number of steps of the algorithm rather than execution time. Moving the main loop of DPLL or CDCL to a tensor computation graph would be a step towards making the algorithms more competitive in terms of the execution time.\n\n## 6 ACKNOWLEDGEMENTS 6 ACKNOWLEDGEMENTSThis was work was supported by (1) the Polish National Science Center grant UMO- 2018/29/B/ST6/02959 (2) the Tensor Flow Research Cloud which granted 50 TPUs (3) the Academic Computer Center Cyfronet at the AGH University of Science and Technology in Krak\u00f3w, Poland. ## REFERENCES REFERENCES[AAB+15] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor Flow: Large- scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.[ACKS16] Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, and Charles A. Sutton. Learning continuous semantic representations of symbolic expressions. Co RR, abs/1611.01423, 2016. [BHB+18] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez- Gonzalez, Vin\u00edcius Flores Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, \u00c7aglar G\u00fcl\u00e7ehre, Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. Co RR, abs/1806.01261, 2018. [ES03] Niklas E\u00e9n and Niklas S\u00f6rensson. An extensible sat- solver. In Theory and Applications of Satisfiability Testing, 6th International Conference, SAT 2003. Santa Margherita Ligure, Italy, May 5- 8, 2003 Selected Revised Papers, pages 502- 518, 2003. [ESA+18] Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural networks understand logical entailment? Co RR, abs/1802.08535, 2018. [IMM18] Alexey Ignatiev, Antonio Morgado, and Joao Marques- Silva. Py SAT: A Python toolkit for prototyping with SAT oracles. In SAT, pages 428- 437, 2018. [Kar72] R. Karp. Reducibility among combinatorial problems. In R. Miller and J. Thatcher, editors, Complexity of Computer Computations, pages 85- 103. Plenum Press, 1972. [KUMO18] Cezary Kaliszyk, Josef Urban, Henryk Michalewski, and Mirek Ols\u00e1k. Reinforcement learning of theorem proving. Co RR, abs/1805.07563, 2018. [LOM+18] Jia Hui Liang, Chanseok Oh, Minu Mathew, Ciza Thomas, Chunxiao Li, and Vijay Ganesh. Machine learning- based restart policy for CDCL SAT solvers. In Theory and Applications of Satisfiability Testing - SAT 2018 - 21st International Conference, SAT 2018, Held as Part of the Federated Logic Conference, Flo C 2018, Oxford, UK, July 9- 12, 2018, Proceedings, pages 94- 110, 2018. [MHN13] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013. [MMZ+01] Matthew W. Moskewicz, Conor F. Madigan, Ying Zhao, Lintao Zhang, and Sharad Malik. Chaff: Engineering an efficient SAT solver. In Proceedings of the 38th Design Automation Conference, DAC 2001, Las Vegas, NV, USA, June 18- 22, 2001, pages 530- 535, 2001. [MS99] Joao Marques- Silva. The impact of branching heuristics in propositional satisfiability algorithms. In EPIA, 1999. [SLB+18] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single- bit supervision. Co RR, abs/1802.03685, 2018.\n\n[SS18] Taro Sekiyama and Kohei Suenaga. Automated proof synthesis for propositional logic with deep neural networks. Co RR, abs/1805.11799, 2018. [VCC+17] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. Co RR, abs/1710.10903, 2017. [VLW+20] Pashootan Vaezipoor, Gil Lederman, Yuhuai Wu, Roger Grosse, and Fahiem Bacchus. Learning clause deletion heuristics with reinforcement learning. In 5th Conference on Artificial Intelligence and Theorem Proving, 2020. [WTWD17] Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise selection for theorem proving by deep graph embedding. Co RR, abs/1709.09994, 2017. [Zho18] Zhang Zhongwei. Simple SAT solver with CDCL implemented in Python. https://github.com/zlii/pysat/, 2018.",
      "level": 1,
      "line_start": 1,
      "line_end": 15
    },
    {
      "heading": "Introduction",
      "content": "general problem of determining equivalence of Boolean (alternatively: arithmetic) expressions (satisfiability can be seen as equivalence to any unsatisfiable formula e.g. \\(a \\wedge \\neg a\\) ). However, the formulas solved by Eq Net have up to 10 variables and 13 symbols, while we tackle formulas beyond one hundred variables and thousands of symbols. Learned Restart Policy [LOM\\(^+\\) 18] presents a different approach to improve a SAT solver with machine learning, where the network decides at each step whether the algorithm should be restarted to follow another random path in the search tree. [VLV\\(^+\\) 20] uses reinforcement learning to train clause deletion heuristics in DPLL based solvers. ## 3 ARCHITECTURE We use a message- passing graph- based neural network architecture similar to Neuro SAT introduced in [SLB\\(^+\\) 18]. The general idea is to represent a formula as a graph with two node types (literal and clause) and two edge types (literal- literal edges represent the negation relation, and clause- literal edges represent relation between each clause and literals it contains). Example formula represented as a graph is shown in Figure 2 Left. Each node has its own state, represented by an embedding vector. Thanks to this representation, we have the following properties: 1. Invariance to variable renaming. 2. Invariance to negation of all occurrences of a variable. 3. Invariance to permutation of literals in a clause. 4. Invariance to permutation of clauses in a formula. <center>Figure 2: Left: A graph representation of formula \\((A \\vee \\neg C \\vee B) \\wedge (\\neg B \\vee C)\\) used in our work. In the model nodes are unlabeled (labels are included only for the reader's convenience). Different colors mark two distinct types of nodes (clause and literal) and two distinct types of edges (literal-literal and clause-literal). Right: Overview of message-passing architecture. In each iteration we take as the input: connection matrix between clauses and literals \\((CCL)\\) , connection between literals and their negations \\((CLL)\\) , literal embeddings from previous iteration \\((LE_{t-1})\\) , and clause embeddings from previous iteration \\((CE_{t-1})\\) . We use 5 separate MLPs, which share parameters across iterations. Aggregation method depends on a model, see the description below. </center> We initialize all embedding vectors with a trainable initial embedding, different for each type of node. Then we run a number of iterations (from 20 to 40 in our experiments), visualized in Figure 2 Right. Each iteration consists of three stages: Stage 1. Message: Each node generates a message vector \\(V\\) (and a vector \\(K\\) if needed) based on its embedding, to every connected node. \\(V\\) and \\(K\\) are generated with a three- layer MLP with Leaky Re LU [MHN13] activation after each hidden layer and linear activation after the last layer. Stage 2. Aggregate: all messages are delivered according to the connection matrix, then aggregated for each receiver with one of the aggregation functions (described in the next paragraph). Stage 3. Update: Each node updates its embedding based on its previous embedding and aggregated received messages. New embedding is computed by a three- layer MLP with Leaky Re LU activation after each hidden layer and sigmoid activation after the last layer. We explore two different aggregation methods. The first is the average of received \\(V\\) vectors. The second method is a modified attention mechanism. As a message, instead of just a single vector \\(V\\) , we send two vectors, \\(V\\) and \\(K\\) . Receiving node generates one vector \\(Q\\) based on its embedding, and the result of aggregation is \\(\\sum_{i} V_{i} \\text{ sigmoid } (K_{i} \\cdot Q)\\) . Thanks to this, each message may be selectively rejected or accepted by the receiver, depending on relation between \\(K\\) and \\(Q\\) . The intuitive difference between this mechanism and the standard attention is as follows: the standard attention as in [VCC\\(^+\\) 17] chooses one message to look at, while our mechanism rejects or accepts messages independently and looks at their sum.\n\nLike Neuro SAT, our architecture learns to predict satisfiability of the whole formula (which we name sat prediction). However, it also predicts, for each literal separately, the existence of a solution with this literal (which we name policy prediction). To get policy prediction we add a logistic regression on top of each literal's embedding in each iteration (with parameters shared across all literals and iterations). To get sat prediction we add a linear regression on top of each literal's embedding in each iteration, and then apply a sigmoid on sum of their outputs. We define sat loss as cross-entropy loss between the sat prediction and the ground truth. We define policy loss as zero if formula is unsatisfiable and as the average of cross-entropy losses between policy predictions and ground truths if formula is satisfiable. To get a loss of the model we sum together both losses for every iteration. ## 4 EXPERIMENTAL RESULTS Dataset and training details. To train and evaluate the models we use a class of SAT problems \\(SR(n)\\) introduced and described in detail in [SLB \\(^{+18}\\) ]. It is parametrized only by \\(n\\) - the number of variables used in a formula. Both the size and the number of clauses vary. The dataset is balanced in terms of number of satisfiable and unsatisfiable examples. Each of the \\(SR(n)\\) samples has two labels (see Section 3): sat indicating whether the formula \\(\\Phi\\) is satisfiable and policy indicating for each literal \\(l\\) whether \\(\\Phi \\wedge l\\) is satisfiable. We generate each of those numbers by running Mini Sat 2.2 [ES03]. Sample random \\(SR(30)\\) formulas are solved by Mini SAT 2.2 in 0.007 seconds, while \\(SR(110)\\) takes 0.137 second and \\(SR(150)\\) takes 3.406 seconds (for a Xeon E5- 2680v3@2,5 GHz computer). We have trained separate models on SR(30), SR(50), SR(70) and SR(100). Table 1 shows the details of the training procedure. Metrics sat error and policy error are defined as mean absolute error of sat or policy prediction versus labels. The presented models are message- passing neural networks with our modified attention mechanism. Table 1: Each of the models was trained on SAT samples drawn from the distribution marked in the first column. The metrics: loss, sat error and policy error are evaluated on an independently generated evaluation set. The values indicate mean and standard deviation over 3-5 trained models. Models were trained using single TPU v2. <table><tr><td>Problem</td><td>Loss</td><td>sat error</td><td>policy error</td><td>Batch size</td><td>Train. steps</td><td>Train. time</td></tr><tr><td>SR(30)</td><td>28.178\u00b10.672</td><td>0.084\u00b10.004</td><td>0.050\u00b10.002</td><td>128</td><td>1200K</td><td>20h</td></tr><tr><td>SR(50)</td><td>32.024\u00b10.555</td><td>0.233\u00b10.017</td><td>0.105\u00b10.006</td><td>64</td><td>600K</td><td>12h</td></tr><tr><td>SR(70)</td><td>33.010\u00b10.482</td><td>0.266\u00b10.033</td><td>0.110\u00b10.007</td><td>64</td><td>600K</td><td>22h</td></tr><tr><td>SR(100)</td><td>34.227\u00b10.127</td><td>0.319\u00b10.007</td><td>0.123\u00b10.002</td><td>32</td><td>1200K</td><td>28h</td></tr></table> Experiment 1: comparison of all models with DLIS and JW- OS heuristics. We evaluated the DPLL algorithm guided by our 4 kinds of models described above and compared to DPLL guided by JW- OS and DLIS. As a performance consideration we decided to stop DPLL after 1000 steps (see Experiment 2 below for a comparison without this restriction) and count the number of solved formulas out of 100 in each class. We present the results in Figure 3. For this and subsequent experiments we only consider satisfiable \\(SR(n)\\) samples. JW- OS proved to be the best on average classes of problems: \\(SR(50)\\) and \\(SR(70)\\) , whereas neural guidance- based algorithms proved to be the best on large problems: \\(SR(90)\\) and \\(SR(110)\\) . <center>Figure 3: Performance of DPLL with different guidance heuristics on specific problem sizes. The \\(x\\) axis indicates the class of the evaluation set: evaluation is performed on fresh randomly chosen one satisfiable hundred \\(SR(x)\\) formulas. The \\(y\\) axis indicates the percent of instances (out of 100) solved by DPLL within 1000 steps. </center> Experiment 2: detailed comparison with the JW- OS heuristic. We have selected the SR(50) model for a detailed comparison of the learned heuristics versus JW- OS and for the sake of this comparison designed hybrid guidance algorithm that uses a model trained on \\(SR(50)\\) (a fixed one of the three similar replicas) and switches to JW- OS when the network predicts sat probability below a threshold of \\(0.3^{2}\\) . We then compared the new hybrid guidance with the heuristic JW- OS without the 1000 step restriction. JW- OS was selected on the basis of Experiment 1. The experiment shows that the hybrid approach is faster in terms of number of steps in a significant majority of cases, both when used with DPLL (Figure 4 Left) and with CDCL (Figure 4 Right).\n\n<center>Figure 4: Left: Comparison of Hybrid (ours) and JW-OS as heuristics in DPLL. We measure the performance of each method according to the number of steps required to find a solution for a given SAT instance. A method wins if it solves a given instance in a smaller number of steps. The blue bar reflects the percentage of formulas where the Hybrid (ours) method won, the green bar means that JW-OS won, and the orange bar means that there was a draw. Right: the same for CDCL. </center> Experiment 3: an ablation for the attention mechanism. From experiments presented in Figure 5 follows that in most cases attention improved evaluation metrics by a significant margin. Only in the case of \\(SR(30)\\) , level 20 attention degraded the model performance. For \\(SR(50)\\) , level 40 the metrics with and without attention stayed within the standard deviation of each other. Reproducibility. For each set of hyperparameters (e.g. \\(SR(30)\\) , level 40), we trained five models. We considered a model not correctly trained if adding it to the set of models raised standard deviation of the losses above 1, see Table 1. We excluded such models (up to 2 models out of 5 for a set of hyperparameters) from further comparisons and left the question of stability of training as a topic of further investigations. The code including hyperparameters is published at https://bit.ly/neurheur. Our code is based on Tensor Flow [AAB+15]. It uses a CDCL implementation by [Zho18]. We access Mini Sat through Py SAT interface [IMM18]. <center>Figure 5: Comparison of policy error with and without attention. The presented values are mean and standard deviation over 3-5 trained models calculated on the evaluation set. </center> 5 CONCLUSIONS AND FUTURE WORKIn this work we have shown three experiments confirming that SAT- solving can be augmented by neural networks. The message- passing architecture augmented by attention performs competitively comparing with standard heuristics when evaluated on relatively large propositional problems, including problems with more than a hundred variables (see Section 4). From the ablation presented in Experiment 3 follows that the message- passing architecture that uses the attention mechanism overall performs better than the same architecture without attention and we attribute it to a selective acceptance of incoming messages made possible by the attention mechanism. We believe that using an appropriately large computing infrastructure the learning process can be extended to more complex examples and that in the near future parallelization combined with a variant of the message- passing architecture can be used to train models which will tackle larger SR problems, and possibly SAT problem classes currently beyond the reach of SAT- solvers. As a future step we consider extending our improved heuristics so that a neural network would be able to control other aspects of the SAT solver behavior, like restarting and backtracking. Eventually, other prediction targets, including expected number of steps, may be beneficial. Once we exhaust the pool of available supervised data it would be interesting to apply reinforcement learning methods, including methods recently presented in [KUMO18]. In this work we focus on the number of steps of the algorithm rather than execution time. Moving the main loop of DPLL or CDCL to a tensor computation graph would be a step towards making the algorithms more competitive in terms of the execution time.",
      "level": 2,
      "line_start": 4,
      "line_end": 12
    },
    {
      "heading": "6 ACKNOWLEDGEMENTS 6 ACKNOWLEDGEMENTSThis was work was supported by (1) the Polish National Science Center grant UMO- 2018/29/B/ST6/02959 (2) the Tensor Flow Research Cloud which granted 50 TPUs (3) the Academic Computer Center Cyfronet at the AGH University of Science and Technology in Krak\u00f3w, Poland. ## REFERENCES REFERENCES[AAB+15] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor Flow: Large- scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.[ACKS16] Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, and Charles A. Sutton. Learning continuous semantic representations of symbolic expressions. Co RR, abs/1611.01423, 2016. [BHB+18] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez- Gonzalez, Vin\u00edcius Flores Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, \u00c7aglar G\u00fcl\u00e7ehre, Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. Co RR, abs/1806.01261, 2018. [ES03] Niklas E\u00e9n and Niklas S\u00f6rensson. An extensible sat- solver. In Theory and Applications of Satisfiability Testing, 6th International Conference, SAT 2003. Santa Margherita Ligure, Italy, May 5- 8, 2003 Selected Revised Papers, pages 502- 518, 2003. [ESA+18] Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural networks understand logical entailment? Co RR, abs/1802.08535, 2018. [IMM18] Alexey Ignatiev, Antonio Morgado, and Joao Marques- Silva. Py SAT: A Python toolkit for prototyping with SAT oracles. In SAT, pages 428- 437, 2018. [Kar72] R. Karp. Reducibility among combinatorial problems. In R. Miller and J. Thatcher, editors, Complexity of Computer Computations, pages 85- 103. Plenum Press, 1972. [KUMO18] Cezary Kaliszyk, Josef Urban, Henryk Michalewski, and Mirek Ols\u00e1k. Reinforcement learning of theorem proving. Co RR, abs/1805.07563, 2018. [LOM+18] Jia Hui Liang, Chanseok Oh, Minu Mathew, Ciza Thomas, Chunxiao Li, and Vijay Ganesh. Machine learning- based restart policy for CDCL SAT solvers. In Theory and Applications of Satisfiability Testing - SAT 2018 - 21st International Conference, SAT 2018, Held as Part of the Federated Logic Conference, Flo C 2018, Oxford, UK, July 9- 12, 2018, Proceedings, pages 94- 110, 2018. [MHN13] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013. [MMZ+01] Matthew W. Moskewicz, Conor F. Madigan, Ying Zhao, Lintao Zhang, and Sharad Malik. Chaff: Engineering an efficient SAT solver. In Proceedings of the 38th Design Automation Conference, DAC 2001, Las Vegas, NV, USA, June 18- 22, 2001, pages 530- 535, 2001. [MS99] Joao Marques- Silva. The impact of branching heuristics in propositional satisfiability algorithms. In EPIA, 1999. [SLB+18] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single- bit supervision. Co RR, abs/1802.03685, 2018.",
      "content": "[SS18] Taro Sekiyama and Kohei Suenaga. Automated proof synthesis for propositional logic with deep neural networks. Co RR, abs/1805.11799, 2018. [VCC+17] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. Co RR, abs/1710.10903, 2017. [VLW+20] Pashootan Vaezipoor, Gil Lederman, Yuhuai Wu, Roger Grosse, and Fahiem Bacchus. Learning clause deletion heuristics with reinforcement learning. In 5th Conference on Artificial Intelligence and Theorem Proving, 2020. [WTWD17] Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise selection for theorem proving by deep graph embedding. Co RR, abs/1709.09994, 2017. [Zho18] Zhang Zhongwei. Simple SAT solver with CDCL implemented in Python. https://github.com/zlii/pysat/, 2018.",
      "level": 2,
      "line_start": 13,
      "line_end": 15
    }
  ]
}