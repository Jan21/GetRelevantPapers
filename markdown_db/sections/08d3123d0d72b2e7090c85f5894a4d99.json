{
  "sections": {
    "deepgate2_functionality_aware_circuit_representati": "## Introduction\n\n\nin Section III. We compare Deep Gate2 with the original Deep Gate and another functionality- aware solution [8] in Section IV. Next, we apply Deep Gate2 onto several downstream tasks in Section V. Finally, Section VI concludes this paper. ## II. RELATED WORK ### A. Circuit Representation Learning A prominent trend in the deep learning community is to learn a general representation from data first and then apply it to various downstream tasks, for example, GPT [16] and BERT [17] learn representations of natural language text that can be fine- tuned for a wide range of natural language processing tasks. Circuit representation learning has also emerged as an attractive research direction, which falls into two categories: structure- aware circuit representation learning [9], [11], [12] and functionality- aware circuit representation learning [7], [8]. Since a circuit can be naturally formulated as a graph, with gates as nodes and wires as edge, the GNN is a powerful tool to capture the interconnections of logic gates and becomes a backbone model to learn circuit representations. For example, TAG [9] is a GNN- based model designed for analog and mixed- signal circuit representation learning and applied for several physical design applications, such as layout matching prediction, wirelength estimation, and net parasitic capacitance prediction. ABGNN [12] learns the representation of digital circuits and handles the arithmetic block identification task. However, these models tend to focus on structural encoding and are not suitable for functionality- related tasks. Consequently, the functionality- aware circuit representation learning frameworks [7], [8] are designed to learn the underlying circuit functionality. For instance, FGNN [8] learns to distinguishes between functionally equivalent and inequivalent circuits by contrastive learning [18]. However, such self- supervised manner relies on data augmentation by perturbing the original circuit to logic equivalence circuit. If the perturbation is not strong and diverse, the model still identifies the functional equivalence circuits based on the invariant local structure, resulting a low generalization ability on capturing underlying functionality. Deep Gate [7] leverages logic- 1 probability under random simulation as supervision, which approximates the statistic of the most direct representation of functionality, i.e. truth table. Despite achieving remarkable progress on testability analysis [5], there are limitations that affect the generalizability of Deep Gate to other EDA tasks. We will elaborate on Deep Gate in the next subsection. ### B. Deep Gate Framework Deep Gate [7] is the first circuit representation learning framework that embeds both structural and functional information of digital circuits. The model pre- processes the input circuits into a unified And- Inverter Graph (AIG) format and obtains rich gate- level representations, which can be applied to various downstream tasks. Deep Gate treats the logic- 1 probability as supervision to learn the functionality. Additionally, the Deep Gate consists of a GNN equipped with an attention- based aggregation function that propagates information of gates in levelized sequential manner. The aggregation function learns to assign high attention weights to controlling fan- in of gates (e.g. the fan- in gate with logic- 0 is the controlling fan- in of AND gate) that mimics the logic computation process. Although it has been applied to testability analysis [5] and SAT problem [13], we argue that the model still encounters with two major shortcomings limiting its generalization ability. <center>Fig. 1. An example of reconvergence structure </center> First, logic probability is not an appropriate supervision for learning functionality. The most direct representation of functionality is the truth table, however, using it as a training label is impractical due to the immeasurable computational overhead. Deep Gate proposes to supervise the model by utilizing the proportion of logic- 1 in the truth table and approximate this proportion as the logic probability through random simulation. However, logic probability is only a statistical information of functionality, indicating the number of logic- 1 values in the truth table rather than which PI assignments lead to logic- 1. Consequently, Deep Gate cannot differentiate the functional difference between two circuits if they have the same probability. Second, Deep Gate is not efficient enough to deal with large circuit. Specifically, Deep Gate requires to perform forward and backward message- passing operations for 20 rounds to embed rich representations. Fig. 1 illustrates the need of this multi- round GNN design in Deep Gate where the nodes in grey color represent PIs. The incoming messages of nodes 5, 6, 5, and 6 during forward propagation are noted in the figure, where \\(h_i\\) is the embedding vector of node \\(i\\) . Since, Deep Gate uses the same initial embeddings for all nodes, the messages of nodes 5, 6, 5, and 6 in the first forward propagation round are identical. Thus, the model can only distinguish node embeddings based on their connections by repeatedly updating PIs through multiple rounds of forward and backward message propagation. We emphasize that the limitations of Deep Gate comes from the lack of effective supervision and weak model design where the unique identification of all PIs are ignored. To address these issues, we propose an efficient one- round GNN design that maintains the unique identification of PIs and uses the pairwise truth- table difference of two gates as an effective supervision. ## III. METHODOLOGY ### A. Problem Formulation The circuit representation learning model aims to map both circuit structure and functionality into embedding space, where the structure represents the connecting relationship of logic gates and the functionality means the logic computational mapping from inputs to outputs. We conclude that the previous models still lack of ability to capture functional information. In this paper, we propose to improve the previous Deep Gate model [7] to represent circuits with similar functionality with the similar embedding vectors. In other words, these circuit representations should have short distance in the embedding space. We take Circuit A, B, C, and D as examples in Fig. 2, where all of them have similar topological structures. Since Circuit A, B and C perform with the same logic probability, Deep Gate [7] tends to produce the similar embeddings for these three circuits. Hence, it is hard to identify the logic equivalent circuits by Deep Gate. Although FGNN [8] is trained to classify logical equivalence and inequivalence circuits by contrastive learning, they cannot differentiate the relative similarity. As shown in the embedding space, the distance between\n\n<center>Fig. 2. Problem statement: the embedding vectors should be close if circuit functions are similar </center> A and B is equal to the distance between A and D. Nonetheless, as indicated in the truth table, Circuit A is equivalent to Circuit C, similar to Circuit B (with only 2 different bits), but dissimilar to Circuit D (with 5 different bits). We expect that the model will bring together or separate the circuits in embedding space according to their truth tables. Therefore, the expected Deep Gate2 model not only identifies the logic equivalent nodes, but also predicts the functional similarity. Thus, we can apply such functionality- aware circuit learning model to provide benefits for the real- world applications. ### B. Dataset Preparation To train the circuit representation learning model, we need to find a supervision contains rich functional information and prepare an effective dataset at first. Truth table, which records the complete logic computational mapping, provides the most direct supervision. However, the length of the truth table increases exponentially with the number of primary inputs, and obtaining a complete truth table requires an immeasurable amount of time. Therefore, a reasonable supervision should be easily obtained and closely related to the truth table. Firstly, we use the Hamming distance between truth tables of two logic gates as supervision. That is, in a way similar to metric learning [19], we map nodes to an embedding space and hope that the distance of the embedding vectors is positive correlated with the Hamming distance of the truth table. Formally, we denote the truth table vector of node \\(i\\) is \\(T_{i}\\) and the embedding vector of node \\(i\\) is \\(h_{i}\\) . \\[distance(h_{i},h_{j})\\propto distance(T_{i},T_{j}) \\quad (1)\\] Secondly, to improve the data efficiency, we regard the each logic gate in circuit as a new circuit (logic cone) with the current gate as output and the original PIs as inputs. By parsing a single original circuit, we obtain a large number of new circuits. Therefore, the task of graph learning becomes the task of learning node- level representation, and the difficulty of data collection is reduced. Thirdly, to ensure the quality of sample pairs and limit the number of sample pairs, we impose the following constraints during sampling node pairs: (1) Two logic cones of the two nodes should have the same PI, which is a necessary condition for comparing the truth table difference. (2) The logic probability, which is the number of logic- 1 percentage in the truth table, should be similar (distance within \\(5\\%\\) ). This is because if the logic probability of two nodes is not consistent, their functions are definitely not consistent. If the logic probability of two nodes is consistent, their functions may be consistent. (3) The difference in logic levels between two nodes should be within 5, because when the two nodes are far apart, their functions are unlikely to be correlated. (4) We only consider the extreme cases, namely, the difference between truth tables is within \\(20\\%\\) or above \\(80\\%\\) . We do not perform the complete simulation, but set a maximum simulation time to obtain the response of each node as an incomplete truth table. It should be noted that we utilize the And- Inverter Graph (AIG) as the circuit netlist format, which is only composed of AND gate and NOT gate. Any other logic gates, including OR, XOR and MUX, can be transformed into a combination of AND and NOT gates in linear time. ### C. Functionality-Aware Loss Function The primary objective of our purposed functionality- aware circuit learning model is to learn node embeddings, where two embedding vectors will be similar if the corresponding two node function are similar. As we sample node pairs \\(\\mathcal{N}\\) in the Section III- B, we can obtain the Hamming distance of truth table \\(D^{T}\\) of each node pair. \\[D_{(i,j)}^{T} = \\frac{Hamming Distance(T_{i},T_{j})}{length(T_{i})},(i,j)\\in \\mathcal{N} \\quad (2)\\] According to Eq. (1), the distance of embedding vectors \\(D^{H}\\) should be proportional to the Hamming distance of the truth table \\(D^{T}\\) . We define the distance of embedding vectors in Eq. (3), where is calculated based on cosine similarity. In other word, the similarity of embedding vectors \\(S_{(i,j)}\\) should be negative related to distance \\(D_{(i,j)}^{T}\\) . \\[\\begin{array}{l}{S_{(i,j)} = Cosine Similarity(h_i,h_j)}\\\\ {D_{(i,j)}^H = 1 - S_{(i,j)}} \\end{array} \\quad (3)\\] Therefore, the training objective is to minimize the difference between \\(D^{H}\\) and \\(D^{T}\\) . We purpose the functionality- aware loss function \\(L_{func}\\) as below. \\[\\begin{array}{l}{D_{(i,j)}^{T^{\\prime}} = Zero Norm(D_{(i,j)}^{T})}\\\\ {D_{(i,j)}^{H^{\\prime}} = Zero Norm(D_{(i,j)}^{H})}\\\\ {L_{func} = \\sum_{(i,j)\\in \\mathcal{N}}(L1Loss(D_{(i,j)}^{T^{\\prime}},D_{(i,j)}^{H^{\\prime}}))} \\end{array} \\quad (4)\\] ### D. One-round GNN Model In this subsection, we propose a GNN model that can capture both functional and structural information for each logic gate through one- round forward propagation. First, we propose to separate the functional embeddings \\(hf\\) and structural embeddings \\(hs\\) , and initialize them in difference ways. We assign the uniform initial functional embeddings to primary inputs (PI), as they all have equivalent logic probability under random simulation. However, we design a PI encoding (PIE) strategy by assigning a unique identification to each PI as its initial structural embedding. Specifically, the initial PI structural embeddings \\(hs_{i}, i \\in PI\\) are orthogonal vectors. This means that the dot product of any two PIs' embeddings is zero. Second, we design four aggregators: \\(agg_{AND}^{r}\\) aggregates the message for structural embedding \\(hs\\) of an AND gate, \\(agg_{AND}^{r}\\) aggregates the message for functional embedding \\(hf\\) of an AND\n\n<center>Fig. 3. One-round GNN propagation process </center> gate. \\(agg r_{NOT}^{s}\\) and \\(agg r_{NOT}^{f}\\) update \\(h s\\) and \\(h f\\) of a NOT gate, respectively. We implement each aggregator using the self- attention mechanism [20], as the output of a logic gate is determined by the controlling values of its fan- in gates. For example, an AND gate must output logic- 0 if any of its fan- in gates has logic- 0. By employing the attention mechanism, the model learns to assign greater importance to the controlling inputs [7]. As illustrated in Eq. (5), \\(w_{q}\\) , \\(w_{k}\\) and \\(w_{v}\\) are three weight matrices and \\(d\\) is the dimension of embedding vectors \\(h\\) . \\[\\begin{array}{l}\\alpha_{j} = softmax(\\frac{w_{q}^{\\top}h_{i}\\cdot(w_{k}^{\\top}h_{j})^{\\top}}{\\sqrt{d}})\\\\ m_{j} = w_{v}^{\\top}h_{j}\\\\ h_{i} = agg r(h_{j}|j\\in \\mathcal{P}(i)) = \\sum_{j\\in \\mathcal{P}(i)}(\\alpha_{j}*m_{j}) \\end{array} \\quad (5)\\] Third, during forward propagation, the structural embeddings are updated only with the structural embeddings of predecessors. As shown in Eq. (6), where the Gate \\(a\\) is AND gate, the Gate \\(b\\) is NOT gate. \\[\\begin{array}{l} h s_{a} = agg r_{A N D}^{s}(h s_{j}|j\\in \\mathcal{P}(a)) \\\\ h s_{b} = agg r_{N O T}^{s}(h s_{j}|j\\in \\mathcal{P}(b)) \\end{array} \\quad (6)\\] At the same time, the gate function is determined by the function and the structural correlations of the fan- in gates. Therefore, the functional embeddings are updated as Eq. (7). \\[\\begin{array}{l} h f_{a} = agg r_{A N D}^{f}([h s_{j},h f_{j}]|j\\in \\mathcal{P}(a))\\\\ h f_{b} = agg r_{N O T}^{f}([h s_{j},h f_{j}]|j\\in \\mathcal{P}(b)) \\end{array} \\quad (7)\\] Therefore, as shown in Fig. 3, the GNN propagation process performs from PI to PO level by level. For the node in level \\(l\\) , its structural embedding \\(h s_{L_{l}}\\) will be updated with the structural embeddings of the node in level \\(l - 1\\) . Additionally, the functional embedding \\(h f_{L_{l}}\\) will be updated with both structural embeddings \\(h s_{L_{l - 1}}\\) and functional embeddings \\(h f_{L_{l - 1}}\\) . The GNN propagation completes after processing \\(N\\) levels. ### E. Model Training Strategies To train the model, we employed multi- stage training strategy, similar to training a model with an easy task and then a harder task in curriculum learning [21]. During each stage, we trained the model with multiple supervisions in multi- task learning manner [22]. In the first stage, we train the one- round GNN model with two simple tasks. The Task 1 involves predicting the logic probability, while the Task 2 entails identifying the structural correlation. To achieve this, we readout the functional embedding \\(h f_{i}\\) to predict the logic probability \\(\\hat{P}_{i}\\) by a multi- layer perceptron (MLP), denoted as \\(MLP_{prob}\\) . In addition, we utilize the structural embeddings \\(h s_{i}\\) and \\(h s_{j}\\) to predict whether node \\(i\\) and node \\(j\\) can be reconvergent by \\(MLP_{rc}\\) . \\[\\begin{array}{r}\\hat{P}_i = MLP_{prob}(h f_i)\\\\ R_{\\langle i,j\\rangle} = MLP_{rc}(h s_i,h s_j) \\end{array} \\quad (8)\\] We define the loss function for Task 1 in Eq. (9), where the \\(P_{i}\\) is the ground truth logic probability obtained through random simulation. \\[L_{prob} = L1Loss(P_i,\\hat{P}_i) \\quad (9)\\] Besides, we define the loss function for Task 2 in Eq. (10). The binary ground truth, denoted as \\(R_{\\langle i,j\\rangle}\\) , indicates whether node pair \\(i\\) and \\(j\\) have a common predecessor. \\[L_{rc} = BCELoss(R_{\\langle i,j\\rangle},R_{\\langle i,j\\rangle}) \\quad (10)\\] Consequently, the loss function for Stage 1 is presented in Eq. (11), where the \\(w_{prob}\\) and \\(w_{rc}\\) are the weight for Task 1 and Task 2, respectively. \\[L_{stage1} = L_{prob}*w_{prob} + L_{rc}*w_{rc} \\quad (11)\\] The second training stage involves another more difficult Task 3. functionality- aware learning, as described in Section III- C. The loss function for Stage 2 is defined below, where \\(w_{func}\\) represents the loss weight of Task 3. \\[L_{stage2} = L_{prob}\\times w_{prob} + L_{rc}\\times w_{rc} + L_{func}\\times w_{func} \\quad (12)\\] Overall, the model can differentiate gates with varying probability in Stage 1. As the logic equivalent pairs only occur when nodes have the same probability, the model in Stage 2 learns to predicting the functional similarity within the probability equivalent class. The effectiveness of the above training strategies is demonstrated in Section IV- E. ## IV. EXPERIMENTS In this section, we demonstrate the ability of our proposed Deep Gate2 to learn functionality- aware circuit representations. Firstly, Section IV- A provides the preliminary of our experiments, including details on dataset preparation, evaluation metrics and model settings. Secondly, we compare the effectiveness and efficiency of our Deep Gate2 against Deep Gate [7] and FGNN [8] on two function- related tasks: logic probability prediction (see Section IV- B) and logic equivalence gates identification (see Section IV- C). Thirdly, we investigate the effectiveness of model design and training strategies in Section IV- D and Section IV- E, respectively. ### A. Experiment Settings 1) Dataset Preparation: We use the circuits in Deep Gate [7], which are extracted from ITC'99 [23], IWLS'05 [24], EPFL [25] and Open Core [26]. These circuits consists of 10,824 AIGs with sizes ranging from 36 to 3,214 logic gates. To obtain the incomplete truth table, we generate 15,000 random patterns and record the corresponding response. Following the data preparation method described in Section III-B, we construct a dataset comprising 894,151 node pairs. We create 80/20 training/test splits for model training and evaluation.\n\n<center>Fig. 4. Functionality-aware circuit learning framework </center> 2) Evaluation Metrics: We assess our Deep Gate2 with two tasks. The first task is to predict the logic probability for each logic gate. We calculate the average prediction error (PE) as Eq. (13), where the set \\(\\mathcal{V}\\) includes all logic gates. \\[PE = \\frac{1}{|\\mathcal{V}|}\\sum_{i\\in \\mathcal{V}}|P_{i} - \\hat{P}_{i}| \\quad (13)\\] The second task is to identify the logic equivalence gates within a circuit. A gate pair \\((i,j)\\) is considered as a positive pair if these two logic gates \\(i\\) and \\(j\\) have the same function, where the pairwise Hamming distance of truth tables \\(D_{(i,j)}^{T} = 0\\) . If the similarity \\(S_{(i,j)}\\) between these two embedding vectors \\(h f_{i}\\) and \\(h f_{j}\\) exceeds a certain threshold, the model will recognize the gate pair \\((i,j)\\) as equivalent. The optimal threshold \\(\\theta\\) is determined based on the receiver operating characteristic (ROC). The evaluation metrics is formally defined in Eq. (14), where \\(TP\\) , \\(TN\\) , \\(FP\\) , \\(FN\\) are true positive, true negative, false positive and false negative, respective, and \\(M\\) is the total number of gate pairs. In the following experiments, the performance on logic equivalence gates identification is measured in terms of Recall, Precision, F1- Score and area under curve (AUC). \\[\\begin{array}{rl} & {TP = \\frac{\\sum((D_{(i,j)}^{T} = =0)\\&(S_{(i,j)} > \\theta))}{M}}\\\\ & {TN = \\frac{\\sum((D_{(i,j)}^{T} > 0)\\&(S_{(i,j)}< \\theta))}{M}}\\\\ & {FP = \\frac{\\sum((D_{(i,j)}^{T} > 0)\\&(S_{(i,j)} > \\theta))}{M}}\\\\ & {FN = \\frac{\\sum((D_{(i,j)}^{T} = =0)\\&(S_{(i,j)}< \\theta))}{M}} \\end{array} \\quad (14)\\] We conduct the following performance comparisons on 10 industrial circuits, with circuit sizes ranging from \\(3.18k\\) gates to \\(40.50k\\) gates. 3) Model Settings: In the one-round GNN model configuration, the dimension of both structural embedding \\(hs\\) and functional embedding \\(hf\\) is 64. Both \\(MLP_{prob}\\) and \\(MLP_{rc}\\) contain 1 hidden layer with 32 neurons and a Re Lu activation function. The model is trained for 60 epochs to ensure each model can converge. The other models [7], [8] mentioned in the following experiments maintain their original settings and are trained until they converge. We train all the models for 80 epochs with batch-size 16 on a single Nvidia V100 GPU. We adopt the Adam optimizer [27] with learning rate \\(10^{- 4}\\) and weight decay \\(10^{- 10}\\) . TABLE I PERFORMANCE OF DEEPGATE (V1) AND OUR PROPOSED DEEPGATE2 (V2) ON LOGIC PROBABILITY PREDICTION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">#Gates</td><td colspan=\"2\">Deep Gate</td><td colspan=\"2\">Deep Gate2</td><td colspan=\"2\">Reduction</td></tr><tr><td>PE</td><td>Time</td><td>PE</td><td>Time</td><td>PE</td><td>Time(x)</td></tr><tr><td>D1</td><td>19,485</td><td>0.0344</td><td>36.89s</td><td>0.0300</td><td>2.23s</td><td>12.79%</td><td>16.56</td></tr><tr><td>D2</td><td>12,648</td><td>0.0356</td><td>16.11s</td><td>0.0309</td><td>1.18s</td><td>13.20%</td><td>13.66</td></tr><tr><td>D3</td><td>14,686</td><td>0.0355</td><td>21.42s</td><td>0.0294</td><td>1.44s</td><td>17.18%</td><td>14.92</td></tr><tr><td>D4</td><td>7,104</td><td>0.0368</td><td>5.89s</td><td>0.0323</td><td>0.50s</td><td>12.23%</td><td>11.89</td></tr><tr><td>D5</td><td>37,279</td><td>0.0356</td><td>131.22s</td><td>0.0316</td><td>7.82s</td><td>11.24%</td><td>16.79</td></tr><tr><td>D6</td><td>37,383</td><td>0.0325</td><td>133.02s</td><td>0.0285</td><td>8.10s</td><td>12.31%</td><td>16.42</td></tr><tr><td>D7</td><td>10,957</td><td>0.0357</td><td>13.02s</td><td>0.0316</td><td>0.90s</td><td>11.48%</td><td>14.44</td></tr><tr><td>D8</td><td>3,183</td><td>0.0406</td><td>1.60s</td><td>0.0341</td><td>0.17s</td><td>16.01%</td><td>9.64</td></tr><tr><td>D9</td><td>27,820</td><td>0.0368</td><td>77.20s</td><td>0.0322</td><td>4.73s</td><td>12.50%</td><td>16.34</td></tr><tr><td>D10</td><td>40,496</td><td>0.0327</td><td>154.00s</td><td>0.0290</td><td>8.89s</td><td>11.31%</td><td>17.33</td></tr><tr><td>Avg.</td><td></td><td>0.0356</td><td>59.04s</td><td>0.0310</td><td>3.59s</td><td>13.08%</td><td>16.43</td></tr></table> ### B. Comparison with Deep Gate on Probability Prediction We compare the probability prediction error (PE, see Eq. (13)) and runtime (Time) with previous Deep Gate. The previous Deep Gate is denoted as Deep Gate and our proposed model with novel loss function and GNN design is named as Deep Gate2 in Table I. Based on the results presented in the table, we make two observations. First, our proposed Deep Gate2 exhibits more accurate predictions of logic probability compared to the previous version. On average, the probability prediction error (PE) of Deep Gate2 is \\(13.08\\%\\) lower than that of Deep Gate. This suggests that using the novel model architecture with embedding initialization strategy can benefit logic representation learning and lead to better results on logic probability prediction. Second, our Deep Gate2 performs more efficient than Deep Gate. Take the circuit D1 as an example, Deep Gate requires 36.89 seconds for inference, but our Deep Gate2 only needs 2.23s, which is 16.56x faster than the previous Deep Gate. Moreover, compared to the previous model, Deep Gate2 achieves an order of magnitude speedup (16.43x on average) in model runtime. This is attributed to the fact that the GNN model in Deep Gate relies on 10 forward and 10 backward message propagation, whereas the proposed one- round GNN model in Deep Gate2 only performs forward propagation for 1 time. Therefore, the new circuit representation learning model is more effective and efficient than Deep Gate, and demonstrates the generalization ability on large- scale circuits.\n\n### C. Comparison with other Models on Logic Equivalence Gates Identification This section compares the functionality- aware accuracy, as defined in Section IV- A2 of Deep Gate2 with that of two other models: Deep Gate [7] and FGNN [8]. The Deep Gate [7] model treats the logic probability as supervision since it contains the statistical information of truth table. The FGNN [8] is trained to differentiate between logic equivalent and inequivalent circuits using contrastive learning. Table II presents the performance of three models on the task of logic equivalence gates identification. Firstly, our proposed approach outperforms the other two models on all circuits with an average F1- score of 0.9434, while Deep Gate and FGNN only achieve F1- Score 0.6778 and 0.4402, respectively. For instance, in circuit D7, our proposed functionality- aware circuit learning approach achieves an F1- Score of 0.9831 and accurately identifies \\(99.15\\%\\) of logic equivalence gate pairs with a precision of \\(97.48\\%\\) , indicating a low false positive rate. In contrast, Deep Gate only achieves an F1- score of 0.6778, while FGNN fails on most of the pairs. Secondly, although Deep Gate has an average recall of \\(91.46\\%\\) , its precision is only \\(54.00\\%\\) , indicating a large number of false positive identifications. This is because Deep Gate can only identify logic equivalent pairs by predicting logic probability, which leads to incorrect identification of gate pairs with similar logic probability. According to our further experiment, in \\(80.83\\%\\) of false positive pairs, the model incorrectly identifies gate pairs with similar logic probability as functionally equivalent. Thirdly, FGNN achieves the lowest performance among the other models, with only 0.4402 F1- Score. The poor performance of FGNN is attributed to the lack of effective supervision. While FGCN learns to identify logic equivalence circuits generated by perturbing local structures slightly, the model tends to consider circuits with similar structures to have the same functionality. However, in the validation dataset and practical applications, two circuits may have the same function even if their topological structures are extremely different. Therefore, the self- supervised approach limits the effectiveness of FGNN in identifying logic equivalence gates. ### D. Effectiveness of PI Encoding Strategy To demonstrate the effectiveness of our proposed PI encoding (PIE) strategy, we trained another model without assigning unique identifications for PIs, which we refer to as w/o PIE. The results are presented in Table III, which show that disabling the PIE reduces the F1- Score of identifying logic equivalence gates from 0.9434 to 0.7541, resulting in an average reduction of \\(20.07\\%\\) . Such reduction can be attributed to the fact that, as demonstrated as the failure case in Section II and Fig. 1, the one- round GNN model without the PIE strategy cannot model the structural information of the circuit. More specifically, the accuracy of the reconvergence structure identification task with w/ PIE model is \\(93.22\\%\\) , while the w/o model only achieve \\(74.56\\%\\) . The functionality of logic gate is affected by both functionality of fan- in gates and whether there is reconvergence between its fan- in gates. Once the reconvergence structure cannot be accurately identified, node functionality cannot be modeled accurately. ### E. Effectiveness of Training Strategies To investigate the effectiveness of our multi- stage training strategy, we train another model (noted as w/o multi- stage model) with all loss functions in only one stage, instead of adding the functionality- aware loss function in the second stage. The original model with multiple stages training strategy is noted as w/ multi- stage model. The w/ multi- stage model learn to predict the logic probability and structural correlation in the first stage and learn the more difficult task, which predicts the functionality in the second stage. The results are shown in Table IV, where the model w/ multi- stage achieves an F1- Score of 0.9434 on average and the model w/o multi- stage achieves only 0.7137. We analyze the reason as follows. The cost of comparing each pair of logic gates in the task of learning functionality is extremely high, which is proportional to the square of the circuit size. We limit the dataset and train the model to learn functional similarity only among pairs with similar logic probability, which is a necessary condition for functional equivalence. Therefore, without the staged multi- stage strategy, be effectively supervised with the simplified dataset, leading to poor performance in learning functionality. As shown in Table V, the differences between the two models in the loss values for predicting logic probability \\((L_{prob})\\) and identifying reconvergence structures \\((L_{rc})\\) are not significant, indicating that they perform similarly in these two tasks. However, compared to the w/o multi- stage model, the w/ multi- stage model performs better in learning functionality with \\(L_{func} = 0.0594\\) , which is \\(51.47\\%\\) smaller than that of w/o multi- stage model. However, the w/ multi- stage model outperforms the model w/o multi- stage in learning functionality task with a significantly lower \\(L_{func}\\) value of 0.0594, which is \\(51.47\\%\\) smaller than that of the latter. ## V. DOWNSTREAM TASKS In this section, we combine our Deep Gate2 with the open- source EDA tools and apply our model to practical EDA tasks: logic synthesis and Boolean satisfiability (SAT) solving. The logic synthesis tools aim to identify logic equivalence gates as quickly as possible. In Section V- A, our proposed functionality- aware circuit learning model provides guidance to the logic synthesis tool about the logic similarity. Additionally, in Section V- B, we apply the learnt functional similarity in SAT solving, where the variables with dissimilar functionality are assigned the same decision value. This approach efficiently shrinks the search space by enabling solvers to encounter more constraints. ### A. Logic Synthesis This subsection shows the effectiveness of our proposed functionality- aware circuit learning framework in SAT- sweeping [28], a common technique of logic synthesis. Fig. 5 illustrates the components of a typical ecosystem for SAT- sweeping engine (also called SAT sweeper), where including equivalence class (EC) manager, SAT- sweeping manager, simulator, and SAT solver. All computations are coordinated by the SAT- sweeping manager [29]. The SAT sweeper starts by computing candidate ECs using several rounds of initial simulation and storing ECs into EC manager. In the next step, the SAT- sweeping manager selects two gates within an EC and then calls the SAT solver to check whether they are equivalent. If so, the EC manager merges these two gates. Otherwise, SAT solver will return a satisfiable assignment as a counterexample for incremental simulation to refine the candidate ECs. To the best of our knowledge, most SAT- sweeping managers select EC only based on the circuit structure, without efficient heuristic strategy considering the functionality of candidate gates. We will introduce the functional information into SAT- sweeping manager to further improve efficiency. 1) Experiment Settings: We combine our Deep Gate2 into SAT sweeper to guide EC selection. To be specific, the updated manager sorts all candidate equivalence classes by computing the cosine similarity of their embeddings. Unlike traditional SAT sweepers, our\n\nTABLE II PERFORMANCE OF DIFFERENT MODELS ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">Size</td><td colspan=\"3\">Func Model</td><td colspan=\"3\">Deep Gate</td><td colspan=\"3\">FGNN</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>19,485</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>94.59%</td><td>52.69%</td><td>0.6768</td><td>63.64%</td><td>41.18%</td><td>0.5000</td></tr><tr><td>D2</td><td>12,648</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>92.07%</td><td>52.98%</td><td>0.6726</td><td>60.87%</td><td>35.00%</td><td>0.4444</td></tr><tr><td>D3</td><td>14,686</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>93.25%</td><td>62.08%</td><td>0.7454</td><td>72.22%</td><td>44.83%</td><td>0.5532</td></tr><tr><td>D4</td><td>7,104</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>90.11%</td><td>46.33%</td><td>0.6120</td><td>62.73%</td><td>23.53%</td><td>0.3422</td></tr><tr><td>D5</td><td>37,279</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>93.69%</td><td>56.72%</td><td>0.7066</td><td>64.00%</td><td>69.57%</td><td>0.6667</td></tr><tr><td>D6</td><td>37,383</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>93.23%</td><td>60.49%</td><td>0.7337</td><td>59.09%</td><td>46.67%</td><td>0.5215</td></tr><tr><td>D7</td><td>10,957</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>88.89%</td><td>48.83%</td><td>0.6303</td><td>36.36%</td><td>23.53%</td><td>0.2857</td></tr><tr><td>D8</td><td>3,183</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>82.86%</td><td>49.90%</td><td>0.6229</td><td>62.63%</td><td>22.22%</td><td>0.3280</td></tr><tr><td>D9</td><td>27,820</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>92.51%</td><td>48.77%</td><td>0.6387</td><td>62.50%</td><td>26.79%</td><td>0.3750</td></tr><tr><td>D10</td><td>40,496</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>93.35%</td><td>61.22%</td><td>0.7395</td><td>47.02%</td><td>32.63%</td><td>0.3853</td></tr><tr><td>Avg.</td><td></td><td>98.73%</td><td>90.49%</td><td>0.9434</td><td>91.46%</td><td>54.00%</td><td>0.6778</td><td>59.11%</td><td>36.60%</td><td>0.4402</td></tr></table> TABLE III PERFORMANCE COMPARISON BETWEEN W/PIE AND W/O PIE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ PIE</td><td colspan=\"3\">w/o PIE</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>70.66%</td><td>64.66%</td><td>0.6753</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>87.20%</td><td>78.57%</td><td>0.8266</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>79.15%</td><td>76.21%</td><td>0.7765</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>87.91%</td><td>59.70%</td><td>0.7111</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>79.23%</td><td>76.73%</td><td>0.7796</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>87.10%</td><td>77.56%</td><td>0.8205</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>84.62%</td><td>64.13%</td><td>0.7296</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>74.29%</td><td>83.87%</td><td>0.7879</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>62.27%</td><td>76.27%</td><td>0.6856</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>87.07%</td><td>65.54%</td><td>0.7479</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7541</td></tr></table> TABLE IV PERFORMANCE COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ multi-stage</td><td colspan=\"3\">w/o multi-stage</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>79.46%</td><td>67.68%</td><td>0.7310</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>74.39%</td><td>63.21%</td><td>0.6835</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>70.46%</td><td>62.78%</td><td>0.6640</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>73.63%</td><td>72.83%</td><td>0.7323</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>68.02%</td><td>77.19%</td><td>0.7232</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>71.46%</td><td>86.01%</td><td>0.7806</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>65.81%</td><td>67.46%</td><td>0.6662</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>71.43%</td><td>86.21%</td><td>0.7813</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>74.52%</td><td>63.17%</td><td>0.6838</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>75.10%</td><td>64.02%</td><td>0.6912</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7137</td></tr></table> TABLE V LOSS COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE <table><tr><td></td><td>w/o multi-stage</td><td>w/ multi-stage</td><td>Reduction</td></tr><tr><td>Lprob</td><td>0.0205</td><td>0.0207</td><td>-0.98%</td></tr><tr><td>Lrc</td><td>0.1186</td><td>0.1115</td><td>5.99%</td></tr><tr><td>Lfunc</td><td>0.1224</td><td>0.0594</td><td>51.47%</td></tr></table> proposed SAT sweeper does not need to validate the equivalence of all candidate ECs in one pass. Instead, node pairs with higher similarity have high priority for SAT solver calls. If the gate pair is formally proved to be equivalent, these two gate are merged. Otherwise, the generated counterexample should contain more conflicts than the <center>Fig. 5. The proposed SAT-sweeping ecosystem. </center> baseline method, resulting in better efficiency for refining candidate ECs. Our model is equipped into ABC [30] as a plug- in and integrated into the SAT sweeper '&frag' [14], which is one of the most efficient and scalable SAT sweeper publicly available at this time. The AIGs derived by merging the resulting equivalence nodes are verified by '&ecc' command in ABC to ensure functional correctness. All experiments are performed on a \\(2.40\\mathrm{GHz}\\) Intel(R) Xeon(R) Silver 4210R CPU with 64GB of main memory. A single core and less than 1GB was used for any test case considered in this subsection. The proposed SAT sweeper (named as Our) is compared against the original engine, &frag. 2) Results: We validate the performance of our SAT sweeper with 6 industrial circuits. As shown in Table VI, section \"Statistics\" lists the number of PI and PO (PI/PO), logic levels (Lev) and internal AND-nodes in the original AIG (And). To ensure the fairness of comparison, the circuits after sweeping should have the same size. Section \"SAT calls\" lists the number of satisfiable SAT calls, performed by the solver employed in each engine. The data shows that our proposed engine decreases the number of satisfiable SAT calls, that explains why it has better results, since more resource are used to prove equivalence gates. In addition, section \"Total runtime\" compares the runtime and section \"Red.\" shows the runtime reduction from &frag to Our. The number of \"SAT calls\" can get an average reduction of \\(53.37\\%\\) (95.88% maximum) through the integration of our Deep Gate2 model. As for \"Total runtime\", this experiment shows that the Deep Gate2- based SAT sweeper outperforms state- of- the- art engines, while reducing the average runtime by \\(49.46\\%\\) (57.77% maximum). Thus, the sweeper formally verifies the equivalence of functionally similar gates\n\nTABLE VI COMPARING THE NUMBER OF SAT CALLS AND THE RUNTIME OF THE SAT SWEEPERS <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"2\">Statistic</td><td colspan=\"2\">SAT calls</td><td colspan=\"2\">Total Runtime (s)</td></tr><tr><td>PI/PO</td><td>Lev</td><td>And</td><td>#frag</td><td>Our Red.</td><td>#frag</td></tr><tr><td>C1</td><td>128/128</td><td>4,372</td><td>57,247</td><td>13,826</td><td>570</td><td>95.88%</td></tr><tr><td>C2</td><td>24/25</td><td>225</td><td>5,416</td><td>100</td><td>74</td><td>26.00%</td></tr><tr><td>C3</td><td>22/1</td><td>29</td><td>703</td><td>4</td><td>1</td><td>75.00%</td></tr><tr><td>C4</td><td>114/1</td><td>91</td><td>19,354</td><td>20</td><td>12</td><td>40.00%</td></tr><tr><td>C5</td><td>126/1</td><td>83</td><td>20,971</td><td>6</td><td>4</td><td>33.33%</td></tr><tr><td>C6</td><td>96/1</td><td>79</td><td>14,389</td><td>10</td><td>5</td><td>50.00%</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>53.37%</td><td>49.46%</td></tr></table> with the guidance from Deep Gate2, thereby reducing the number of invalid SAT calls and improving efficiency of SAT sweeping. Take C1 as a representative example, the baseline &frag selects gates in EC for formal verification without considering their behaviour, thus, many solver calls return satisfiable results, and few gates can be merged. However, with the guidance of Deep Gate2, the sweeper can prioritize the selection of gates with similar behavior, resulting in a significant reduction of \\(95.88\\%\\) in SAT calls and \\(57.77\\%\\) in runtime. ### B. Boolean Satisfiability Solving Boolean satisfiability (SAT) solving is a long- standing and fundamental NP- complete problem with applications in many areas, especially in electronic design automation (EDA) [31]- [33]. The existing SAT solvers are designed to incorporate efficient heuristics [34]- [36] to expedite the solving process. For instance, [34] proposes to utilize the correlation of logic gate functionality to enforce variable decision for solving circuit- based SAT instances. Although the solution achieves remarkable speedup over SAT solvers, it still relies on the time- consuming logic simulation to obtain the functionality. Based on [34], we demonstrate how the Deep Gate2 models functional correlation efficiently and accelerates SAT solving. 1) Experiment Settings: We integrate our Deep Gate2 into a modern SAT solver, Ca Di Cal [15] to solve the instances from logic equivalence checking (LEC) task. Firstly, we obtain gate-level embeddings of the original circuit and predict the pairwise functional similarity between these gates. Given the one-to-one mapping [37] between circuits and conjunctive normal form (a problem format required by SAT solvers), we can easily transfer the gate functional similarity to variable behavioral similarity. If two logic gates have similar representations (and therefore similar functionality), their corresponding variables should be correlated and grouped together during the variable decision process. Secondly, we incorporate the learnt knowledge into the SAT solver. As shown in Algorithm 1, when the current variable \\(s\\) is assigned a value \\(v\\) , we identify all unassigned variables \\(s'\\) in the set \\(S\\) that contains correlated variables with \\(s\\) . As modern SAT solvers reduce searching space by detecting conflicts as much as possible [38], we assign the reverse value \\(\\bar{v}\\) to \\(s'\\) to promptly cause conflict for joint decision. Besides, the threshold \\(\\delta\\) in Algorithm 1 is set to \\(1e - 5\\) . Thirdly, to evaluate the efficacy of our model in accelerating SAT solving, we compare the aforementioned hybrid solver (labeled as Our) with original Ca Di Cal [15] (labeled as Baseline) on 5 industrial instances. All experiments are conducted with a single 2.40GHz Intel(R) Xeon(R) E5- 2640 v4 CPU. 2) Results: The runtime comparison between Baseline and Our are listed in Table VII. To ensure a fair comparison, we aggregate the Deep Gate2 model inference time (Model) and SAT solver runtime (Solver) as the Overall runtime. We have the following observations. Algorithm 1 Variable Decision Function with Deep Gate2 Current variable \\(s\\) just being assigned a value \\(v\\) ; Set \\(S\\) containing the correlated variables with \\(s\\) ; Function \\(\\mathrm{Sim}(s_i,s_j)\\) to calculate the behaviour similarity of two variables; \\(\\delta\\) is the threshold for the joint decision Function \\(V(s)\\) to get the assigned value of variable \\(s\\) ; Function Decision \\((s,v)\\) to assign the decision value \\(v\\) to the current decision variable \\(s\\) . 1: Decision \\((s,v)\\) 2: for \\(s'\\) in \\(S\\) do 3: if \\(s' \\neq s\\) and \\(V(s') = \\text{None}\\) then 4: if \\(\\mathrm{Sim}(s',s) > 1 - \\delta\\) then 5: Decision \\((s',\\bar{v})\\) 6: end if 7: end if 8: end for TABLE VII COMPARING THE RUNTIME BETWEEN BASELINE AND OUR SOLVERS <table><tr><td rowspan=\"2\">Instance</td><td rowspan=\"2\">Size</td><td rowspan=\"2\">Baseline(s)</td><td colspan=\"2\">Our(s)</td><td rowspan=\"2\">Reduction</td></tr><tr><td>Model</td><td>Solver</td></tr><tr><td>I1</td><td>17,495</td><td>88.01</td><td>1.77</td><td>30.25</td><td>32.02</td></tr><tr><td>I2</td><td>21,952</td><td>29.36</td><td>2.85</td><td>6.01</td><td>8.86</td></tr><tr><td>I3</td><td>23,810</td><td>61.24</td><td>3.25</td><td>32.88</td><td>36.13</td></tr><tr><td>I4</td><td>27,606</td><td>158.04</td><td>4.36</td><td>137.77</td><td>142.13</td></tr><tr><td>I5</td><td>28,672</td><td>89.89</td><td>4.78</td><td>70.95</td><td>75.73</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>40.05%</td></tr></table> First, our method achieves a substantial reduction in total runtime for all test cases, with an average runtime reduction of \\(40.05\\%\\) . Take I1 as an example, the plain solver requires \\(88.01\\mathrm{s}\\) to solve the problem, but by combining with our model, the new solver produces results in only 32.02s, reducing runtime by \\(63.62\\%\\) . Second, our model only takes a few seconds to obtain embeddings, occupying less than \\(10\\%\\) of overall runtime on average. It should be noted that our Deep Gate2 is able to infer within polynomial time that is only proportional to the size of instance. Third, while the two largest instances I4 and I5 show less reduction than the others, it does not necessarily mean that our model is unable to generalize to larger instances. As evidenced by the results for I2, an instance with a similar size to I4 and I5 also demonstrates a significant reduction. The reduction caused by our model should be determined by the characteristics of instance. In summary, our model is effective in speeding up downstream SAT solving. ## VI. CONCLUSION This paper introduces Deep Gate2, a novel functionally- aware framework for circuit representation learning. Our approach leverages the pairwise truth table differences of logic gates as a supervisory signal, providing rich functionality supervision and proving scalable for large circuits. Moreover, Deep Gate2 differentiates and concurrently updates structural and functional embeddings in two dedicated flows, acquiring comprehensive representations through a single round of GNN forward message- passing. In comparison to its predecessor, Deep Gate2 demonstrates enhanced performance in logic probability prediction and logic equivalent gate identification, while simultaneously improving model efficiency tenfold. The applications of Deep Gate2 onto multiple downstream tasks further demonstrate its effectiveness and potential utility in the EDA field.\n\n## REFERENCES [1] J. Chen, J. Kuang, G. Zhao, D. J.- H. Huang, and E. F. Young, \"Pros: A plug- in for routability optimization applied in the state- of- the- art commercial eda tool using deep learning,\" in Proceedings of the 39th International Conference on Computer- Aided Design, 2020, pp. 1- 8. [2] A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.- J. Lee, E. Johnson, O. Pathak, A. Nazi et al., \"A graph placement methodology for fast chip design,\" Nature, vol. 594, no. 7862, pp. 207- 212, 2021. [3] W. L. Neto, M. Austin, S. Temple, L. Amaru, X. Tang, and P.- E. Gaillardon, \"Losaic: A logic synthesis framework driven by artificial intelligence,\" in 2019 IEEE/ACM International Conference on Computer- Aided Design (ICCAD). IEEE, 2019, pp. 1- 6. [4] W. Haaswijk, E. Collins, B. Seguin, M. Soeken, F. Kaplan, S. Susstrunk, and G. De Micheli, \"Deep learning for logic optimization algorithms,\" in 2018 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE, 2018, pp. 1- 4. [5] Z. Shi, M. Li, S. Khan, L. Wang, N. Wang, Y. Huang, and Q. Xu, \"Deeptpi: Test point insertion with deep reinforcement learning,\" in 2022 IEEE International Test Conference (ITC). IEEE, 2022, pp. 194- 203. [6] J. Huang, H.- L. Zhen, N. Wang, H. Mao, M. Yuan, and Y. Huang, \"Neural fault analysis for sat- based atpg,\" in 2022 IEEE International Test Conference (ITC). IEEE, 2022, pp. 36- 45. [7] M. Li, S. Khan, Z. Shi, N. Wang, H. Yu, and Q. Xu, \"Deepgate: Learning neural representations of logic gates,\" in Proceedings of the 59th ACM/IEEE Design Automation Conference, 2022, pp. 667- 672. [8] Z. Wang, C. Bai, Z. He, G. Zhang, Q. Xu, T.- Y. Ho, B. Yu, and Y. Huang, \"Functionality matters in netlist representation learning,\" in Proceedings of the 59th ACM/IEEE Design Automation Conference, 2022, pp. 61- 66. [9] K. Zhu, H. Chen, W. J. Turner, G. F. Kokai, P.- H. Wei, D. Z. Pan, and H. Ren, \"Tag: Learning circuit spatial embedding from layouts,\" in Proceedings of the 41st IEEE/ACM International Conference on Computer- Aided Design, 2022, pp. 1- 9. [10] Y. Lai, Y. Mu, and P. Luo, \"Maskplace: Fast chip placement via reinforced visual representation learning,\" ar Xiv preprint ar Xiv:2211.13382, 2022. [11] A. Fayyazi, S. Shababi, P. Nuzzo, S. Nazarian, and M. Pedram, \"Deep learning- based circuit recognition using sparse mapping and level- dependent decaying sum circuit representations,\" in 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2019, pp. 638- 641. [12] Z. He, Z. Wang, C. Bail, H. Yang, and B. Yu, \"Graph learning- based arithmetic block identification,\" in 2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD). IEEE, 2021, pp. 1- 8. [13] M. Li, Z. Shi, Q. Lai, S. Khan, and Q. Xu, \"Deepsat: An eda- driven learning framework for sat,\" ar Xiv preprint ar Xiv:2205.13745, 2022. [14] A. Mishchenko, S. Chatterjee, R. Jiang, and R. K. Brayton, \"Fraigs: A unifying representation for logic synthesis and verification,\" ERL Technical Report, Tech. Rep., 2005. [15] S. D. QUEUE, \"Cadical at the sat race 2019,\" SAT RACE 2019, p. 8, 2019. [16] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \"Language models are few- shot learners,\" Advances in neural information processing systems, vol. 33, pp. 1877- 1901, 2020. [17] J. Devlin, M.- W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre- training of deep bidirectional transformers for language understanding,\" ar Xiv preprint ar Xiv:1810.04805, 2018. [18] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, \"Unsupervised feature learning via non- parametric instance discrimination,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3733- 3742. [19] E. Hoffer and N. Ailon, \"Deep metric learning using triplet network,\" in Similarity- Based Pattern Recognition: Third International Workshop, SIMBAD 2015, Copenhagen, Denmark, October 12- 14, 2015. Proceedings 3. Springer, 2015, pp. 84- 92. [20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" Advances in neural information processing systems, vol. 30, 2017. [21] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \"Curriculum learning,\" in Proceedings of the 26th annual international conference on machine learning, 2009, pp. 41- 48. [22] S. Ruder, \"An overview of multi- task learning in deep neural networks,\" ar Xiv preprint ar Xiv:1706.05098, 2017. [23] S. Davidson, \"Characteristics of the itc'99 benchmark circuits,\" in ITSW, 1999. [24] C. Albrecht, \"Iwls 2005 benchmarks,\" in IWLS, 2005. [25] L. Amar\u00fa, P.- E. Gaillardon, and G. De Micheli, \"The epfl combinational benchmark suite,\" in IWLS, no. CONF, 2015. [26] O. Team, \"Opencores,\" https://opencores.org/. [27] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" ar Xiv preprint ar Xiv:1412.6980, 2014. [28] A. Kuehlmann, V. Paruthi, F. Krohm, and M. K. Ganai, \"Robust boolean reasoning for equivalence checking and functional property verification,\" IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, vol. 21, no. 12, pp. 1377- 1394, 2002. [29] A. Mishchenko and R. Brayton, \"Integrating an aig package, simulator, and sat solver,\" in International Workshop on Logic and Synthesis (IWLS), 2018, pp. 11- 16. [30] B. L. Synthesis and V. Group, \"Abc: A system for sequential synthesis and verification.\" http://www.cad.eecs.berkeley.edu/alanmi/abc, 2023. [31] E. I. Goldberg, M. R. Prasad, and R. K. Brayton, \"Using sat for combinational equivalence checking,\" in Proceedings Design, Automation and Test in Europe, Conference and Exhibition 2001. IEEE, 2001, pp. 114- 121. [32] K. L. Mc Millan, \"Interpolation and sat- based model checking,\" in Computer Aided Verification: 15th International Conference, CAV 2003, Boulder, CO, USA, July 8- 12, 2003. Proceedings 15. Springer, 2003, pp. 1- 13. [33] K. Yang, K.- T. Cheng, and L.- C. Wang, \"Trangen: A sat- based atpg for path- oriented transition faults,\" in ASP- DAC 2004: Asia and South Pacific Design Automation Conference 2004 (IEEE Cat. No. 04EX753). IEEE, 2004, pp. 92- 97. [34] F. Lu, L.- C. Wang, K.- T. Cheng, and R.- Y. Huang, \"A circuit sat solver with signal correlation guided learning,\" in 2003 Design, Automation and Test in Europe Conference and Exhibition. IEEE, 2003, pp. 892- 897. [35] G. Audemard and L. Simon, \"Glucose: a solver that predicts learnt clauses quality,\" SAT Competition, pp. 7- 8, 2009. [36] \"On the glucose sat solver,\" International Journal on Artificial Intelligence Tools, vol. 27, no. 01, p. 1840001, 2018. [37] G. S. Tseitin, \"On the complexity of derivation in propositional calculus,\" Automation of reasoning: 2. Classical papers on computational logic 1967- 1970, pp. 466- 483, 1983. [38] J. Marques- Silva, I. Lynce, and S. Malik, \"Conflict- driven clause learning sat solvers,\" in Handbook of satisfiability. IOS press, 2021, pp. 133- 182.",
    "introduction": "in Section III. We compare Deep Gate2 with the original Deep Gate and another functionality- aware solution [8] in Section IV. Next, we apply Deep Gate2 onto several downstream tasks in Section V. Finally, Section VI concludes this paper. ## II. RELATED WORK ### A. Circuit Representation Learning A prominent trend in the deep learning community is to learn a general representation from data first and then apply it to various downstream tasks, for example, GPT [16] and BERT [17] learn representations of natural language text that can be fine- tuned for a wide range of natural language processing tasks. Circuit representation learning has also emerged as an attractive research direction, which falls into two categories: structure- aware circuit representation learning [9], [11], [12] and functionality- aware circuit representation learning [7], [8]. Since a circuit can be naturally formulated as a graph, with gates as nodes and wires as edge, the GNN is a powerful tool to capture the interconnections of logic gates and becomes a backbone model to learn circuit representations. For example, TAG [9] is a GNN- based model designed for analog and mixed- signal circuit representation learning and applied for several physical design applications, such as layout matching prediction, wirelength estimation, and net parasitic capacitance prediction. ABGNN [12] learns the representation of digital circuits and handles the arithmetic block identification task. However, these models tend to focus on structural encoding and are not suitable for functionality- related tasks. Consequently, the functionality- aware circuit representation learning frameworks [7], [8] are designed to learn the underlying circuit functionality. For instance, FGNN [8] learns to distinguishes between functionally equivalent and inequivalent circuits by contrastive learning [18]. However, such self- supervised manner relies on data augmentation by perturbing the original circuit to logic equivalence circuit. If the perturbation is not strong and diverse, the model still identifies the functional equivalence circuits based on the invariant local structure, resulting a low generalization ability on capturing underlying functionality. Deep Gate [7] leverages logic- 1 probability under random simulation as supervision, which approximates the statistic of the most direct representation of functionality, i.e. truth table. Despite achieving remarkable progress on testability analysis [5], there are limitations that affect the generalizability of Deep Gate to other EDA tasks. We will elaborate on Deep Gate in the next subsection. ### B. Deep Gate Framework Deep Gate [7] is the first circuit representation learning framework that embeds both structural and functional information of digital circuits. The model pre- processes the input circuits into a unified And- Inverter Graph (AIG) format and obtains rich gate- level representations, which can be applied to various downstream tasks. Deep Gate treats the logic- 1 probability as supervision to learn the functionality. Additionally, the Deep Gate consists of a GNN equipped with an attention- based aggregation function that propagates information of gates in levelized sequential manner. The aggregation function learns to assign high attention weights to controlling fan- in of gates (e.g. the fan- in gate with logic- 0 is the controlling fan- in of AND gate) that mimics the logic computation process. Although it has been applied to testability analysis [5] and SAT problem [13], we argue that the model still encounters with two major shortcomings limiting its generalization ability. <center>Fig. 1. An example of reconvergence structure </center> First, logic probability is not an appropriate supervision for learning functionality. The most direct representation of functionality is the truth table, however, using it as a training label is impractical due to the immeasurable computational overhead. Deep Gate proposes to supervise the model by utilizing the proportion of logic- 1 in the truth table and approximate this proportion as the logic probability through random simulation. However, logic probability is only a statistical information of functionality, indicating the number of logic- 1 values in the truth table rather than which PI assignments lead to logic- 1. Consequently, Deep Gate cannot differentiate the functional difference between two circuits if they have the same probability. Second, Deep Gate is not efficient enough to deal with large circuit. Specifically, Deep Gate requires to perform forward and backward message- passing operations for 20 rounds to embed rich representations. Fig. 1 illustrates the need of this multi- round GNN design in Deep Gate where the nodes in grey color represent PIs. The incoming messages of nodes 5, 6, 5, and 6 during forward propagation are noted in the figure, where \\(h_i\\) is the embedding vector of node \\(i\\) . Since, Deep Gate uses the same initial embeddings for all nodes, the messages of nodes 5, 6, 5, and 6 in the first forward propagation round are identical. Thus, the model can only distinguish node embeddings based on their connections by repeatedly updating PIs through multiple rounds of forward and backward message propagation. We emphasize that the limitations of Deep Gate comes from the lack of effective supervision and weak model design where the unique identification of all PIs are ignored. To address these issues, we propose an efficient one- round GNN design that maintains the unique identification of PIs and uses the pairwise truth- table difference of two gates as an effective supervision. ## III. METHODOLOGY ### A. Problem Formulation The circuit representation learning model aims to map both circuit structure and functionality into embedding space, where the structure represents the connecting relationship of logic gates and the functionality means the logic computational mapping from inputs to outputs. We conclude that the previous models still lack of ability to capture functional information. In this paper, we propose to improve the previous Deep Gate model [7] to represent circuits with similar functionality with the similar embedding vectors. In other words, these circuit representations should have short distance in the embedding space. We take Circuit A, B, C, and D as examples in Fig. 2, where all of them have similar topological structures. Since Circuit A, B and C perform with the same logic probability, Deep Gate [7] tends to produce the similar embeddings for these three circuits. Hence, it is hard to identify the logic equivalent circuits by Deep Gate. Although FGNN [8] is trained to classify logical equivalence and inequivalence circuits by contrastive learning, they cannot differentiate the relative similarity. As shown in the embedding space, the distance between\n\n<center>Fig. 2. Problem statement: the embedding vectors should be close if circuit functions are similar </center> A and B is equal to the distance between A and D. Nonetheless, as indicated in the truth table, Circuit A is equivalent to Circuit C, similar to Circuit B (with only 2 different bits), but dissimilar to Circuit D (with 5 different bits). We expect that the model will bring together or separate the circuits in embedding space according to their truth tables. Therefore, the expected Deep Gate2 model not only identifies the logic equivalent nodes, but also predicts the functional similarity. Thus, we can apply such functionality- aware circuit learning model to provide benefits for the real- world applications. ### B. Dataset Preparation To train the circuit representation learning model, we need to find a supervision contains rich functional information and prepare an effective dataset at first. Truth table, which records the complete logic computational mapping, provides the most direct supervision. However, the length of the truth table increases exponentially with the number of primary inputs, and obtaining a complete truth table requires an immeasurable amount of time. Therefore, a reasonable supervision should be easily obtained and closely related to the truth table. Firstly, we use the Hamming distance between truth tables of two logic gates as supervision. That is, in a way similar to metric learning [19], we map nodes to an embedding space and hope that the distance of the embedding vectors is positive correlated with the Hamming distance of the truth table. Formally, we denote the truth table vector of node \\(i\\) is \\(T_{i}\\) and the embedding vector of node \\(i\\) is \\(h_{i}\\) . \\[distance(h_{i},h_{j})\\propto distance(T_{i},T_{j}) \\quad (1)\\] Secondly, to improve the data efficiency, we regard the each logic gate in circuit as a new circuit (logic cone) with the current gate as output and the original PIs as inputs. By parsing a single original circuit, we obtain a large number of new circuits. Therefore, the task of graph learning becomes the task of learning node- level representation, and the difficulty of data collection is reduced. Thirdly, to ensure the quality of sample pairs and limit the number of sample pairs, we impose the following constraints during sampling node pairs: (1) Two logic cones of the two nodes should have the same PI, which is a necessary condition for comparing the truth table difference. (2) The logic probability, which is the number of logic- 1 percentage in the truth table, should be similar (distance within \\(5\\%\\) ). This is because if the logic probability of two nodes is not consistent, their functions are definitely not consistent. If the logic probability of two nodes is consistent, their functions may be consistent. (3) The difference in logic levels between two nodes should be within 5, because when the two nodes are far apart, their functions are unlikely to be correlated. (4) We only consider the extreme cases, namely, the difference between truth tables is within \\(20\\%\\) or above \\(80\\%\\) . We do not perform the complete simulation, but set a maximum simulation time to obtain the response of each node as an incomplete truth table. It should be noted that we utilize the And- Inverter Graph (AIG) as the circuit netlist format, which is only composed of AND gate and NOT gate. Any other logic gates, including OR, XOR and MUX, can be transformed into a combination of AND and NOT gates in linear time. ### C. Functionality-Aware Loss Function The primary objective of our purposed functionality- aware circuit learning model is to learn node embeddings, where two embedding vectors will be similar if the corresponding two node function are similar. As we sample node pairs \\(\\mathcal{N}\\) in the Section III- B, we can obtain the Hamming distance of truth table \\(D^{T}\\) of each node pair. \\[D_{(i,j)}^{T} = \\frac{Hamming Distance(T_{i},T_{j})}{length(T_{i})},(i,j)\\in \\mathcal{N} \\quad (2)\\] According to Eq. (1), the distance of embedding vectors \\(D^{H}\\) should be proportional to the Hamming distance of the truth table \\(D^{T}\\) . We define the distance of embedding vectors in Eq. (3), where is calculated based on cosine similarity. In other word, the similarity of embedding vectors \\(S_{(i,j)}\\) should be negative related to distance \\(D_{(i,j)}^{T}\\) . \\[\\begin{array}{l}{S_{(i,j)} = Cosine Similarity(h_i,h_j)}\\\\ {D_{(i,j)}^H = 1 - S_{(i,j)}} \\end{array} \\quad (3)\\] Therefore, the training objective is to minimize the difference between \\(D^{H}\\) and \\(D^{T}\\) . We purpose the functionality- aware loss function \\(L_{func}\\) as below. \\[\\begin{array}{l}{D_{(i,j)}^{T^{\\prime}} = Zero Norm(D_{(i,j)}^{T})}\\\\ {D_{(i,j)}^{H^{\\prime}} = Zero Norm(D_{(i,j)}^{H})}\\\\ {L_{func} = \\sum_{(i,j)\\in \\mathcal{N}}(L1Loss(D_{(i,j)}^{T^{\\prime}},D_{(i,j)}^{H^{\\prime}}))} \\end{array} \\quad (4)\\] ### D. One-round GNN Model In this subsection, we propose a GNN model that can capture both functional and structural information for each logic gate through one- round forward propagation. First, we propose to separate the functional embeddings \\(hf\\) and structural embeddings \\(hs\\) , and initialize them in difference ways. We assign the uniform initial functional embeddings to primary inputs (PI), as they all have equivalent logic probability under random simulation. However, we design a PI encoding (PIE) strategy by assigning a unique identification to each PI as its initial structural embedding. Specifically, the initial PI structural embeddings \\(hs_{i}, i \\in PI\\) are orthogonal vectors. This means that the dot product of any two PIs' embeddings is zero. Second, we design four aggregators: \\(agg_{AND}^{r}\\) aggregates the message for structural embedding \\(hs\\) of an AND gate, \\(agg_{AND}^{r}\\) aggregates the message for functional embedding \\(hf\\) of an AND\n\n<center>Fig. 3. One-round GNN propagation process </center> gate. \\(agg r_{NOT}^{s}\\) and \\(agg r_{NOT}^{f}\\) update \\(h s\\) and \\(h f\\) of a NOT gate, respectively. We implement each aggregator using the self- attention mechanism [20], as the output of a logic gate is determined by the controlling values of its fan- in gates. For example, an AND gate must output logic- 0 if any of its fan- in gates has logic- 0. By employing the attention mechanism, the model learns to assign greater importance to the controlling inputs [7]. As illustrated in Eq. (5), \\(w_{q}\\) , \\(w_{k}\\) and \\(w_{v}\\) are three weight matrices and \\(d\\) is the dimension of embedding vectors \\(h\\) . \\[\\begin{array}{l}\\alpha_{j} = softmax(\\frac{w_{q}^{\\top}h_{i}\\cdot(w_{k}^{\\top}h_{j})^{\\top}}{\\sqrt{d}})\\\\ m_{j} = w_{v}^{\\top}h_{j}\\\\ h_{i} = agg r(h_{j}|j\\in \\mathcal{P}(i)) = \\sum_{j\\in \\mathcal{P}(i)}(\\alpha_{j}*m_{j}) \\end{array} \\quad (5)\\] Third, during forward propagation, the structural embeddings are updated only with the structural embeddings of predecessors. As shown in Eq. (6), where the Gate \\(a\\) is AND gate, the Gate \\(b\\) is NOT gate. \\[\\begin{array}{l} h s_{a} = agg r_{A N D}^{s}(h s_{j}|j\\in \\mathcal{P}(a)) \\\\ h s_{b} = agg r_{N O T}^{s}(h s_{j}|j\\in \\mathcal{P}(b)) \\end{array} \\quad (6)\\] At the same time, the gate function is determined by the function and the structural correlations of the fan- in gates. Therefore, the functional embeddings are updated as Eq. (7). \\[\\begin{array}{l} h f_{a} = agg r_{A N D}^{f}([h s_{j},h f_{j}]|j\\in \\mathcal{P}(a))\\\\ h f_{b} = agg r_{N O T}^{f}([h s_{j},h f_{j}]|j\\in \\mathcal{P}(b)) \\end{array} \\quad (7)\\] Therefore, as shown in Fig. 3, the GNN propagation process performs from PI to PO level by level. For the node in level \\(l\\) , its structural embedding \\(h s_{L_{l}}\\) will be updated with the structural embeddings of the node in level \\(l - 1\\) . Additionally, the functional embedding \\(h f_{L_{l}}\\) will be updated with both structural embeddings \\(h s_{L_{l - 1}}\\) and functional embeddings \\(h f_{L_{l - 1}}\\) . The GNN propagation completes after processing \\(N\\) levels. ### E. Model Training Strategies To train the model, we employed multi- stage training strategy, similar to training a model with an easy task and then a harder task in curriculum learning [21]. During each stage, we trained the model with multiple supervisions in multi- task learning manner [22]. In the first stage, we train the one- round GNN model with two simple tasks. The Task 1 involves predicting the logic probability, while the Task 2 entails identifying the structural correlation. To achieve this, we readout the functional embedding \\(h f_{i}\\) to predict the logic probability \\(\\hat{P}_{i}\\) by a multi- layer perceptron (MLP), denoted as \\(MLP_{prob}\\) . In addition, we utilize the structural embeddings \\(h s_{i}\\) and \\(h s_{j}\\) to predict whether node \\(i\\) and node \\(j\\) can be reconvergent by \\(MLP_{rc}\\) . \\[\\begin{array}{r}\\hat{P}_i = MLP_{prob}(h f_i)\\\\ R_{\\langle i,j\\rangle} = MLP_{rc}(h s_i,h s_j) \\end{array} \\quad (8)\\] We define the loss function for Task 1 in Eq. (9), where the \\(P_{i}\\) is the ground truth logic probability obtained through random simulation. \\[L_{prob} = L1Loss(P_i,\\hat{P}_i) \\quad (9)\\] Besides, we define the loss function for Task 2 in Eq. (10). The binary ground truth, denoted as \\(R_{\\langle i,j\\rangle}\\) , indicates whether node pair \\(i\\) and \\(j\\) have a common predecessor. \\[L_{rc} = BCELoss(R_{\\langle i,j\\rangle},R_{\\langle i,j\\rangle}) \\quad (10)\\] Consequently, the loss function for Stage 1 is presented in Eq. (11), where the \\(w_{prob}\\) and \\(w_{rc}\\) are the weight for Task 1 and Task 2, respectively. \\[L_{stage1} = L_{prob}*w_{prob} + L_{rc}*w_{rc} \\quad (11)\\] The second training stage involves another more difficult Task 3. functionality- aware learning, as described in Section III- C. The loss function for Stage 2 is defined below, where \\(w_{func}\\) represents the loss weight of Task 3. \\[L_{stage2} = L_{prob}\\times w_{prob} + L_{rc}\\times w_{rc} + L_{func}\\times w_{func} \\quad (12)\\] Overall, the model can differentiate gates with varying probability in Stage 1. As the logic equivalent pairs only occur when nodes have the same probability, the model in Stage 2 learns to predicting the functional similarity within the probability equivalent class. The effectiveness of the above training strategies is demonstrated in Section IV- E. ## IV. EXPERIMENTS In this section, we demonstrate the ability of our proposed Deep Gate2 to learn functionality- aware circuit representations. Firstly, Section IV- A provides the preliminary of our experiments, including details on dataset preparation, evaluation metrics and model settings. Secondly, we compare the effectiveness and efficiency of our Deep Gate2 against Deep Gate [7] and FGNN [8] on two function- related tasks: logic probability prediction (see Section IV- B) and logic equivalence gates identification (see Section IV- C). Thirdly, we investigate the effectiveness of model design and training strategies in Section IV- D and Section IV- E, respectively. ### A. Experiment Settings 1) Dataset Preparation: We use the circuits in Deep Gate [7], which are extracted from ITC'99 [23], IWLS'05 [24], EPFL [25] and Open Core [26]. These circuits consists of 10,824 AIGs with sizes ranging from 36 to 3,214 logic gates. To obtain the incomplete truth table, we generate 15,000 random patterns and record the corresponding response. Following the data preparation method described in Section III-B, we construct a dataset comprising 894,151 node pairs. We create 80/20 training/test splits for model training and evaluation.\n\n<center>Fig. 4. Functionality-aware circuit learning framework </center> 2) Evaluation Metrics: We assess our Deep Gate2 with two tasks. The first task is to predict the logic probability for each logic gate. We calculate the average prediction error (PE) as Eq. (13), where the set \\(\\mathcal{V}\\) includes all logic gates. \\[PE = \\frac{1}{|\\mathcal{V}|}\\sum_{i\\in \\mathcal{V}}|P_{i} - \\hat{P}_{i}| \\quad (13)\\] The second task is to identify the logic equivalence gates within a circuit. A gate pair \\((i,j)\\) is considered as a positive pair if these two logic gates \\(i\\) and \\(j\\) have the same function, where the pairwise Hamming distance of truth tables \\(D_{(i,j)}^{T} = 0\\) . If the similarity \\(S_{(i,j)}\\) between these two embedding vectors \\(h f_{i}\\) and \\(h f_{j}\\) exceeds a certain threshold, the model will recognize the gate pair \\((i,j)\\) as equivalent. The optimal threshold \\(\\theta\\) is determined based on the receiver operating characteristic (ROC). The evaluation metrics is formally defined in Eq. (14), where \\(TP\\) , \\(TN\\) , \\(FP\\) , \\(FN\\) are true positive, true negative, false positive and false negative, respective, and \\(M\\) is the total number of gate pairs. In the following experiments, the performance on logic equivalence gates identification is measured in terms of Recall, Precision, F1- Score and area under curve (AUC). \\[\\begin{array}{rl} & {TP = \\frac{\\sum((D_{(i,j)}^{T} = =0)\\&(S_{(i,j)} > \\theta))}{M}}\\\\ & {TN = \\frac{\\sum((D_{(i,j)}^{T} > 0)\\&(S_{(i,j)}< \\theta))}{M}}\\\\ & {FP = \\frac{\\sum((D_{(i,j)}^{T} > 0)\\&(S_{(i,j)} > \\theta))}{M}}\\\\ & {FN = \\frac{\\sum((D_{(i,j)}^{T} = =0)\\&(S_{(i,j)}< \\theta))}{M}} \\end{array} \\quad (14)\\] We conduct the following performance comparisons on 10 industrial circuits, with circuit sizes ranging from \\(3.18k\\) gates to \\(40.50k\\) gates. 3) Model Settings: In the one-round GNN model configuration, the dimension of both structural embedding \\(hs\\) and functional embedding \\(hf\\) is 64. Both \\(MLP_{prob}\\) and \\(MLP_{rc}\\) contain 1 hidden layer with 32 neurons and a Re Lu activation function. The model is trained for 60 epochs to ensure each model can converge. The other models [7], [8] mentioned in the following experiments maintain their original settings and are trained until they converge. We train all the models for 80 epochs with batch-size 16 on a single Nvidia V100 GPU. We adopt the Adam optimizer [27] with learning rate \\(10^{- 4}\\) and weight decay \\(10^{- 10}\\) . TABLE I PERFORMANCE OF DEEPGATE (V1) AND OUR PROPOSED DEEPGATE2 (V2) ON LOGIC PROBABILITY PREDICTION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">#Gates</td><td colspan=\"2\">Deep Gate</td><td colspan=\"2\">Deep Gate2</td><td colspan=\"2\">Reduction</td></tr><tr><td>PE</td><td>Time</td><td>PE</td><td>Time</td><td>PE</td><td>Time(x)</td></tr><tr><td>D1</td><td>19,485</td><td>0.0344</td><td>36.89s</td><td>0.0300</td><td>2.23s</td><td>12.79%</td><td>16.56</td></tr><tr><td>D2</td><td>12,648</td><td>0.0356</td><td>16.11s</td><td>0.0309</td><td>1.18s</td><td>13.20%</td><td>13.66</td></tr><tr><td>D3</td><td>14,686</td><td>0.0355</td><td>21.42s</td><td>0.0294</td><td>1.44s</td><td>17.18%</td><td>14.92</td></tr><tr><td>D4</td><td>7,104</td><td>0.0368</td><td>5.89s</td><td>0.0323</td><td>0.50s</td><td>12.23%</td><td>11.89</td></tr><tr><td>D5</td><td>37,279</td><td>0.0356</td><td>131.22s</td><td>0.0316</td><td>7.82s</td><td>11.24%</td><td>16.79</td></tr><tr><td>D6</td><td>37,383</td><td>0.0325</td><td>133.02s</td><td>0.0285</td><td>8.10s</td><td>12.31%</td><td>16.42</td></tr><tr><td>D7</td><td>10,957</td><td>0.0357</td><td>13.02s</td><td>0.0316</td><td>0.90s</td><td>11.48%</td><td>14.44</td></tr><tr><td>D8</td><td>3,183</td><td>0.0406</td><td>1.60s</td><td>0.0341</td><td>0.17s</td><td>16.01%</td><td>9.64</td></tr><tr><td>D9</td><td>27,820</td><td>0.0368</td><td>77.20s</td><td>0.0322</td><td>4.73s</td><td>12.50%</td><td>16.34</td></tr><tr><td>D10</td><td>40,496</td><td>0.0327</td><td>154.00s</td><td>0.0290</td><td>8.89s</td><td>11.31%</td><td>17.33</td></tr><tr><td>Avg.</td><td></td><td>0.0356</td><td>59.04s</td><td>0.0310</td><td>3.59s</td><td>13.08%</td><td>16.43</td></tr></table> ### B. Comparison with Deep Gate on Probability Prediction We compare the probability prediction error (PE, see Eq. (13)) and runtime (Time) with previous Deep Gate. The previous Deep Gate is denoted as Deep Gate and our proposed model with novel loss function and GNN design is named as Deep Gate2 in Table I. Based on the results presented in the table, we make two observations. First, our proposed Deep Gate2 exhibits more accurate predictions of logic probability compared to the previous version. On average, the probability prediction error (PE) of Deep Gate2 is \\(13.08\\%\\) lower than that of Deep Gate. This suggests that using the novel model architecture with embedding initialization strategy can benefit logic representation learning and lead to better results on logic probability prediction. Second, our Deep Gate2 performs more efficient than Deep Gate. Take the circuit D1 as an example, Deep Gate requires 36.89 seconds for inference, but our Deep Gate2 only needs 2.23s, which is 16.56x faster than the previous Deep Gate. Moreover, compared to the previous model, Deep Gate2 achieves an order of magnitude speedup (16.43x on average) in model runtime. This is attributed to the fact that the GNN model in Deep Gate relies on 10 forward and 10 backward message propagation, whereas the proposed one- round GNN model in Deep Gate2 only performs forward propagation for 1 time. Therefore, the new circuit representation learning model is more effective and efficient than Deep Gate, and demonstrates the generalization ability on large- scale circuits.\n\n### C. Comparison with other Models on Logic Equivalence Gates Identification This section compares the functionality- aware accuracy, as defined in Section IV- A2 of Deep Gate2 with that of two other models: Deep Gate [7] and FGNN [8]. The Deep Gate [7] model treats the logic probability as supervision since it contains the statistical information of truth table. The FGNN [8] is trained to differentiate between logic equivalent and inequivalent circuits using contrastive learning. Table II presents the performance of three models on the task of logic equivalence gates identification. Firstly, our proposed approach outperforms the other two models on all circuits with an average F1- score of 0.9434, while Deep Gate and FGNN only achieve F1- Score 0.6778 and 0.4402, respectively. For instance, in circuit D7, our proposed functionality- aware circuit learning approach achieves an F1- Score of 0.9831 and accurately identifies \\(99.15\\%\\) of logic equivalence gate pairs with a precision of \\(97.48\\%\\) , indicating a low false positive rate. In contrast, Deep Gate only achieves an F1- score of 0.6778, while FGNN fails on most of the pairs. Secondly, although Deep Gate has an average recall of \\(91.46\\%\\) , its precision is only \\(54.00\\%\\) , indicating a large number of false positive identifications. This is because Deep Gate can only identify logic equivalent pairs by predicting logic probability, which leads to incorrect identification of gate pairs with similar logic probability. According to our further experiment, in \\(80.83\\%\\) of false positive pairs, the model incorrectly identifies gate pairs with similar logic probability as functionally equivalent. Thirdly, FGNN achieves the lowest performance among the other models, with only 0.4402 F1- Score. The poor performance of FGNN is attributed to the lack of effective supervision. While FGCN learns to identify logic equivalence circuits generated by perturbing local structures slightly, the model tends to consider circuits with similar structures to have the same functionality. However, in the validation dataset and practical applications, two circuits may have the same function even if their topological structures are extremely different. Therefore, the self- supervised approach limits the effectiveness of FGNN in identifying logic equivalence gates. ### D. Effectiveness of PI Encoding Strategy To demonstrate the effectiveness of our proposed PI encoding (PIE) strategy, we trained another model without assigning unique identifications for PIs, which we refer to as w/o PIE. The results are presented in Table III, which show that disabling the PIE reduces the F1- Score of identifying logic equivalence gates from 0.9434 to 0.7541, resulting in an average reduction of \\(20.07\\%\\) . Such reduction can be attributed to the fact that, as demonstrated as the failure case in Section II and Fig. 1, the one- round GNN model without the PIE strategy cannot model the structural information of the circuit. More specifically, the accuracy of the reconvergence structure identification task with w/ PIE model is \\(93.22\\%\\) , while the w/o model only achieve \\(74.56\\%\\) . The functionality of logic gate is affected by both functionality of fan- in gates and whether there is reconvergence between its fan- in gates. Once the reconvergence structure cannot be accurately identified, node functionality cannot be modeled accurately. ### E. Effectiveness of Training Strategies To investigate the effectiveness of our multi- stage training strategy, we train another model (noted as w/o multi- stage model) with all loss functions in only one stage, instead of adding the functionality- aware loss function in the second stage. The original model with multiple stages training strategy is noted as w/ multi- stage model. The w/ multi- stage model learn to predict the logic probability and structural correlation in the first stage and learn the more difficult task, which predicts the functionality in the second stage. The results are shown in Table IV, where the model w/ multi- stage achieves an F1- Score of 0.9434 on average and the model w/o multi- stage achieves only 0.7137. We analyze the reason as follows. The cost of comparing each pair of logic gates in the task of learning functionality is extremely high, which is proportional to the square of the circuit size. We limit the dataset and train the model to learn functional similarity only among pairs with similar logic probability, which is a necessary condition for functional equivalence. Therefore, without the staged multi- stage strategy, be effectively supervised with the simplified dataset, leading to poor performance in learning functionality. As shown in Table V, the differences between the two models in the loss values for predicting logic probability \\((L_{prob})\\) and identifying reconvergence structures \\((L_{rc})\\) are not significant, indicating that they perform similarly in these two tasks. However, compared to the w/o multi- stage model, the w/ multi- stage model performs better in learning functionality with \\(L_{func} = 0.0594\\) , which is \\(51.47\\%\\) smaller than that of w/o multi- stage model. However, the w/ multi- stage model outperforms the model w/o multi- stage in learning functionality task with a significantly lower \\(L_{func}\\) value of 0.0594, which is \\(51.47\\%\\) smaller than that of the latter. ## V. DOWNSTREAM TASKS In this section, we combine our Deep Gate2 with the open- source EDA tools and apply our model to practical EDA tasks: logic synthesis and Boolean satisfiability (SAT) solving. The logic synthesis tools aim to identify logic equivalence gates as quickly as possible. In Section V- A, our proposed functionality- aware circuit learning model provides guidance to the logic synthesis tool about the logic similarity. Additionally, in Section V- B, we apply the learnt functional similarity in SAT solving, where the variables with dissimilar functionality are assigned the same decision value. This approach efficiently shrinks the search space by enabling solvers to encounter more constraints. ### A. Logic Synthesis This subsection shows the effectiveness of our proposed functionality- aware circuit learning framework in SAT- sweeping [28], a common technique of logic synthesis. Fig. 5 illustrates the components of a typical ecosystem for SAT- sweeping engine (also called SAT sweeper), where including equivalence class (EC) manager, SAT- sweeping manager, simulator, and SAT solver. All computations are coordinated by the SAT- sweeping manager [29]. The SAT sweeper starts by computing candidate ECs using several rounds of initial simulation and storing ECs into EC manager. In the next step, the SAT- sweeping manager selects two gates within an EC and then calls the SAT solver to check whether they are equivalent. If so, the EC manager merges these two gates. Otherwise, SAT solver will return a satisfiable assignment as a counterexample for incremental simulation to refine the candidate ECs. To the best of our knowledge, most SAT- sweeping managers select EC only based on the circuit structure, without efficient heuristic strategy considering the functionality of candidate gates. We will introduce the functional information into SAT- sweeping manager to further improve efficiency. 1) Experiment Settings: We combine our Deep Gate2 into SAT sweeper to guide EC selection. To be specific, the updated manager sorts all candidate equivalence classes by computing the cosine similarity of their embeddings. Unlike traditional SAT sweepers, our\n\nTABLE II PERFORMANCE OF DIFFERENT MODELS ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">Size</td><td colspan=\"3\">Func Model</td><td colspan=\"3\">Deep Gate</td><td colspan=\"3\">FGNN</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>19,485</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>94.59%</td><td>52.69%</td><td>0.6768</td><td>63.64%</td><td>41.18%</td><td>0.5000</td></tr><tr><td>D2</td><td>12,648</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>92.07%</td><td>52.98%</td><td>0.6726</td><td>60.87%</td><td>35.00%</td><td>0.4444</td></tr><tr><td>D3</td><td>14,686</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>93.25%</td><td>62.08%</td><td>0.7454</td><td>72.22%</td><td>44.83%</td><td>0.5532</td></tr><tr><td>D4</td><td>7,104</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>90.11%</td><td>46.33%</td><td>0.6120</td><td>62.73%</td><td>23.53%</td><td>0.3422</td></tr><tr><td>D5</td><td>37,279</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>93.69%</td><td>56.72%</td><td>0.7066</td><td>64.00%</td><td>69.57%</td><td>0.6667</td></tr><tr><td>D6</td><td>37,383</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>93.23%</td><td>60.49%</td><td>0.7337</td><td>59.09%</td><td>46.67%</td><td>0.5215</td></tr><tr><td>D7</td><td>10,957</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>88.89%</td><td>48.83%</td><td>0.6303</td><td>36.36%</td><td>23.53%</td><td>0.2857</td></tr><tr><td>D8</td><td>3,183</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>82.86%</td><td>49.90%</td><td>0.6229</td><td>62.63%</td><td>22.22%</td><td>0.3280</td></tr><tr><td>D9</td><td>27,820</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>92.51%</td><td>48.77%</td><td>0.6387</td><td>62.50%</td><td>26.79%</td><td>0.3750</td></tr><tr><td>D10</td><td>40,496</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>93.35%</td><td>61.22%</td><td>0.7395</td><td>47.02%</td><td>32.63%</td><td>0.3853</td></tr><tr><td>Avg.</td><td></td><td>98.73%</td><td>90.49%</td><td>0.9434</td><td>91.46%</td><td>54.00%</td><td>0.6778</td><td>59.11%</td><td>36.60%</td><td>0.4402</td></tr></table> TABLE III PERFORMANCE COMPARISON BETWEEN W/PIE AND W/O PIE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ PIE</td><td colspan=\"3\">w/o PIE</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>70.66%</td><td>64.66%</td><td>0.6753</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>87.20%</td><td>78.57%</td><td>0.8266</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>79.15%</td><td>76.21%</td><td>0.7765</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>87.91%</td><td>59.70%</td><td>0.7111</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>79.23%</td><td>76.73%</td><td>0.7796</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>87.10%</td><td>77.56%</td><td>0.8205</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>84.62%</td><td>64.13%</td><td>0.7296</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>74.29%</td><td>83.87%</td><td>0.7879</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>62.27%</td><td>76.27%</td><td>0.6856</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>87.07%</td><td>65.54%</td><td>0.7479</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7541</td></tr></table> TABLE IV PERFORMANCE COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ multi-stage</td><td colspan=\"3\">w/o multi-stage</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>79.46%</td><td>67.68%</td><td>0.7310</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>74.39%</td><td>63.21%</td><td>0.6835</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>70.46%</td><td>62.78%</td><td>0.6640</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>73.63%</td><td>72.83%</td><td>0.7323</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>68.02%</td><td>77.19%</td><td>0.7232</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>71.46%</td><td>86.01%</td><td>0.7806</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>65.81%</td><td>67.46%</td><td>0.6662</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>71.43%</td><td>86.21%</td><td>0.7813</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>74.52%</td><td>63.17%</td><td>0.6838</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>75.10%</td><td>64.02%</td><td>0.6912</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7137</td></tr></table> TABLE V LOSS COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE <table><tr><td></td><td>w/o multi-stage</td><td>w/ multi-stage</td><td>Reduction</td></tr><tr><td>Lprob</td><td>0.0205</td><td>0.0207</td><td>-0.98%</td></tr><tr><td>Lrc</td><td>0.1186</td><td>0.1115</td><td>5.99%</td></tr><tr><td>Lfunc</td><td>0.1224</td><td>0.0594</td><td>51.47%</td></tr></table> proposed SAT sweeper does not need to validate the equivalence of all candidate ECs in one pass. Instead, node pairs with higher similarity have high priority for SAT solver calls. If the gate pair is formally proved to be equivalent, these two gate are merged. Otherwise, the generated counterexample should contain more conflicts than the <center>Fig. 5. The proposed SAT-sweeping ecosystem. </center> baseline method, resulting in better efficiency for refining candidate ECs. Our model is equipped into ABC [30] as a plug- in and integrated into the SAT sweeper '&frag' [14], which is one of the most efficient and scalable SAT sweeper publicly available at this time. The AIGs derived by merging the resulting equivalence nodes are verified by '&ecc' command in ABC to ensure functional correctness. All experiments are performed on a \\(2.40\\mathrm{GHz}\\) Intel(R) Xeon(R) Silver 4210R CPU with 64GB of main memory. A single core and less than 1GB was used for any test case considered in this subsection. The proposed SAT sweeper (named as Our) is compared against the original engine, &frag. 2) Results: We validate the performance of our SAT sweeper with 6 industrial circuits. As shown in Table VI, section \"Statistics\" lists the number of PI and PO (PI/PO), logic levels (Lev) and internal AND-nodes in the original AIG (And). To ensure the fairness of comparison, the circuits after sweeping should have the same size. Section \"SAT calls\" lists the number of satisfiable SAT calls, performed by the solver employed in each engine. The data shows that our proposed engine decreases the number of satisfiable SAT calls, that explains why it has better results, since more resource are used to prove equivalence gates. In addition, section \"Total runtime\" compares the runtime and section \"Red.\" shows the runtime reduction from &frag to Our. The number of \"SAT calls\" can get an average reduction of \\(53.37\\%\\) (95.88% maximum) through the integration of our Deep Gate2 model. As for \"Total runtime\", this experiment shows that the Deep Gate2- based SAT sweeper outperforms state- of- the- art engines, while reducing the average runtime by \\(49.46\\%\\) (57.77% maximum). Thus, the sweeper formally verifies the equivalence of functionally similar gates\n\nTABLE VI COMPARING THE NUMBER OF SAT CALLS AND THE RUNTIME OF THE SAT SWEEPERS <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"2\">Statistic</td><td colspan=\"2\">SAT calls</td><td colspan=\"2\">Total Runtime (s)</td></tr><tr><td>PI/PO</td><td>Lev</td><td>And</td><td>#frag</td><td>Our Red.</td><td>#frag</td></tr><tr><td>C1</td><td>128/128</td><td>4,372</td><td>57,247</td><td>13,826</td><td>570</td><td>95.88%</td></tr><tr><td>C2</td><td>24/25</td><td>225</td><td>5,416</td><td>100</td><td>74</td><td>26.00%</td></tr><tr><td>C3</td><td>22/1</td><td>29</td><td>703</td><td>4</td><td>1</td><td>75.00%</td></tr><tr><td>C4</td><td>114/1</td><td>91</td><td>19,354</td><td>20</td><td>12</td><td>40.00%</td></tr><tr><td>C5</td><td>126/1</td><td>83</td><td>20,971</td><td>6</td><td>4</td><td>33.33%</td></tr><tr><td>C6</td><td>96/1</td><td>79</td><td>14,389</td><td>10</td><td>5</td><td>50.00%</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>53.37%</td><td>49.46%</td></tr></table> with the guidance from Deep Gate2, thereby reducing the number of invalid SAT calls and improving efficiency of SAT sweeping. Take C1 as a representative example, the baseline &frag selects gates in EC for formal verification without considering their behaviour, thus, many solver calls return satisfiable results, and few gates can be merged. However, with the guidance of Deep Gate2, the sweeper can prioritize the selection of gates with similar behavior, resulting in a significant reduction of \\(95.88\\%\\) in SAT calls and \\(57.77\\%\\) in runtime. ### B. Boolean Satisfiability Solving Boolean satisfiability (SAT) solving is a long- standing and fundamental NP- complete problem with applications in many areas, especially in electronic design automation (EDA) [31]- [33]. The existing SAT solvers are designed to incorporate efficient heuristics [34]- [36] to expedite the solving process. For instance, [34] proposes to utilize the correlation of logic gate functionality to enforce variable decision for solving circuit- based SAT instances. Although the solution achieves remarkable speedup over SAT solvers, it still relies on the time- consuming logic simulation to obtain the functionality. Based on [34], we demonstrate how the Deep Gate2 models functional correlation efficiently and accelerates SAT solving. 1) Experiment Settings: We integrate our Deep Gate2 into a modern SAT solver, Ca Di Cal [15] to solve the instances from logic equivalence checking (LEC) task. Firstly, we obtain gate-level embeddings of the original circuit and predict the pairwise functional similarity between these gates. Given the one-to-one mapping [37] between circuits and conjunctive normal form (a problem format required by SAT solvers), we can easily transfer the gate functional similarity to variable behavioral similarity. If two logic gates have similar representations (and therefore similar functionality), their corresponding variables should be correlated and grouped together during the variable decision process. Secondly, we incorporate the learnt knowledge into the SAT solver. As shown in Algorithm 1, when the current variable \\(s\\) is assigned a value \\(v\\) , we identify all unassigned variables \\(s'\\) in the set \\(S\\) that contains correlated variables with \\(s\\) . As modern SAT solvers reduce searching space by detecting conflicts as much as possible [38], we assign the reverse value \\(\\bar{v}\\) to \\(s'\\) to promptly cause conflict for joint decision. Besides, the threshold \\(\\delta\\) in Algorithm 1 is set to \\(1e - 5\\) . Thirdly, to evaluate the efficacy of our model in accelerating SAT solving, we compare the aforementioned hybrid solver (labeled as Our) with original Ca Di Cal [15] (labeled as Baseline) on 5 industrial instances. All experiments are conducted with a single 2.40GHz Intel(R) Xeon(R) E5- 2640 v4 CPU. 2) Results: The runtime comparison between Baseline and Our are listed in Table VII. To ensure a fair comparison, we aggregate the Deep Gate2 model inference time (Model) and SAT solver runtime (Solver) as the Overall runtime. We have the following observations. Algorithm 1 Variable Decision Function with Deep Gate2 Current variable \\(s\\) just being assigned a value \\(v\\) ; Set \\(S\\) containing the correlated variables with \\(s\\) ; Function \\(\\mathrm{Sim}(s_i,s_j)\\) to calculate the behaviour similarity of two variables; \\(\\delta\\) is the threshold for the joint decision Function \\(V(s)\\) to get the assigned value of variable \\(s\\) ; Function Decision \\((s,v)\\) to assign the decision value \\(v\\) to the current decision variable \\(s\\) . 1: Decision \\((s,v)\\) 2: for \\(s'\\) in \\(S\\) do 3: if \\(s' \\neq s\\) and \\(V(s') = \\text{None}\\) then 4: if \\(\\mathrm{Sim}(s',s) > 1 - \\delta\\) then 5: Decision \\((s',\\bar{v})\\) 6: end if 7: end if 8: end for TABLE VII COMPARING THE RUNTIME BETWEEN BASELINE AND OUR SOLVERS <table><tr><td rowspan=\"2\">Instance</td><td rowspan=\"2\">Size</td><td rowspan=\"2\">Baseline(s)</td><td colspan=\"2\">Our(s)</td><td rowspan=\"2\">Reduction</td></tr><tr><td>Model</td><td>Solver</td></tr><tr><td>I1</td><td>17,495</td><td>88.01</td><td>1.77</td><td>30.25</td><td>32.02</td></tr><tr><td>I2</td><td>21,952</td><td>29.36</td><td>2.85</td><td>6.01</td><td>8.86</td></tr><tr><td>I3</td><td>23,810</td><td>61.24</td><td>3.25</td><td>32.88</td><td>36.13</td></tr><tr><td>I4</td><td>27,606</td><td>158.04</td><td>4.36</td><td>137.77</td><td>142.13</td></tr><tr><td>I5</td><td>28,672</td><td>89.89</td><td>4.78</td><td>70.95</td><td>75.73</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>40.05%</td></tr></table> First, our method achieves a substantial reduction in total runtime for all test cases, with an average runtime reduction of \\(40.05\\%\\) . Take I1 as an example, the plain solver requires \\(88.01\\mathrm{s}\\) to solve the problem, but by combining with our model, the new solver produces results in only 32.02s, reducing runtime by \\(63.62\\%\\) . Second, our model only takes a few seconds to obtain embeddings, occupying less than \\(10\\%\\) of overall runtime on average. It should be noted that our Deep Gate2 is able to infer within polynomial time that is only proportional to the size of instance. Third, while the two largest instances I4 and I5 show less reduction than the others, it does not necessarily mean that our model is unable to generalize to larger instances. As evidenced by the results for I2, an instance with a similar size to I4 and I5 also demonstrates a significant reduction. The reduction caused by our model should be determined by the characteristics of instance. In summary, our model is effective in speeding up downstream SAT solving. ## VI. CONCLUSION This paper introduces Deep Gate2, a novel functionally- aware framework for circuit representation learning. Our approach leverages the pairwise truth table differences of logic gates as a supervisory signal, providing rich functionality supervision and proving scalable for large circuits. Moreover, Deep Gate2 differentiates and concurrently updates structural and functional embeddings in two dedicated flows, acquiring comprehensive representations through a single round of GNN forward message- passing. In comparison to its predecessor, Deep Gate2 demonstrates enhanced performance in logic probability prediction and logic equivalent gate identification, while simultaneously improving model efficiency tenfold. The applications of Deep Gate2 onto multiple downstream tasks further demonstrate its effectiveness and potential utility in the EDA field.",
    "c_comparison_with_other_models_on_logic_equivalence_gates_identification_this_section_compares_the_functionality-_aware_accuracy_as_defined_in_section_iv-_a2_of_deep_gate2_with_that_of_two_other_models_deep_gate_7_and_fgnn_8_the_deep_gate_7_model_treats_the_logic_probability_as_supervision_since_it_contains_the_statistical_information_of_truth_table_the_fgnn_8_is_trained_to_differentiate_between_logic_equivalent_and_inequivalent_circuits_using_contrastive_learning_table_ii_presents_the_performance_of_three_models_on_the_task_of_logic_equivalence_gates_identification_firstly_our_proposed_approach_outperforms_the_other_two_models_on_all_circuits_with_an_average_f1-_score_of_09434_while_deep_gate_and_fgnn_only_achieve_f1-_score_06778_and_04402_respectively_for_instance_in_circuit_d7_our_proposed_functionality-_aware_circuit_learning_approach_achieves_an_f1-_score_of_09831_and_accurately_identifies_9915_of_logic_equivalence_gate_pairs_with_a_precision_of_9748_indicating_a_low_false_positive_rate_in_contrast_deep_gate_only_achieves_an_f1-_score_of_06778_while_fgnn_fails_on_most_of_the_pairs_secondly_although_deep_gate_has_an_average_recall_of_9146_its_precision_is_only_5400_indicating_a_large_number_of_false_positive_identifications_this_is_because_deep_gate_can_only_identify_logic_equivalent_pairs_by_predicting_logic_probability_which_leads_to_incorrect_identification_of_gate_pairs_with_similar_logic_probability_according_to_our_further_experiment_in_8083_of_false_positive_pairs_the_model_incorrectly_identifies_gate_pairs_with_similar_logic_probability_as_functionally_equivalent_thirdly_fgnn_achieves_the_lowest_performance_among_the_other_models_with_only_04402_f1-_score_the_poor_performance_of_fgnn_is_attributed_to_the_lack_of_effective_supervision_while_fgcn_learns_to_identify_logic_equivalence_circuits_generated_by_perturbing_local_structures_slightly_the_model_tends_to_consider_circuits_with_similar_structures_to_have_the_same_functionality_however_in_the_validation_dataset_and_practical_applications_two_circuits_may_have_the_same_function_even_if_their_topological_structures_are_extremely_different_therefore_the_self-_supervised_approach_limits_the_effectiveness_of_fgnn_in_identifying_logic_equivalence_gates_d_effectiveness_of_pi_encoding_strategy_to_demonstrate_the_effectiveness_of_our_proposed_pi_encoding_pie_strategy_we_trained_another_model_without_assigning_unique_identifications_for_pis_which_we_refer_to_as_wo_pie_the_results_are_presented_in_table_iii_which_show_that_disabling_the_pie_reduces_the_f1-_score_of_identifying_logic_equivalence_gates_from_09434_to_07541_resulting_in_an_average_reduction_of_2007_such_reduction_can_be_attributed_to_the_fact_that_as_demonstrated_as_the_failure_case_in_section_ii_and_fig_1_the_one-_round_gnn_model_without_the_pie_strategy_cannot_model_the_structural_information_of_the_circuit_more_specifically_the_accuracy_of_the_reconvergence_structure_identification_task_with_w_pie_model_is_9322_while_the_wo_model_only_achieve_7456_the_functionality_of_logic_gate_is_affected_by_both_functionality_of_fan-_in_gates_and_whether_there_is_reconvergence_between_its_fan-_in_gates_once_the_reconvergence_structure_cannot_be_accurately_identified_node_functionality_cannot_be_modeled_accurately_e_effectiveness_of_training_strategies_to_investigate_the_effectiveness_of_our_multi-_stage_training_strategy_we_train_another_model_noted_as_wo_multi-_stage_model_with_all_loss_functions_in_only_one_stage_instead_of_adding_the_functionality-_aware_loss_function_in_the_second_stage_the_original_model_with_multiple_stages_training_strategy_is_noted_as_w_multi-_stage_model_the_w_multi-_stage_model_learn_to_predict_the_logic_probability_and_structural_correlation_in_the_first_stage_and_learn_the_more_difficult_task_which_predicts_the_functionality_in_the_second_stage_the_results_are_shown_in_table_iv_where_the_model_w_multi-_stage_achieves_an_f1-_score_of_09434_on_average_and_the_model_wo_multi-_stage_achieves_only_07137_we_analyze_the_reason_as_follows_the_cost_of_comparing_each_pair_of_logic_gates_in_the_task_of_learning_functionality_is_extremely_high_which_is_proportional_to_the_square_of_the_circuit_size_we_limit_the_dataset_and_train_the_model_to_learn_functional_similarity_only_among_pairs_with_similar_logic_probability_which_is_a_necessary_condition_for_functional_equivalence_therefore_without_the_staged_multi-_stage_strategy_be_effectively_supervised_with_the_simplified_dataset_leading_to_poor_performance_in_learning_functionality_as_shown_in_table_v_the_differences_between_the_two_models_in_the_loss_values_for_predicting_logic_probability_l_prob_and_identifying_reconvergence_structures_l_rc_are_not_significant_indicating_that_they_perform_similarly_in_these_two_tasks_however_compared_to_the_wo_multi-_stage_model_the_w_multi-_stage_model_performs_better_in_learning_functionality_with_l_func_00594_which_is_5147_smaller_than_that_of_wo_multi-_stage_model_however_the_w_multi-_stage_model_outperforms_the_model_wo_multi-_stage_in_learning_functionality_task_with_a_significantly_lower_l_func_value_of_00594_which_is_5147_smaller_than_that_of_the_latter_v_downstream_tasks_in_this_section_we_combine_our_deep_gate2_with_the_open-_source_eda_tools_and_apply_our_model_to_practical_eda_tasks_logic_synthesis_and_boolean_satisfiability_sat_solving_the_logic_synthesis_tools_aim_to_identify_logic_equivalence_gates_as_quickly_as_possible_in_section_v-_a_our_proposed_functionality-_aware_circuit_learning_model_provides_guidance_to_the_logic_synthesis_tool_about_the_logic_similarity_additionally_in_section_v-_b_we_apply_the_learnt_functional_similarity_in_sat_solving_where_the_variables_with_dissimilar_functionality_are_assigned_the_same_decision_value_this_approach_efficiently_shrinks_the_search_space_by_enabling_solvers_to_encounter_more_constraints_a_logic_synthesis_this_subsection_shows_the_effectiveness_of_our_proposed_functionality-_aware_circuit_learning_framework_in_sat-_sweeping_28_a_common_technique_of_logic_synthesis_fig_5_illustrates_the_components_of_a_typical_ecosystem_for_sat-_sweeping_engine_also_called_sat_sweeper_where_including_equivalence_class_ec_manager_sat-_sweeping_manager_simulator_and_sat_solver_all_computations_are_coordinated_by_the_sat-_sweeping_manager_29_the_sat_sweeper_starts_by_computing_candidate_ecs_using_several_rounds_of_initial_simulation_and_storing_ecs_into_ec_manager_in_the_next_step_the_sat-_sweeping_manager_selects_two_gates_within_an_ec_and_then_calls_the_sat_solver_to_check_whether_they_are_equivalent_if_so_the_ec_manager_merges_these_two_gates_otherwise_sat_solver_will_return_a_satisfiable_assignment_as_a_counterexample_for_incremental_simulation_to_refine_the_candidate_ecs_to_the_best_of_our_knowledge_most_sat-_sweeping_managers_select_ec_only_based_on_the_circuit_structure_without_efficient_heuristic_strategy_considering_the_functionality_of_candidate_gates_we_will_introduce_the_functional_information_into_sat-_sweeping_manager_to_further_improve_efficiency_1_experiment_settings_we_combine_our_deep_gate2_into_sat_sweeper_to_guide_ec_selection_to_be_specific_the_updated_manager_sorts_all_candidate_equivalence_classes_by_computing_the_cosine_similarity_of_their_embeddings_unlike_traditional_sat_sweepers_our": "TABLE II PERFORMANCE OF DIFFERENT MODELS ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">Size</td><td colspan=\"3\">Func Model</td><td colspan=\"3\">Deep Gate</td><td colspan=\"3\">FGNN</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>19,485</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>94.59%</td><td>52.69%</td><td>0.6768</td><td>63.64%</td><td>41.18%</td><td>0.5000</td></tr><tr><td>D2</td><td>12,648</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>92.07%</td><td>52.98%</td><td>0.6726</td><td>60.87%</td><td>35.00%</td><td>0.4444</td></tr><tr><td>D3</td><td>14,686</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>93.25%</td><td>62.08%</td><td>0.7454</td><td>72.22%</td><td>44.83%</td><td>0.5532</td></tr><tr><td>D4</td><td>7,104</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>90.11%</td><td>46.33%</td><td>0.6120</td><td>62.73%</td><td>23.53%</td><td>0.3422</td></tr><tr><td>D5</td><td>37,279</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>93.69%</td><td>56.72%</td><td>0.7066</td><td>64.00%</td><td>69.57%</td><td>0.6667</td></tr><tr><td>D6</td><td>37,383</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>93.23%</td><td>60.49%</td><td>0.7337</td><td>59.09%</td><td>46.67%</td><td>0.5215</td></tr><tr><td>D7</td><td>10,957</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>88.89%</td><td>48.83%</td><td>0.6303</td><td>36.36%</td><td>23.53%</td><td>0.2857</td></tr><tr><td>D8</td><td>3,183</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>82.86%</td><td>49.90%</td><td>0.6229</td><td>62.63%</td><td>22.22%</td><td>0.3280</td></tr><tr><td>D9</td><td>27,820</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>92.51%</td><td>48.77%</td><td>0.6387</td><td>62.50%</td><td>26.79%</td><td>0.3750</td></tr><tr><td>D10</td><td>40,496</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>93.35%</td><td>61.22%</td><td>0.7395</td><td>47.02%</td><td>32.63%</td><td>0.3853</td></tr><tr><td>Avg.</td><td></td><td>98.73%</td><td>90.49%</td><td>0.9434</td><td>91.46%</td><td>54.00%</td><td>0.6778</td><td>59.11%</td><td>36.60%</td><td>0.4402</td></tr></table> TABLE III PERFORMANCE COMPARISON BETWEEN W/PIE AND W/O PIE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ PIE</td><td colspan=\"3\">w/o PIE</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>70.66%</td><td>64.66%</td><td>0.6753</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>87.20%</td><td>78.57%</td><td>0.8266</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>79.15%</td><td>76.21%</td><td>0.7765</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>87.91%</td><td>59.70%</td><td>0.7111</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>79.23%</td><td>76.73%</td><td>0.7796</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>87.10%</td><td>77.56%</td><td>0.8205</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>84.62%</td><td>64.13%</td><td>0.7296</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>74.29%</td><td>83.87%</td><td>0.7879</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>62.27%</td><td>76.27%</td><td>0.6856</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>87.07%</td><td>65.54%</td><td>0.7479</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7541</td></tr></table> TABLE IV PERFORMANCE COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ multi-stage</td><td colspan=\"3\">w/o multi-stage</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>79.46%</td><td>67.68%</td><td>0.7310</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>74.39%</td><td>63.21%</td><td>0.6835</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>70.46%</td><td>62.78%</td><td>0.6640</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>73.63%</td><td>72.83%</td><td>0.7323</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>68.02%</td><td>77.19%</td><td>0.7232</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>71.46%</td><td>86.01%</td><td>0.7806</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>65.81%</td><td>67.46%</td><td>0.6662</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>71.43%</td><td>86.21%</td><td>0.7813</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>74.52%</td><td>63.17%</td><td>0.6838</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>75.10%</td><td>64.02%</td><td>0.6912</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7137</td></tr></table> TABLE V LOSS COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE <table><tr><td></td><td>w/o multi-stage</td><td>w/ multi-stage</td><td>Reduction</td></tr><tr><td>Lprob</td><td>0.0205</td><td>0.0207</td><td>-0.98%</td></tr><tr><td>Lrc</td><td>0.1186</td><td>0.1115</td><td>5.99%</td></tr><tr><td>Lfunc</td><td>0.1224</td><td>0.0594</td><td>51.47%</td></tr></table> proposed SAT sweeper does not need to validate the equivalence of all candidate ECs in one pass. Instead, node pairs with higher similarity have high priority for SAT solver calls. If the gate pair is formally proved to be equivalent, these two gate are merged. Otherwise, the generated counterexample should contain more conflicts than the <center>Fig. 5. The proposed SAT-sweeping ecosystem. </center> baseline method, resulting in better efficiency for refining candidate ECs. Our model is equipped into ABC [30] as a plug- in and integrated into the SAT sweeper '&frag' [14], which is one of the most efficient and scalable SAT sweeper publicly available at this time. The AIGs derived by merging the resulting equivalence nodes are verified by '&ecc' command in ABC to ensure functional correctness. All experiments are performed on a \\(2.40\\mathrm{GHz}\\) Intel(R) Xeon(R) Silver 4210R CPU with 64GB of main memory. A single core and less than 1GB was used for any test case considered in this subsection. The proposed SAT sweeper (named as Our) is compared against the original engine, &frag. 2) Results: We validate the performance of our SAT sweeper with 6 industrial circuits. As shown in Table VI, section \"Statistics\" lists the number of PI and PO (PI/PO), logic levels (Lev) and internal AND-nodes in the original AIG (And). To ensure the fairness of comparison, the circuits after sweeping should have the same size. Section \"SAT calls\" lists the number of satisfiable SAT calls, performed by the solver employed in each engine. The data shows that our proposed engine decreases the number of satisfiable SAT calls, that explains why it has better results, since more resource are used to prove equivalence gates. In addition, section \"Total runtime\" compares the runtime and section \"Red.\" shows the runtime reduction from &frag to Our. The number of \"SAT calls\" can get an average reduction of \\(53.37\\%\\) (95.88% maximum) through the integration of our Deep Gate2 model. As for \"Total runtime\", this experiment shows that the Deep Gate2- based SAT sweeper outperforms state- of- the- art engines, while reducing the average runtime by \\(49.46\\%\\) (57.77% maximum). Thus, the sweeper formally verifies the equivalence of functionally similar gates\n\nTABLE VI COMPARING THE NUMBER OF SAT CALLS AND THE RUNTIME OF THE SAT SWEEPERS <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"2\">Statistic</td><td colspan=\"2\">SAT calls</td><td colspan=\"2\">Total Runtime (s)</td></tr><tr><td>PI/PO</td><td>Lev</td><td>And</td><td>#frag</td><td>Our Red.</td><td>#frag</td></tr><tr><td>C1</td><td>128/128</td><td>4,372</td><td>57,247</td><td>13,826</td><td>570</td><td>95.88%</td></tr><tr><td>C2</td><td>24/25</td><td>225</td><td>5,416</td><td>100</td><td>74</td><td>26.00%</td></tr><tr><td>C3</td><td>22/1</td><td>29</td><td>703</td><td>4</td><td>1</td><td>75.00%</td></tr><tr><td>C4</td><td>114/1</td><td>91</td><td>19,354</td><td>20</td><td>12</td><td>40.00%</td></tr><tr><td>C5</td><td>126/1</td><td>83</td><td>20,971</td><td>6</td><td>4</td><td>33.33%</td></tr><tr><td>C6</td><td>96/1</td><td>79</td><td>14,389</td><td>10</td><td>5</td><td>50.00%</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>53.37%</td><td>49.46%</td></tr></table> with the guidance from Deep Gate2, thereby reducing the number of invalid SAT calls and improving efficiency of SAT sweeping. Take C1 as a representative example, the baseline &frag selects gates in EC for formal verification without considering their behaviour, thus, many solver calls return satisfiable results, and few gates can be merged. However, with the guidance of Deep Gate2, the sweeper can prioritize the selection of gates with similar behavior, resulting in a significant reduction of \\(95.88\\%\\) in SAT calls and \\(57.77\\%\\) in runtime. ### B. Boolean Satisfiability Solving Boolean satisfiability (SAT) solving is a long- standing and fundamental NP- complete problem with applications in many areas, especially in electronic design automation (EDA) [31]- [33]. The existing SAT solvers are designed to incorporate efficient heuristics [34]- [36] to expedite the solving process. For instance, [34] proposes to utilize the correlation of logic gate functionality to enforce variable decision for solving circuit- based SAT instances. Although the solution achieves remarkable speedup over SAT solvers, it still relies on the time- consuming logic simulation to obtain the functionality. Based on [34], we demonstrate how the Deep Gate2 models functional correlation efficiently and accelerates SAT solving. 1) Experiment Settings: We integrate our Deep Gate2 into a modern SAT solver, Ca Di Cal [15] to solve the instances from logic equivalence checking (LEC) task. Firstly, we obtain gate-level embeddings of the original circuit and predict the pairwise functional similarity between these gates. Given the one-to-one mapping [37] between circuits and conjunctive normal form (a problem format required by SAT solvers), we can easily transfer the gate functional similarity to variable behavioral similarity. If two logic gates have similar representations (and therefore similar functionality), their corresponding variables should be correlated and grouped together during the variable decision process. Secondly, we incorporate the learnt knowledge into the SAT solver. As shown in Algorithm 1, when the current variable \\(s\\) is assigned a value \\(v\\) , we identify all unassigned variables \\(s'\\) in the set \\(S\\) that contains correlated variables with \\(s\\) . As modern SAT solvers reduce searching space by detecting conflicts as much as possible [38], we assign the reverse value \\(\\bar{v}\\) to \\(s'\\) to promptly cause conflict for joint decision. Besides, the threshold \\(\\delta\\) in Algorithm 1 is set to \\(1e - 5\\) . Thirdly, to evaluate the efficacy of our model in accelerating SAT solving, we compare the aforementioned hybrid solver (labeled as Our) with original Ca Di Cal [15] (labeled as Baseline) on 5 industrial instances. All experiments are conducted with a single 2.40GHz Intel(R) Xeon(R) E5- 2640 v4 CPU. 2) Results: The runtime comparison between Baseline and Our are listed in Table VII. To ensure a fair comparison, we aggregate the Deep Gate2 model inference time (Model) and SAT solver runtime (Solver) as the Overall runtime. We have the following observations. Algorithm 1 Variable Decision Function with Deep Gate2 Current variable \\(s\\) just being assigned a value \\(v\\) ; Set \\(S\\) containing the correlated variables with \\(s\\) ; Function \\(\\mathrm{Sim}(s_i,s_j)\\) to calculate the behaviour similarity of two variables; \\(\\delta\\) is the threshold for the joint decision Function \\(V(s)\\) to get the assigned value of variable \\(s\\) ; Function Decision \\((s,v)\\) to assign the decision value \\(v\\) to the current decision variable \\(s\\) . 1: Decision \\((s,v)\\) 2: for \\(s'\\) in \\(S\\) do 3: if \\(s' \\neq s\\) and \\(V(s') = \\text{None}\\) then 4: if \\(\\mathrm{Sim}(s',s) > 1 - \\delta\\) then 5: Decision \\((s',\\bar{v})\\) 6: end if 7: end if 8: end for TABLE VII COMPARING THE RUNTIME BETWEEN BASELINE AND OUR SOLVERS <table><tr><td rowspan=\"2\">Instance</td><td rowspan=\"2\">Size</td><td rowspan=\"2\">Baseline(s)</td><td colspan=\"2\">Our(s)</td><td rowspan=\"2\">Reduction</td></tr><tr><td>Model</td><td>Solver</td></tr><tr><td>I1</td><td>17,495</td><td>88.01</td><td>1.77</td><td>30.25</td><td>32.02</td></tr><tr><td>I2</td><td>21,952</td><td>29.36</td><td>2.85</td><td>6.01</td><td>8.86</td></tr><tr><td>I3</td><td>23,810</td><td>61.24</td><td>3.25</td><td>32.88</td><td>36.13</td></tr><tr><td>I4</td><td>27,606</td><td>158.04</td><td>4.36</td><td>137.77</td><td>142.13</td></tr><tr><td>I5</td><td>28,672</td><td>89.89</td><td>4.78</td><td>70.95</td><td>75.73</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>40.05%</td></tr></table> First, our method achieves a substantial reduction in total runtime for all test cases, with an average runtime reduction of \\(40.05\\%\\) . Take I1 as an example, the plain solver requires \\(88.01\\mathrm{s}\\) to solve the problem, but by combining with our model, the new solver produces results in only 32.02s, reducing runtime by \\(63.62\\%\\) . Second, our model only takes a few seconds to obtain embeddings, occupying less than \\(10\\%\\) of overall runtime on average. It should be noted that our Deep Gate2 is able to infer within polynomial time that is only proportional to the size of instance. Third, while the two largest instances I4 and I5 show less reduction than the others, it does not necessarily mean that our model is unable to generalize to larger instances. As evidenced by the results for I2, an instance with a similar size to I4 and I5 also demonstrates a significant reduction. The reduction caused by our model should be determined by the characteristics of instance. In summary, our model is effective in speeding up downstream SAT solving. ## VI. CONCLUSION This paper introduces Deep Gate2, a novel functionally- aware framework for circuit representation learning. Our approach leverages the pairwise truth table differences of logic gates as a supervisory signal, providing rich functionality supervision and proving scalable for large circuits. Moreover, Deep Gate2 differentiates and concurrently updates structural and functional embeddings in two dedicated flows, acquiring comprehensive representations through a single round of GNN forward message- passing. In comparison to its predecessor, Deep Gate2 demonstrates enhanced performance in logic probability prediction and logic equivalent gate identification, while simultaneously improving model efficiency tenfold. The applications of Deep Gate2 onto multiple downstream tasks further demonstrate its effectiveness and potential utility in the EDA field.",
    "references_1_j_chen_j_kuang_g_zhao_d_j-_h_huang_and_e_f_young_pros_a_plug-_in_for_routability_optimization_applied_in_the_state-_of-_the-_art_commercial_eda_tool_using_deep_learning_in_proceedings_of_the_39th_international_conference_on_computer-_aided_design_2020_pp_1-_8_2_a_mirhoseini_a_goldie_m_yazgan_j_w_jiang_e_songhori_s_wang_y-_j_lee_e_johnson_o_pathak_a_nazi_et_al_a_graph_placement_methodology_for_fast_chip_design_nature_vol_594_no_7862_pp_207-_212_2021_3_w_l_neto_m_austin_s_temple_l_amaru_x_tang_and_p-_e_gaillardon_losaic_a_logic_synthesis_framework_driven_by_artificial_intelligence_in_2019_ieeeacm_international_conference_on_computer-_aided_design_iccad_ieee_2019_pp_1-_6_4_w_haaswijk_e_collins_b_seguin_m_soeken_f_kaplan_s_susstrunk_and_g_de_micheli_deep_learning_for_logic_optimization_algorithms_in_2018_ieee_international_symposium_on_circuits_and_systems_iscas_ieee_2018_pp_1-_4_5_z_shi_m_li_s_khan_l_wang_n_wang_y_huang_and_q_xu_deeptpi_test_point_insertion_with_deep_reinforcement_learning_in_2022_ieee_international_test_conference_itc_ieee_2022_pp_194-_203_6_j_huang_h-_l_zhen_n_wang_h_mao_m_yuan_and_y_huang_neural_fault_analysis_for_sat-_based_atpg_in_2022_ieee_international_test_conference_itc_ieee_2022_pp_36-_45_7_m_li_s_khan_z_shi_n_wang_h_yu_and_q_xu_deepgate_learning_neural_representations_of_logic_gates_in_proceedings_of_the_59th_acmieee_design_automation_conference_2022_pp_667-_672_8_z_wang_c_bai_z_he_g_zhang_q_xu_t-_y_ho_b_yu_and_y_huang_functionality_matters_in_netlist_representation_learning_in_proceedings_of_the_59th_acmieee_design_automation_conference_2022_pp_61-_66_9_k_zhu_h_chen_w_j_turner_g_f_kokai_p-_h_wei_d_z_pan_and_h_ren_tag_learning_circuit_spatial_embedding_from_layouts_in_proceedings_of_the_41st_ieeeacm_international_conference_on_computer-_aided_design_2022_pp_1-_9_10_y_lai_y_mu_and_p_luo_maskplace_fast_chip_placement_via_reinforced_visual_representation_learning_ar_xiv_preprint_ar_xiv221113382_2022_11_a_fayyazi_s_shababi_p_nuzzo_s_nazarian_and_m_pedram_deep_learning-_based_circuit_recognition_using_sparse_mapping_and_level-_dependent_decaying_sum_circuit_representations_in_2019_design_automation_test_in_europe_conference_exhibition_date_ieee_2019_pp_638-_641_12_z_he_z_wang_c_bail_h_yang_and_b_yu_graph_learning-_based_arithmetic_block_identification_in_2021_ieeeacm_international_conference_on_computer_aided_design_iccad_ieee_2021_pp_1-_8_13_m_li_z_shi_q_lai_s_khan_and_q_xu_deepsat_an_eda-_driven_learning_framework_for_sat_ar_xiv_preprint_ar_xiv220513745_2022_14_a_mishchenko_s_chatterjee_r_jiang_and_r_k_brayton_fraigs_a_unifying_representation_for_logic_synthesis_and_verification_erl_technical_report_tech_rep_2005_15_s_d_queue_cadical_at_the_sat_race_2019_sat_race_2019_p_8_2019_16_t_brown_b_mann_n_ryder_m_subbiah_j_d_kaplan_p_dhariwal_a_neelakantan_p_shyam_g_sastry_a_askell_et_al_language_models_are_few-_shot_learners_advances_in_neural_information_processing_systems_vol_33_pp_1877-_1901_2020_17_j_devlin_m-_w_chang_k_lee_and_k_toutanova_bert_pre-_training_of_deep_bidirectional_transformers_for_language_understanding_ar_xiv_preprint_ar_xiv181004805_2018_18_z_wu_y_xiong_s_x_yu_and_d_lin_unsupervised_feature_learning_via_non-_parametric_instance_discrimination_in_proceedings_of_the_ieee_conference_on_computer_vision_and_pattern_recognition_2018_pp_3733-_3742_19_e_hoffer_and_n_ailon_deep_metric_learning_using_triplet_network_in_similarity-_based_pattern_recognition_third_international_workshop_simbad_2015_copenhagen_denmark_october_12-_14_2015_proceedings_3_springer_2015_pp_84-_92_20_a_vaswani_n_shazeer_n_parmar_j_uszkoreit_l_jones_a_n_gomez_l_kaiser_and_i_polosukhin_attention_is_all_you_need_advances_in_neural_information_processing_systems_vol_30_2017_21_y_bengio_j_louradour_r_collobert_and_j_weston_curriculum_learning_in_proceedings_of_the_26th_annual_international_conference_on_machine_learning_2009_pp_41-_48_22_s_ruder_an_overview_of_multi-_task_learning_in_deep_neural_networks_ar_xiv_preprint_ar_xiv170605098_2017_23_s_davidson_characteristics_of_the_itc99_benchmark_circuits_in_itsw_1999_24_c_albrecht_iwls_2005_benchmarks_in_iwls_2005_25_l_amar\u00fa_p-_e_gaillardon_and_g_de_micheli_the_epfl_combinational_benchmark_suite_in_iwls_no_conf_2015_26_o_team_opencores_httpsopencoresorg_27_d_p_kingma_and_j_ba_adam_a_method_for_stochastic_optimization_ar_xiv_preprint_ar_xiv14126980_2014_28_a_kuehlmann_v_paruthi_f_krohm_and_m_k_ganai_robust_boolean_reasoning_for_equivalence_checking_and_functional_property_verification_ieee_transactions_on_computer-_aided_design_of_integrated_circuits_and_systems_vol_21_no_12_pp_1377-_1394_2002_29_a_mishchenko_and_r_brayton_integrating_an_aig_package_simulator_and_sat_solver_in_international_workshop_on_logic_and_synthesis_iwls_2018_pp_11-_16_30_b_l_synthesis_and_v_group_abc_a_system_for_sequential_synthesis_and_verification_httpwwwcadeecsberkeleyedualanmiabc_2023_31_e_i_goldberg_m_r_prasad_and_r_k_brayton_using_sat_for_combinational_equivalence_checking_in_proceedings_design_automation_and_test_in_europe_conference_and_exhibition_2001_ieee_2001_pp_114-_121_32_k_l_mc_millan_interpolation_and_sat-_based_model_checking_in_computer_aided_verification_15th_international_conference_cav_2003_boulder_co_usa_july_8-_12_2003_proceedings_15_springer_2003_pp_1-_13_33_k_yang_k-_t_cheng_and_l-_c_wang_trangen_a_sat-_based_atpg_for_path-_oriented_transition_faults_in_asp-_dac_2004_asia_and_south_pacific_design_automation_conference_2004_ieee_cat_no_04ex753_ieee_2004_pp_92-_97_34_f_lu_l-_c_wang_k-_t_cheng_and_r-_y_huang_a_circuit_sat_solver_with_signal_correlation_guided_learning_in_2003_design_automation_and_test_in_europe_conference_and_exhibition_ieee_2003_pp_892-_897_35_g_audemard_and_l_simon_glucose_a_solver_that_predicts_learnt_clauses_quality_sat_competition_pp_7-_8_2009_36_on_the_glucose_sat_solver_international_journal_on_artificial_intelligence_tools_vol_27_no_01_p_1840001_2018_37_g_s_tseitin_on_the_complexity_of_derivation_in_propositional_calculus_automation_of_reasoning_2_classical_papers_on_computational_logic_1967-_1970_pp_466-_483_1983_38_j_marques-_silva_i_lynce_and_s_malik_conflict-_driven_clause_learning_sat_solvers_in_handbook_of_satisfiability_ios_press_2021_pp_133-_182": ""
  },
  "section_objects": [
    {
      "heading": "DeepGate2 Functionality Aware Circuit Representati",
      "content": "## Introduction\n\n\nin Section III. We compare Deep Gate2 with the original Deep Gate and another functionality- aware solution [8] in Section IV. Next, we apply Deep Gate2 onto several downstream tasks in Section V. Finally, Section VI concludes this paper. ## II. RELATED WORK ### A. Circuit Representation Learning A prominent trend in the deep learning community is to learn a general representation from data first and then apply it to various downstream tasks, for example, GPT [16] and BERT [17] learn representations of natural language text that can be fine- tuned for a wide range of natural language processing tasks. Circuit representation learning has also emerged as an attractive research direction, which falls into two categories: structure- aware circuit representation learning [9], [11], [12] and functionality- aware circuit representation learning [7], [8]. Since a circuit can be naturally formulated as a graph, with gates as nodes and wires as edge, the GNN is a powerful tool to capture the interconnections of logic gates and becomes a backbone model to learn circuit representations. For example, TAG [9] is a GNN- based model designed for analog and mixed- signal circuit representation learning and applied for several physical design applications, such as layout matching prediction, wirelength estimation, and net parasitic capacitance prediction. ABGNN [12] learns the representation of digital circuits and handles the arithmetic block identification task. However, these models tend to focus on structural encoding and are not suitable for functionality- related tasks. Consequently, the functionality- aware circuit representation learning frameworks [7], [8] are designed to learn the underlying circuit functionality. For instance, FGNN [8] learns to distinguishes between functionally equivalent and inequivalent circuits by contrastive learning [18]. However, such self- supervised manner relies on data augmentation by perturbing the original circuit to logic equivalence circuit. If the perturbation is not strong and diverse, the model still identifies the functional equivalence circuits based on the invariant local structure, resulting a low generalization ability on capturing underlying functionality. Deep Gate [7] leverages logic- 1 probability under random simulation as supervision, which approximates the statistic of the most direct representation of functionality, i.e. truth table. Despite achieving remarkable progress on testability analysis [5], there are limitations that affect the generalizability of Deep Gate to other EDA tasks. We will elaborate on Deep Gate in the next subsection. ### B. Deep Gate Framework Deep Gate [7] is the first circuit representation learning framework that embeds both structural and functional information of digital circuits. The model pre- processes the input circuits into a unified And- Inverter Graph (AIG) format and obtains rich gate- level representations, which can be applied to various downstream tasks. Deep Gate treats the logic- 1 probability as supervision to learn the functionality. Additionally, the Deep Gate consists of a GNN equipped with an attention- based aggregation function that propagates information of gates in levelized sequential manner. The aggregation function learns to assign high attention weights to controlling fan- in of gates (e.g. the fan- in gate with logic- 0 is the controlling fan- in of AND gate) that mimics the logic computation process. Although it has been applied to testability analysis [5] and SAT problem [13], we argue that the model still encounters with two major shortcomings limiting its generalization ability. <center>Fig. 1. An example of reconvergence structure </center> First, logic probability is not an appropriate supervision for learning functionality. The most direct representation of functionality is the truth table, however, using it as a training label is impractical due to the immeasurable computational overhead. Deep Gate proposes to supervise the model by utilizing the proportion of logic- 1 in the truth table and approximate this proportion as the logic probability through random simulation. However, logic probability is only a statistical information of functionality, indicating the number of logic- 1 values in the truth table rather than which PI assignments lead to logic- 1. Consequently, Deep Gate cannot differentiate the functional difference between two circuits if they have the same probability. Second, Deep Gate is not efficient enough to deal with large circuit. Specifically, Deep Gate requires to perform forward and backward message- passing operations for 20 rounds to embed rich representations. Fig. 1 illustrates the need of this multi- round GNN design in Deep Gate where the nodes in grey color represent PIs. The incoming messages of nodes 5, 6, 5, and 6 during forward propagation are noted in the figure, where \\(h_i\\) is the embedding vector of node \\(i\\) . Since, Deep Gate uses the same initial embeddings for all nodes, the messages of nodes 5, 6, 5, and 6 in the first forward propagation round are identical. Thus, the model can only distinguish node embeddings based on their connections by repeatedly updating PIs through multiple rounds of forward and backward message propagation. We emphasize that the limitations of Deep Gate comes from the lack of effective supervision and weak model design where the unique identification of all PIs are ignored. To address these issues, we propose an efficient one- round GNN design that maintains the unique identification of PIs and uses the pairwise truth- table difference of two gates as an effective supervision. ## III. METHODOLOGY ### A. Problem Formulation The circuit representation learning model aims to map both circuit structure and functionality into embedding space, where the structure represents the connecting relationship of logic gates and the functionality means the logic computational mapping from inputs to outputs. We conclude that the previous models still lack of ability to capture functional information. In this paper, we propose to improve the previous Deep Gate model [7] to represent circuits with similar functionality with the similar embedding vectors. In other words, these circuit representations should have short distance in the embedding space. We take Circuit A, B, C, and D as examples in Fig. 2, where all of them have similar topological structures. Since Circuit A, B and C perform with the same logic probability, Deep Gate [7] tends to produce the similar embeddings for these three circuits. Hence, it is hard to identify the logic equivalent circuits by Deep Gate. Although FGNN [8] is trained to classify logical equivalence and inequivalence circuits by contrastive learning, they cannot differentiate the relative similarity. As shown in the embedding space, the distance between\n\n<center>Fig. 2. Problem statement: the embedding vectors should be close if circuit functions are similar </center> A and B is equal to the distance between A and D. Nonetheless, as indicated in the truth table, Circuit A is equivalent to Circuit C, similar to Circuit B (with only 2 different bits), but dissimilar to Circuit D (with 5 different bits). We expect that the model will bring together or separate the circuits in embedding space according to their truth tables. Therefore, the expected Deep Gate2 model not only identifies the logic equivalent nodes, but also predicts the functional similarity. Thus, we can apply such functionality- aware circuit learning model to provide benefits for the real- world applications. ### B. Dataset Preparation To train the circuit representation learning model, we need to find a supervision contains rich functional information and prepare an effective dataset at first. Truth table, which records the complete logic computational mapping, provides the most direct supervision. However, the length of the truth table increases exponentially with the number of primary inputs, and obtaining a complete truth table requires an immeasurable amount of time. Therefore, a reasonable supervision should be easily obtained and closely related to the truth table. Firstly, we use the Hamming distance between truth tables of two logic gates as supervision. That is, in a way similar to metric learning [19], we map nodes to an embedding space and hope that the distance of the embedding vectors is positive correlated with the Hamming distance of the truth table. Formally, we denote the truth table vector of node \\(i\\) is \\(T_{i}\\) and the embedding vector of node \\(i\\) is \\(h_{i}\\) . \\[distance(h_{i},h_{j})\\propto distance(T_{i},T_{j}) \\quad (1)\\] Secondly, to improve the data efficiency, we regard the each logic gate in circuit as a new circuit (logic cone) with the current gate as output and the original PIs as inputs. By parsing a single original circuit, we obtain a large number of new circuits. Therefore, the task of graph learning becomes the task of learning node- level representation, and the difficulty of data collection is reduced. Thirdly, to ensure the quality of sample pairs and limit the number of sample pairs, we impose the following constraints during sampling node pairs: (1) Two logic cones of the two nodes should have the same PI, which is a necessary condition for comparing the truth table difference. (2) The logic probability, which is the number of logic- 1 percentage in the truth table, should be similar (distance within \\(5\\%\\) ). This is because if the logic probability of two nodes is not consistent, their functions are definitely not consistent. If the logic probability of two nodes is consistent, their functions may be consistent. (3) The difference in logic levels between two nodes should be within 5, because when the two nodes are far apart, their functions are unlikely to be correlated. (4) We only consider the extreme cases, namely, the difference between truth tables is within \\(20\\%\\) or above \\(80\\%\\) . We do not perform the complete simulation, but set a maximum simulation time to obtain the response of each node as an incomplete truth table. It should be noted that we utilize the And- Inverter Graph (AIG) as the circuit netlist format, which is only composed of AND gate and NOT gate. Any other logic gates, including OR, XOR and MUX, can be transformed into a combination of AND and NOT gates in linear time. ### C. Functionality-Aware Loss Function The primary objective of our purposed functionality- aware circuit learning model is to learn node embeddings, where two embedding vectors will be similar if the corresponding two node function are similar. As we sample node pairs \\(\\mathcal{N}\\) in the Section III- B, we can obtain the Hamming distance of truth table \\(D^{T}\\) of each node pair. \\[D_{(i,j)}^{T} = \\frac{Hamming Distance(T_{i},T_{j})}{length(T_{i})},(i,j)\\in \\mathcal{N} \\quad (2)\\] According to Eq. (1), the distance of embedding vectors \\(D^{H}\\) should be proportional to the Hamming distance of the truth table \\(D^{T}\\) . We define the distance of embedding vectors in Eq. (3), where is calculated based on cosine similarity. In other word, the similarity of embedding vectors \\(S_{(i,j)}\\) should be negative related to distance \\(D_{(i,j)}^{T}\\) . \\[\\begin{array}{l}{S_{(i,j)} = Cosine Similarity(h_i,h_j)}\\\\ {D_{(i,j)}^H = 1 - S_{(i,j)}} \\end{array} \\quad (3)\\] Therefore, the training objective is to minimize the difference between \\(D^{H}\\) and \\(D^{T}\\) . We purpose the functionality- aware loss function \\(L_{func}\\) as below. \\[\\begin{array}{l}{D_{(i,j)}^{T^{\\prime}} = Zero Norm(D_{(i,j)}^{T})}\\\\ {D_{(i,j)}^{H^{\\prime}} = Zero Norm(D_{(i,j)}^{H})}\\\\ {L_{func} = \\sum_{(i,j)\\in \\mathcal{N}}(L1Loss(D_{(i,j)}^{T^{\\prime}},D_{(i,j)}^{H^{\\prime}}))} \\end{array} \\quad (4)\\] ### D. One-round GNN Model In this subsection, we propose a GNN model that can capture both functional and structural information for each logic gate through one- round forward propagation. First, we propose to separate the functional embeddings \\(hf\\) and structural embeddings \\(hs\\) , and initialize them in difference ways. We assign the uniform initial functional embeddings to primary inputs (PI), as they all have equivalent logic probability under random simulation. However, we design a PI encoding (PIE) strategy by assigning a unique identification to each PI as its initial structural embedding. Specifically, the initial PI structural embeddings \\(hs_{i}, i \\in PI\\) are orthogonal vectors. This means that the dot product of any two PIs' embeddings is zero. Second, we design four aggregators: \\(agg_{AND}^{r}\\) aggregates the message for structural embedding \\(hs\\) of an AND gate, \\(agg_{AND}^{r}\\) aggregates the message for functional embedding \\(hf\\) of an AND\n\n<center>Fig. 3. One-round GNN propagation process </center> gate. \\(agg r_{NOT}^{s}\\) and \\(agg r_{NOT}^{f}\\) update \\(h s\\) and \\(h f\\) of a NOT gate, respectively. We implement each aggregator using the self- attention mechanism [20], as the output of a logic gate is determined by the controlling values of its fan- in gates. For example, an AND gate must output logic- 0 if any of its fan- in gates has logic- 0. By employing the attention mechanism, the model learns to assign greater importance to the controlling inputs [7]. As illustrated in Eq. (5), \\(w_{q}\\) , \\(w_{k}\\) and \\(w_{v}\\) are three weight matrices and \\(d\\) is the dimension of embedding vectors \\(h\\) . \\[\\begin{array}{l}\\alpha_{j} = softmax(\\frac{w_{q}^{\\top}h_{i}\\cdot(w_{k}^{\\top}h_{j})^{\\top}}{\\sqrt{d}})\\\\ m_{j} = w_{v}^{\\top}h_{j}\\\\ h_{i} = agg r(h_{j}|j\\in \\mathcal{P}(i)) = \\sum_{j\\in \\mathcal{P}(i)}(\\alpha_{j}*m_{j}) \\end{array} \\quad (5)\\] Third, during forward propagation, the structural embeddings are updated only with the structural embeddings of predecessors. As shown in Eq. (6), where the Gate \\(a\\) is AND gate, the Gate \\(b\\) is NOT gate. \\[\\begin{array}{l} h s_{a} = agg r_{A N D}^{s}(h s_{j}|j\\in \\mathcal{P}(a)) \\\\ h s_{b} = agg r_{N O T}^{s}(h s_{j}|j\\in \\mathcal{P}(b)) \\end{array} \\quad (6)\\] At the same time, the gate function is determined by the function and the structural correlations of the fan- in gates. Therefore, the functional embeddings are updated as Eq. (7). \\[\\begin{array}{l} h f_{a} = agg r_{A N D}^{f}([h s_{j},h f_{j}]|j\\in \\mathcal{P}(a))\\\\ h f_{b} = agg r_{N O T}^{f}([h s_{j},h f_{j}]|j\\in \\mathcal{P}(b)) \\end{array} \\quad (7)\\] Therefore, as shown in Fig. 3, the GNN propagation process performs from PI to PO level by level. For the node in level \\(l\\) , its structural embedding \\(h s_{L_{l}}\\) will be updated with the structural embeddings of the node in level \\(l - 1\\) . Additionally, the functional embedding \\(h f_{L_{l}}\\) will be updated with both structural embeddings \\(h s_{L_{l - 1}}\\) and functional embeddings \\(h f_{L_{l - 1}}\\) . The GNN propagation completes after processing \\(N\\) levels. ### E. Model Training Strategies To train the model, we employed multi- stage training strategy, similar to training a model with an easy task and then a harder task in curriculum learning [21]. During each stage, we trained the model with multiple supervisions in multi- task learning manner [22]. In the first stage, we train the one- round GNN model with two simple tasks. The Task 1 involves predicting the logic probability, while the Task 2 entails identifying the structural correlation. To achieve this, we readout the functional embedding \\(h f_{i}\\) to predict the logic probability \\(\\hat{P}_{i}\\) by a multi- layer perceptron (MLP), denoted as \\(MLP_{prob}\\) . In addition, we utilize the structural embeddings \\(h s_{i}\\) and \\(h s_{j}\\) to predict whether node \\(i\\) and node \\(j\\) can be reconvergent by \\(MLP_{rc}\\) . \\[\\begin{array}{r}\\hat{P}_i = MLP_{prob}(h f_i)\\\\ R_{\\langle i,j\\rangle} = MLP_{rc}(h s_i,h s_j) \\end{array} \\quad (8)\\] We define the loss function for Task 1 in Eq. (9), where the \\(P_{i}\\) is the ground truth logic probability obtained through random simulation. \\[L_{prob} = L1Loss(P_i,\\hat{P}_i) \\quad (9)\\] Besides, we define the loss function for Task 2 in Eq. (10). The binary ground truth, denoted as \\(R_{\\langle i,j\\rangle}\\) , indicates whether node pair \\(i\\) and \\(j\\) have a common predecessor. \\[L_{rc} = BCELoss(R_{\\langle i,j\\rangle},R_{\\langle i,j\\rangle}) \\quad (10)\\] Consequently, the loss function for Stage 1 is presented in Eq. (11), where the \\(w_{prob}\\) and \\(w_{rc}\\) are the weight for Task 1 and Task 2, respectively. \\[L_{stage1} = L_{prob}*w_{prob} + L_{rc}*w_{rc} \\quad (11)\\] The second training stage involves another more difficult Task 3. functionality- aware learning, as described in Section III- C. The loss function for Stage 2 is defined below, where \\(w_{func}\\) represents the loss weight of Task 3. \\[L_{stage2} = L_{prob}\\times w_{prob} + L_{rc}\\times w_{rc} + L_{func}\\times w_{func} \\quad (12)\\] Overall, the model can differentiate gates with varying probability in Stage 1. As the logic equivalent pairs only occur when nodes have the same probability, the model in Stage 2 learns to predicting the functional similarity within the probability equivalent class. The effectiveness of the above training strategies is demonstrated in Section IV- E. ## IV. EXPERIMENTS In this section, we demonstrate the ability of our proposed Deep Gate2 to learn functionality- aware circuit representations. Firstly, Section IV- A provides the preliminary of our experiments, including details on dataset preparation, evaluation metrics and model settings. Secondly, we compare the effectiveness and efficiency of our Deep Gate2 against Deep Gate [7] and FGNN [8] on two function- related tasks: logic probability prediction (see Section IV- B) and logic equivalence gates identification (see Section IV- C). Thirdly, we investigate the effectiveness of model design and training strategies in Section IV- D and Section IV- E, respectively. ### A. Experiment Settings 1) Dataset Preparation: We use the circuits in Deep Gate [7], which are extracted from ITC'99 [23], IWLS'05 [24], EPFL [25] and Open Core [26]. These circuits consists of 10,824 AIGs with sizes ranging from 36 to 3,214 logic gates. To obtain the incomplete truth table, we generate 15,000 random patterns and record the corresponding response. Following the data preparation method described in Section III-B, we construct a dataset comprising 894,151 node pairs. We create 80/20 training/test splits for model training and evaluation.\n\n<center>Fig. 4. Functionality-aware circuit learning framework </center> 2) Evaluation Metrics: We assess our Deep Gate2 with two tasks. The first task is to predict the logic probability for each logic gate. We calculate the average prediction error (PE) as Eq. (13), where the set \\(\\mathcal{V}\\) includes all logic gates. \\[PE = \\frac{1}{|\\mathcal{V}|}\\sum_{i\\in \\mathcal{V}}|P_{i} - \\hat{P}_{i}| \\quad (13)\\] The second task is to identify the logic equivalence gates within a circuit. A gate pair \\((i,j)\\) is considered as a positive pair if these two logic gates \\(i\\) and \\(j\\) have the same function, where the pairwise Hamming distance of truth tables \\(D_{(i,j)}^{T} = 0\\) . If the similarity \\(S_{(i,j)}\\) between these two embedding vectors \\(h f_{i}\\) and \\(h f_{j}\\) exceeds a certain threshold, the model will recognize the gate pair \\((i,j)\\) as equivalent. The optimal threshold \\(\\theta\\) is determined based on the receiver operating characteristic (ROC). The evaluation metrics is formally defined in Eq. (14), where \\(TP\\) , \\(TN\\) , \\(FP\\) , \\(FN\\) are true positive, true negative, false positive and false negative, respective, and \\(M\\) is the total number of gate pairs. In the following experiments, the performance on logic equivalence gates identification is measured in terms of Recall, Precision, F1- Score and area under curve (AUC). \\[\\begin{array}{rl} & {TP = \\frac{\\sum((D_{(i,j)}^{T} = =0)\\&(S_{(i,j)} > \\theta))}{M}}\\\\ & {TN = \\frac{\\sum((D_{(i,j)}^{T} > 0)\\&(S_{(i,j)}< \\theta))}{M}}\\\\ & {FP = \\frac{\\sum((D_{(i,j)}^{T} > 0)\\&(S_{(i,j)} > \\theta))}{M}}\\\\ & {FN = \\frac{\\sum((D_{(i,j)}^{T} = =0)\\&(S_{(i,j)}< \\theta))}{M}} \\end{array} \\quad (14)\\] We conduct the following performance comparisons on 10 industrial circuits, with circuit sizes ranging from \\(3.18k\\) gates to \\(40.50k\\) gates. 3) Model Settings: In the one-round GNN model configuration, the dimension of both structural embedding \\(hs\\) and functional embedding \\(hf\\) is 64. Both \\(MLP_{prob}\\) and \\(MLP_{rc}\\) contain 1 hidden layer with 32 neurons and a Re Lu activation function. The model is trained for 60 epochs to ensure each model can converge. The other models [7], [8] mentioned in the following experiments maintain their original settings and are trained until they converge. We train all the models for 80 epochs with batch-size 16 on a single Nvidia V100 GPU. We adopt the Adam optimizer [27] with learning rate \\(10^{- 4}\\) and weight decay \\(10^{- 10}\\) . TABLE I PERFORMANCE OF DEEPGATE (V1) AND OUR PROPOSED DEEPGATE2 (V2) ON LOGIC PROBABILITY PREDICTION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">#Gates</td><td colspan=\"2\">Deep Gate</td><td colspan=\"2\">Deep Gate2</td><td colspan=\"2\">Reduction</td></tr><tr><td>PE</td><td>Time</td><td>PE</td><td>Time</td><td>PE</td><td>Time(x)</td></tr><tr><td>D1</td><td>19,485</td><td>0.0344</td><td>36.89s</td><td>0.0300</td><td>2.23s</td><td>12.79%</td><td>16.56</td></tr><tr><td>D2</td><td>12,648</td><td>0.0356</td><td>16.11s</td><td>0.0309</td><td>1.18s</td><td>13.20%</td><td>13.66</td></tr><tr><td>D3</td><td>14,686</td><td>0.0355</td><td>21.42s</td><td>0.0294</td><td>1.44s</td><td>17.18%</td><td>14.92</td></tr><tr><td>D4</td><td>7,104</td><td>0.0368</td><td>5.89s</td><td>0.0323</td><td>0.50s</td><td>12.23%</td><td>11.89</td></tr><tr><td>D5</td><td>37,279</td><td>0.0356</td><td>131.22s</td><td>0.0316</td><td>7.82s</td><td>11.24%</td><td>16.79</td></tr><tr><td>D6</td><td>37,383</td><td>0.0325</td><td>133.02s</td><td>0.0285</td><td>8.10s</td><td>12.31%</td><td>16.42</td></tr><tr><td>D7</td><td>10,957</td><td>0.0357</td><td>13.02s</td><td>0.0316</td><td>0.90s</td><td>11.48%</td><td>14.44</td></tr><tr><td>D8</td><td>3,183</td><td>0.0406</td><td>1.60s</td><td>0.0341</td><td>0.17s</td><td>16.01%</td><td>9.64</td></tr><tr><td>D9</td><td>27,820</td><td>0.0368</td><td>77.20s</td><td>0.0322</td><td>4.73s</td><td>12.50%</td><td>16.34</td></tr><tr><td>D10</td><td>40,496</td><td>0.0327</td><td>154.00s</td><td>0.0290</td><td>8.89s</td><td>11.31%</td><td>17.33</td></tr><tr><td>Avg.</td><td></td><td>0.0356</td><td>59.04s</td><td>0.0310</td><td>3.59s</td><td>13.08%</td><td>16.43</td></tr></table> ### B. Comparison with Deep Gate on Probability Prediction We compare the probability prediction error (PE, see Eq. (13)) and runtime (Time) with previous Deep Gate. The previous Deep Gate is denoted as Deep Gate and our proposed model with novel loss function and GNN design is named as Deep Gate2 in Table I. Based on the results presented in the table, we make two observations. First, our proposed Deep Gate2 exhibits more accurate predictions of logic probability compared to the previous version. On average, the probability prediction error (PE) of Deep Gate2 is \\(13.08\\%\\) lower than that of Deep Gate. This suggests that using the novel model architecture with embedding initialization strategy can benefit logic representation learning and lead to better results on logic probability prediction. Second, our Deep Gate2 performs more efficient than Deep Gate. Take the circuit D1 as an example, Deep Gate requires 36.89 seconds for inference, but our Deep Gate2 only needs 2.23s, which is 16.56x faster than the previous Deep Gate. Moreover, compared to the previous model, Deep Gate2 achieves an order of magnitude speedup (16.43x on average) in model runtime. This is attributed to the fact that the GNN model in Deep Gate relies on 10 forward and 10 backward message propagation, whereas the proposed one- round GNN model in Deep Gate2 only performs forward propagation for 1 time. Therefore, the new circuit representation learning model is more effective and efficient than Deep Gate, and demonstrates the generalization ability on large- scale circuits.\n\n### C. Comparison with other Models on Logic Equivalence Gates Identification This section compares the functionality- aware accuracy, as defined in Section IV- A2 of Deep Gate2 with that of two other models: Deep Gate [7] and FGNN [8]. The Deep Gate [7] model treats the logic probability as supervision since it contains the statistical information of truth table. The FGNN [8] is trained to differentiate between logic equivalent and inequivalent circuits using contrastive learning. Table II presents the performance of three models on the task of logic equivalence gates identification. Firstly, our proposed approach outperforms the other two models on all circuits with an average F1- score of 0.9434, while Deep Gate and FGNN only achieve F1- Score 0.6778 and 0.4402, respectively. For instance, in circuit D7, our proposed functionality- aware circuit learning approach achieves an F1- Score of 0.9831 and accurately identifies \\(99.15\\%\\) of logic equivalence gate pairs with a precision of \\(97.48\\%\\) , indicating a low false positive rate. In contrast, Deep Gate only achieves an F1- score of 0.6778, while FGNN fails on most of the pairs. Secondly, although Deep Gate has an average recall of \\(91.46\\%\\) , its precision is only \\(54.00\\%\\) , indicating a large number of false positive identifications. This is because Deep Gate can only identify logic equivalent pairs by predicting logic probability, which leads to incorrect identification of gate pairs with similar logic probability. According to our further experiment, in \\(80.83\\%\\) of false positive pairs, the model incorrectly identifies gate pairs with similar logic probability as functionally equivalent. Thirdly, FGNN achieves the lowest performance among the other models, with only 0.4402 F1- Score. The poor performance of FGNN is attributed to the lack of effective supervision. While FGCN learns to identify logic equivalence circuits generated by perturbing local structures slightly, the model tends to consider circuits with similar structures to have the same functionality. However, in the validation dataset and practical applications, two circuits may have the same function even if their topological structures are extremely different. Therefore, the self- supervised approach limits the effectiveness of FGNN in identifying logic equivalence gates. ### D. Effectiveness of PI Encoding Strategy To demonstrate the effectiveness of our proposed PI encoding (PIE) strategy, we trained another model without assigning unique identifications for PIs, which we refer to as w/o PIE. The results are presented in Table III, which show that disabling the PIE reduces the F1- Score of identifying logic equivalence gates from 0.9434 to 0.7541, resulting in an average reduction of \\(20.07\\%\\) . Such reduction can be attributed to the fact that, as demonstrated as the failure case in Section II and Fig. 1, the one- round GNN model without the PIE strategy cannot model the structural information of the circuit. More specifically, the accuracy of the reconvergence structure identification task with w/ PIE model is \\(93.22\\%\\) , while the w/o model only achieve \\(74.56\\%\\) . The functionality of logic gate is affected by both functionality of fan- in gates and whether there is reconvergence between its fan- in gates. Once the reconvergence structure cannot be accurately identified, node functionality cannot be modeled accurately. ### E. Effectiveness of Training Strategies To investigate the effectiveness of our multi- stage training strategy, we train another model (noted as w/o multi- stage model) with all loss functions in only one stage, instead of adding the functionality- aware loss function in the second stage. The original model with multiple stages training strategy is noted as w/ multi- stage model. The w/ multi- stage model learn to predict the logic probability and structural correlation in the first stage and learn the more difficult task, which predicts the functionality in the second stage. The results are shown in Table IV, where the model w/ multi- stage achieves an F1- Score of 0.9434 on average and the model w/o multi- stage achieves only 0.7137. We analyze the reason as follows. The cost of comparing each pair of logic gates in the task of learning functionality is extremely high, which is proportional to the square of the circuit size. We limit the dataset and train the model to learn functional similarity only among pairs with similar logic probability, which is a necessary condition for functional equivalence. Therefore, without the staged multi- stage strategy, be effectively supervised with the simplified dataset, leading to poor performance in learning functionality. As shown in Table V, the differences between the two models in the loss values for predicting logic probability \\((L_{prob})\\) and identifying reconvergence structures \\((L_{rc})\\) are not significant, indicating that they perform similarly in these two tasks. However, compared to the w/o multi- stage model, the w/ multi- stage model performs better in learning functionality with \\(L_{func} = 0.0594\\) , which is \\(51.47\\%\\) smaller than that of w/o multi- stage model. However, the w/ multi- stage model outperforms the model w/o multi- stage in learning functionality task with a significantly lower \\(L_{func}\\) value of 0.0594, which is \\(51.47\\%\\) smaller than that of the latter. ## V. DOWNSTREAM TASKS In this section, we combine our Deep Gate2 with the open- source EDA tools and apply our model to practical EDA tasks: logic synthesis and Boolean satisfiability (SAT) solving. The logic synthesis tools aim to identify logic equivalence gates as quickly as possible. In Section V- A, our proposed functionality- aware circuit learning model provides guidance to the logic synthesis tool about the logic similarity. Additionally, in Section V- B, we apply the learnt functional similarity in SAT solving, where the variables with dissimilar functionality are assigned the same decision value. This approach efficiently shrinks the search space by enabling solvers to encounter more constraints. ### A. Logic Synthesis This subsection shows the effectiveness of our proposed functionality- aware circuit learning framework in SAT- sweeping [28], a common technique of logic synthesis. Fig. 5 illustrates the components of a typical ecosystem for SAT- sweeping engine (also called SAT sweeper), where including equivalence class (EC) manager, SAT- sweeping manager, simulator, and SAT solver. All computations are coordinated by the SAT- sweeping manager [29]. The SAT sweeper starts by computing candidate ECs using several rounds of initial simulation and storing ECs into EC manager. In the next step, the SAT- sweeping manager selects two gates within an EC and then calls the SAT solver to check whether they are equivalent. If so, the EC manager merges these two gates. Otherwise, SAT solver will return a satisfiable assignment as a counterexample for incremental simulation to refine the candidate ECs. To the best of our knowledge, most SAT- sweeping managers select EC only based on the circuit structure, without efficient heuristic strategy considering the functionality of candidate gates. We will introduce the functional information into SAT- sweeping manager to further improve efficiency. 1) Experiment Settings: We combine our Deep Gate2 into SAT sweeper to guide EC selection. To be specific, the updated manager sorts all candidate equivalence classes by computing the cosine similarity of their embeddings. Unlike traditional SAT sweepers, our\n\nTABLE II PERFORMANCE OF DIFFERENT MODELS ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">Size</td><td colspan=\"3\">Func Model</td><td colspan=\"3\">Deep Gate</td><td colspan=\"3\">FGNN</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>19,485</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>94.59%</td><td>52.69%</td><td>0.6768</td><td>63.64%</td><td>41.18%</td><td>0.5000</td></tr><tr><td>D2</td><td>12,648</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>92.07%</td><td>52.98%</td><td>0.6726</td><td>60.87%</td><td>35.00%</td><td>0.4444</td></tr><tr><td>D3</td><td>14,686</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>93.25%</td><td>62.08%</td><td>0.7454</td><td>72.22%</td><td>44.83%</td><td>0.5532</td></tr><tr><td>D4</td><td>7,104</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>90.11%</td><td>46.33%</td><td>0.6120</td><td>62.73%</td><td>23.53%</td><td>0.3422</td></tr><tr><td>D5</td><td>37,279</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>93.69%</td><td>56.72%</td><td>0.7066</td><td>64.00%</td><td>69.57%</td><td>0.6667</td></tr><tr><td>D6</td><td>37,383</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>93.23%</td><td>60.49%</td><td>0.7337</td><td>59.09%</td><td>46.67%</td><td>0.5215</td></tr><tr><td>D7</td><td>10,957</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>88.89%</td><td>48.83%</td><td>0.6303</td><td>36.36%</td><td>23.53%</td><td>0.2857</td></tr><tr><td>D8</td><td>3,183</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>82.86%</td><td>49.90%</td><td>0.6229</td><td>62.63%</td><td>22.22%</td><td>0.3280</td></tr><tr><td>D9</td><td>27,820</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>92.51%</td><td>48.77%</td><td>0.6387</td><td>62.50%</td><td>26.79%</td><td>0.3750</td></tr><tr><td>D10</td><td>40,496</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>93.35%</td><td>61.22%</td><td>0.7395</td><td>47.02%</td><td>32.63%</td><td>0.3853</td></tr><tr><td>Avg.</td><td></td><td>98.73%</td><td>90.49%</td><td>0.9434</td><td>91.46%</td><td>54.00%</td><td>0.6778</td><td>59.11%</td><td>36.60%</td><td>0.4402</td></tr></table> TABLE III PERFORMANCE COMPARISON BETWEEN W/PIE AND W/O PIE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ PIE</td><td colspan=\"3\">w/o PIE</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>70.66%</td><td>64.66%</td><td>0.6753</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>87.20%</td><td>78.57%</td><td>0.8266</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>79.15%</td><td>76.21%</td><td>0.7765</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>87.91%</td><td>59.70%</td><td>0.7111</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>79.23%</td><td>76.73%</td><td>0.7796</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>87.10%</td><td>77.56%</td><td>0.8205</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>84.62%</td><td>64.13%</td><td>0.7296</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>74.29%</td><td>83.87%</td><td>0.7879</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>62.27%</td><td>76.27%</td><td>0.6856</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>87.07%</td><td>65.54%</td><td>0.7479</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7541</td></tr></table> TABLE IV PERFORMANCE COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ multi-stage</td><td colspan=\"3\">w/o multi-stage</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>79.46%</td><td>67.68%</td><td>0.7310</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>74.39%</td><td>63.21%</td><td>0.6835</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>70.46%</td><td>62.78%</td><td>0.6640</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>73.63%</td><td>72.83%</td><td>0.7323</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>68.02%</td><td>77.19%</td><td>0.7232</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>71.46%</td><td>86.01%</td><td>0.7806</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>65.81%</td><td>67.46%</td><td>0.6662</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>71.43%</td><td>86.21%</td><td>0.7813</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>74.52%</td><td>63.17%</td><td>0.6838</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>75.10%</td><td>64.02%</td><td>0.6912</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7137</td></tr></table> TABLE V LOSS COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE <table><tr><td></td><td>w/o multi-stage</td><td>w/ multi-stage</td><td>Reduction</td></tr><tr><td>Lprob</td><td>0.0205</td><td>0.0207</td><td>-0.98%</td></tr><tr><td>Lrc</td><td>0.1186</td><td>0.1115</td><td>5.99%</td></tr><tr><td>Lfunc</td><td>0.1224</td><td>0.0594</td><td>51.47%</td></tr></table> proposed SAT sweeper does not need to validate the equivalence of all candidate ECs in one pass. Instead, node pairs with higher similarity have high priority for SAT solver calls. If the gate pair is formally proved to be equivalent, these two gate are merged. Otherwise, the generated counterexample should contain more conflicts than the <center>Fig. 5. The proposed SAT-sweeping ecosystem. </center> baseline method, resulting in better efficiency for refining candidate ECs. Our model is equipped into ABC [30] as a plug- in and integrated into the SAT sweeper '&frag' [14], which is one of the most efficient and scalable SAT sweeper publicly available at this time. The AIGs derived by merging the resulting equivalence nodes are verified by '&ecc' command in ABC to ensure functional correctness. All experiments are performed on a \\(2.40\\mathrm{GHz}\\) Intel(R) Xeon(R) Silver 4210R CPU with 64GB of main memory. A single core and less than 1GB was used for any test case considered in this subsection. The proposed SAT sweeper (named as Our) is compared against the original engine, &frag. 2) Results: We validate the performance of our SAT sweeper with 6 industrial circuits. As shown in Table VI, section \"Statistics\" lists the number of PI and PO (PI/PO), logic levels (Lev) and internal AND-nodes in the original AIG (And). To ensure the fairness of comparison, the circuits after sweeping should have the same size. Section \"SAT calls\" lists the number of satisfiable SAT calls, performed by the solver employed in each engine. The data shows that our proposed engine decreases the number of satisfiable SAT calls, that explains why it has better results, since more resource are used to prove equivalence gates. In addition, section \"Total runtime\" compares the runtime and section \"Red.\" shows the runtime reduction from &frag to Our. The number of \"SAT calls\" can get an average reduction of \\(53.37\\%\\) (95.88% maximum) through the integration of our Deep Gate2 model. As for \"Total runtime\", this experiment shows that the Deep Gate2- based SAT sweeper outperforms state- of- the- art engines, while reducing the average runtime by \\(49.46\\%\\) (57.77% maximum). Thus, the sweeper formally verifies the equivalence of functionally similar gates\n\nTABLE VI COMPARING THE NUMBER OF SAT CALLS AND THE RUNTIME OF THE SAT SWEEPERS <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"2\">Statistic</td><td colspan=\"2\">SAT calls</td><td colspan=\"2\">Total Runtime (s)</td></tr><tr><td>PI/PO</td><td>Lev</td><td>And</td><td>#frag</td><td>Our Red.</td><td>#frag</td></tr><tr><td>C1</td><td>128/128</td><td>4,372</td><td>57,247</td><td>13,826</td><td>570</td><td>95.88%</td></tr><tr><td>C2</td><td>24/25</td><td>225</td><td>5,416</td><td>100</td><td>74</td><td>26.00%</td></tr><tr><td>C3</td><td>22/1</td><td>29</td><td>703</td><td>4</td><td>1</td><td>75.00%</td></tr><tr><td>C4</td><td>114/1</td><td>91</td><td>19,354</td><td>20</td><td>12</td><td>40.00%</td></tr><tr><td>C5</td><td>126/1</td><td>83</td><td>20,971</td><td>6</td><td>4</td><td>33.33%</td></tr><tr><td>C6</td><td>96/1</td><td>79</td><td>14,389</td><td>10</td><td>5</td><td>50.00%</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>53.37%</td><td>49.46%</td></tr></table> with the guidance from Deep Gate2, thereby reducing the number of invalid SAT calls and improving efficiency of SAT sweeping. Take C1 as a representative example, the baseline &frag selects gates in EC for formal verification without considering their behaviour, thus, many solver calls return satisfiable results, and few gates can be merged. However, with the guidance of Deep Gate2, the sweeper can prioritize the selection of gates with similar behavior, resulting in a significant reduction of \\(95.88\\%\\) in SAT calls and \\(57.77\\%\\) in runtime. ### B. Boolean Satisfiability Solving Boolean satisfiability (SAT) solving is a long- standing and fundamental NP- complete problem with applications in many areas, especially in electronic design automation (EDA) [31]- [33]. The existing SAT solvers are designed to incorporate efficient heuristics [34]- [36] to expedite the solving process. For instance, [34] proposes to utilize the correlation of logic gate functionality to enforce variable decision for solving circuit- based SAT instances. Although the solution achieves remarkable speedup over SAT solvers, it still relies on the time- consuming logic simulation to obtain the functionality. Based on [34], we demonstrate how the Deep Gate2 models functional correlation efficiently and accelerates SAT solving. 1) Experiment Settings: We integrate our Deep Gate2 into a modern SAT solver, Ca Di Cal [15] to solve the instances from logic equivalence checking (LEC) task. Firstly, we obtain gate-level embeddings of the original circuit and predict the pairwise functional similarity between these gates. Given the one-to-one mapping [37] between circuits and conjunctive normal form (a problem format required by SAT solvers), we can easily transfer the gate functional similarity to variable behavioral similarity. If two logic gates have similar representations (and therefore similar functionality), their corresponding variables should be correlated and grouped together during the variable decision process. Secondly, we incorporate the learnt knowledge into the SAT solver. As shown in Algorithm 1, when the current variable \\(s\\) is assigned a value \\(v\\) , we identify all unassigned variables \\(s'\\) in the set \\(S\\) that contains correlated variables with \\(s\\) . As modern SAT solvers reduce searching space by detecting conflicts as much as possible [38], we assign the reverse value \\(\\bar{v}\\) to \\(s'\\) to promptly cause conflict for joint decision. Besides, the threshold \\(\\delta\\) in Algorithm 1 is set to \\(1e - 5\\) . Thirdly, to evaluate the efficacy of our model in accelerating SAT solving, we compare the aforementioned hybrid solver (labeled as Our) with original Ca Di Cal [15] (labeled as Baseline) on 5 industrial instances. All experiments are conducted with a single 2.40GHz Intel(R) Xeon(R) E5- 2640 v4 CPU. 2) Results: The runtime comparison between Baseline and Our are listed in Table VII. To ensure a fair comparison, we aggregate the Deep Gate2 model inference time (Model) and SAT solver runtime (Solver) as the Overall runtime. We have the following observations. Algorithm 1 Variable Decision Function with Deep Gate2 Current variable \\(s\\) just being assigned a value \\(v\\) ; Set \\(S\\) containing the correlated variables with \\(s\\) ; Function \\(\\mathrm{Sim}(s_i,s_j)\\) to calculate the behaviour similarity of two variables; \\(\\delta\\) is the threshold for the joint decision Function \\(V(s)\\) to get the assigned value of variable \\(s\\) ; Function Decision \\((s,v)\\) to assign the decision value \\(v\\) to the current decision variable \\(s\\) . 1: Decision \\((s,v)\\) 2: for \\(s'\\) in \\(S\\) do 3: if \\(s' \\neq s\\) and \\(V(s') = \\text{None}\\) then 4: if \\(\\mathrm{Sim}(s',s) > 1 - \\delta\\) then 5: Decision \\((s',\\bar{v})\\) 6: end if 7: end if 8: end for TABLE VII COMPARING THE RUNTIME BETWEEN BASELINE AND OUR SOLVERS <table><tr><td rowspan=\"2\">Instance</td><td rowspan=\"2\">Size</td><td rowspan=\"2\">Baseline(s)</td><td colspan=\"2\">Our(s)</td><td rowspan=\"2\">Reduction</td></tr><tr><td>Model</td><td>Solver</td></tr><tr><td>I1</td><td>17,495</td><td>88.01</td><td>1.77</td><td>30.25</td><td>32.02</td></tr><tr><td>I2</td><td>21,952</td><td>29.36</td><td>2.85</td><td>6.01</td><td>8.86</td></tr><tr><td>I3</td><td>23,810</td><td>61.24</td><td>3.25</td><td>32.88</td><td>36.13</td></tr><tr><td>I4</td><td>27,606</td><td>158.04</td><td>4.36</td><td>137.77</td><td>142.13</td></tr><tr><td>I5</td><td>28,672</td><td>89.89</td><td>4.78</td><td>70.95</td><td>75.73</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>40.05%</td></tr></table> First, our method achieves a substantial reduction in total runtime for all test cases, with an average runtime reduction of \\(40.05\\%\\) . Take I1 as an example, the plain solver requires \\(88.01\\mathrm{s}\\) to solve the problem, but by combining with our model, the new solver produces results in only 32.02s, reducing runtime by \\(63.62\\%\\) . Second, our model only takes a few seconds to obtain embeddings, occupying less than \\(10\\%\\) of overall runtime on average. It should be noted that our Deep Gate2 is able to infer within polynomial time that is only proportional to the size of instance. Third, while the two largest instances I4 and I5 show less reduction than the others, it does not necessarily mean that our model is unable to generalize to larger instances. As evidenced by the results for I2, an instance with a similar size to I4 and I5 also demonstrates a significant reduction. The reduction caused by our model should be determined by the characteristics of instance. In summary, our model is effective in speeding up downstream SAT solving. ## VI. CONCLUSION This paper introduces Deep Gate2, a novel functionally- aware framework for circuit representation learning. Our approach leverages the pairwise truth table differences of logic gates as a supervisory signal, providing rich functionality supervision and proving scalable for large circuits. Moreover, Deep Gate2 differentiates and concurrently updates structural and functional embeddings in two dedicated flows, acquiring comprehensive representations through a single round of GNN forward message- passing. In comparison to its predecessor, Deep Gate2 demonstrates enhanced performance in logic probability prediction and logic equivalent gate identification, while simultaneously improving model efficiency tenfold. The applications of Deep Gate2 onto multiple downstream tasks further demonstrate its effectiveness and potential utility in the EDA field.\n\n## REFERENCES [1] J. Chen, J. Kuang, G. Zhao, D. J.- H. Huang, and E. F. Young, \"Pros: A plug- in for routability optimization applied in the state- of- the- art commercial eda tool using deep learning,\" in Proceedings of the 39th International Conference on Computer- Aided Design, 2020, pp. 1- 8. [2] A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.- J. Lee, E. Johnson, O. Pathak, A. Nazi et al., \"A graph placement methodology for fast chip design,\" Nature, vol. 594, no. 7862, pp. 207- 212, 2021. [3] W. L. Neto, M. Austin, S. Temple, L. Amaru, X. Tang, and P.- E. Gaillardon, \"Losaic: A logic synthesis framework driven by artificial intelligence,\" in 2019 IEEE/ACM International Conference on Computer- Aided Design (ICCAD). IEEE, 2019, pp. 1- 6. [4] W. Haaswijk, E. Collins, B. Seguin, M. Soeken, F. Kaplan, S. Susstrunk, and G. De Micheli, \"Deep learning for logic optimization algorithms,\" in 2018 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE, 2018, pp. 1- 4. [5] Z. Shi, M. Li, S. Khan, L. Wang, N. Wang, Y. Huang, and Q. Xu, \"Deeptpi: Test point insertion with deep reinforcement learning,\" in 2022 IEEE International Test Conference (ITC). IEEE, 2022, pp. 194- 203. [6] J. Huang, H.- L. Zhen, N. Wang, H. Mao, M. Yuan, and Y. Huang, \"Neural fault analysis for sat- based atpg,\" in 2022 IEEE International Test Conference (ITC). IEEE, 2022, pp. 36- 45. [7] M. Li, S. Khan, Z. Shi, N. Wang, H. Yu, and Q. Xu, \"Deepgate: Learning neural representations of logic gates,\" in Proceedings of the 59th ACM/IEEE Design Automation Conference, 2022, pp. 667- 672. [8] Z. Wang, C. Bai, Z. He, G. Zhang, Q. Xu, T.- Y. Ho, B. Yu, and Y. Huang, \"Functionality matters in netlist representation learning,\" in Proceedings of the 59th ACM/IEEE Design Automation Conference, 2022, pp. 61- 66. [9] K. Zhu, H. Chen, W. J. Turner, G. F. Kokai, P.- H. Wei, D. Z. Pan, and H. Ren, \"Tag: Learning circuit spatial embedding from layouts,\" in Proceedings of the 41st IEEE/ACM International Conference on Computer- Aided Design, 2022, pp. 1- 9. [10] Y. Lai, Y. Mu, and P. Luo, \"Maskplace: Fast chip placement via reinforced visual representation learning,\" ar Xiv preprint ar Xiv:2211.13382, 2022. [11] A. Fayyazi, S. Shababi, P. Nuzzo, S. Nazarian, and M. Pedram, \"Deep learning- based circuit recognition using sparse mapping and level- dependent decaying sum circuit representations,\" in 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2019, pp. 638- 641. [12] Z. He, Z. Wang, C. Bail, H. Yang, and B. Yu, \"Graph learning- based arithmetic block identification,\" in 2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD). IEEE, 2021, pp. 1- 8. [13] M. Li, Z. Shi, Q. Lai, S. Khan, and Q. Xu, \"Deepsat: An eda- driven learning framework for sat,\" ar Xiv preprint ar Xiv:2205.13745, 2022. [14] A. Mishchenko, S. Chatterjee, R. Jiang, and R. K. Brayton, \"Fraigs: A unifying representation for logic synthesis and verification,\" ERL Technical Report, Tech. Rep., 2005. [15] S. D. QUEUE, \"Cadical at the sat race 2019,\" SAT RACE 2019, p. 8, 2019. [16] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \"Language models are few- shot learners,\" Advances in neural information processing systems, vol. 33, pp. 1877- 1901, 2020. [17] J. Devlin, M.- W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre- training of deep bidirectional transformers for language understanding,\" ar Xiv preprint ar Xiv:1810.04805, 2018. [18] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, \"Unsupervised feature learning via non- parametric instance discrimination,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3733- 3742. [19] E. Hoffer and N. Ailon, \"Deep metric learning using triplet network,\" in Similarity- Based Pattern Recognition: Third International Workshop, SIMBAD 2015, Copenhagen, Denmark, October 12- 14, 2015. Proceedings 3. Springer, 2015, pp. 84- 92. [20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" Advances in neural information processing systems, vol. 30, 2017. [21] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \"Curriculum learning,\" in Proceedings of the 26th annual international conference on machine learning, 2009, pp. 41- 48. [22] S. Ruder, \"An overview of multi- task learning in deep neural networks,\" ar Xiv preprint ar Xiv:1706.05098, 2017. [23] S. Davidson, \"Characteristics of the itc'99 benchmark circuits,\" in ITSW, 1999. [24] C. Albrecht, \"Iwls 2005 benchmarks,\" in IWLS, 2005. [25] L. Amar\u00fa, P.- E. Gaillardon, and G. De Micheli, \"The epfl combinational benchmark suite,\" in IWLS, no. CONF, 2015. [26] O. Team, \"Opencores,\" https://opencores.org/. [27] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" ar Xiv preprint ar Xiv:1412.6980, 2014. [28] A. Kuehlmann, V. Paruthi, F. Krohm, and M. K. Ganai, \"Robust boolean reasoning for equivalence checking and functional property verification,\" IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, vol. 21, no. 12, pp. 1377- 1394, 2002. [29] A. Mishchenko and R. Brayton, \"Integrating an aig package, simulator, and sat solver,\" in International Workshop on Logic and Synthesis (IWLS), 2018, pp. 11- 16. [30] B. L. Synthesis and V. Group, \"Abc: A system for sequential synthesis and verification.\" http://www.cad.eecs.berkeley.edu/alanmi/abc, 2023. [31] E. I. Goldberg, M. R. Prasad, and R. K. Brayton, \"Using sat for combinational equivalence checking,\" in Proceedings Design, Automation and Test in Europe, Conference and Exhibition 2001. IEEE, 2001, pp. 114- 121. [32] K. L. Mc Millan, \"Interpolation and sat- based model checking,\" in Computer Aided Verification: 15th International Conference, CAV 2003, Boulder, CO, USA, July 8- 12, 2003. Proceedings 15. Springer, 2003, pp. 1- 13. [33] K. Yang, K.- T. Cheng, and L.- C. Wang, \"Trangen: A sat- based atpg for path- oriented transition faults,\" in ASP- DAC 2004: Asia and South Pacific Design Automation Conference 2004 (IEEE Cat. No. 04EX753). IEEE, 2004, pp. 92- 97. [34] F. Lu, L.- C. Wang, K.- T. Cheng, and R.- Y. Huang, \"A circuit sat solver with signal correlation guided learning,\" in 2003 Design, Automation and Test in Europe Conference and Exhibition. IEEE, 2003, pp. 892- 897. [35] G. Audemard and L. Simon, \"Glucose: a solver that predicts learnt clauses quality,\" SAT Competition, pp. 7- 8, 2009. [36] \"On the glucose sat solver,\" International Journal on Artificial Intelligence Tools, vol. 27, no. 01, p. 1840001, 2018. [37] G. S. Tseitin, \"On the complexity of derivation in propositional calculus,\" Automation of reasoning: 2. Classical papers on computational logic 1967- 1970, pp. 466- 483, 1983. [38] J. Marques- Silva, I. Lynce, and S. Malik, \"Conflict- driven clause learning sat solvers,\" in Handbook of satisfiability. IOS press, 2021, pp. 133- 182.",
      "level": 1,
      "line_start": 1,
      "line_end": 21
    },
    {
      "heading": "Introduction",
      "content": "in Section III. We compare Deep Gate2 with the original Deep Gate and another functionality- aware solution [8] in Section IV. Next, we apply Deep Gate2 onto several downstream tasks in Section V. Finally, Section VI concludes this paper. ## II. RELATED WORK ### A. Circuit Representation Learning A prominent trend in the deep learning community is to learn a general representation from data first and then apply it to various downstream tasks, for example, GPT [16] and BERT [17] learn representations of natural language text that can be fine- tuned for a wide range of natural language processing tasks. Circuit representation learning has also emerged as an attractive research direction, which falls into two categories: structure- aware circuit representation learning [9], [11], [12] and functionality- aware circuit representation learning [7], [8]. Since a circuit can be naturally formulated as a graph, with gates as nodes and wires as edge, the GNN is a powerful tool to capture the interconnections of logic gates and becomes a backbone model to learn circuit representations. For example, TAG [9] is a GNN- based model designed for analog and mixed- signal circuit representation learning and applied for several physical design applications, such as layout matching prediction, wirelength estimation, and net parasitic capacitance prediction. ABGNN [12] learns the representation of digital circuits and handles the arithmetic block identification task. However, these models tend to focus on structural encoding and are not suitable for functionality- related tasks. Consequently, the functionality- aware circuit representation learning frameworks [7], [8] are designed to learn the underlying circuit functionality. For instance, FGNN [8] learns to distinguishes between functionally equivalent and inequivalent circuits by contrastive learning [18]. However, such self- supervised manner relies on data augmentation by perturbing the original circuit to logic equivalence circuit. If the perturbation is not strong and diverse, the model still identifies the functional equivalence circuits based on the invariant local structure, resulting a low generalization ability on capturing underlying functionality. Deep Gate [7] leverages logic- 1 probability under random simulation as supervision, which approximates the statistic of the most direct representation of functionality, i.e. truth table. Despite achieving remarkable progress on testability analysis [5], there are limitations that affect the generalizability of Deep Gate to other EDA tasks. We will elaborate on Deep Gate in the next subsection. ### B. Deep Gate Framework Deep Gate [7] is the first circuit representation learning framework that embeds both structural and functional information of digital circuits. The model pre- processes the input circuits into a unified And- Inverter Graph (AIG) format and obtains rich gate- level representations, which can be applied to various downstream tasks. Deep Gate treats the logic- 1 probability as supervision to learn the functionality. Additionally, the Deep Gate consists of a GNN equipped with an attention- based aggregation function that propagates information of gates in levelized sequential manner. The aggregation function learns to assign high attention weights to controlling fan- in of gates (e.g. the fan- in gate with logic- 0 is the controlling fan- in of AND gate) that mimics the logic computation process. Although it has been applied to testability analysis [5] and SAT problem [13], we argue that the model still encounters with two major shortcomings limiting its generalization ability. <center>Fig. 1. An example of reconvergence structure </center> First, logic probability is not an appropriate supervision for learning functionality. The most direct representation of functionality is the truth table, however, using it as a training label is impractical due to the immeasurable computational overhead. Deep Gate proposes to supervise the model by utilizing the proportion of logic- 1 in the truth table and approximate this proportion as the logic probability through random simulation. However, logic probability is only a statistical information of functionality, indicating the number of logic- 1 values in the truth table rather than which PI assignments lead to logic- 1. Consequently, Deep Gate cannot differentiate the functional difference between two circuits if they have the same probability. Second, Deep Gate is not efficient enough to deal with large circuit. Specifically, Deep Gate requires to perform forward and backward message- passing operations for 20 rounds to embed rich representations. Fig. 1 illustrates the need of this multi- round GNN design in Deep Gate where the nodes in grey color represent PIs. The incoming messages of nodes 5, 6, 5, and 6 during forward propagation are noted in the figure, where \\(h_i\\) is the embedding vector of node \\(i\\) . Since, Deep Gate uses the same initial embeddings for all nodes, the messages of nodes 5, 6, 5, and 6 in the first forward propagation round are identical. Thus, the model can only distinguish node embeddings based on their connections by repeatedly updating PIs through multiple rounds of forward and backward message propagation. We emphasize that the limitations of Deep Gate comes from the lack of effective supervision and weak model design where the unique identification of all PIs are ignored. To address these issues, we propose an efficient one- round GNN design that maintains the unique identification of PIs and uses the pairwise truth- table difference of two gates as an effective supervision. ## III. METHODOLOGY ### A. Problem Formulation The circuit representation learning model aims to map both circuit structure and functionality into embedding space, where the structure represents the connecting relationship of logic gates and the functionality means the logic computational mapping from inputs to outputs. We conclude that the previous models still lack of ability to capture functional information. In this paper, we propose to improve the previous Deep Gate model [7] to represent circuits with similar functionality with the similar embedding vectors. In other words, these circuit representations should have short distance in the embedding space. We take Circuit A, B, C, and D as examples in Fig. 2, where all of them have similar topological structures. Since Circuit A, B and C perform with the same logic probability, Deep Gate [7] tends to produce the similar embeddings for these three circuits. Hence, it is hard to identify the logic equivalent circuits by Deep Gate. Although FGNN [8] is trained to classify logical equivalence and inequivalence circuits by contrastive learning, they cannot differentiate the relative similarity. As shown in the embedding space, the distance between\n\n<center>Fig. 2. Problem statement: the embedding vectors should be close if circuit functions are similar </center> A and B is equal to the distance between A and D. Nonetheless, as indicated in the truth table, Circuit A is equivalent to Circuit C, similar to Circuit B (with only 2 different bits), but dissimilar to Circuit D (with 5 different bits). We expect that the model will bring together or separate the circuits in embedding space according to their truth tables. Therefore, the expected Deep Gate2 model not only identifies the logic equivalent nodes, but also predicts the functional similarity. Thus, we can apply such functionality- aware circuit learning model to provide benefits for the real- world applications. ### B. Dataset Preparation To train the circuit representation learning model, we need to find a supervision contains rich functional information and prepare an effective dataset at first. Truth table, which records the complete logic computational mapping, provides the most direct supervision. However, the length of the truth table increases exponentially with the number of primary inputs, and obtaining a complete truth table requires an immeasurable amount of time. Therefore, a reasonable supervision should be easily obtained and closely related to the truth table. Firstly, we use the Hamming distance between truth tables of two logic gates as supervision. That is, in a way similar to metric learning [19], we map nodes to an embedding space and hope that the distance of the embedding vectors is positive correlated with the Hamming distance of the truth table. Formally, we denote the truth table vector of node \\(i\\) is \\(T_{i}\\) and the embedding vector of node \\(i\\) is \\(h_{i}\\) . \\[distance(h_{i},h_{j})\\propto distance(T_{i},T_{j}) \\quad (1)\\] Secondly, to improve the data efficiency, we regard the each logic gate in circuit as a new circuit (logic cone) with the current gate as output and the original PIs as inputs. By parsing a single original circuit, we obtain a large number of new circuits. Therefore, the task of graph learning becomes the task of learning node- level representation, and the difficulty of data collection is reduced. Thirdly, to ensure the quality of sample pairs and limit the number of sample pairs, we impose the following constraints during sampling node pairs: (1) Two logic cones of the two nodes should have the same PI, which is a necessary condition for comparing the truth table difference. (2) The logic probability, which is the number of logic- 1 percentage in the truth table, should be similar (distance within \\(5\\%\\) ). This is because if the logic probability of two nodes is not consistent, their functions are definitely not consistent. If the logic probability of two nodes is consistent, their functions may be consistent. (3) The difference in logic levels between two nodes should be within 5, because when the two nodes are far apart, their functions are unlikely to be correlated. (4) We only consider the extreme cases, namely, the difference between truth tables is within \\(20\\%\\) or above \\(80\\%\\) . We do not perform the complete simulation, but set a maximum simulation time to obtain the response of each node as an incomplete truth table. It should be noted that we utilize the And- Inverter Graph (AIG) as the circuit netlist format, which is only composed of AND gate and NOT gate. Any other logic gates, including OR, XOR and MUX, can be transformed into a combination of AND and NOT gates in linear time. ### C. Functionality-Aware Loss Function The primary objective of our purposed functionality- aware circuit learning model is to learn node embeddings, where two embedding vectors will be similar if the corresponding two node function are similar. As we sample node pairs \\(\\mathcal{N}\\) in the Section III- B, we can obtain the Hamming distance of truth table \\(D^{T}\\) of each node pair. \\[D_{(i,j)}^{T} = \\frac{Hamming Distance(T_{i},T_{j})}{length(T_{i})},(i,j)\\in \\mathcal{N} \\quad (2)\\] According to Eq. (1), the distance of embedding vectors \\(D^{H}\\) should be proportional to the Hamming distance of the truth table \\(D^{T}\\) . We define the distance of embedding vectors in Eq. (3), where is calculated based on cosine similarity. In other word, the similarity of embedding vectors \\(S_{(i,j)}\\) should be negative related to distance \\(D_{(i,j)}^{T}\\) . \\[\\begin{array}{l}{S_{(i,j)} = Cosine Similarity(h_i,h_j)}\\\\ {D_{(i,j)}^H = 1 - S_{(i,j)}} \\end{array} \\quad (3)\\] Therefore, the training objective is to minimize the difference between \\(D^{H}\\) and \\(D^{T}\\) . We purpose the functionality- aware loss function \\(L_{func}\\) as below. \\[\\begin{array}{l}{D_{(i,j)}^{T^{\\prime}} = Zero Norm(D_{(i,j)}^{T})}\\\\ {D_{(i,j)}^{H^{\\prime}} = Zero Norm(D_{(i,j)}^{H})}\\\\ {L_{func} = \\sum_{(i,j)\\in \\mathcal{N}}(L1Loss(D_{(i,j)}^{T^{\\prime}},D_{(i,j)}^{H^{\\prime}}))} \\end{array} \\quad (4)\\] ### D. One-round GNN Model In this subsection, we propose a GNN model that can capture both functional and structural information for each logic gate through one- round forward propagation. First, we propose to separate the functional embeddings \\(hf\\) and structural embeddings \\(hs\\) , and initialize them in difference ways. We assign the uniform initial functional embeddings to primary inputs (PI), as they all have equivalent logic probability under random simulation. However, we design a PI encoding (PIE) strategy by assigning a unique identification to each PI as its initial structural embedding. Specifically, the initial PI structural embeddings \\(hs_{i}, i \\in PI\\) are orthogonal vectors. This means that the dot product of any two PIs' embeddings is zero. Second, we design four aggregators: \\(agg_{AND}^{r}\\) aggregates the message for structural embedding \\(hs\\) of an AND gate, \\(agg_{AND}^{r}\\) aggregates the message for functional embedding \\(hf\\) of an AND\n\n<center>Fig. 3. One-round GNN propagation process </center> gate. \\(agg r_{NOT}^{s}\\) and \\(agg r_{NOT}^{f}\\) update \\(h s\\) and \\(h f\\) of a NOT gate, respectively. We implement each aggregator using the self- attention mechanism [20], as the output of a logic gate is determined by the controlling values of its fan- in gates. For example, an AND gate must output logic- 0 if any of its fan- in gates has logic- 0. By employing the attention mechanism, the model learns to assign greater importance to the controlling inputs [7]. As illustrated in Eq. (5), \\(w_{q}\\) , \\(w_{k}\\) and \\(w_{v}\\) are three weight matrices and \\(d\\) is the dimension of embedding vectors \\(h\\) . \\[\\begin{array}{l}\\alpha_{j} = softmax(\\frac{w_{q}^{\\top}h_{i}\\cdot(w_{k}^{\\top}h_{j})^{\\top}}{\\sqrt{d}})\\\\ m_{j} = w_{v}^{\\top}h_{j}\\\\ h_{i} = agg r(h_{j}|j\\in \\mathcal{P}(i)) = \\sum_{j\\in \\mathcal{P}(i)}(\\alpha_{j}*m_{j}) \\end{array} \\quad (5)\\] Third, during forward propagation, the structural embeddings are updated only with the structural embeddings of predecessors. As shown in Eq. (6), where the Gate \\(a\\) is AND gate, the Gate \\(b\\) is NOT gate. \\[\\begin{array}{l} h s_{a} = agg r_{A N D}^{s}(h s_{j}|j\\in \\mathcal{P}(a)) \\\\ h s_{b} = agg r_{N O T}^{s}(h s_{j}|j\\in \\mathcal{P}(b)) \\end{array} \\quad (6)\\] At the same time, the gate function is determined by the function and the structural correlations of the fan- in gates. Therefore, the functional embeddings are updated as Eq. (7). \\[\\begin{array}{l} h f_{a} = agg r_{A N D}^{f}([h s_{j},h f_{j}]|j\\in \\mathcal{P}(a))\\\\ h f_{b} = agg r_{N O T}^{f}([h s_{j},h f_{j}]|j\\in \\mathcal{P}(b)) \\end{array} \\quad (7)\\] Therefore, as shown in Fig. 3, the GNN propagation process performs from PI to PO level by level. For the node in level \\(l\\) , its structural embedding \\(h s_{L_{l}}\\) will be updated with the structural embeddings of the node in level \\(l - 1\\) . Additionally, the functional embedding \\(h f_{L_{l}}\\) will be updated with both structural embeddings \\(h s_{L_{l - 1}}\\) and functional embeddings \\(h f_{L_{l - 1}}\\) . The GNN propagation completes after processing \\(N\\) levels. ### E. Model Training Strategies To train the model, we employed multi- stage training strategy, similar to training a model with an easy task and then a harder task in curriculum learning [21]. During each stage, we trained the model with multiple supervisions in multi- task learning manner [22]. In the first stage, we train the one- round GNN model with two simple tasks. The Task 1 involves predicting the logic probability, while the Task 2 entails identifying the structural correlation. To achieve this, we readout the functional embedding \\(h f_{i}\\) to predict the logic probability \\(\\hat{P}_{i}\\) by a multi- layer perceptron (MLP), denoted as \\(MLP_{prob}\\) . In addition, we utilize the structural embeddings \\(h s_{i}\\) and \\(h s_{j}\\) to predict whether node \\(i\\) and node \\(j\\) can be reconvergent by \\(MLP_{rc}\\) . \\[\\begin{array}{r}\\hat{P}_i = MLP_{prob}(h f_i)\\\\ R_{\\langle i,j\\rangle} = MLP_{rc}(h s_i,h s_j) \\end{array} \\quad (8)\\] We define the loss function for Task 1 in Eq. (9), where the \\(P_{i}\\) is the ground truth logic probability obtained through random simulation. \\[L_{prob} = L1Loss(P_i,\\hat{P}_i) \\quad (9)\\] Besides, we define the loss function for Task 2 in Eq. (10). The binary ground truth, denoted as \\(R_{\\langle i,j\\rangle}\\) , indicates whether node pair \\(i\\) and \\(j\\) have a common predecessor. \\[L_{rc} = BCELoss(R_{\\langle i,j\\rangle},R_{\\langle i,j\\rangle}) \\quad (10)\\] Consequently, the loss function for Stage 1 is presented in Eq. (11), where the \\(w_{prob}\\) and \\(w_{rc}\\) are the weight for Task 1 and Task 2, respectively. \\[L_{stage1} = L_{prob}*w_{prob} + L_{rc}*w_{rc} \\quad (11)\\] The second training stage involves another more difficult Task 3. functionality- aware learning, as described in Section III- C. The loss function for Stage 2 is defined below, where \\(w_{func}\\) represents the loss weight of Task 3. \\[L_{stage2} = L_{prob}\\times w_{prob} + L_{rc}\\times w_{rc} + L_{func}\\times w_{func} \\quad (12)\\] Overall, the model can differentiate gates with varying probability in Stage 1. As the logic equivalent pairs only occur when nodes have the same probability, the model in Stage 2 learns to predicting the functional similarity within the probability equivalent class. The effectiveness of the above training strategies is demonstrated in Section IV- E. ## IV. EXPERIMENTS In this section, we demonstrate the ability of our proposed Deep Gate2 to learn functionality- aware circuit representations. Firstly, Section IV- A provides the preliminary of our experiments, including details on dataset preparation, evaluation metrics and model settings. Secondly, we compare the effectiveness and efficiency of our Deep Gate2 against Deep Gate [7] and FGNN [8] on two function- related tasks: logic probability prediction (see Section IV- B) and logic equivalence gates identification (see Section IV- C). Thirdly, we investigate the effectiveness of model design and training strategies in Section IV- D and Section IV- E, respectively. ### A. Experiment Settings 1) Dataset Preparation: We use the circuits in Deep Gate [7], which are extracted from ITC'99 [23], IWLS'05 [24], EPFL [25] and Open Core [26]. These circuits consists of 10,824 AIGs with sizes ranging from 36 to 3,214 logic gates. To obtain the incomplete truth table, we generate 15,000 random patterns and record the corresponding response. Following the data preparation method described in Section III-B, we construct a dataset comprising 894,151 node pairs. We create 80/20 training/test splits for model training and evaluation.\n\n<center>Fig. 4. Functionality-aware circuit learning framework </center> 2) Evaluation Metrics: We assess our Deep Gate2 with two tasks. The first task is to predict the logic probability for each logic gate. We calculate the average prediction error (PE) as Eq. (13), where the set \\(\\mathcal{V}\\) includes all logic gates. \\[PE = \\frac{1}{|\\mathcal{V}|}\\sum_{i\\in \\mathcal{V}}|P_{i} - \\hat{P}_{i}| \\quad (13)\\] The second task is to identify the logic equivalence gates within a circuit. A gate pair \\((i,j)\\) is considered as a positive pair if these two logic gates \\(i\\) and \\(j\\) have the same function, where the pairwise Hamming distance of truth tables \\(D_{(i,j)}^{T} = 0\\) . If the similarity \\(S_{(i,j)}\\) between these two embedding vectors \\(h f_{i}\\) and \\(h f_{j}\\) exceeds a certain threshold, the model will recognize the gate pair \\((i,j)\\) as equivalent. The optimal threshold \\(\\theta\\) is determined based on the receiver operating characteristic (ROC). The evaluation metrics is formally defined in Eq. (14), where \\(TP\\) , \\(TN\\) , \\(FP\\) , \\(FN\\) are true positive, true negative, false positive and false negative, respective, and \\(M\\) is the total number of gate pairs. In the following experiments, the performance on logic equivalence gates identification is measured in terms of Recall, Precision, F1- Score and area under curve (AUC). \\[\\begin{array}{rl} & {TP = \\frac{\\sum((D_{(i,j)}^{T} = =0)\\&(S_{(i,j)} > \\theta))}{M}}\\\\ & {TN = \\frac{\\sum((D_{(i,j)}^{T} > 0)\\&(S_{(i,j)}< \\theta))}{M}}\\\\ & {FP = \\frac{\\sum((D_{(i,j)}^{T} > 0)\\&(S_{(i,j)} > \\theta))}{M}}\\\\ & {FN = \\frac{\\sum((D_{(i,j)}^{T} = =0)\\&(S_{(i,j)}< \\theta))}{M}} \\end{array} \\quad (14)\\] We conduct the following performance comparisons on 10 industrial circuits, with circuit sizes ranging from \\(3.18k\\) gates to \\(40.50k\\) gates. 3) Model Settings: In the one-round GNN model configuration, the dimension of both structural embedding \\(hs\\) and functional embedding \\(hf\\) is 64. Both \\(MLP_{prob}\\) and \\(MLP_{rc}\\) contain 1 hidden layer with 32 neurons and a Re Lu activation function. The model is trained for 60 epochs to ensure each model can converge. The other models [7], [8] mentioned in the following experiments maintain their original settings and are trained until they converge. We train all the models for 80 epochs with batch-size 16 on a single Nvidia V100 GPU. We adopt the Adam optimizer [27] with learning rate \\(10^{- 4}\\) and weight decay \\(10^{- 10}\\) . TABLE I PERFORMANCE OF DEEPGATE (V1) AND OUR PROPOSED DEEPGATE2 (V2) ON LOGIC PROBABILITY PREDICTION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">#Gates</td><td colspan=\"2\">Deep Gate</td><td colspan=\"2\">Deep Gate2</td><td colspan=\"2\">Reduction</td></tr><tr><td>PE</td><td>Time</td><td>PE</td><td>Time</td><td>PE</td><td>Time(x)</td></tr><tr><td>D1</td><td>19,485</td><td>0.0344</td><td>36.89s</td><td>0.0300</td><td>2.23s</td><td>12.79%</td><td>16.56</td></tr><tr><td>D2</td><td>12,648</td><td>0.0356</td><td>16.11s</td><td>0.0309</td><td>1.18s</td><td>13.20%</td><td>13.66</td></tr><tr><td>D3</td><td>14,686</td><td>0.0355</td><td>21.42s</td><td>0.0294</td><td>1.44s</td><td>17.18%</td><td>14.92</td></tr><tr><td>D4</td><td>7,104</td><td>0.0368</td><td>5.89s</td><td>0.0323</td><td>0.50s</td><td>12.23%</td><td>11.89</td></tr><tr><td>D5</td><td>37,279</td><td>0.0356</td><td>131.22s</td><td>0.0316</td><td>7.82s</td><td>11.24%</td><td>16.79</td></tr><tr><td>D6</td><td>37,383</td><td>0.0325</td><td>133.02s</td><td>0.0285</td><td>8.10s</td><td>12.31%</td><td>16.42</td></tr><tr><td>D7</td><td>10,957</td><td>0.0357</td><td>13.02s</td><td>0.0316</td><td>0.90s</td><td>11.48%</td><td>14.44</td></tr><tr><td>D8</td><td>3,183</td><td>0.0406</td><td>1.60s</td><td>0.0341</td><td>0.17s</td><td>16.01%</td><td>9.64</td></tr><tr><td>D9</td><td>27,820</td><td>0.0368</td><td>77.20s</td><td>0.0322</td><td>4.73s</td><td>12.50%</td><td>16.34</td></tr><tr><td>D10</td><td>40,496</td><td>0.0327</td><td>154.00s</td><td>0.0290</td><td>8.89s</td><td>11.31%</td><td>17.33</td></tr><tr><td>Avg.</td><td></td><td>0.0356</td><td>59.04s</td><td>0.0310</td><td>3.59s</td><td>13.08%</td><td>16.43</td></tr></table> ### B. Comparison with Deep Gate on Probability Prediction We compare the probability prediction error (PE, see Eq. (13)) and runtime (Time) with previous Deep Gate. The previous Deep Gate is denoted as Deep Gate and our proposed model with novel loss function and GNN design is named as Deep Gate2 in Table I. Based on the results presented in the table, we make two observations. First, our proposed Deep Gate2 exhibits more accurate predictions of logic probability compared to the previous version. On average, the probability prediction error (PE) of Deep Gate2 is \\(13.08\\%\\) lower than that of Deep Gate. This suggests that using the novel model architecture with embedding initialization strategy can benefit logic representation learning and lead to better results on logic probability prediction. Second, our Deep Gate2 performs more efficient than Deep Gate. Take the circuit D1 as an example, Deep Gate requires 36.89 seconds for inference, but our Deep Gate2 only needs 2.23s, which is 16.56x faster than the previous Deep Gate. Moreover, compared to the previous model, Deep Gate2 achieves an order of magnitude speedup (16.43x on average) in model runtime. This is attributed to the fact that the GNN model in Deep Gate relies on 10 forward and 10 backward message propagation, whereas the proposed one- round GNN model in Deep Gate2 only performs forward propagation for 1 time. Therefore, the new circuit representation learning model is more effective and efficient than Deep Gate, and demonstrates the generalization ability on large- scale circuits.\n\n### C. Comparison with other Models on Logic Equivalence Gates Identification This section compares the functionality- aware accuracy, as defined in Section IV- A2 of Deep Gate2 with that of two other models: Deep Gate [7] and FGNN [8]. The Deep Gate [7] model treats the logic probability as supervision since it contains the statistical information of truth table. The FGNN [8] is trained to differentiate between logic equivalent and inequivalent circuits using contrastive learning. Table II presents the performance of three models on the task of logic equivalence gates identification. Firstly, our proposed approach outperforms the other two models on all circuits with an average F1- score of 0.9434, while Deep Gate and FGNN only achieve F1- Score 0.6778 and 0.4402, respectively. For instance, in circuit D7, our proposed functionality- aware circuit learning approach achieves an F1- Score of 0.9831 and accurately identifies \\(99.15\\%\\) of logic equivalence gate pairs with a precision of \\(97.48\\%\\) , indicating a low false positive rate. In contrast, Deep Gate only achieves an F1- score of 0.6778, while FGNN fails on most of the pairs. Secondly, although Deep Gate has an average recall of \\(91.46\\%\\) , its precision is only \\(54.00\\%\\) , indicating a large number of false positive identifications. This is because Deep Gate can only identify logic equivalent pairs by predicting logic probability, which leads to incorrect identification of gate pairs with similar logic probability. According to our further experiment, in \\(80.83\\%\\) of false positive pairs, the model incorrectly identifies gate pairs with similar logic probability as functionally equivalent. Thirdly, FGNN achieves the lowest performance among the other models, with only 0.4402 F1- Score. The poor performance of FGNN is attributed to the lack of effective supervision. While FGCN learns to identify logic equivalence circuits generated by perturbing local structures slightly, the model tends to consider circuits with similar structures to have the same functionality. However, in the validation dataset and practical applications, two circuits may have the same function even if their topological structures are extremely different. Therefore, the self- supervised approach limits the effectiveness of FGNN in identifying logic equivalence gates. ### D. Effectiveness of PI Encoding Strategy To demonstrate the effectiveness of our proposed PI encoding (PIE) strategy, we trained another model without assigning unique identifications for PIs, which we refer to as w/o PIE. The results are presented in Table III, which show that disabling the PIE reduces the F1- Score of identifying logic equivalence gates from 0.9434 to 0.7541, resulting in an average reduction of \\(20.07\\%\\) . Such reduction can be attributed to the fact that, as demonstrated as the failure case in Section II and Fig. 1, the one- round GNN model without the PIE strategy cannot model the structural information of the circuit. More specifically, the accuracy of the reconvergence structure identification task with w/ PIE model is \\(93.22\\%\\) , while the w/o model only achieve \\(74.56\\%\\) . The functionality of logic gate is affected by both functionality of fan- in gates and whether there is reconvergence between its fan- in gates. Once the reconvergence structure cannot be accurately identified, node functionality cannot be modeled accurately. ### E. Effectiveness of Training Strategies To investigate the effectiveness of our multi- stage training strategy, we train another model (noted as w/o multi- stage model) with all loss functions in only one stage, instead of adding the functionality- aware loss function in the second stage. The original model with multiple stages training strategy is noted as w/ multi- stage model. The w/ multi- stage model learn to predict the logic probability and structural correlation in the first stage and learn the more difficult task, which predicts the functionality in the second stage. The results are shown in Table IV, where the model w/ multi- stage achieves an F1- Score of 0.9434 on average and the model w/o multi- stage achieves only 0.7137. We analyze the reason as follows. The cost of comparing each pair of logic gates in the task of learning functionality is extremely high, which is proportional to the square of the circuit size. We limit the dataset and train the model to learn functional similarity only among pairs with similar logic probability, which is a necessary condition for functional equivalence. Therefore, without the staged multi- stage strategy, be effectively supervised with the simplified dataset, leading to poor performance in learning functionality. As shown in Table V, the differences between the two models in the loss values for predicting logic probability \\((L_{prob})\\) and identifying reconvergence structures \\((L_{rc})\\) are not significant, indicating that they perform similarly in these two tasks. However, compared to the w/o multi- stage model, the w/ multi- stage model performs better in learning functionality with \\(L_{func} = 0.0594\\) , which is \\(51.47\\%\\) smaller than that of w/o multi- stage model. However, the w/ multi- stage model outperforms the model w/o multi- stage in learning functionality task with a significantly lower \\(L_{func}\\) value of 0.0594, which is \\(51.47\\%\\) smaller than that of the latter. ## V. DOWNSTREAM TASKS In this section, we combine our Deep Gate2 with the open- source EDA tools and apply our model to practical EDA tasks: logic synthesis and Boolean satisfiability (SAT) solving. The logic synthesis tools aim to identify logic equivalence gates as quickly as possible. In Section V- A, our proposed functionality- aware circuit learning model provides guidance to the logic synthesis tool about the logic similarity. Additionally, in Section V- B, we apply the learnt functional similarity in SAT solving, where the variables with dissimilar functionality are assigned the same decision value. This approach efficiently shrinks the search space by enabling solvers to encounter more constraints. ### A. Logic Synthesis This subsection shows the effectiveness of our proposed functionality- aware circuit learning framework in SAT- sweeping [28], a common technique of logic synthesis. Fig. 5 illustrates the components of a typical ecosystem for SAT- sweeping engine (also called SAT sweeper), where including equivalence class (EC) manager, SAT- sweeping manager, simulator, and SAT solver. All computations are coordinated by the SAT- sweeping manager [29]. The SAT sweeper starts by computing candidate ECs using several rounds of initial simulation and storing ECs into EC manager. In the next step, the SAT- sweeping manager selects two gates within an EC and then calls the SAT solver to check whether they are equivalent. If so, the EC manager merges these two gates. Otherwise, SAT solver will return a satisfiable assignment as a counterexample for incremental simulation to refine the candidate ECs. To the best of our knowledge, most SAT- sweeping managers select EC only based on the circuit structure, without efficient heuristic strategy considering the functionality of candidate gates. We will introduce the functional information into SAT- sweeping manager to further improve efficiency. 1) Experiment Settings: We combine our Deep Gate2 into SAT sweeper to guide EC selection. To be specific, the updated manager sorts all candidate equivalence classes by computing the cosine similarity of their embeddings. Unlike traditional SAT sweepers, our\n\nTABLE II PERFORMANCE OF DIFFERENT MODELS ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">Size</td><td colspan=\"3\">Func Model</td><td colspan=\"3\">Deep Gate</td><td colspan=\"3\">FGNN</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>19,485</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>94.59%</td><td>52.69%</td><td>0.6768</td><td>63.64%</td><td>41.18%</td><td>0.5000</td></tr><tr><td>D2</td><td>12,648</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>92.07%</td><td>52.98%</td><td>0.6726</td><td>60.87%</td><td>35.00%</td><td>0.4444</td></tr><tr><td>D3</td><td>14,686</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>93.25%</td><td>62.08%</td><td>0.7454</td><td>72.22%</td><td>44.83%</td><td>0.5532</td></tr><tr><td>D4</td><td>7,104</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>90.11%</td><td>46.33%</td><td>0.6120</td><td>62.73%</td><td>23.53%</td><td>0.3422</td></tr><tr><td>D5</td><td>37,279</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>93.69%</td><td>56.72%</td><td>0.7066</td><td>64.00%</td><td>69.57%</td><td>0.6667</td></tr><tr><td>D6</td><td>37,383</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>93.23%</td><td>60.49%</td><td>0.7337</td><td>59.09%</td><td>46.67%</td><td>0.5215</td></tr><tr><td>D7</td><td>10,957</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>88.89%</td><td>48.83%</td><td>0.6303</td><td>36.36%</td><td>23.53%</td><td>0.2857</td></tr><tr><td>D8</td><td>3,183</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>82.86%</td><td>49.90%</td><td>0.6229</td><td>62.63%</td><td>22.22%</td><td>0.3280</td></tr><tr><td>D9</td><td>27,820</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>92.51%</td><td>48.77%</td><td>0.6387</td><td>62.50%</td><td>26.79%</td><td>0.3750</td></tr><tr><td>D10</td><td>40,496</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>93.35%</td><td>61.22%</td><td>0.7395</td><td>47.02%</td><td>32.63%</td><td>0.3853</td></tr><tr><td>Avg.</td><td></td><td>98.73%</td><td>90.49%</td><td>0.9434</td><td>91.46%</td><td>54.00%</td><td>0.6778</td><td>59.11%</td><td>36.60%</td><td>0.4402</td></tr></table> TABLE III PERFORMANCE COMPARISON BETWEEN W/PIE AND W/O PIE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ PIE</td><td colspan=\"3\">w/o PIE</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>70.66%</td><td>64.66%</td><td>0.6753</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>87.20%</td><td>78.57%</td><td>0.8266</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>79.15%</td><td>76.21%</td><td>0.7765</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>87.91%</td><td>59.70%</td><td>0.7111</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>79.23%</td><td>76.73%</td><td>0.7796</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>87.10%</td><td>77.56%</td><td>0.8205</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>84.62%</td><td>64.13%</td><td>0.7296</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>74.29%</td><td>83.87%</td><td>0.7879</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>62.27%</td><td>76.27%</td><td>0.6856</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>87.07%</td><td>65.54%</td><td>0.7479</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7541</td></tr></table> TABLE IV PERFORMANCE COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ multi-stage</td><td colspan=\"3\">w/o multi-stage</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>79.46%</td><td>67.68%</td><td>0.7310</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>74.39%</td><td>63.21%</td><td>0.6835</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>70.46%</td><td>62.78%</td><td>0.6640</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>73.63%</td><td>72.83%</td><td>0.7323</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>68.02%</td><td>77.19%</td><td>0.7232</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>71.46%</td><td>86.01%</td><td>0.7806</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>65.81%</td><td>67.46%</td><td>0.6662</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>71.43%</td><td>86.21%</td><td>0.7813</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>74.52%</td><td>63.17%</td><td>0.6838</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>75.10%</td><td>64.02%</td><td>0.6912</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7137</td></tr></table> TABLE V LOSS COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE <table><tr><td></td><td>w/o multi-stage</td><td>w/ multi-stage</td><td>Reduction</td></tr><tr><td>Lprob</td><td>0.0205</td><td>0.0207</td><td>-0.98%</td></tr><tr><td>Lrc</td><td>0.1186</td><td>0.1115</td><td>5.99%</td></tr><tr><td>Lfunc</td><td>0.1224</td><td>0.0594</td><td>51.47%</td></tr></table> proposed SAT sweeper does not need to validate the equivalence of all candidate ECs in one pass. Instead, node pairs with higher similarity have high priority for SAT solver calls. If the gate pair is formally proved to be equivalent, these two gate are merged. Otherwise, the generated counterexample should contain more conflicts than the <center>Fig. 5. The proposed SAT-sweeping ecosystem. </center> baseline method, resulting in better efficiency for refining candidate ECs. Our model is equipped into ABC [30] as a plug- in and integrated into the SAT sweeper '&frag' [14], which is one of the most efficient and scalable SAT sweeper publicly available at this time. The AIGs derived by merging the resulting equivalence nodes are verified by '&ecc' command in ABC to ensure functional correctness. All experiments are performed on a \\(2.40\\mathrm{GHz}\\) Intel(R) Xeon(R) Silver 4210R CPU with 64GB of main memory. A single core and less than 1GB was used for any test case considered in this subsection. The proposed SAT sweeper (named as Our) is compared against the original engine, &frag. 2) Results: We validate the performance of our SAT sweeper with 6 industrial circuits. As shown in Table VI, section \"Statistics\" lists the number of PI and PO (PI/PO), logic levels (Lev) and internal AND-nodes in the original AIG (And). To ensure the fairness of comparison, the circuits after sweeping should have the same size. Section \"SAT calls\" lists the number of satisfiable SAT calls, performed by the solver employed in each engine. The data shows that our proposed engine decreases the number of satisfiable SAT calls, that explains why it has better results, since more resource are used to prove equivalence gates. In addition, section \"Total runtime\" compares the runtime and section \"Red.\" shows the runtime reduction from &frag to Our. The number of \"SAT calls\" can get an average reduction of \\(53.37\\%\\) (95.88% maximum) through the integration of our Deep Gate2 model. As for \"Total runtime\", this experiment shows that the Deep Gate2- based SAT sweeper outperforms state- of- the- art engines, while reducing the average runtime by \\(49.46\\%\\) (57.77% maximum). Thus, the sweeper formally verifies the equivalence of functionally similar gates\n\nTABLE VI COMPARING THE NUMBER OF SAT CALLS AND THE RUNTIME OF THE SAT SWEEPERS <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"2\">Statistic</td><td colspan=\"2\">SAT calls</td><td colspan=\"2\">Total Runtime (s)</td></tr><tr><td>PI/PO</td><td>Lev</td><td>And</td><td>#frag</td><td>Our Red.</td><td>#frag</td></tr><tr><td>C1</td><td>128/128</td><td>4,372</td><td>57,247</td><td>13,826</td><td>570</td><td>95.88%</td></tr><tr><td>C2</td><td>24/25</td><td>225</td><td>5,416</td><td>100</td><td>74</td><td>26.00%</td></tr><tr><td>C3</td><td>22/1</td><td>29</td><td>703</td><td>4</td><td>1</td><td>75.00%</td></tr><tr><td>C4</td><td>114/1</td><td>91</td><td>19,354</td><td>20</td><td>12</td><td>40.00%</td></tr><tr><td>C5</td><td>126/1</td><td>83</td><td>20,971</td><td>6</td><td>4</td><td>33.33%</td></tr><tr><td>C6</td><td>96/1</td><td>79</td><td>14,389</td><td>10</td><td>5</td><td>50.00%</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>53.37%</td><td>49.46%</td></tr></table> with the guidance from Deep Gate2, thereby reducing the number of invalid SAT calls and improving efficiency of SAT sweeping. Take C1 as a representative example, the baseline &frag selects gates in EC for formal verification without considering their behaviour, thus, many solver calls return satisfiable results, and few gates can be merged. However, with the guidance of Deep Gate2, the sweeper can prioritize the selection of gates with similar behavior, resulting in a significant reduction of \\(95.88\\%\\) in SAT calls and \\(57.77\\%\\) in runtime. ### B. Boolean Satisfiability Solving Boolean satisfiability (SAT) solving is a long- standing and fundamental NP- complete problem with applications in many areas, especially in electronic design automation (EDA) [31]- [33]. The existing SAT solvers are designed to incorporate efficient heuristics [34]- [36] to expedite the solving process. For instance, [34] proposes to utilize the correlation of logic gate functionality to enforce variable decision for solving circuit- based SAT instances. Although the solution achieves remarkable speedup over SAT solvers, it still relies on the time- consuming logic simulation to obtain the functionality. Based on [34], we demonstrate how the Deep Gate2 models functional correlation efficiently and accelerates SAT solving. 1) Experiment Settings: We integrate our Deep Gate2 into a modern SAT solver, Ca Di Cal [15] to solve the instances from logic equivalence checking (LEC) task. Firstly, we obtain gate-level embeddings of the original circuit and predict the pairwise functional similarity between these gates. Given the one-to-one mapping [37] between circuits and conjunctive normal form (a problem format required by SAT solvers), we can easily transfer the gate functional similarity to variable behavioral similarity. If two logic gates have similar representations (and therefore similar functionality), their corresponding variables should be correlated and grouped together during the variable decision process. Secondly, we incorporate the learnt knowledge into the SAT solver. As shown in Algorithm 1, when the current variable \\(s\\) is assigned a value \\(v\\) , we identify all unassigned variables \\(s'\\) in the set \\(S\\) that contains correlated variables with \\(s\\) . As modern SAT solvers reduce searching space by detecting conflicts as much as possible [38], we assign the reverse value \\(\\bar{v}\\) to \\(s'\\) to promptly cause conflict for joint decision. Besides, the threshold \\(\\delta\\) in Algorithm 1 is set to \\(1e - 5\\) . Thirdly, to evaluate the efficacy of our model in accelerating SAT solving, we compare the aforementioned hybrid solver (labeled as Our) with original Ca Di Cal [15] (labeled as Baseline) on 5 industrial instances. All experiments are conducted with a single 2.40GHz Intel(R) Xeon(R) E5- 2640 v4 CPU. 2) Results: The runtime comparison between Baseline and Our are listed in Table VII. To ensure a fair comparison, we aggregate the Deep Gate2 model inference time (Model) and SAT solver runtime (Solver) as the Overall runtime. We have the following observations. Algorithm 1 Variable Decision Function with Deep Gate2 Current variable \\(s\\) just being assigned a value \\(v\\) ; Set \\(S\\) containing the correlated variables with \\(s\\) ; Function \\(\\mathrm{Sim}(s_i,s_j)\\) to calculate the behaviour similarity of two variables; \\(\\delta\\) is the threshold for the joint decision Function \\(V(s)\\) to get the assigned value of variable \\(s\\) ; Function Decision \\((s,v)\\) to assign the decision value \\(v\\) to the current decision variable \\(s\\) . 1: Decision \\((s,v)\\) 2: for \\(s'\\) in \\(S\\) do 3: if \\(s' \\neq s\\) and \\(V(s') = \\text{None}\\) then 4: if \\(\\mathrm{Sim}(s',s) > 1 - \\delta\\) then 5: Decision \\((s',\\bar{v})\\) 6: end if 7: end if 8: end for TABLE VII COMPARING THE RUNTIME BETWEEN BASELINE AND OUR SOLVERS <table><tr><td rowspan=\"2\">Instance</td><td rowspan=\"2\">Size</td><td rowspan=\"2\">Baseline(s)</td><td colspan=\"2\">Our(s)</td><td rowspan=\"2\">Reduction</td></tr><tr><td>Model</td><td>Solver</td></tr><tr><td>I1</td><td>17,495</td><td>88.01</td><td>1.77</td><td>30.25</td><td>32.02</td></tr><tr><td>I2</td><td>21,952</td><td>29.36</td><td>2.85</td><td>6.01</td><td>8.86</td></tr><tr><td>I3</td><td>23,810</td><td>61.24</td><td>3.25</td><td>32.88</td><td>36.13</td></tr><tr><td>I4</td><td>27,606</td><td>158.04</td><td>4.36</td><td>137.77</td><td>142.13</td></tr><tr><td>I5</td><td>28,672</td><td>89.89</td><td>4.78</td><td>70.95</td><td>75.73</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>40.05%</td></tr></table> First, our method achieves a substantial reduction in total runtime for all test cases, with an average runtime reduction of \\(40.05\\%\\) . Take I1 as an example, the plain solver requires \\(88.01\\mathrm{s}\\) to solve the problem, but by combining with our model, the new solver produces results in only 32.02s, reducing runtime by \\(63.62\\%\\) . Second, our model only takes a few seconds to obtain embeddings, occupying less than \\(10\\%\\) of overall runtime on average. It should be noted that our Deep Gate2 is able to infer within polynomial time that is only proportional to the size of instance. Third, while the two largest instances I4 and I5 show less reduction than the others, it does not necessarily mean that our model is unable to generalize to larger instances. As evidenced by the results for I2, an instance with a similar size to I4 and I5 also demonstrates a significant reduction. The reduction caused by our model should be determined by the characteristics of instance. In summary, our model is effective in speeding up downstream SAT solving. ## VI. CONCLUSION This paper introduces Deep Gate2, a novel functionally- aware framework for circuit representation learning. Our approach leverages the pairwise truth table differences of logic gates as a supervisory signal, providing rich functionality supervision and proving scalable for large circuits. Moreover, Deep Gate2 differentiates and concurrently updates structural and functional embeddings in two dedicated flows, acquiring comprehensive representations through a single round of GNN forward message- passing. In comparison to its predecessor, Deep Gate2 demonstrates enhanced performance in logic probability prediction and logic equivalent gate identification, while simultaneously improving model efficiency tenfold. The applications of Deep Gate2 onto multiple downstream tasks further demonstrate its effectiveness and potential utility in the EDA field.",
      "level": 2,
      "line_start": 4,
      "line_end": 20
    },
    {
      "heading": "C. Comparison with other Models on Logic Equivalence Gates Identification This section compares the functionality- aware accuracy, as defined in Section IV- A2 of Deep Gate2 with that of two other models: Deep Gate [7] and FGNN [8]. The Deep Gate [7] model treats the logic probability as supervision since it contains the statistical information of truth table. The FGNN [8] is trained to differentiate between logic equivalent and inequivalent circuits using contrastive learning. Table II presents the performance of three models on the task of logic equivalence gates identification. Firstly, our proposed approach outperforms the other two models on all circuits with an average F1- score of 0.9434, while Deep Gate and FGNN only achieve F1- Score 0.6778 and 0.4402, respectively. For instance, in circuit D7, our proposed functionality- aware circuit learning approach achieves an F1- Score of 0.9831 and accurately identifies \\(99.15\\%\\) of logic equivalence gate pairs with a precision of \\(97.48\\%\\) , indicating a low false positive rate. In contrast, Deep Gate only achieves an F1- score of 0.6778, while FGNN fails on most of the pairs. Secondly, although Deep Gate has an average recall of \\(91.46\\%\\) , its precision is only \\(54.00\\%\\) , indicating a large number of false positive identifications. This is because Deep Gate can only identify logic equivalent pairs by predicting logic probability, which leads to incorrect identification of gate pairs with similar logic probability. According to our further experiment, in \\(80.83\\%\\) of false positive pairs, the model incorrectly identifies gate pairs with similar logic probability as functionally equivalent. Thirdly, FGNN achieves the lowest performance among the other models, with only 0.4402 F1- Score. The poor performance of FGNN is attributed to the lack of effective supervision. While FGCN learns to identify logic equivalence circuits generated by perturbing local structures slightly, the model tends to consider circuits with similar structures to have the same functionality. However, in the validation dataset and practical applications, two circuits may have the same function even if their topological structures are extremely different. Therefore, the self- supervised approach limits the effectiveness of FGNN in identifying logic equivalence gates. ### D. Effectiveness of PI Encoding Strategy To demonstrate the effectiveness of our proposed PI encoding (PIE) strategy, we trained another model without assigning unique identifications for PIs, which we refer to as w/o PIE. The results are presented in Table III, which show that disabling the PIE reduces the F1- Score of identifying logic equivalence gates from 0.9434 to 0.7541, resulting in an average reduction of \\(20.07\\%\\) . Such reduction can be attributed to the fact that, as demonstrated as the failure case in Section II and Fig. 1, the one- round GNN model without the PIE strategy cannot model the structural information of the circuit. More specifically, the accuracy of the reconvergence structure identification task with w/ PIE model is \\(93.22\\%\\) , while the w/o model only achieve \\(74.56\\%\\) . The functionality of logic gate is affected by both functionality of fan- in gates and whether there is reconvergence between its fan- in gates. Once the reconvergence structure cannot be accurately identified, node functionality cannot be modeled accurately. ### E. Effectiveness of Training Strategies To investigate the effectiveness of our multi- stage training strategy, we train another model (noted as w/o multi- stage model) with all loss functions in only one stage, instead of adding the functionality- aware loss function in the second stage. The original model with multiple stages training strategy is noted as w/ multi- stage model. The w/ multi- stage model learn to predict the logic probability and structural correlation in the first stage and learn the more difficult task, which predicts the functionality in the second stage. The results are shown in Table IV, where the model w/ multi- stage achieves an F1- Score of 0.9434 on average and the model w/o multi- stage achieves only 0.7137. We analyze the reason as follows. The cost of comparing each pair of logic gates in the task of learning functionality is extremely high, which is proportional to the square of the circuit size. We limit the dataset and train the model to learn functional similarity only among pairs with similar logic probability, which is a necessary condition for functional equivalence. Therefore, without the staged multi- stage strategy, be effectively supervised with the simplified dataset, leading to poor performance in learning functionality. As shown in Table V, the differences between the two models in the loss values for predicting logic probability \\((L_{prob})\\) and identifying reconvergence structures \\((L_{rc})\\) are not significant, indicating that they perform similarly in these two tasks. However, compared to the w/o multi- stage model, the w/ multi- stage model performs better in learning functionality with \\(L_{func} = 0.0594\\) , which is \\(51.47\\%\\) smaller than that of w/o multi- stage model. However, the w/ multi- stage model outperforms the model w/o multi- stage in learning functionality task with a significantly lower \\(L_{func}\\) value of 0.0594, which is \\(51.47\\%\\) smaller than that of the latter. ## V. DOWNSTREAM TASKS In this section, we combine our Deep Gate2 with the open- source EDA tools and apply our model to practical EDA tasks: logic synthesis and Boolean satisfiability (SAT) solving. The logic synthesis tools aim to identify logic equivalence gates as quickly as possible. In Section V- A, our proposed functionality- aware circuit learning model provides guidance to the logic synthesis tool about the logic similarity. Additionally, in Section V- B, we apply the learnt functional similarity in SAT solving, where the variables with dissimilar functionality are assigned the same decision value. This approach efficiently shrinks the search space by enabling solvers to encounter more constraints. ### A. Logic Synthesis This subsection shows the effectiveness of our proposed functionality- aware circuit learning framework in SAT- sweeping [28], a common technique of logic synthesis. Fig. 5 illustrates the components of a typical ecosystem for SAT- sweeping engine (also called SAT sweeper), where including equivalence class (EC) manager, SAT- sweeping manager, simulator, and SAT solver. All computations are coordinated by the SAT- sweeping manager [29]. The SAT sweeper starts by computing candidate ECs using several rounds of initial simulation and storing ECs into EC manager. In the next step, the SAT- sweeping manager selects two gates within an EC and then calls the SAT solver to check whether they are equivalent. If so, the EC manager merges these two gates. Otherwise, SAT solver will return a satisfiable assignment as a counterexample for incremental simulation to refine the candidate ECs. To the best of our knowledge, most SAT- sweeping managers select EC only based on the circuit structure, without efficient heuristic strategy considering the functionality of candidate gates. We will introduce the functional information into SAT- sweeping manager to further improve efficiency. 1) Experiment Settings: We combine our Deep Gate2 into SAT sweeper to guide EC selection. To be specific, the updated manager sorts all candidate equivalence classes by computing the cosine similarity of their embeddings. Unlike traditional SAT sweepers, our",
      "content": "TABLE II PERFORMANCE OF DIFFERENT MODELS ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td rowspan=\"2\">Size</td><td colspan=\"3\">Func Model</td><td colspan=\"3\">Deep Gate</td><td colspan=\"3\">FGNN</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>19,485</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>94.59%</td><td>52.69%</td><td>0.6768</td><td>63.64%</td><td>41.18%</td><td>0.5000</td></tr><tr><td>D2</td><td>12,648</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>92.07%</td><td>52.98%</td><td>0.6726</td><td>60.87%</td><td>35.00%</td><td>0.4444</td></tr><tr><td>D3</td><td>14,686</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>93.25%</td><td>62.08%</td><td>0.7454</td><td>72.22%</td><td>44.83%</td><td>0.5532</td></tr><tr><td>D4</td><td>7,104</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>90.11%</td><td>46.33%</td><td>0.6120</td><td>62.73%</td><td>23.53%</td><td>0.3422</td></tr><tr><td>D5</td><td>37,279</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>93.69%</td><td>56.72%</td><td>0.7066</td><td>64.00%</td><td>69.57%</td><td>0.6667</td></tr><tr><td>D6</td><td>37,383</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>93.23%</td><td>60.49%</td><td>0.7337</td><td>59.09%</td><td>46.67%</td><td>0.5215</td></tr><tr><td>D7</td><td>10,957</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>88.89%</td><td>48.83%</td><td>0.6303</td><td>36.36%</td><td>23.53%</td><td>0.2857</td></tr><tr><td>D8</td><td>3,183</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>82.86%</td><td>49.90%</td><td>0.6229</td><td>62.63%</td><td>22.22%</td><td>0.3280</td></tr><tr><td>D9</td><td>27,820</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>92.51%</td><td>48.77%</td><td>0.6387</td><td>62.50%</td><td>26.79%</td><td>0.3750</td></tr><tr><td>D10</td><td>40,496</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>93.35%</td><td>61.22%</td><td>0.7395</td><td>47.02%</td><td>32.63%</td><td>0.3853</td></tr><tr><td>Avg.</td><td></td><td>98.73%</td><td>90.49%</td><td>0.9434</td><td>91.46%</td><td>54.00%</td><td>0.6778</td><td>59.11%</td><td>36.60%</td><td>0.4402</td></tr></table> TABLE III PERFORMANCE COMPARISON BETWEEN W/PIE AND W/O PIE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ PIE</td><td colspan=\"3\">w/o PIE</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>70.66%</td><td>64.66%</td><td>0.6753</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>87.20%</td><td>78.57%</td><td>0.8266</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>79.15%</td><td>76.21%</td><td>0.7765</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>87.91%</td><td>59.70%</td><td>0.7111</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>79.23%</td><td>76.73%</td><td>0.7796</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>87.10%</td><td>77.56%</td><td>0.8205</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>84.62%</td><td>64.13%</td><td>0.7296</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>74.29%</td><td>83.87%</td><td>0.7879</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>62.27%</td><td>76.27%</td><td>0.6856</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>87.07%</td><td>65.54%</td><td>0.7479</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7541</td></tr></table> TABLE IV PERFORMANCE COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE ON LOGIC EQUIVALENCE GATES IDENTIFICATION <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"3\">w/ multi-stage</td><td colspan=\"3\">w/o multi-stage</td></tr><tr><td>Recall</td><td>Precision</td><td>F1-Score</td><td>Recall</td><td>Precision</td><td>F1-Score</td></tr><tr><td>D1</td><td>98.46%</td><td>84.34%</td><td>0.9085</td><td>79.46%</td><td>67.68%</td><td>0.7310</td></tr><tr><td>D2</td><td>99.39%</td><td>89.07%</td><td>0.9395</td><td>74.39%</td><td>63.21%</td><td>0.6835</td></tr><tr><td>D3</td><td>97.89%</td><td>84.36%</td><td>0.9062</td><td>70.46%</td><td>62.78%</td><td>0.6640</td></tr><tr><td>D4</td><td>98.90%</td><td>97.83%</td><td>0.9836</td><td>73.63%</td><td>72.83%</td><td>0.7323</td></tr><tr><td>D5</td><td>99.80%</td><td>88.49%</td><td>0.9381</td><td>68.02%</td><td>77.19%</td><td>0.7232</td></tr><tr><td>D6</td><td>98.31%</td><td>93.19%</td><td>0.9568</td><td>71.46%</td><td>86.01%</td><td>0.7806</td></tr><tr><td>D7</td><td>99.15%</td><td>97.48%</td><td>0.9831</td><td>65.81%</td><td>67.46%</td><td>0.6662</td></tr><tr><td>D8</td><td>97.14%</td><td>97.14%</td><td>0.9714</td><td>71.43%</td><td>86.21%</td><td>0.7813</td></tr><tr><td>D9</td><td>99.74%</td><td>85.16%</td><td>0.9188</td><td>74.52%</td><td>63.17%</td><td>0.6838</td></tr><tr><td>D10</td><td>98.48%</td><td>87.80%</td><td>0.9283</td><td>75.10%</td><td>64.02%</td><td>0.6912</td></tr><tr><td>Avg.</td><td></td><td></td><td>0.9434</td><td></td><td></td><td>0.7137</td></tr></table> TABLE V LOSS COMPARISON BETWEEN W/MULTI-STAGE AND W/O MULTI-STAGE <table><tr><td></td><td>w/o multi-stage</td><td>w/ multi-stage</td><td>Reduction</td></tr><tr><td>Lprob</td><td>0.0205</td><td>0.0207</td><td>-0.98%</td></tr><tr><td>Lrc</td><td>0.1186</td><td>0.1115</td><td>5.99%</td></tr><tr><td>Lfunc</td><td>0.1224</td><td>0.0594</td><td>51.47%</td></tr></table> proposed SAT sweeper does not need to validate the equivalence of all candidate ECs in one pass. Instead, node pairs with higher similarity have high priority for SAT solver calls. If the gate pair is formally proved to be equivalent, these two gate are merged. Otherwise, the generated counterexample should contain more conflicts than the <center>Fig. 5. The proposed SAT-sweeping ecosystem. </center> baseline method, resulting in better efficiency for refining candidate ECs. Our model is equipped into ABC [30] as a plug- in and integrated into the SAT sweeper '&frag' [14], which is one of the most efficient and scalable SAT sweeper publicly available at this time. The AIGs derived by merging the resulting equivalence nodes are verified by '&ecc' command in ABC to ensure functional correctness. All experiments are performed on a \\(2.40\\mathrm{GHz}\\) Intel(R) Xeon(R) Silver 4210R CPU with 64GB of main memory. A single core and less than 1GB was used for any test case considered in this subsection. The proposed SAT sweeper (named as Our) is compared against the original engine, &frag. 2) Results: We validate the performance of our SAT sweeper with 6 industrial circuits. As shown in Table VI, section \"Statistics\" lists the number of PI and PO (PI/PO), logic levels (Lev) and internal AND-nodes in the original AIG (And). To ensure the fairness of comparison, the circuits after sweeping should have the same size. Section \"SAT calls\" lists the number of satisfiable SAT calls, performed by the solver employed in each engine. The data shows that our proposed engine decreases the number of satisfiable SAT calls, that explains why it has better results, since more resource are used to prove equivalence gates. In addition, section \"Total runtime\" compares the runtime and section \"Red.\" shows the runtime reduction from &frag to Our. The number of \"SAT calls\" can get an average reduction of \\(53.37\\%\\) (95.88% maximum) through the integration of our Deep Gate2 model. As for \"Total runtime\", this experiment shows that the Deep Gate2- based SAT sweeper outperforms state- of- the- art engines, while reducing the average runtime by \\(49.46\\%\\) (57.77% maximum). Thus, the sweeper formally verifies the equivalence of functionally similar gates\n\nTABLE VI COMPARING THE NUMBER OF SAT CALLS AND THE RUNTIME OF THE SAT SWEEPERS <table><tr><td rowspan=\"2\">Circuit</td><td colspan=\"2\">Statistic</td><td colspan=\"2\">SAT calls</td><td colspan=\"2\">Total Runtime (s)</td></tr><tr><td>PI/PO</td><td>Lev</td><td>And</td><td>#frag</td><td>Our Red.</td><td>#frag</td></tr><tr><td>C1</td><td>128/128</td><td>4,372</td><td>57,247</td><td>13,826</td><td>570</td><td>95.88%</td></tr><tr><td>C2</td><td>24/25</td><td>225</td><td>5,416</td><td>100</td><td>74</td><td>26.00%</td></tr><tr><td>C3</td><td>22/1</td><td>29</td><td>703</td><td>4</td><td>1</td><td>75.00%</td></tr><tr><td>C4</td><td>114/1</td><td>91</td><td>19,354</td><td>20</td><td>12</td><td>40.00%</td></tr><tr><td>C5</td><td>126/1</td><td>83</td><td>20,971</td><td>6</td><td>4</td><td>33.33%</td></tr><tr><td>C6</td><td>96/1</td><td>79</td><td>14,389</td><td>10</td><td>5</td><td>50.00%</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>53.37%</td><td>49.46%</td></tr></table> with the guidance from Deep Gate2, thereby reducing the number of invalid SAT calls and improving efficiency of SAT sweeping. Take C1 as a representative example, the baseline &frag selects gates in EC for formal verification without considering their behaviour, thus, many solver calls return satisfiable results, and few gates can be merged. However, with the guidance of Deep Gate2, the sweeper can prioritize the selection of gates with similar behavior, resulting in a significant reduction of \\(95.88\\%\\) in SAT calls and \\(57.77\\%\\) in runtime. ### B. Boolean Satisfiability Solving Boolean satisfiability (SAT) solving is a long- standing and fundamental NP- complete problem with applications in many areas, especially in electronic design automation (EDA) [31]- [33]. The existing SAT solvers are designed to incorporate efficient heuristics [34]- [36] to expedite the solving process. For instance, [34] proposes to utilize the correlation of logic gate functionality to enforce variable decision for solving circuit- based SAT instances. Although the solution achieves remarkable speedup over SAT solvers, it still relies on the time- consuming logic simulation to obtain the functionality. Based on [34], we demonstrate how the Deep Gate2 models functional correlation efficiently and accelerates SAT solving. 1) Experiment Settings: We integrate our Deep Gate2 into a modern SAT solver, Ca Di Cal [15] to solve the instances from logic equivalence checking (LEC) task. Firstly, we obtain gate-level embeddings of the original circuit and predict the pairwise functional similarity between these gates. Given the one-to-one mapping [37] between circuits and conjunctive normal form (a problem format required by SAT solvers), we can easily transfer the gate functional similarity to variable behavioral similarity. If two logic gates have similar representations (and therefore similar functionality), their corresponding variables should be correlated and grouped together during the variable decision process. Secondly, we incorporate the learnt knowledge into the SAT solver. As shown in Algorithm 1, when the current variable \\(s\\) is assigned a value \\(v\\) , we identify all unassigned variables \\(s'\\) in the set \\(S\\) that contains correlated variables with \\(s\\) . As modern SAT solvers reduce searching space by detecting conflicts as much as possible [38], we assign the reverse value \\(\\bar{v}\\) to \\(s'\\) to promptly cause conflict for joint decision. Besides, the threshold \\(\\delta\\) in Algorithm 1 is set to \\(1e - 5\\) . Thirdly, to evaluate the efficacy of our model in accelerating SAT solving, we compare the aforementioned hybrid solver (labeled as Our) with original Ca Di Cal [15] (labeled as Baseline) on 5 industrial instances. All experiments are conducted with a single 2.40GHz Intel(R) Xeon(R) E5- 2640 v4 CPU. 2) Results: The runtime comparison between Baseline and Our are listed in Table VII. To ensure a fair comparison, we aggregate the Deep Gate2 model inference time (Model) and SAT solver runtime (Solver) as the Overall runtime. We have the following observations. Algorithm 1 Variable Decision Function with Deep Gate2 Current variable \\(s\\) just being assigned a value \\(v\\) ; Set \\(S\\) containing the correlated variables with \\(s\\) ; Function \\(\\mathrm{Sim}(s_i,s_j)\\) to calculate the behaviour similarity of two variables; \\(\\delta\\) is the threshold for the joint decision Function \\(V(s)\\) to get the assigned value of variable \\(s\\) ; Function Decision \\((s,v)\\) to assign the decision value \\(v\\) to the current decision variable \\(s\\) . 1: Decision \\((s,v)\\) 2: for \\(s'\\) in \\(S\\) do 3: if \\(s' \\neq s\\) and \\(V(s') = \\text{None}\\) then 4: if \\(\\mathrm{Sim}(s',s) > 1 - \\delta\\) then 5: Decision \\((s',\\bar{v})\\) 6: end if 7: end if 8: end for TABLE VII COMPARING THE RUNTIME BETWEEN BASELINE AND OUR SOLVERS <table><tr><td rowspan=\"2\">Instance</td><td rowspan=\"2\">Size</td><td rowspan=\"2\">Baseline(s)</td><td colspan=\"2\">Our(s)</td><td rowspan=\"2\">Reduction</td></tr><tr><td>Model</td><td>Solver</td></tr><tr><td>I1</td><td>17,495</td><td>88.01</td><td>1.77</td><td>30.25</td><td>32.02</td></tr><tr><td>I2</td><td>21,952</td><td>29.36</td><td>2.85</td><td>6.01</td><td>8.86</td></tr><tr><td>I3</td><td>23,810</td><td>61.24</td><td>3.25</td><td>32.88</td><td>36.13</td></tr><tr><td>I4</td><td>27,606</td><td>158.04</td><td>4.36</td><td>137.77</td><td>142.13</td></tr><tr><td>I5</td><td>28,672</td><td>89.89</td><td>4.78</td><td>70.95</td><td>75.73</td></tr><tr><td>Avg.</td><td></td><td></td><td></td><td></td><td>40.05%</td></tr></table> First, our method achieves a substantial reduction in total runtime for all test cases, with an average runtime reduction of \\(40.05\\%\\) . Take I1 as an example, the plain solver requires \\(88.01\\mathrm{s}\\) to solve the problem, but by combining with our model, the new solver produces results in only 32.02s, reducing runtime by \\(63.62\\%\\) . Second, our model only takes a few seconds to obtain embeddings, occupying less than \\(10\\%\\) of overall runtime on average. It should be noted that our Deep Gate2 is able to infer within polynomial time that is only proportional to the size of instance. Third, while the two largest instances I4 and I5 show less reduction than the others, it does not necessarily mean that our model is unable to generalize to larger instances. As evidenced by the results for I2, an instance with a similar size to I4 and I5 also demonstrates a significant reduction. The reduction caused by our model should be determined by the characteristics of instance. In summary, our model is effective in speeding up downstream SAT solving. ## VI. CONCLUSION This paper introduces Deep Gate2, a novel functionally- aware framework for circuit representation learning. Our approach leverages the pairwise truth table differences of logic gates as a supervisory signal, providing rich functionality supervision and proving scalable for large circuits. Moreover, Deep Gate2 differentiates and concurrently updates structural and functional embeddings in two dedicated flows, acquiring comprehensive representations through a single round of GNN forward message- passing. In comparison to its predecessor, Deep Gate2 demonstrates enhanced performance in logic probability prediction and logic equivalent gate identification, while simultaneously improving model efficiency tenfold. The applications of Deep Gate2 onto multiple downstream tasks further demonstrate its effectiveness and potential utility in the EDA field.",
      "level": 3,
      "line_start": 15,
      "line_end": 20
    },
    {
      "heading": "REFERENCES [1] J. Chen, J. Kuang, G. Zhao, D. J.- H. Huang, and E. F. Young, \"Pros: A plug- in for routability optimization applied in the state- of- the- art commercial eda tool using deep learning,\" in Proceedings of the 39th International Conference on Computer- Aided Design, 2020, pp. 1- 8. [2] A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.- J. Lee, E. Johnson, O. Pathak, A. Nazi et al., \"A graph placement methodology for fast chip design,\" Nature, vol. 594, no. 7862, pp. 207- 212, 2021. [3] W. L. Neto, M. Austin, S. Temple, L. Amaru, X. Tang, and P.- E. Gaillardon, \"Losaic: A logic synthesis framework driven by artificial intelligence,\" in 2019 IEEE/ACM International Conference on Computer- Aided Design (ICCAD). IEEE, 2019, pp. 1- 6. [4] W. Haaswijk, E. Collins, B. Seguin, M. Soeken, F. Kaplan, S. Susstrunk, and G. De Micheli, \"Deep learning for logic optimization algorithms,\" in 2018 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE, 2018, pp. 1- 4. [5] Z. Shi, M. Li, S. Khan, L. Wang, N. Wang, Y. Huang, and Q. Xu, \"Deeptpi: Test point insertion with deep reinforcement learning,\" in 2022 IEEE International Test Conference (ITC). IEEE, 2022, pp. 194- 203. [6] J. Huang, H.- L. Zhen, N. Wang, H. Mao, M. Yuan, and Y. Huang, \"Neural fault analysis for sat- based atpg,\" in 2022 IEEE International Test Conference (ITC). IEEE, 2022, pp. 36- 45. [7] M. Li, S. Khan, Z. Shi, N. Wang, H. Yu, and Q. Xu, \"Deepgate: Learning neural representations of logic gates,\" in Proceedings of the 59th ACM/IEEE Design Automation Conference, 2022, pp. 667- 672. [8] Z. Wang, C. Bai, Z. He, G. Zhang, Q. Xu, T.- Y. Ho, B. Yu, and Y. Huang, \"Functionality matters in netlist representation learning,\" in Proceedings of the 59th ACM/IEEE Design Automation Conference, 2022, pp. 61- 66. [9] K. Zhu, H. Chen, W. J. Turner, G. F. Kokai, P.- H. Wei, D. Z. Pan, and H. Ren, \"Tag: Learning circuit spatial embedding from layouts,\" in Proceedings of the 41st IEEE/ACM International Conference on Computer- Aided Design, 2022, pp. 1- 9. [10] Y. Lai, Y. Mu, and P. Luo, \"Maskplace: Fast chip placement via reinforced visual representation learning,\" ar Xiv preprint ar Xiv:2211.13382, 2022. [11] A. Fayyazi, S. Shababi, P. Nuzzo, S. Nazarian, and M. Pedram, \"Deep learning- based circuit recognition using sparse mapping and level- dependent decaying sum circuit representations,\" in 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2019, pp. 638- 641. [12] Z. He, Z. Wang, C. Bail, H. Yang, and B. Yu, \"Graph learning- based arithmetic block identification,\" in 2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD). IEEE, 2021, pp. 1- 8. [13] M. Li, Z. Shi, Q. Lai, S. Khan, and Q. Xu, \"Deepsat: An eda- driven learning framework for sat,\" ar Xiv preprint ar Xiv:2205.13745, 2022. [14] A. Mishchenko, S. Chatterjee, R. Jiang, and R. K. Brayton, \"Fraigs: A unifying representation for logic synthesis and verification,\" ERL Technical Report, Tech. Rep., 2005. [15] S. D. QUEUE, \"Cadical at the sat race 2019,\" SAT RACE 2019, p. 8, 2019. [16] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \"Language models are few- shot learners,\" Advances in neural information processing systems, vol. 33, pp. 1877- 1901, 2020. [17] J. Devlin, M.- W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre- training of deep bidirectional transformers for language understanding,\" ar Xiv preprint ar Xiv:1810.04805, 2018. [18] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, \"Unsupervised feature learning via non- parametric instance discrimination,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3733- 3742. [19] E. Hoffer and N. Ailon, \"Deep metric learning using triplet network,\" in Similarity- Based Pattern Recognition: Third International Workshop, SIMBAD 2015, Copenhagen, Denmark, October 12- 14, 2015. Proceedings 3. Springer, 2015, pp. 84- 92. [20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" Advances in neural information processing systems, vol. 30, 2017. [21] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \"Curriculum learning,\" in Proceedings of the 26th annual international conference on machine learning, 2009, pp. 41- 48. [22] S. Ruder, \"An overview of multi- task learning in deep neural networks,\" ar Xiv preprint ar Xiv:1706.05098, 2017. [23] S. Davidson, \"Characteristics of the itc'99 benchmark circuits,\" in ITSW, 1999. [24] C. Albrecht, \"Iwls 2005 benchmarks,\" in IWLS, 2005. [25] L. Amar\u00fa, P.- E. Gaillardon, and G. De Micheli, \"The epfl combinational benchmark suite,\" in IWLS, no. CONF, 2015. [26] O. Team, \"Opencores,\" https://opencores.org/. [27] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" ar Xiv preprint ar Xiv:1412.6980, 2014. [28] A. Kuehlmann, V. Paruthi, F. Krohm, and M. K. Ganai, \"Robust boolean reasoning for equivalence checking and functional property verification,\" IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, vol. 21, no. 12, pp. 1377- 1394, 2002. [29] A. Mishchenko and R. Brayton, \"Integrating an aig package, simulator, and sat solver,\" in International Workshop on Logic and Synthesis (IWLS), 2018, pp. 11- 16. [30] B. L. Synthesis and V. Group, \"Abc: A system for sequential synthesis and verification.\" http://www.cad.eecs.berkeley.edu/alanmi/abc, 2023. [31] E. I. Goldberg, M. R. Prasad, and R. K. Brayton, \"Using sat for combinational equivalence checking,\" in Proceedings Design, Automation and Test in Europe, Conference and Exhibition 2001. IEEE, 2001, pp. 114- 121. [32] K. L. Mc Millan, \"Interpolation and sat- based model checking,\" in Computer Aided Verification: 15th International Conference, CAV 2003, Boulder, CO, USA, July 8- 12, 2003. Proceedings 15. Springer, 2003, pp. 1- 13. [33] K. Yang, K.- T. Cheng, and L.- C. Wang, \"Trangen: A sat- based atpg for path- oriented transition faults,\" in ASP- DAC 2004: Asia and South Pacific Design Automation Conference 2004 (IEEE Cat. No. 04EX753). IEEE, 2004, pp. 92- 97. [34] F. Lu, L.- C. Wang, K.- T. Cheng, and R.- Y. Huang, \"A circuit sat solver with signal correlation guided learning,\" in 2003 Design, Automation and Test in Europe Conference and Exhibition. IEEE, 2003, pp. 892- 897. [35] G. Audemard and L. Simon, \"Glucose: a solver that predicts learnt clauses quality,\" SAT Competition, pp. 7- 8, 2009. [36] \"On the glucose sat solver,\" International Journal on Artificial Intelligence Tools, vol. 27, no. 01, p. 1840001, 2018. [37] G. S. Tseitin, \"On the complexity of derivation in propositional calculus,\" Automation of reasoning: 2. Classical papers on computational logic 1967- 1970, pp. 466- 483, 1983. [38] J. Marques- Silva, I. Lynce, and S. Malik, \"Conflict- driven clause learning sat solvers,\" in Handbook of satisfiability. IOS press, 2021, pp. 133- 182.",
      "content": "",
      "level": 2,
      "line_start": 21,
      "line_end": 21
    }
  ]
}