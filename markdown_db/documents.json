{
  "5296fb85e84d6890d37f97710f30f93a": {
    "file_path": "sample_papers/tensorflow_large_scale.md",
    "title": "Large-Scale Self-Supervised Learning with TensorFlow",
    "file_hash": "7b8130454e0aaa0ab777d50dabe1a8c4",
    "parsed_at": "2025-12-08T11:51:53.214853",
    "section_count": 13,
    "section_keys": [
      "large-scale_self-supervised_learning_with_tensorflow",
      "abstract",
      "introduction",
      "methodology",
      "architecture",
      "training_procedure",
      "experiments",
      "dataset",
      "training_setup",
      "results",
      "implementation",
      "conclusion",
      "references"
    ]
  },
  "2978d81eacedcc22568390d974b63d9b": {
    "file_path": "sample_papers/pytorch_classification.md",
    "title": "Deep Learning for Image Classification with PyTorch",
    "file_hash": "615b8ddff2f8a00e86da557d196f2f4c",
    "parsed_at": "2025-12-08T11:51:53.214996",
    "section_count": 11,
    "section_keys": [
      "deep_learning_for_image_classification_with_pytorch",
      "abstract",
      "introduction",
      "methodology",
      "experiments",
      "dataset",
      "training_setup",
      "results",
      "implementation",
      "conclusion",
      "references"
    ]
  },
  "c5c51bb0ce0b29139bc147b16406a611": {
    "file_path": "sample_papers/pytorch_mnist.md",
    "title": "Efficient Neural Networks for MNIST Classification",
    "file_hash": "369070f72f3a5f7ada12330a9c3153e1",
    "parsed_at": "2025-12-08T11:51:53.215069",
    "section_count": 13,
    "section_keys": [
      "efficient_neural_networks_for_mnist_classification",
      "abstract",
      "introduction",
      "methodology",
      "model_architecture",
      "training",
      "experiments",
      "dataset",
      "training_configuration",
      "results",
      "code_availability",
      "conclusion",
      "references"
    ]
  },
  "db18ac8883db1a9144cb82763df8b8b0": {
    "file_path": "test_papers/pytorch_paper.md",
    "title": "Efficient CNN Architecture for CIFAR-10 Classification",
    "file_hash": "acadaf16ca91ab95464ca077146a38d4",
    "parsed_at": "2025-12-08T14:04:06.922620",
    "section_count": 10,
    "section_keys": [
      "efficient_cnn_architecture_for_cifar-10_classification",
      "abstract",
      "introduction",
      "methodology",
      "experiments",
      "dataset",
      "training_setup",
      "results",
      "code_availability",
      "conclusion"
    ]
  },
  "c315cbf5fc60285b9e90d40ff7090b5a": {
    "file_path": "converted_papers/2205.04423_Graph_Neural_Networks_for_Propositional_Model_Coun.md",
    "title": "Graph Neural Networks for Propositional Model Coun",
    "file_hash": "625f0c8428491862480ae97e7ac3a16a",
    "parsed_at": "2025-12-08T15:13:54.058343",
    "section_count": 6,
    "section_keys": [
      "graph_neural_networks_for_propositional_model_coun",
      "introduction",
      "c_configuration_and_ablation_studies_c1_bpgat_parameter_tuning_before_setting_bpgat_architectures_parameters_to_the_ones_detailed_in_section_41_we_performed_several_tuning_experiments_learning_the_damping_parameter_in_order_to_assess_whether_it_is_better_to_use_a_fixed_scalar_parameter_alpha_or_a_learned_operator_delta_as_damping_parameter_of_equation_4_we_tried_all_possible_configurations_each_configuration_is_trained_as_specified_in_section_41_in_table_10_bpgat_refers_to_the_architecture_with_damping_parameter_computed_by_an_mlp_delta_for_factor-_to-_variable_messages_and_fixed_to_05_for_variable-_to-_factor_messages_bpgat_vf_refers_to_the_architecture_with_damping_parameter_computed_by_an_mlp_delta_for_variable-_to-_factor_messages_and_fixed_to_05_for_factor-_to-_variable_messages_bpgat_all_refers_to_the_model_trained_with_an_mlp_delta_for_transforming_both_kind_of_messages_bpgat_none_refers_to_the_model_trained_with_both_damping_parameter_fixed_to_05_results_justify_our_choice_of_using_a_learned_operator_delta_only_for_updating_factor-_to-_variable_messages_table_10_rmsemre_comparison_of_bpgat_trained_with_different_configurations_of_the_damping_parameter_tabletrtddatasettdtdbpgattdtdbpgat_vftdtdbpgat_alltdtdbpgat_nonetdtrtrtdtest_1tdtd012760001366tdtd023510002762tdtd031570003930tdtd041580006443tdtrtrtdtest_2tdtd031000003201tdtd08525001001tdtd09493001601tdtd09238001555tdtrtrtdtest_3tdtd017480001471tdtd021090002998tdtd025890003041tdtd030130003344tdtrtable_number_of_message_passing_iterations_another_parameter_that_we_explored_is_the_number_t_of_message_passing_rounds_ie_the_number_of_times_equations_11_and_12_are_iterated_before_the_readout_phase_equation_9_is_executed_results_are_shown_in_table_11_the_model_achieves_the_highest_performance_when_t_5_table_11_rmsemre_comparison_for_different_number_of_message_passing_iterations_t_bpgat_bpgat_10_and_bpgat_15_refer_to_the_model_trained_with_t_51015_respectively_tabletrtddatasettdtdbpgattdtdbpgat_10tdtdbpgat_15tdtrtrtdtest_1tdtd012760001366tdtd016840003415tdtd017750003694tdtrtrtdtest_2tdtd031000003201tdtd037510005622tdtd053280006294tdtrtrtdtest_3tdtd017480001471tdtd025550003165tdtd027840005979tdtrtable",
      "c2_ablation_studies_on_gat_layers_to_assess_the_relevance_of_using_a_gat-_style_attention_mechanism_in_transforming_both_factor-_to-_variable_and_variable-_to-_factor_messages_we_performed_the_following_ablation_studies_-_hybrid_bp-bpgat_results_are_shown_in_table_12_we_tested_the_model_transforming_factor-to-variable_messages_using_gat_equation_12_and_variable-to-factor_messages_using_bp_equation_2_this_is_denoted_as_fvgat-vfnone_in_table_12_conversely_we_tested_the_model_transforming_variable-to-factor_messages_using_bp_equation_2_and_factor-to-variable_messages_using_gat_equation_11_this_is_denoted_as_fvnone-vfgat_in_table_12_-_hybrid_bpnn-bpgat_results_are_shown_in_table_13_we_tested_the_model_transforming_factor-to-variable_messages_using_gat_equation_12_and_variable-to-factor_messages_using_bpnns_corresponding_update_equation_8_this_is_denoted_as_fvgat-vfmlp_in_table_13_conversely_we_tested_the_model_transforming_variable-to-factor_messages_using_bpnns_update_equation_8_and_factor-to-variable_messages_using_gat_equation_11_this_is_denoted_as_fvmlp-vfgat_in_table_13_results_highlight_the_fact_that_transforming_both_factor-_to-_variable_and_variable-_to-_factor_messages_using_a_gat_attention_mechanism_is_beneficial_for_the_overall_performance_of_the_model_table_12_rmsemre_results_of_the_ablation_studies_on_the_attention_mechanism_tabletrtddatasettdtdbpgattdtdfvgat-vfnonetdtdfvnone-vfgattdtrtrtdtest_1tdtd012760001366tdtd0156800015721tdtd017860001740tdtrtrtdtest_2tdtd031000003201tdtd042920004640tdtd058700006392tdtrtrtdtest_3tdtd017480001471tdtd020090001964tdtd026990002197tdtrtable_table_13_rmsemre_results_of_the_ablation_studies_between_message_transformation_performed_by_bpgat_and_bpnn_tabletrtddatasettdtdbpgattdtdfvgat-vfmlptdtdfvmlp-vfgattdtrtrtdtest_1tdtd012760001366tdtd036130004713tdtd040560005256tdtrtrtdtest_2tdtd031000003201tdtd15787002002tdtd17065002211tdtrtrtdtest_3tdtd017480001471tdtd027510004689tdtd029020004872tdtrtable",
      "3_method_the_objective_of_this_work_is_to_tackle_the_sat_problem_using_a_gnn_model_as_an_approximate_solver_31_belief_propagation_neural_networks_the_starting_point_of_our_investigation_is_one_of_the_variants_of_the_architecture_proposed_in_15_that_we_recall_here_briefly_this_model_called_belief_propagation_neural_network_bpnn_generalizes_bp_by_means_of_gnns_taking_as_input_the_factor_graph_representing_a_cnf_sat_formula_and_giving_as_output_an_estimation_of_the_logarithm_of_the_factor_graphs_partition_function_z_of_equation_1_given_an_input_factor_graph_g_ve_where_nodes_are_partitioned_into_factor_nodes_f_j_j_1m_and_variable_nodes_x_i_i_1n_both_factor-_to-_variable_messages_hatm_jrightarrow_i_and_variable-_to-_factor_hatm_irightarrow_j_messages_are_initialized_to_1_then_the_following_message_passing_phase_is_performed_in_the_log-_space_for_t_iterations_beginarrayr_l_hatm_irightarrow_jk_1x_i_sum_cin_mathcalnx_ibackslash_jmathrmmlp_1hatm_crightarrow_ikx_i_hatm_jrightarrow_ik_1x_i_mathrmlse_x_1ldots_x_kin_mathcalnf_jbackslash_x_ibiggf_jx_ix_1ldots_x_k_qquad_sum_x_vin_mathcalnf_jbackslash_x_imathrmmlp_2hatm_vrightarrow_ikx_vbigg_endarray_quad_8_where_lse_is_a_shorthand_for_the_log-_sum-_exp_function_that_is_bpnn_augments_the_standard_bp_message_passing_scheme_of_equation_2_in_the_log-_space_by_transforming_the_messages_using_mlps_after_the_message_passing_phase_is_completed_the_following_readout_phase_is_executed_which_outputs_an_estimation_ln_hatz_of_the_natural_logarithm_of_the_partition_function_z_of_the_input_factor_graph_beginarrayr_l_ln_hatz_mathrmmlp_3leftmathrmconcat_k_1tleftmathrmconcatleftsum_j_1mb_jkx_mathcalnf_jln_f_jx_mathcalnf_jrightright_left-sum_j_1mb_jkx_mathcalnf_jln_b_jkx_mathcalnf_jright_leftleftsum_i_1nmathrmdegx_i_-_1b_ikx_iln_b_ikx_irightright_endarray_quad_9_where_b_ikx_i_and_b_jkx_mathcalnf_j_refer_to_an_approximation_of_variable_and_factor_beliefs_at_iteration_k_respectively_computed_as_in_standard_belief_propagation_we_defer_to_the_supplementary_material_section_a_for_further_details_hence_this_final_layer",
      "42_results_the_objective_of_our_experiments_is_twofold_evaluating_both_bpgat_scalability_and_generalization_capabilities_in_order_to_assess_the_performance_of_the_model_both_root_mean_squared_error_rmse_and_mean_relative_error_mre_metrics_are_reported_evaluated_between_the_logarithm_of_the_ground_truth_number_of_models_of_the_input_formulae_ln_z_and_the_output_of_the_model_ln_hatz_as_a_baseline_we_used_approx_mc_923_the_state-_of-_the-_art_approximate_sat_solver_which_is_a_randomized_hashing_algorithm_that_provides_probably_approximately_correct_pac_guarantees_it_would_have_been_meaningful_and_interesting_to_compare_bpgat_against_other_guarantee-_less_counters_such_as_approx_count_29_or_sample_approx_28_but_unfortunately_we_couldnt_access_any_open-_source_implementation_of_them_scalability_in_order_to_assess_the_ability_of_our_model_to_scale_to_larger_problem_sizes_than_the_one_seen_during_training_we_generated_several_datasets_following_the_procedure_detailed_in_section_41_table_1_shows_the_statistics_of_the_datasets_used_in_this_testing_phase_each_containing_300_labeled_instances_all_datasets_are_much_larger_than_the_one_seen_during_training_and_in_particular_test_4_contains_formulae_having_a_number_of_variables_which_is_more_than_ten_times_more_the_one_in_the_training_set_and_a_number_of_clauses_which_is_almost_ten_times_more_than_the_ones_in_the_training_set_table_2_shows_the_results_obtained_in_terms_of_rmse_and_mre_by_bpgat_and_approx_mc_it_is_worth_noting_that_for_all_the_datasets_tested_bpgat_outperforms_approx_mc_in_terms_of_mre_although_not_in_terms_of_rmse_such_higher_rmse_is_a_consequence_of_few_outliers_with_a_large_prediction_error_for_bpgat_as_shown_in_figure_1_while_most_of_its_predictions_are_close_to_the_ground_truth_labels_as_certified_by_the_consistently_lower_mre_table_1_average_number_of_variables_average_number_of_clauses_average_number_of_solutions_and_average_time_employed_in_seconds_by_the_exact_solver_sharp_sat_of_the_datasets_used_to_test_scalability_tabletrtddatasettdtdavgvartdtdavgcltdtdavgsoltdtdavg_t_stdtrtrtdtest_1tdtd618tdtd7689tdtd176e19tdtd2671tdtrtrtdtest_2tdtd6043tdtd14361tdtd623e14tdtd21145tdtrtrtdtest_3tdtd12407tdtd7526tdtd114e21tdtd285tdtrtrtdtest_4tdtd37759tdtd27511tdtd718e145tdtd28695tdtrtable_out-_of-_distribution_generalization_the_second_set_of_tests_we_performed_aims_at_evaluating_the_generalization_capabilities_of_our_model_the_problem_classes_we_perform_experiments_with_are_both_sat-_encoded_combinatorial_problems_k_-_dominating_set_graph"
    ]
  },
  "818d75376cda814e0f95d7b14883ea8d": {
    "file_path": "converted_papers/2506.11057_STRCMP_Integrating_Graph_Structural_Priors_with_La.md",
    "title": "STRCMP Integrating Graph Structural Priors with La",
    "file_hash": "58d4715f3075abedba2c19a4f3bab1cd",
    "parsed_at": "2025-12-08T15:13:54.059727",
    "section_count": 9,
    "section_keys": [
      "strcmp_integrating_graph_structural_priors_with_la",
      "introduction",
      "references_1_david_m_ryan_and_brian_a_foster_an_integer_programming_approach_to_scheduling_computer_scheduling_of_public_transport_urban_passenger_vehicle_and_crew_scheduling_pages_269-_280_1981_2_xijun_li_mingxuan_yuan_di_chen_jianguo_yao_and_jia_zeng_a_data-_driven_three-_layer_algorithm_for_split_delivery_vehicle_routing_problem_with_3d_container_loading_constraint_in_proceedings_of_the_24th_acm_sigkdd_international_conference_on_knowledge_discovery_data_mining_pages_528-_536_2018_3_bo_liu_yuqian_jiang_xiaohan_zhang_qiang_liu_shiqi_zhang_joydeep_biswas_and_peter_stone_mathrmlim_p_empowering_large_language_models_with_optimal_planning_proficiency_ar_xiv_preprint_ar_xiv230411477_2023_4_etienne_de_klerk_exploiting_special_structure_in_semidefinite_programming_a_survey_of_theory_and_applications_european_journal_of_operational_research_20111-_10_2010_5_maria_kandyba-_chimani_exact_algorithms_for_network_design_problems_using_graph_orientations_ph_d_thesis_citeseer_2011_6_bruce_hendrickson_the_molecule_problem_exploiting_structure_in_global_optimization_siam_journal_on_optimization_54835-_857_1995_7_yoshua_bengio_andrea_lodi_and_antoine_prouvost_machine_learning_for_combinatorial_optimization_a_methodological_tour_dhorizon_european_journal_of_operational_research_2902405-_421_2021_8_ashish_vaswani_noam_shazeer_niki_parmar_jakob_uszkoreit_llion_jones_aidan_n_gomez_\u0142ukasz_kaiser_and_illia_polosukhin_attention_is_all_you_need_advances_in_neural_information_processing_systems_30_2017_9_irwan_bello_hieu_pham_quoc_v_le_mohammad_norouzi_and_samy_bengio_neural_combinatorial_optimization_with_reinforcement_learning_ar_xiv_preprint_ar_xiv161109940_2016_10_elias_b_khalil_christopher_morris_and_andrea_lodi_mip-_gnn_a_data-_driven_framework_for_guiding_combinatorial_solvers_in_proceedings_of_the_aaai_conference_on_artificial_intelligence_volume_36_pages_10219-_10227_2022_11_carlos_ans\u00f3tegui_britta_heymann_josep_pon_meinolf_sellmann_and_kevin_tierney_hyperreactive_tabu_search_for_maxsat_in_learning_and_intelligent_optimization_12th_international_conference_lion_12_kalamata_greece_june_10-_15_2018_revised_selected_papers_12_pages_309-_325_springer_2019_12_markus_kruber_marco_e_l\u00fcbbcke_and_axel_parmentier_learning_when_to_use_a_decomposition_in_international_conference_on_ai_and_or_techniques_in_constraint_programming_for_combinatorial_optimization_problems_pages_202-_210_springer_2017_13_pierre_bonami_andrea_lodi_and_giulia_zarpellon_learning_a_classification_of_mixed-_integer_quadratic_programming_problems_in_international_conference_on_the_integration_of_constraint_programming_artificial_intelligence_and_operations_research_pages_595-_604_springer_2018_14_maxime_gasse_didier_ch\u00e9telat_nicola_ferroni_laurent_charlin_and_andrea_lodi_exact_combinatorial_optimization_with_graph_convolutional_neural_networks_ar_xiv_preprint_ar_xiv190601629_2019_15_zhihai_wang_xijun_li_jie_wang_yufei_kuang_mingxuan_yuan_jia_zeng_yongdong_zhang_and_feng_wu_learning_cut_selection_for_mixed-_integer_linear_programming_via_hierarchical_sequence_model_ar_xiv_preprint_ar_xiv230200244_2023_16_jie_wang_zhihai_wang_xijun_li_yufei_kuang_zhihao_shi_fangzhou_zhu_mingxuan_yuan_jia_zeng_yongdong_zhang_and_feng_wu_learning_to_cut_via_hierarchical_sequenceset_model_for_efficient_mixed-_integer_programming_ieee_transactions_on_pattern_analysis_and_machine_intelligence_2024",
      "contents_1_introduction_1_2_related_work_3_3_problem_statement_3_4_proposed_solution_4_41_methodology_4_42_theoretical_analysis_6_5_experimental_evaluation_6_51_settings_7_52_implementation_7_53_results_8_6_conclusion_9_a_proofs_15_b_implementation_details_16_b1_combinatorial_structure_extraction_16_b2_structure-aware_code_generation_18_c_baseline_details_19_c1_solver_adoption_19_c2_baselines_setting_19_c21_neural_combinatorial_optimization_19_c22_evolutionary_code_optimization_20_d_dataset_details_21_d1_sat_domain_21_d2_milp_domain_21_e_metric_details_22_e1_metrics_related_to_milp_22_e2_metrics_related_to_sat_22_f_discussion_23_g_additional_experimental_results_24_g1_optimization_performance_result_24_g2_convergence_comparison_result_25_g3_ablation_studies_result_27_h_experiment_statistical_significance_28",
      "a_proofs_theorem_1_given_a_prior_dataset_mathcald_whose_data_samples_comprises_m_types_of_prior_mathcalc_mathbfc_1dotsmathbfc_m_the_distinct_priors_follow_respective_true_distribution_pmathbfc_imathbfw_let_mathbfc_i_be_a_sample_drawn_from_the_distribution_ie_mathbfc_isim_pmathbfc_imathbfw_a_generative_model_with_an_additional_type_of_prior_will_not_lower_the_upper_bound_of_model_performance_sup_mathcalp_proof_assume_that_there_is_a_generative_model_which_is_associated_with_a_set_of_prior_mathcalc_k_mathbfc_1dotsmathbfc_k_its_upper_bound_of_model_performance_is_denoted_by_sup_mathcalp_mathcalc_k_according_to_definition_1_sup_mathcalp_mathcalc_k_can_be_expanded_as_beginarrayr_l_sup_mathcalp_mathcalc_k_sum_mathbfcin_mathcalc_ksum_mathbfcin_mathcalcpmathbfcmax_mathbfwpmathbfwmathbfcphi_mathbfw_qquad_sum_mathbfcin_mathcalc_ksum_mathbfcin_mathcalcpmathbfcunderset_mathbfwmaxsum_mathbfc_iin_mathcalc_ipmathbfc_imathbfcpmathbfwmathbfcmathbfc_iphi_mathbfw_qquad_leq_sum_mathbfcin_mathcalc_ksum_mathbfcin_mathcalcsum_mathbfc_iin_mathcalc_ipmathbfcpmathbfc_imathbfcunderset_mathbfwmaxpmathbfwmathbfcmathbfc_iphi_mathbfw_qquad_sum_mathbfcin_mathcalc_ksum_mathbfcin_mathcalcsum_mathbfc_iin_mathcalc_ipmathbfc_imathbfcunderset_mathbfwmaxpmathbfwmathbfcmathbfc_iphi_mathbfw_qquad_sum_mathbfcin_mathcalc_ksum_mathbfcin_mathcalcsum_mathbfc_iin_mathcalc_ipmathbfc_imathbfcunderset_mathbfwmaxpmathbfwmathbfcmathbfc_iphi_mathbfw_qquad_sup_mathcalp_mathcalc_kcup_mathbfc_i_endarray_quad_7_above_proof_indicates_that_for_a_given_generative_model_with_any_additional_modal_prior_mathbfc_i_its_performance_upper_bound_sup_mathcalp_mathcalc_kcup_mathbfc_i_is_greater_or_equal_to_its_original_performance_upper_bound_sup_mathcalp_mathcalc_k_square_theorem_2_if_prior_mathbfc_p_e_is_a_performance-_enhancing_prior_a_generative_model_neglecting_prior_mathbfc_p_e_will_decrease_the_upper_bound_of_model_performance_it_can_be_expressed_as_sup_mathcalp_mathcalcbackslash_mathbfc_p_e_sup_mathcalp_mathcalc_quad_6_proof_if_prior_mathbfc_p_e_is_a_performance-_enhancing_prior_according_to_definition_2_we_have_beginarrayr_l_underset_mathbfwmaxpmathbfwmathbfcphi_mathbfw_mathbfcin_mathcalcbackslash_mathbfc_p_e_underset_mathbfwmaxpmathbfwmathbfcphi_mathbfw_mathbfcin_mathcalc_qquad_underset_mathbfc_p_ein_mathcalc_p_esumpmathbfc_p_emathbfcunderset_mathbfwmaxpmathbfwmathbfcphi_mathbfw_mathbfcin_mathcalc_endarray_quad_8_then_according_to_definition_1_sup_mathcalp_mathcalcbackslash_mathbfc_p_e_can_be_expanded_as_sup_mathcalp_mathcalcbackslash_mathbfc_p_e_sum_mathbfcin_mathcalcbackslash_mathbfc_p_esum_mathbfcin_mathcalcpmathbfcmax_mathbfwpmathbfwmathbfcphi_mathbfw_quad_9_substitute_eq8_into_eq9_to_derive_beginarrayr_l_sup_mathcalp_mathcalcbackslash_mathbfc_p_e_sum_mathbfcin_mathcalcbackslash_mathbfc_p_esum_mathbfcin_mathcalcpmathbfcmax_mathbfwpmathbfwmathbfcphi_mathbfw_qquad_sum_mathbfcin_mathcalcbackslash_mathbfc_p_esum_mathbfcin_mathbfcpmathbfcsum_mathbfc_p_ein_mathcalc_p_epmathbfc_p_emathbfcmax_mathbfwpmathbfwmathbfcphi_mathbfw_qquad_sum_mathbfcin_mathcalcbackslash_mathbfc_p_esum_mathbfcin_mathcalcsum_mathbfc_p_ein_mathcalc_p_epmathbfcpmathbfc_p_emathbfcmax_mathbfwpmathbfwmathbfcphi_mathbfw_qquad_sum_mathbfcin_mathcalcbackslash_mathbfc_p_esum_mathbfcin_mathcalcsum_mathbfc_p_ein_mathcalc_p_epmathbfc_p_emathbfcmax_mathbfwpmathbfwmathbfcphi_mathbfw_qquad_sum_mathbfcin_mathcalcsum_mathbfcin_mathcalcpmathbfcmax_mathbfwpmathbfwmathbfcphi_mathbfw_qquad_sup_mathcalp_mathcalc_endarray_quad_10",
      "b_implementation_details_b1_combinatorial_structure_extraction_data_representation_of_milp_problem_a_mixed-_integer_linear_programming_milp_problem_is_formally_defined_as_min_pmb_xin_mathbbrnpmb_wtoppmb_xquad_mathrmstpmb_apmb_xleq_pmb_bpmb_lleq_pmb_xleq_pmb_upmb_x_jin_mathbbzforall_jin_mathbbi_quad_11_where_pmb_win_mathbbrn_ain_mathbbrmtimes_n_bin_mathbbrm_lin_mathbbrcup_-_infty_n_pmb_uin_mathbbrcup_pm_infty_n_and_the_index_set_mathbbisubset_12dots_n_specifies_integer-_constrained_variables_to_encode_milp_instances_we_design_a_bipartite_graph_mathcalg_mathcalccup_mathcalvmathcale_with_constraint-_variable_interactions_the_constraint_node_set_mathcalc_c_1dots_c_m_corresponds_to_rows_of_apmb_xleq_pmbb_where_each_node_c_i_is_associated_with_a_1d_feature_vector_c_i_b_i_representing_its_right-_hand_side_value_the_variable_node_set_mathcalv_v_1dots_v_n_represents_decision_variables_each_equipped_with_a_9d_feature_vector_v_j_containing_the_objective_coefficient_w_j_variable_type_indicator_integercontinuous_and_bound_parameters_l_j_u_j_edges_mathcale_e_ij_connect_constraint_c_i_to_variable_v_j_iff_a_ijneq_0_with_edge_features_e_ij_a_ij_encoding_the_constraint_coefficients_furthermore_a_mixed-_integer_linear_programming_milp_instance_is_encoded_as_a_weighted_bipartite_graph_with_feature_matrices_pmb_g_pmb_cpmb_vpmb_e_where_pmb_c_pmbv_and_pmbe_aggregate_constraint_node_features_c_i_variable_node_features_v_j_and_edge_features_e_ij_respectively_the_full_specification_of_these_features_is_summarized_in_table_2_which_preserves_all_structural_and_numerical_information_of_the_original_milp_problem_following_standard_practice_we_utilize_the_observation_function_from_ecole_50_to_generate_these_bipartite_graph_representations_from_milp_instances_table_2_description_of_the_constraint_variable_and_edge_features_in_our_bipartite_graph_representation_for_milp_instance_tabletrtdtensortdtdfeaturetdtddescriptiontdtrtrtd_rowspan3ctdtdconstraint_coefficienttdtdaverage_of_all_coefficients_in_the_constrainttdtrtrtdconstraint_degreetdtddegree_of_constraint_nodestdtrtrtdbiastdtdnormalized_right-hand-side_of_the_constrainttdtrtrtd_rowspan4vtdtdobjectivetdtdnormalized_objective_coefficienttdtrtrtdvariable_coefficienttdtdaverage_variable_coefficient_in_all_constraintstdtrtrtdvariable_degreetdtddegree_of_the_variable_node_in_the_bipartite_graph_representationtdtrtrtdmaximum_variable_coefficienttdtdmaximum_variable_coefficient_in_all_constraintstdtrtrtdtdtdminimum_variable_coefficienttdtdminimum_variable_coefficient_in_all_constraintstdtrtrtdetdtdcoefficienttdtdconstraint_coefficienttdtrtable_data_representation_of_sat_problem_a_boolean_satisfiability_sat_problem_consists_of_variables_x_i_and_logical_operators_wedge_vee_and_neg_a_formula_is_satisfiable_if_there_exists_a_variable_assignment_that_makes_all_clauses_evaluate_to_true_following_51_we_focus_on_formulas_in_conjunctive_normal_form_cnf_conjunctions_of_clauses_where_each_clause_is_a_disjunction_of_literals_variables_x_i_or_their_negations_neg_x_i_any_sat_formula_can_be_converted_to_an_equisatisfiable_cnf_formula_in_linear_time_for_example_x_1_vee_neg_x_2_wedge_x_2_vee_neg_x_3_represents_a_cnf_formula_with_two_clauses_we_utilize_the_variable-_clause_graph_vcg_to_represent_sat_formulas_for_a_sat_formula_the_vcg_is_constructed_with_nodes_representing_literals_and_clauses_and_edges_indicating_the_inclusion_of_a_literal_in_a_clause_the_bipartite_structure_of_vcgs_ensures_a_one-_to-_one_correspondence_between_cnf_formulas_and_their_graph_representations_formally_a_bipartite_graph_mathcalg_mathcalv_1_cup_mathcalv_2_mathcale_is_defined_by_its_vertex_set_mathcalv_1_cup_mathcalv_2_v_1_ldots_v_n_and_edge_set_mathcale_subseteq_v_i_v_j_v_i_v_j_in_mathcalv_the_vertex_set_is_partitioned_into_two_disjoint_subsets_mathcalv_1_and_mathcalv_2_with_edges_restricted_to_connections_between_nodes_in_distinct_partitions_mathcale_subseteq_v_i_v_j_v_i_in_mathcalv_1_v_j_in_mathcalv_2_in_the_context_of_vcgs_a_cnf_formula_with_n",
      "b2_structure-aware_code_generation_data_curation_we_begin_by_assembling_a_curated_collection_of_mathematical_models_for_co_problems_which_are_directly_compatible_with_corresponding_co_solvers_alongside_their_natural_language_descriptions_subsequently_for_each_co_problem_we_compile_a_prompt_that_integrates_the_natural_language_description_with_specific_code_generation_requirements_for_instance_the_code_generation_requirements_may_include_details_such_as_the_function_name_inputoutput_parameters_expected_function_behavior_relevant_background_knowledge_etc_this_prompt_is_then_fed_into_an_llm_to_generate_a_code_snippet_note_that_to_maximize_the_diversity_of_collected_code_snippets_we_employ_multiple_llm_queries_per_prompt_under_high_temperature_settings_to_sample_distinct_candidate_implementations_each_generated_code_snippet_is_evaluated_by_embedding_it_into_the_target_solver_and_solving_the_corresponding_co_problem_thereby_obtaining_performance_metrics_for_the_code_snippet_note_that_based_on_the_principles_of_data_curation_we_collect_the_needed_data_via_querying_qwen25-_coder-_7b-_instructor_same_as_the_model_adopted_in_the_following_post_training_procedure_post_training_specifically_we_first_curate_the_collected_data_by_processing_each_co_problem_q_i_with_its_associated_prompt_x_i_and_corresponding_multiple_generated_code_snippets_y_ijj_1dotsm_these_code_snippets_are_systematically_ranked_based_on_previously_obtained_performance_metrics_to_establish_quality_ordering_the_highest-_performing_code-_prompt_pairs_are_selected_to_form_the_sft_dataset_mathcald_s_f_t_x_iy_i_i_1n_additionally_by_leveraging_pairwise_comparisons_extracted_from_the_established_ranking_hierarchy_we_derive_the_preference_dataset_mathcald_d_p_o_x_iy_wy_l_i_1n_where_y_wy_l_are_preferreddispreferred_code_snippets_the_training_process_can_be_formulated_as_follows_mathcall_mathrmdpo_-mathbbe_xy_wy_lsim_mathcald_d_p_oleftlog_sigma_leftbeta_ln_fracpi_theta_ly_wxpi_mathrmrefy_wx_-beta_ln_fracpi_theta_ly_lxpi_mathrmrefy_lxrightright_quad_16_where_sigma_is_the_sigmoid_function_beta_is_the_hyperparameter_that_governs_the_trade-_off_between_reward_maximization_and_kl_divergence_minimization_pi_mathrmref_is_the_reference_model_trained_on_mathcald_s_f_t_note_that_both_pi_mathrmref_and_pi_theta_l_updates_only_the_parameter_theta_l_of_the_composite_model_through_above_posttraining_we_obtain_a_generative_model_capable_of_structure-_aware_code_generation_that_simultaneously_respects_combinatorial_optimization_problems_inherent_topological_constraints_and_solver-_specific_syntactic_requirements_note_that_based_on_above_principle_of_data_curation_and_post-_training_we_collect_8k_and_4k_post-_training_instances_for_milp_and_sat_instances_respectively_implementation_training_details_to_implement_the_proposed_composite_model_we_first_design_a_structure-_prior-_aware_forward_propagation_mechanism_and_corresponding_adapted_inference_framework_based_on_the_qwen25-_coder-_7b-_instructor_model_via_the_transformers_library_bullet_structure-_prior-_aware_forward_propagation_mechanism_specifically_we_process_the_input_prompt_through_the_tokenizer_and_the_models_embedding_layer_to_obtain_i_n_p_u_t__e_m_b_e_d_s_which_are_then_merged_with_structural_feature_vectors_of_combinatorial_optimization_problems_extracted_by_the_previous_graph_neural_network_first_we_align_the_dimensionality_of_the_combinatorial_optimization_problem_feature_vector_pmbh_qin_mathbbrd_with_the_hidden_layer_dimensions_of_the_large_language_model_via_zero-_padding_given_the_text_embedding_shape_e_m_b_e_d_sin_mathbbrbtimes_stimes_h_where_b_is_the_batch_size_s_is_the_sequence_length_number_of_tokens_and_h_is_the_hidden_dimension_the_dimension-_adapted_feature_vector_is_obtained_as_mathbfh_q_z_e_r_o_p_a_d_d_i_n_gmathbfh_qoplus_mathbf0h_-_din_mathbbrbtimes_h_quad_17_where_zero_padding_denotes_zero-_padding_and_mathbfh_q_represents_the_padded_graph_structural_feature_vector_subsequently_we_fuse_text_and_structural_features_between_the_embedding_layer_and_decoder_layer_by_prepending_the_graph_feature_vector_to_the_text_embedding_sequence_forming_a_hybrid_input_mathcalee_m_b_e_d_smathbfh_q_mathrmclsoplus_mathbfh_qoplus_e_m_b_e_d_s1_in_mathbbrbtimes_1_stimes_h_quad_18_here_e_m_b_e_d_s_is_the_input_embeds_obtained_from_processing_the_textual_prompt_via_the_tokenizer_and_embedding_layer_enabling_the_self-_attention_mechanism_to_jointly_model_textual_semantics_and_graph_structural_features_a_mask_of_all_true_values_is_constructed_and_merged_with_the_attention_mask_to_match_the_shape_of_the_combined_input_embeds_ensuring_mathbfh_q_participates_in_attention_computation_let_the_original_attention_mask_shape_be_m_01btimes_s_the_merged_attention_mask_becomes_m_i_n_m_01mathbf1bm_1sin_01btimes_1_s_quad_19",
      "g_additional_experimental_results_g1_optimization_performance_result_table_5_the_optimization_performance_result_wrt_par-2_score_between_different_methods_over_sat_domain_tabletrtd_rowspan2compared_methodstdtd_colspan4par-2_score_tdtrtrtdcnptdtdcoins_gridtdtdprptdtdzamkellertdtrtrtdauto_sattdtd64472tdtd109869tdtd63934tdtd80775tdtrtrtdeasy_sattdtd64942tdtd159575tdtd2000tdtd82938tdtrtrtdstrcmptdtd62415tdtd112409tdtd180446tdtd27062tdtrtrtdstrcmp_dpo_onlytdtd64345tdtd109869tdtd48292tdtd26594tdtrtrtdstrcmp_sft_onlytdtd64323tdtd109847tdtd183786tdtd22769tdtrtrtdstrcmp_wo_gnntdtd64567tdtd109834tdtd182038tdtd64612tdtrtable_table_6_the_optimization_performance_result_wrt_solving_time_between_different_methods_over_sat_domain_tabletrtd_rowspan2compared_methodstdtd_colspan4solving_time_tdtrtrtdcnptdtdcoins_gridtdtdprptdtdzamkellertdtrtrtdauto_sattdtd32472tdtd19158tdtd22967tdtd20772tdtrtrtdeasy_sattdtd32942tdtd26064tdtd50000tdtd21810tdtrtrtdstrcmptdtd31415tdtd18971tdtd46223tdtd7990tdtrtrtdstrcmp_dpo_onlytdtd32345tdtd19158tdtd21146tdtd7765tdtrtrtdstrcmp_sft_onlytdtd32323tdtd19151tdtd46893tdtd6929tdtrtrtdstrcmp_wo_gnntdtd32567tdtd19147tdtd47019tdtd17014tdtrtable_centerfigure_8_convergence_comparison_wrt_par-2_between_evolutionary-based_algorithm_discovery_frameworks_on_prp_dataset_of_sat_domain_center",
      "research_questions_rq1_does_the_proposed_strcmp_identify_superior_algorithmic_implementations_compared_to_existing_algorithm_discovery_approaches_rq2_does_the_proposed_composite_model_effectively_reduce_computational_overhead_in_existing_algorithm_discovery_frameworks_rq3_does_the_structural_prior_benefit_the_generative_model_in_solving_combinatorial_optimization_problems_51_settings_baselines_we_evaluate_our_framework_against_two_categories_of_baselines_neural_combinatorial_optimization_methods_and_llm-_based_evolutionary_code_optimization_frameworks_covering_mixed-_integer_linear_programming_milp_and_boolean_satisfiability_sat_more_details_of_baselines_and_used_backend_solvers_can_be_found_in_appendix_c_-_neural_combinatorial_optimization_for_milp_we_compare_with_the_seminal_work_l2b_14_which_employs_graph_convolutional_networks_for_variable_selection_to_replace_the_strong_branching_policy_and_hem_15_16_a_hierarchical_sequence_model_for_cut_selection_in_branch-and-bound_solvers_for_sat_we_include_neuro_sat_40_a_message-passing_neural_network_trained_with_single-bit_supervision_for_sat_solving_-_evolutionary_code_optimization_while_numerous_llm-based_evolutionary_code_optimization_approaches_exist_for_combinatorial_optimization_we_specifically_compare_with_two_methods_specialized_for_sat_and_milp_auto_sat_29_and_llm4solver_30_auto_sat_leverages_llms_to_automate_heuristic_optimization_in_sat_solvers_minimizing_manual_intervention_llm4solver_integrates_llms_with_multi-objective_evolutionary_algorithms_to_automatically_design_effective_diving_heuristics_for_milp_solverssup2sup_dataset_we_perform_the_empirical_evaluation_over_ten_widely-_used_benchmark_dataset_for_sat_and_milp_respectively_more_details_and_statistics_of_the_used_datasets_can_be_found_in_appendix_d_-_mixed-integer_linear_programming_milp_the_datasets_for_milp_solver_evaluation_contain_three_difficulty_tiers_15_16_30_1_easy_features_synthetic_benchmarks_set_covering_41_maximum_independent_set_42_multiple_knapsack_43_generated_using_protocols_from_44_45_2_medium_includes_mik_46_and_corlat_47_3_hard_contains_the_google-inspired_load_balancing_problem_and_the_industrial-scale_anonymous_problem_48_-_boolean_satisfiability_sat_the_dataset_comprises_two_sources_sat_competition_problems_49_and_automatically_generated_instances_via_picat_29_sat_competition_data_includes_profitable-robust-product_prp_and_chromatic-number-of-the-plane_cnp_problems_while_picat-generated_data_contains_coins_grid_and_zamkeller_instances_we_adhere_to_the_generation_protocol_established_in_29_metrics_to_evaluate_the_effectiveness_of_the_proposed_framework_we_analyze_solving_time_to_optimality_or_solution_quality_attainable_within_a_fixed_time_budget_for_solving_efficiency_evaluation_we_measure_the_number_of_iterations_or_training_steps_required_for_convergence_specifically_for_milp_domain_critical_metrics_include_solving_time_and_primal-dual_pd_integral_which_are_widely_used_in_benchmarking_the_milp_solvers_15_16_14_for_sat_domain_key_metrics_encompass_solving_time_par-_2_and_number_of_timeout_frequently_measured_in_evaluating_sat_solvers_29_40_metrics_such_as_solving_time_pd_integral_number_of_timeout_and_par-_2_are_minimized_through_optimization_more_details_of_the_used_metrics_are_presented_in_appendix_e_52_implementation_model_architecture_the_proposed_composite_model_comprises_two_components_a_gnn_and_a_structure-_prior-_aware_llm_we_implement_the_gnn_using_graph_convolution_operators_from_torch_geometric_structured_with_three_sequential_convolutional_layers_terminated_by_global_mean"
    ]
  },
  "7c9d87a54a294365ad41a782a0200f78": {
    "file_path": "converted_papers/2304.08738_Addressing_Variable_Dependency_in_GNN-based_SAT_So.md",
    "title": "Addressing Variable Dependency in GNN based SAT So",
    "file_hash": "3c0cca1b285ba52ba190ab3fcb50009e",
    "parsed_at": "2025-12-08T15:13:54.060229",
    "section_count": 3,
    "section_keys": [
      "addressing_variable_dependency_in_gnn_based_sat_so",
      "introduction",
      "references_amizadeh_et_al_2018_saeed_amizadeh_sergiy_matusevych_and_markus_weimer_learning_to_solve_circuit-_sat_an_unsupervised_differentiable_approach_in_international_conference_on_learning_representations_2018_audemard_and_simon_2014_gilles_audemard_and_laurent_simon_glucose_in_the_sat_2014_competition_proceedings_of_sat_competition_201431_2014_brand_1993_daniel_brand_verification_of_large_synthesized_designs_in_proceedings_of_1993_international_conference_on_computer_aided_design_iccad_pages_534-_537_ieee_1993_cho_et_al_2014_kyunghyun_cho_bart_van_merri\u00ebnboer_caglar_gulcehre_dzmitry_bahdanau_fethi_bougares_holger_schwenk_and_yoshua_bengio_learning_phrase_representations_using_rnn_encoder-_decoder_for_statistical_machine_translation_ar_xiv_preprint_ar_xiv14061078_2014_duan_et_al_2022_haonan_duan_pashootan_vaezipoor_max_b_paulus_yangjun_ruan_and_chris_j_maddison_augment_with_care_contrastive_learning_for_the_boolean_satisfiability_problem_ar_xiv_preprint_ar_xiv220208396_2022_gupta_et_al_2006_aarti_gupta_malay_k_ganai_and_chao_wang_sat-_based_verification_methods_and_applications_in_hardware_verification_in_international_school_on_formal_methods_for_the_design_of_computer_communication_and_software_systems_pages_108-_143_springer_2006_jacobs_and_sakr_2021_swen_jacobs_and_mouhammad_sakr_aigen_random_generation_of_symbolic_transition_systems_in_international_conference_on_computer_aided_verification_pages_435-_446_springer_2021_li_et_al_2022_min_li_zhengyuan_shi_qiuxia_lai_sadaf_khan_and_qiang_xu_deep_sat_an_eda-_driven_learning_framework_for_sat_ar_xiv_preprint_ar_xiv220513745_2022_marques-_silva_and_sakallah_1999_joao_p_marques-_silva_and_karem_a_sakallah_grasp_a_search_algorithm_for_propositional_satisfiability_ieee_transactions_on_computers_485506-_521_1999_moskewicz_et_al_2001_matthew_w_moskewicz_conor_f_madigan_ying_zhao_lintao_zhang_and_sharad_malik_chaff_engineering_an_efficient_sat_solver_in_proceedings_of_the_38th_annual_design_automation_conference_pages_530-_535_2001_ozolins_et_al_2021_emils_ozolins_karlis_freivalds_andis_draguns_eliza_gaile_ronalds_zakovskis_and_sergejs_kozlovics_goal-_aware_neural_sat_solver_ar_xiv_preprint_ar_xiv210607162_2021_prestwich_2009_steven_david_prestwich_cnf_encodings_handbook_of_satisfiability_18575-_97_2009_selsam_and_bjorner_2019_daniel_selsam_and_nikolaj_bjorner_neuro_core_guiding_high-_performance_sat_solvers_with_unsat-_core_predictions_2019_selsam_et_al_2018_daniel_selsam_matthew_lamm_benedikt_b\u00fcnz_percy_liang_leonardo_de_moura_and_david_l_dill_learning_a_sat_solver_from_single-_bit_supervision_ar_xiv_preprint_ar_xiv180203685_2018_selsam_2018_daniel_selsam_guidance_on_reproducing_experiments_in_paper_httpsgithubcomdelsamneurosatissues2_2018_shi_et_al_2021_feng_shi_chonghan_lee_mohammad_khairul_bashar_nikhil_shukla_song-_chun_zhu_and_vijaykrishnan_narayanan_transformer-_based_machine_learning_for_fast_sat_solvers_and_logic_synthesis_ar_xiv_preprint_ar_xiv210707116_2021_shuai_et_al_2016_bing_shuai_zhen_zuo_bing_wang_and_gang_wang_dag-_recurrent_neural_networks_for_scene_labeling_in_proceedings_of_the_ieee_conference_on_computer_vision_and_pattern_recognition_pages_3620-_3629_2016_sorensson_and_een_2005_niklas_sorensson_and_niklas_een_minisat_v1_13-_a_sat_solver_with_conflict-_clause_minimization_sat_2005531-_2_2005_sun_et_al_2022_zeyu_sun_wenjie_zhang_lili_mou_qihao_zhu_yingfei_xiong_and_lu_zhang_generalized_equivariance_and_preferential_labeling_for_gnn_node_classification_2022_tseitin_1983_grigori_s_tseitin_on_the_complexity_of_derivation_in_propositional_calculus_in_automation_of_reasoning_pages_466-_483_springer_1983_yolcu_and_p\u00f3czos_2019_emre_yolcu_and_barnab\u00e1s_p\u00f3czos_learning_local_search_heuristics_for_boolean_satisfiability_advances_in_neural_information_processing_systems_32_2019_zhang_et_al_2020_wenjie_zhang_zeyu_sun_qihao_zhu_ge_li_shaowei_cai_yingfei_xiong_and_lu_zhang_nlocal_sat_boosting_local_search_with_solution_prediction_ar_xiv_preprint_ar_xiv200109398_2020"
    ]
  },
  "83ab2bd7b585c97101b99d26fafebf19": {
    "file_path": "converted_papers/2005.13406_Neural_heuristics_for_SAT_solving.md",
    "title": "Neural heuristics for SAT solving",
    "file_hash": "02fef77792c5224488795385ce9709e4",
    "parsed_at": "2025-12-08T15:13:54.060610",
    "section_count": 3,
    "section_keys": [
      "neural_heuristics_for_sat_solving",
      "introduction",
      "6_acknowledgements_6_acknowledgementsthis_was_work_was_supported_by_1_the_polish_national_science_center_grant_umo-_201829bst602959_2_the_tensor_flow_research_cloud_which_granted_50_tpus_3_the_academic_computer_center_cyfronet_at_the_agh_university_of_science_and_technology_in_krak\u00f3w_poland_references_referencesaab15_mart\u00edn_abadi_ashish_agarwal_paul_barham_eugene_brevdo_zhifeng_chen_craig_citro_greg_s_corrado_andy_davis_jeffrey_dean_matthieu_devin_sanjay_ghemawat_ian_goodfellow_andrew_harp_geoffrey_irving_michael_isard_yangqing_jia_rafal_jozefowicz_lukasz_kaiser_manjunath_kudlur_josh_levenberg_dandelion_man\u00e9_rajat_monga_sherry_moore_derek_murray_chris_olah_mike_schuster_jonathon_shlens_benoit_steiner_ilya_sutskever_kunal_talwar_paul_tucker_vincent_vanhoucke_vijay_vasudevan_fernanda_vi\u00e9gas_oriol_vinyals_pete_warden_martin_wattenberg_martin_wicke_yuan_yu_and_xiaoqiang_zheng_tensor_flow_large-_scale_machine_learning_on_heterogeneous_systems_2015_software_available_from_tensorfloworgacks16_miltiadis_allamanis_pankajan_chanthirasegaran_pushmeet_kohli_and_charles_a_sutton_learning_continuous_semantic_representations_of_symbolic_expressions_co_rr_abs161101423_2016_bhb18_peter_w_battaglia_jessica_b_hamrick_victor_bapst_alvaro_sanchez-_gonzalez_vin\u00edcius_flores_zambaldi_mateusz_malinowski_andrea_tacchetti_david_raposo_adam_santoro_ryan_faulkner_\u00e7aglar_g\u00fcl\u00e7ehre_francis_song_andrew_j_ballard_justin_gilmer_george_e_dahl_ashish_vaswani_kelsey_allen_charles_nash_victoria_langston_chris_dyer_nicolas_heess_daan_wierstra_pushmeet_kohli_matthew_botvinick_oriol_vinyals_yujia_li_and_razvan_pascanu_relational_inductive_biases_deep_learning_and_graph_networks_co_rr_abs180601261_2018_es03_niklas_e\u00e9n_and_niklas_s\u00f6rensson_an_extensible_sat-_solver_in_theory_and_applications_of_satisfiability_testing_6th_international_conference_sat_2003_santa_margherita_ligure_italy_may_5-_8_2003_selected_revised_papers_pages_502-_518_2003_esa18_richard_evans_david_saxton_david_amos_pushmeet_kohli_and_edward_grefenstette_can_neural_networks_understand_logical_entailment_co_rr_abs180208535_2018_imm18_alexey_ignatiev_antonio_morgado_and_joao_marques-_silva_py_sat_a_python_toolkit_for_prototyping_with_sat_oracles_in_sat_pages_428-_437_2018_kar72_r_karp_reducibility_among_combinatorial_problems_in_r_miller_and_j_thatcher_editors_complexity_of_computer_computations_pages_85-_103_plenum_press_1972_kumo18_cezary_kaliszyk_josef_urban_henryk_michalewski_and_mirek_ols\u00e1k_reinforcement_learning_of_theorem_proving_co_rr_abs180507563_2018_lom18_jia_hui_liang_chanseok_oh_minu_mathew_ciza_thomas_chunxiao_li_and_vijay_ganesh_machine_learning-_based_restart_policy_for_cdcl_sat_solvers_in_theory_and_applications_of_satisfiability_testing_-_sat_2018_-_21st_international_conference_sat_2018_held_as_part_of_the_federated_logic_conference_flo_c_2018_oxford_uk_july_9-_12_2018_proceedings_pages_94-_110_2018_mhn13_andrew_l_maas_awni_y_hannun_and_andrew_y_ng_rectifier_nonlinearities_improve_neural_network_acoustic_models_in_in_icml_workshop_on_deep_learning_for_audio_speech_and_language_processing_2013_mmz01_matthew_w_moskewicz_conor_f_madigan_ying_zhao_lintao_zhang_and_sharad_malik_chaff_engineering_an_efficient_sat_solver_in_proceedings_of_the_38th_design_automation_conference_dac_2001_las_vegas_nv_usa_june_18-_22_2001_pages_530-_535_2001_ms99_joao_marques-_silva_the_impact_of_branching_heuristics_in_propositional_satisfiability_algorithms_in_epia_1999_slb18_daniel_selsam_matthew_lamm_benedikt_b\u00fcnz_percy_liang_leonardo_de_moura_and_david_l_dill_learning_a_sat_solver_from_single-_bit_supervision_co_rr_abs180203685_2018"
    ]
  },
  "a0ac4c54c1df325b9494211e467bfc4d": {
    "file_path": "converted_papers/2211.14405_Learning_Branching_Heuristics_from_Graph_Neural_Ne.md",
    "title": "Learning Branching Heuristics from Graph Neural Ne",
    "file_hash": "2e9e7feaec3bc9fb81ce5a69d99c2896",
    "parsed_at": "2025-12-08T15:13:54.061307",
    "section_count": 5,
    "section_keys": [
      "learning_branching_heuristics_from_graph_neural_ne",
      "introduction",
      "6_acknowledgement_6_acknowledgement_this_research_was_supported_in_part_through_computational_resources_and_services_provided_by_ubc_advanced_research_computing_ubc_arc_sockeye_ubc_advanced_research_computing_2022_doi_1014288sockeye_references_references1_lin_xu_frank_hutter_holger_h_hoos_and_kevin_leyton-_brown_satzilla_portfolio-_based_algorithm_selection_for_sat_journal_of_artificial_intelligence_research_32565-_606_2008_2_hong_xu_sven_koenig_and_tk_kumar_towards_effective_deep_learning_for_constraint_satisfaction_problems_in_international_conference_on_principles_and_practice_of_constraint_programming_pages_588-_597_springer_2018_3_daniel_selsam_matthew_lamm_benedikt_b\u00fcnz_percy_liang_leonardo_de_moura_and_david_l_dill_learning_a_sat_solver_from_single-_bit_supervision_in_international_conference_on_learning_representations_2019_4_oriol_vinyals_meire_fortunato_and_navdeep_jaitly_pointer_networks_advances_in_neural_information_processing_systems_28_2015_5_zhuwen_li_qifeng_chen_and_vladlen_koltun_combinatorial_optimization_with_graph_convolutional_networks_and_guided_tree_search_advances_in_neural_information_processing_systems_31_2018_6_elias_khalil_hanjun_dai_yuyu_zhang_bistra_dilkina_and_le_song_learning_combinatorial_optimization_algorithms_over_graphs_advances_in_neural_information_processing_systems_30_2017_7_wouter_kool_herke_van_hoof_and_max_welling_attention_learn_to_solve_routing_problems_in_international_conference_on_learning_representations_2019_8_nikolaos_karalias_and_andreas_loukas_erdos_goes_neural_an_unsupervised_learning_framework_for_combinatorial_optimization_on_graphs_in_advances_in_neural_information_processing_systems_volume_33_pages_6659-_6672_2020_9_joseph_culberson_yong_gao_and_calin_anton_phase_transitions_of_dominating_clique_problem_and_their_implications_to_heuristics_in_satisfiability_search_in_international_joint_conference_on_artificial_intelligence_volume_19_page_78_2005_10_dieter_kratsch_algorithms_in_domination_in_graphs_advanced_topics_pages_191-_232_routledge_2017",
      "b_preliminary_test_for_dominating_cliques_to_check_whether_equation_5_works_effectively_as_the_loss_function_of_the_gnn_model_for_finding_dominating_cliques_we_create_other_gnp_instances_with_different_n_and_p_in_particular_n_ranges_from_25_to_400_in_increments_of_25_and_p_ranges_from_01_to_09_in_increments_of_02_so_we_have_80_pairs_of_n_and_p_then_we_generate_32_instances_from_gnp_for_each_pair_of_n_and_p_and_check_the_values_of_loss_function_of_these_instances_output_from_the_well-_trained_probabilistic-_method_gnn_model_from_the_above_phase_transition_the_instances_generated_from_gnp_where_p_01_have_low_likelihood_of_containing_a_dominating_clique_on_the_contrary_the_instances_generated_from_gnp_where_p_09_are_more_likely_to_have_dominating_cliques_figure_3_shows_the_average_of_the_values_of_loss_function_of_the_32_instances_for_each_pair_of_n_and_p_from_figure_3_we_can_see_that_the_average_of_the_values_of_loss_function_of_the_instances_generated_from_gnp_where_p_01_are_much_greater_than_the_corresponding_average_of_the_instances_generated_from_gnp_where_p_09_which_is_consistent_with_the_known_phase_transition_because_the_greater_loss-_function_values_imply_the_lower_probabilities_of_existing_a_dominating_clique_centerfigure_3_averages_of_the_values_of_loss_function_of_32_instances_for_each_pair_of_n_and_p_the_x-axis_is_the_size_of_graphs_the_y-axis_is_the_average_of_the_values_of_loss_function_center_c_training_data_for_minimum_dominating_cliques_our_data_is_generated_from_gnp_the_work_in_9_shows_that_the_phase_transition_of_dominating_cliques_happens_at_the_exact_threshold_p_frac3_-_sqrt52_approx_0381_it_is_also_mentioned_in_9_that_p_around_0371_is_a_good_empirical_probability_to_create_the_instances_that_it_is_hard_to_predict_the_existence_of_dominating_cliques_we_call_such_instances_as_hard_instances",
      "2_dominating_clique_and_probabilistic-method_gnn_given_a_graph_g_ve_a_dominating_clique_dc_is_a_subset_s_of_v_such_that_s_is_a_clique_and_dominates_vbackslash_s_ie_forall_uin_vbackslash_s_exists_vin_s_such_that_vu_in_e_checking_the_existence_of_a_dominating_clique_in_a_graph_is_an_np-_complete_problem_10_our_gnn_model_is_similar_to_what_has_been_proposed_in_8_its_architecture_is_standard_and_is_trained_in_an_unsupervised_way_to_learn_a_probability_distribution_for_each_vertex_unlike_8_where_the_distribution_is_directly_used_to_sample_a_solution_we_use_the_distribution_to_design_branching_heuristics_due_to_the_fact_that_a_solution_to_the_dominating-_clique_problem_needs_to_satisfy_both_local_and_global_properties_designing_a_good_loss_function_is_much_more_challenging_than_the_those_used_in_8_for_solutions_that_largely_require_certain_local_properties_we_build_a_probability_space_by_defining_a_bernoulli_distribution_for_each_vertex_of_a_given_graph_by_the_correlation_inequality_in_the_probabilistic_method_we_can_get_an_approximation_of_the_probability_measure_of_dominating_cliques_in_the_graph_using_the_approximation_as_the_loss_function_of_a_gnn_model_we_hope_that_the_approximation_grows_larger_as_much_as_possible_we_note_that_in_8_the_first-_moment_method_is_used_in_the_loss_function_of_their_gnn_model_which_requires_that_the_value_of_loss_function_after_training_is_less_than_a_constant_depending_on_specific_graphs_if_the_requirement_is_not_met_the_learned_probability_distributions_do_not_guarantee_the_usefulness_for_the_solving_problem_our_loss_function_avoid_such_restrictions_in_our_approach_under_the_learned_probability_space_the_information_entropy_of_vertices_is_used_in_the_new_branching_heuristic_to_replace_the_original_branching_heuristic_of_the_dominating-_clique_solver_in_9_our_experimental_results_show_that_the_learned_branching_heuristic_can_prune_more_search_space_than_the_original_branching_heuristic_a_recent_attempt_towards_learning_branching_choices_in_a_combinatorial_search_was_given_by_li_et_al_2018_5_where_they_approached_the_maximum-_independent-_set_problem_by_choosing_a_node_that_is_expected_to_be_in_an_optimal_solution_this_choice_is_made_by_assigning_probabilities_to_the_unselected_nodes_and_choosing_a_node_of_high_probability_and_these_probabilities_are_learned_in_a_sense_it_performs_a_greedy_selection_of_vertices_to_be_added_to_a_maximum_independent_set_this_can_be_parallelized_into_a_tree_search_by_considering_multiple_probability_maps_of_the_vertices_at_each_stage_according_to_a_pre-_chosen_branching_factor_parameter_and_continuing_the_selection_process_in_each_probability_map_the_largest_independent_set_is_found_over_all_branches_later_bother_et_al_2022_11_showed_that_the_performance_of_such_a_tree_search_of_selections_can_be_matched_by_randomly_generating_values_in_place_of_the_learned_probability_map_for_this_combinatorial_problem_this_is_comparable_to_a_monte_carlo_tree_search_we_contrast_our_work_here_to_that_of_li_et_als_tree_search_in_that_their_search"
    ]
  },
  "da14976f78933e5c4c39d53db9705621": {
    "file_path": "converted_papers/2208.10227_One_Model_Any_CSP_Graph_Neural_Networks_as_Fast_Gl.md",
    "title": "One Model Any CSP Graph Neural Networks as Fast Gl",
    "file_hash": "3c09f1bf5001cff4216b71b50fb11637",
    "parsed_at": "2025-12-08T15:13:54.062901",
    "section_count": 5,
    "section_keys": [
      "one_model_any_csp_graph_neural_networks_as_fast_gl",
      "introduction",
      "a_method_details_here_we_will_provide_a_formal_definition_of_our_architecture_and_training_procedure_we_also_give_information_on_model_selection_and_hyperparameters_and_discuss_some_implementation_details_a1_architecture_let_us_formalize_the_architecture_of_our_policy_gnn_pi_theta_recall_that_the_main_hyperparameters_of_pi_theta_are_the_latent_dimension_din_mathbbn_and_the_aggregation_function_bigoplus_which_we_either_choose_as_an_element-_wise_sum_mean_or_max_function_our_gnn_is_then_composed_of_the_following_trainable_components_a_gru-_cell_mathbfgmathbbrdtimes_mathbbrdto_mathbbrd_and_its_trainable_initial_state_mathbfhin_mathbbrd_this_cell_is_used_to_update_the_recurrent_value_states_a_value_encoder_mathrmmlpmathbfemathbbrd_1to_mathbbrd_which_merges_the_information_of_the_recurrent_state_and_the_binary_label_of_each_value_two_linear_perceptrons_mathbfm_mathcalvmathbfm_mathcalemathbbrdto_mathbbr2d_these_functions_are_used_to_generate_the_messages_that_are_sent_from_values_to_constraints_and_from_constraints_to_values_respectively_three_mlps_mathbfu_mathcalvmathbfu_mathcalemathbfu_mathcalxmathbbrdto_mathbbrd_for_combining_aggregated_messages_for_values_constraints_and_variables_respectively_the_output_mlp_mathbfomathbbrdto_mathbbr_which_generates_the_logit_scores_for_each_value_before_we_apply_the_domain-_wise_softmax_the_combined_trainable_weights_of_these_functions_form_the_parameter_vector_theta_the_mlps_mathbfemathbfu_mathcalvmathbfu_mathcalemathbfu_mathcalx_and_mathbfo_all_have_two_layers_the_hidden_layer_is_re_lu-_activated_and_has_dimension_d_while_the_second_layer_is_linear_we_also_note_that_mathbfemathbfm_mathcalvmathbfm_mathcalemathbfu_mathcalvmathbfu_mathcale_and_mathbfu_mathcalx_each_apply_layer_norm_to_their_output_which_we_found_to_significantly_improve_convergence_during_training_in_iteration_t_we_associate_a_recurrent_state_htnuin_mathbbrd_with_each_value_nu_in_mathcalv_these_states_are_passed_on_from_the_previous_iteration_t_-_1_and_initialized_as_h0nu_mathbfh_pi_theta_then_performs_the_following_message_passing_procedure_in_each_iteration_t_first_each_value_nu_in_mathcalv_generates_a_latent_state_xtnu_by_applying_the_encoder_mathbfe_to_its_recurrent_state_and_its_binary_label_xtnu_mathbfebigbight_-_1nul_vt_-_1nubigbig_quad_5_here_denotes_concatenation_of_vectors_the_latent_state_is_then_used_to_generate_two_messages_for_each_value_by_applying_the_message_generation_mlp_mathbfm_mathcalv_mtnu_0mtnu_1_mathbfm_mathcalvbigxtnubig_quad_6_note_that_the_output_of_mathbfm_mathcalv_has_dimension_2d_and_is_the_stack_of_both_d_-_dimensional_messages_the_message_mtnu_i_is_send_along_all_constraint_edges_cnu_with_label_l_ecnu_i_hence_the_edge_labels_are_incorporated_by_generating_different_messages_for_each_label_the_constraints_aggregate_these_messages_and_process_the_result_with_their_message_generation_function_mathbfm_mathcale_beginarrayr_l_ytc_bigoplus_vin_mathcalncmtbignu_l_ecnubig_mtc0mtc1_mathbfm_mathcalebigytcbig_endarray_quad_7_these_messages_are_then_aggregated_by_the_values_that_combine_the_information_with_their_latent_state_x_by_applying_the_update_mlp_mathbfu_mathcalv_ytnu_bigoplus_cin_mathcalnnucap_mathcalcmtcl_ecnu_quad_9_ztnu_mathbfu_mathcalvbigxtnu_ytnubig_xtnu_quad_10_note_that_we_added_a_residual_connection_around_mathbfu_mathcalv_for_better_gradient_flow_in_the_next_phase_of_our_message_passing_procedure_values_exchange_messages_with_their_respective_variables_to_this_end_each_variable_xin_mathcalx_pools_the_latent_states_of_their_respective_values_and_applies_mathbfu_mathcalx_to_obtain_a_variable-_level_latent_representation_ztx_ztx_mathbfu_mathcalxbigbigoplus_nu_in_d_xztnubig_quad_11_this_representation_is_send_back_to_each_value_nu_in_mathcalv_x_of_x_where_it_is_combined_with_the_value-_level_latent_state_by_a_simple_addition_note_that_this_final_message_pass_needs_no_aggregation_as_every_value_is_connected_to_exactly_one_variable_the_result_is_used_as_input_to_the_gru-_cell_mathbfg_which_updates_the_recurrent_states_of_the_values_htnu_mathbfgbight_-_1nuztnu_ztxbig_quad_12_finally_pi_theta_computes_a_soft_assignment_phit_for_mathcali_to_this_end_the_mlp_mathbfo_maps_the_new_recurrent_state_of_each_value_nu_in_mathcalv_x_of_each_variable_x_to_a_scalar_real_number_otnu_mathbfohtnu_we_can_then_apply_the_softmax_function_within_each_domain_to_produce_a_soft_value_assignment_phitnu_fracexpleftotnurightsum_nuprimeinmathcalv_xexpleftotnuprimeright_quad_13_figure_5_provides_a_visual_representation_of_our_message_passing_procedure_we_also_provide_the_forward_pass_of_anycsp_as_pseudocode_in_algorithm_1_figure_6_visualizes_a_run_of_a_trained_anycsp_model_on_a_2-_coloring_problem_for_a_grid_graph_a2_training_let_us_formalize_how_we_apply_reinforce_when_training_an_anycsp_model_recall_that_our_action_space_is_extremely_large_as_we_choose_one_assignment_from_the_set_of_all_possible_assignments_in_each_step_we_can_handle_this_action_space_efficiently_because_we_model_probability_distributions_over_this_space_as_soft_assignments_from_which_a_new_value_is_sampled_independently_for_every_variable_the_probability_with_which_a_hard_assignment_alpha_is_sampled_from_a_soft_assignment_phi_is_therefore_given_by_mathbfpalpha_phi_prod_xin_mathcalxphi_alpha_x_quad_14",
      "b_experiment_details_in_this_section_we_will_provide_additional_details_on_our_experimental_setup_baselines_we_also_provide_detailed_instancelevel_results_for_our_graph_coloring_and_maxcut_experiments_since_these_use_structured_benchmark_instances_in_table_10_we_provide_an_overview_of_all_external_software_used_in_our_experiments_b1_model_rb_the_model_rb_defines_an_easy_way_to_generate_theoretically_hard_random_csp_instances_by_randomly_choosing_a_number_of_disallowed_tuples_of_a_fixed_arity_a_class_of_random_csp_instances_of_model_rb_is_denoted_mathrmrbknalpha_rp_where_each_instance_consists_of_ngeq_2_variables_with_domain_size_d_nalpha_for_a_0_each_instance_has_m_r_nln_n_constraints_for_r_0_of_arity_kgeq_2_with_each_constraint_disallowing_t_p_dk_randomly_selected_tuples_note_that_the_selection_of_scopes_and_tuples_is_performed_with_repetition_this_is_due_to_the_fact_that_the_number_of_repeated_constraints_and_tuples_are_asymptotically_smaller_than_the_total_number_of_constraints_and_tuples_and_thus_can_be_neglected_the_hardest_of_the_model_rb_instances_occur_around_the_critical_value_p_cr_1_-_e-_alpha_r_of_p_xu_and_li_2003_data_our_training_data_consists_of_randomly_generated_model_rb_instances_with_30_variables_and_arity_2_we_randomly_select_din_n1_k2n1_k_and_min_nlog_kd2nlog_kd_and_generate_instances_with_p_09p_cr_slightly_smaller_than_the_critical_values_of_p_to_increase_the_number_of_satisfiable_instances_seen_during_training_to_generate_one_instance_we_build_m_constraints_each_by_randomly_selecting_scope_of_k_distinct_variables_with_repetition_and_then_randomly_selecting_with_repetition_a_relation_of_t_distinct_disallowed_tuples_our_validation_data_contains_200_instances_sampled_from_the_exact_same_distribution_the_test_dataset_is_obtained_from_the_xcsp_project_audemard_et_al_2020_and_contains_50_satisfiable_model_rb_instances_with_50_variables_each_with_domain_size_22_and_about_500_constraints_of_arity_2_more_specifically_we_use_all_instances_of_the_random-_rb-_2-_50-_23f_dataset_as_our_test_data_baselines_we_used_three_state-_of-_the-_art_csp-_solvers_from_the_xcsp_competition_as_baselines_picat_zhou_2022_ace_lecoutre_2022_and_co_so_co_audemard_2018_picat_is_a_sat-_based_solver_and_the_winner_of_the_most_recent_xcsp_competition_audemard_et_al_2020_ace_and_co_so_co_are_based_on_constraint_propagation_we_include_co_so_co_because_it_demonstrated_very_strong_performance_specifically_on_binary_model_rb_instances_in_previous_csp_competitions_indeed_it_also_is_the_best_performing_baseline_in_our_experiment_b2_vertex_coloring_a_csp_instance_of_k_-_col_with_the_input_graph_g_ve_has_a_variable_x_v_for_each_vertex_vin_gv_the_domain_mathcald_1dotsk_for_each_variable_and_a_constraint_c_x_vx_ur_neqk_for_each_edge_vuin_ge_here_the_relation_r_neqk_ij1leq_ijleq_kineq_j_implies_the_color_inequality_of_connected_nodes_we_consider_the_decision_problem_of_k_-_col_that_is_we_provide_the_number_of_colors_k_as_part_of_the_input_instance_and_ask_whether_or_not_a_conflict-_free_k_-_coloring_exists_for_the_given_graph_if_anycsp_fails_at_this_task_then_it_produces_a_coloring_with_unsatisfied_constraints_we_do_point_out_that_not_all_of_our_baselines_use_this_setup_the_greedy_heuristic_and_dsatur_are_constructive_and_yield_solutions_that_are_always_conflict-_free_but_may_have_a_sub-_optimal_number_of_colors_hybrid_ea_initially_constructs_a_sub-_optimal_conflict-_free_coloring_and_then_iteratively_attempts_to_lower_the_number_of_colors_through_tabu_search_and_evolutionary_optimization_for_all_of_these_methods_we_can_measure_whether_or_not_they_produce_a_conflict-_free_solution_with_the_optimal_number_of_colors_within_a_given_timeout_however_we_should_keep_these_differences_in_mind_during_a_comparison_data_to_generate_training_graphs_we_mix_the_following_3_distributions_uniformly_-_erd\u0151s-r\u00e9nyi_graphs_with_n_50_vertices_and_edge_probability_p_sim_u01_03_-_barab\u00e1si-albert_graphs_with_n_50_vertices_and_parameter_m_sim_u2_10_-_random_geometric_graphs_with_n_50_vertices_distributed_uniformly_at_random_in_a_2-dimensional_1_times_1_square_the_edge_threshold_radius_is_drawn_uniformly_from_r_sim_u015_03_for_each_graph_g_drawn_from_this_distribution_we_then_choose_a_number_of_colors_k_in_3_10_as_follows_we_first_apply_a_linear_time_greedy_coloring_heuristic_as_implemented_by_network_x_hagberg_swart_and_s_chult_2008_to_color_the_graph_without_conflict_if_the_greedy_heuristic_required_k_colors_for_g_then_we_pose_the_problem_of_coloring_g_with_k_colors_as_the_training_csp_instance_where_k_is_chosen_as_k_max_3min_10k_-_1_quad_22_intuitively_anycsp_has_to_color_each_graph_with_1_color_less_than_the_greedy_heuristic_some_of_these_instances_are_unsatisfiable_which_is_not_a_problem_for_our_reward_scheme_and_training_procedure_we_found_this_simple_method_to_be_very_effective_at_quickly_generating_graph_coloring_instances_around_the_threshold_of_satisfiability_with_minimal_fine-_tuning_the_200_validation_instances_are_generated_with_the_same_parameters_and_procedure_except_that_we_increase_the_number_of_vertices_for_all_three_graph_types_to_n_200_baselines_runcsp_was_trained_on_the_same_data_distribution_used_in_its_experiments_on_structured_coloring_instances_in_the_appendix_of_t\u00f6nshoff_et_al_2021_we_use_a_py_torch_implementation_of_runcsp_and_train_each_model_for_a_total_of_100k_steps_to_ensure_convergence_recall_that_runcsp_requires_us_to_fix_one_k_before_training_and_we_train_one_model_for_each_k_in_4_ldots_9_we_consider_a_graph_solved_by_runcsp_if_the_model_trained_for_the_graphs_chromatic_number_is_able_to_find_a_conflict-_free_coloring_to_evaluate_the_csp_solvers_picat_and_co_so_co_in_this_experiment_we_reduce_each_coloring_instance_to_a_csp_instance",
      "c_ablation_we_provide_an_empirical_ablation_study_for_two_major_design_choices_of_anyc_sp_firstly_we_want_to_study_the_benefit_of_our_exponentially_sized_action_space_when_compared_to_a_more_conventional_local_search_setting_secondly_we_aim_to_validate_the_reward_scheme_we_constructed_in_section_42_to_this_end_we_will_evaluate_two_modified_versions_of_anyc_sp_1_anyc_sploc_a_version_of_anyc_sp_designed_to_be_a_local_search_heuristic_we_modify_pi_theta_such_that_the_softmax_over_the_scores_in_otboldsymbol_v_is_not_performed_separately_within_each_domain_but_over_all_values_in_the_disjoint_union_of_domains_mathcalv_phit_1boldsymbol_v_fracexpleftotboldsymbolvrightsum_boldsymbolv_in_mathcalvexpleftotboldsymbolv_right_quad_23_the_output_phit_1_of_pi_theta_in_iteration_t_1_is_therefore_not_a_soft_assignment_but_a_probability_distribution_over_the_disjoint_union_of_domains_to_obtain_a_new_hard_assignment_alphat_1_we_sample_a_single_value_vin_mathcalc_from_this_distribution_and_set_it_as_the_value_for_its_respective_variable_x_v_with_vin_mathcalv_x_v_beginarraycboldsymbol_vsim_phit_1_alphat_1_alphatx_v_boldsymbol_v_endarray_quad_25_all_variables_other_than_x_v_remain_unchanged_in_iteration_t_1_with_this_modification_mathrmanycsp_loc_becomes_a_local_search_heuristic_that_only_changes_one_variable_at_a_time_the_remaining_architecture_and_training_procedure_are_identical_to_anyc_sp_including_the_reward_scheme_2_anycspqual_a_version_of_anyc_sp_trained_by_using_the_quality_q_mathcalialphat_of_the_current_assignment_as_a_reward_for_this_configuration_to_train_well_we_found_it_helpful_to_use_the_quality_of_the_initial_assignment_as_a_baseline_rt_q_mathcalialphat_-_q_mathcalialpha0_quad_26_without_the_subtractive_baseline_we_found_the_training_to_be_very_unstable_note_that_the_baseline_does_not_solve_the_fundamental_problem_of_the_reward_scheme_a_heuristic_can_not_leave_a_local_maximum_without_being_immediately_punished_for_doing_so_here_we_will_study_how_this_proposed_issue_actually_effects_performance_empirically_we_perform_our_ablation_experiments_on_the_graph_coloring_maxcut_and_max-_k_-_sat_problems_for_each_problem_we_train_both_modifications_with_the_same_training_data_and_hyperparameters_as_anyc_sp_c1_results_tables_11_12_and_13_contain_the_results_of_our_ablation_study_for_k_-_col_maxcut_and_max-_k_-_sat_respectively_the_metrics_in_each_table_are_identical_to_those_used_in_our_main_experiment_for_graph_coloring_mathrmanycsp_qual_performs_significantly_worse_than_the_other_two_versions_of_our_method_compared_to_our_main_baselines_it_only_outperforms_runcsp_table_11_ablation_results_on_graph_coloring_tabletrtdmethodtdtdcolamplt10tdtdcol10tdtrtrtdanycsploctdtd49tdtd37tdtrtrtdanycspqualtdtd37tdtd25tdtrtrtdanycsptdtd50tdtd40tdtrtable_and_the_simple_greedy_approach_mathrmanycsp_loc_actually_performs_reasonably_well_as_it_only_solves_four_graphs_less_than_anycsp_across_all_100_test_instances_in_this_experiment_the_reward_scheme_seems_to_contribute_more_to_the_performance_than_the_global_search_action_space_however_only_the_combination_of_both_in_anycsp_yields_the_best_results_on_the_maxcut_problem_there_is_no_clear_hierarchy_between_mathrmanycsp_loc_and_mathrmanycsp_qual_however_both_ablation_versions_perform_significantly_worse_than_anycsp_the_same_seems_to_hold_on_the_max-_k_-_sat_problem_the_two_modified_versions_yield_similar_results_but_perform_far_worse_than_anycsp_figure_7_investigates_the_differences_on_the_max-_k_-_sat_problem_further_we_plot_how_the_number_of_unsatisfied_clauses_in_the_best_found_solution_evolves_throughout_the_60k_search_steps_performed_by_anycsp_in_20_minutes_the_curves_are_averaged_over_all_50_instances_in_out_max-_5-_sat_test_data_both_mathrmanycsp_loc_and_mathrmanycsp_qual_are_unable_to_converge_to_solutions_as_good_as_those_found_by_anycsp_but_for_different_reasons_mathrmanycsp_loc_converges_slowly_but_steadily_due_to_the_slow_convergence_compared_to_anycsp_it_is_not_able_to_find_equivalent_solutions_in_the_same_amount_of_time_mathrmanycsp_qual_initially_converges_as_fast_as_anycsp_this_is_expected_since_this_version_also_performs_global_search_and_can_refine_the_whole_solution_in_parallel_however_it_tapers_of_significantly_earlier_compared_to_anycsp_and_the_solution_quality_remains_virtually_constant_after_20k_search_steps_this_is_the_expected_problem_our_reward_scheme_intends_to_solve_during_training_mathrmanycsp_qual_can_not_leave_local_maxima_without_being_punished_for_doing_so_by_the_simple_reward_scheme_this_inhibits_exploration_and_encourages_stagnation_after_60k_steps_mathrmanycsp_loc_actually_catches_up_to_mathrmanycsp_qual_and_both_ablation_versions_yield_similar_results_once_the_20_minute_timeout_is_reached_overall_our_experiments_and_ablation_study_suggests_that_our_two_main_design_choices_are_crucial_to_consistently_obtaining_strong_search_heuristics_1_a_global_search_space_is_necessary_to_refine_the_whole_solution_in_parallel_and_speed_up_the_search_without_this_advantage_gnn-based_heuristics_can_not_compensate_for_their_comparatively_high_computational_cost_2_a_well-chosen_reward_scheme_that_encourages_exploration_is_equally_important_without_it_global_search_simply_gets_stuck_faster_than_local_search_a_simple_reward_proportional_to_the_quality_is_not_suitable_in_this_regard_anycsp_combines_these_insights_in_one_generic_architecture_for_all_csps"
    ]
  },
  "71cb5cd4a4e3658c9e73df930df6a0e1": {
    "file_path": "converted_papers/2504.01173_Neural_Approaches_to_SAT_Solving_Design_Choices_an.md",
    "title": "Neural Approaches to SAT Solving Design Choices an",
    "file_hash": "8435b1a284281d15ebfb2b24fb61f2af",
    "parsed_at": "2025-12-08T15:13:54.063946",
    "section_count": 6,
    "section_keys": [
      "neural_approaches_to_sat_solving_design_choices_an",
      "introduction",
      "6_interpreting_the_trained_model_61_embedding_space_analysis_our_analysis_of_variable_embeddings_reveals_patterns_that_explain_how_gnns_learn_to_solve_sat_problems_when_visualizing_these_embeddings_using_dimensionality_reduction_mc_innes_et_al_2018_we_observe_that_they_form_distinct_clusters_corresponding_to_optimal_variable_assignments_as_shown_in_figure_5_variable_embeddings_start_randomly_distributed_but_gradually_organize_into_two_clusters_through_message_passing_iterations_by_applying_k-_means_clustering_k_2_to_these_embeddings_we_can_recover_variable_assignments_that_approximate_optimal_solutions_even_from_networks_trained_only_to_predict_satisfiability_status_62_iterative_optimization_behavior_by_tracking_clause_satisfaction_across_iterations_we_observe_that_gnns_solve_sat_problems_through_progressive_local_refinement_the_gap_number_of_unsatisfied_clauses_decreases_following_a_trajectory_typical_of_iterative_optimization_methods_rapid_initial_improvement_followed_by_gradual_refinement_this_behavior_supports_the_interpretation_that_gnns_implicitly_learn_to_perform_continuous_optimization_in_a_high-_dimensional_space_similar_to_sdp_relaxations_for_sat_the_effectiveness_of_additional_message_passing_iterations_during_inference_further_strengthens_this_connection_a_difference_from_the_sdp_relaxation_is_that_the_objective_function_which_the_gnn_implicitely_optimizes_is_non-_convex_because_we_observed_that_it_can_get_stuck_in_local_optima_or_converge_to_different_solutions_when_initialized_multiple_times_by_different_random_embeddings_figure_3_illustrates_how_the_average_gap_decreases_with_increasing_iterations_the_trajectory_suggests_a_rapid_improvement_phase_followed_by_more_gradual_refinement_individual_instance_trajectories_reveal_that_while_most_instances_show_steady_improvement_toward_optimal_solutions_some_exhibit_fluctuations_particularly_unsatisfiable_instances_this_observation_supports_the_potential_value_of_early_stopping_techniques_as_in_rare_cases_the_gap_at_later_iterations_might_be_higher_than_a_previously_achieved_minimum_gap_the_bi-_level_optimization_perspectivewhere_message_passing_performs_an_inner_optimization_loop_finding_variable_assignments_guided_by_network_parameters_optimized_at_the_outer_level_during_traininghelps_explain_the_networks_ability_to_generalize_to_novel_problem_instances_and_larger_problems_than_those_seen_during_training_in_section_7_we_discuss_more_details_about_a_possibility_of_manual_derivation_of_the_gnn_equations_from_and_explicit_objective_function_7_discussion_in_this_section_we_discuss_the_limitations_of_our_work_along_with_an_outlook_for_future_research_the_primary_limitation_of_the_methods_presented_here_is_that_they_are_not_competitive_with_state-_of-_the-_art_sat_solvers_on_benchmarks_derived_from_real-_world_problems_current_sat_solvers_can_handle_formulas_with_millions_of_variables_which_is_not_feasible_for_the_gnn_in_its_current_form_however_as_mentioned_in_the_introduction_our_motivation_for_studying_these_models_is_to_better_understand_the_reasoning_capabilities_of_neural_networks_in_a_simplified_context_the_test-_time_scaling_experiments_clearly_demonstrate_that_the_gnns_can_successfully_generalize_beyond_their_training_distribution_and_do_not_merely_learn_superficial_statistical_patterns_the_qualitative_results_presented_in_section_6_further_suggest_that_it_is_possible_to_fully_understand_the_mechanisms_by_which_the_gnn_solves_a_given_formula_figure_3_illustrates_that_the_trained_gnn_functions_as_an_implicit_max_sat_solver_incrementally_maximizing_the_number",
      "b_sdp_for_max-2-sat_semidefinite_programming_sdp_is_a_mathematical_optimization_technique_primarily_used_for_problems_involving_positive_semidefinite_matrices_in_sdp_a_linear_objective_function_is_optimized_over_a_feasible_region_given_by_a_spectrahedron_an_intersection_of_a_convex_cone_formed_by_positive_semidefinite_matrices_and_an_affine_subspace_ramana_and_goldman_1995_along_with_the_broad_scope_of_applications_sdp_has_been_used_to_design_approximation_algorithms_for_discrete_np-_hard_problems_gartner_and_matousek_2012_this_is_achieved_by_lifting_variables_of_a_problem_to_a_vector_space_and_optimizing_a_loss_function_expressed_in_terms_of_these_vectors_in_this_section_we_provide_a_detailed_derivation_of_the_sdp_relaxation_for_max-_2-_sat_the_goal_is_to_write_an_objective_function_for_2-_cnf_formulae_which_consist_of_clauses_c_1ldots_c_k_over_variables_x_1ldots_x_n_with_at_most_two_literals_per_clause_b1_derivation_of_the_sdp_relaxation_for_each_boolean_variable_x_i_where_iin_12ldots_n_a_new_variable_y_iin_-_11_is_associated_and_an_additional_variable_y_0in_-_11_is_introduced_this_additional_variable_is_introduced_to_unambiguously_assign_the_truth_value_in_the_original_problem_from_values_of_the_relaxed_problem_it_is_not_possible_to_just_assign_true_false_to_x_i_if_y_i_1-_1_because_quadratic_terms_cannot_distinguish_between_y_icdot_y_j_and_-_y_icdot_-_y_j_instead_the_truth_value_of_x_i_is_assigned_by_comparing_y_i_with_y_0_x_i_is_true_if_and_only_if_y_i_y_0_otherwise_it_is_false_the_assignment_is_therefore_invariant_to_negating_all_variables_to_determine_the_value_of_a_formula_we_sum_the_value_of_its_clauses_c_which_are_given_by_the_value_function_vc_here_are_examples_of_the_value_function_for_different_clauses_beginarraycvx_i_frac1_y_0cdot_y_i2_vneg_x_i_1_-_vx_i_frac1_-_y_0cdot_y_i2_vx_ivee_neg_x_j_1_-_vneg_x_iwedge_x_j_1_-_frac1_-_y_0cdot_y_i2cdot_frac1_y_0cdot_y_j2_frac14_1_y_0cdot_y_i_frac14_1_-_y_0cdot_y_j_frac14_1_y_icdot_y_j_endarray_quad_15_by_summing_over_all_clauses_c_in_the_boolean_formula_the_following_integer_quadratic_program_for_max-_2-_sat_is_obtained_beginarrayr_l_mathrmmaximizequad_sum_cin_cvc_mathrmsubjecttoquad_y_iin_-11_mathrmforalliin_01ldots_n_mathrmsubjecttoquad_y_iin_-11_mathrmforalljin_01ldots_n_endarray_quad_21_this_can_be_rewritten_by_collecting_coefficients_of_y_icdot_y_j_for_ijin_01ldots_n_and_putting_them_symmetrically_into_a_n_1times_n_1_coefficient_matrix_w_the_terms_y_icdot_y_j_can_be_collected_in_a_matrix_y_with_the_same_dimensions_as_w_the_elements_y_ij_correspond_to_y_icdot_y_j_for_ijin_01ldots_n_both_matrices_are_symmetric_hence_the_sum_of_all_elements_in_their_element-_wise_product_which_is_the_objective_function_can_be_compactly_expressed_by_using_the_trace_operation_this_leads_to_the_following_version_of_the_same_integer_program_beginarrayr_l_mathrmmaximizequad_mathrmtrw_y_mathrmsubjecttoquad_y_i_i_1mathrmforalliin_01ldots_n_qquad_y_i_j_y_icdot_y_jmathrmforallijin_01ldots_n_qquad_y_iin_-11_mathrmforalliin_01ldots_n_endarray_quad_22",
      "b2_relaxation_to_semidefinite_programming_to_make_the_discrete_program_continuous_we_first_allow_the_value_of_the_variables_y_i_to_be_any_real_number_between_-_1_and_1_however_semidefinite_programming_goes_further_and_allows_variables_to_be_n_1_-_dimensional_unit_vectors_y_0ldots_y_nlongrightarrow_mathbfy_0ldots_mathbfy_n_as_schematically_depicted_in_figure_7_in_this_relaxation_the_binary_products_y_icdot_y_j_in_the_objective_function_are_replaced_by_inner_products_langle_mathbfy_imathbfy_jrangle_this_can_be_compactly_represented_in_matrix_form_by_substituting_each_inner_product_langle_mathbfy_imathbfy_jrangle_with_a_scalar_y_ij_of_a_matrix_y_the_fact_that_these_scalars_correspond_to_inner_products_is_encoded_by_the_restriction_to_positive-_semidefinite_matrices_ysucceq_0_the_sdp_relaxation_of_max-_2-_sat_can_thus_be_formulated_as_beginarrayrl_mathrmmaximizequad_mathrmtrw_y_mathrmsubjecttoquad_y_i_i_1mathrmforalliin_01ldots_n_qquad_ysucceq_0_endarray_quad_27_positive_semidefiniteness_of_matrix_y_ensures_that_it_can_be_uniquely_factorized_as_y_yfrac12yfrac12t_we_can_then_obtain_real_unit_vectors_mathbfy_i_for_all_iin_0ldots_n_such_that_y_ij_langle_mathbfy_imathbfy_jrangle_for_all_ijin_0ldots_n_the_constraints_y_ii_1_ensure_that_all_vectors_mathbfy_i_lie_on_an_n_1_-_dimensional_unit_sphere_centerfigure_7_lifting_the_variables_to_a_higher_dimension_demonstrated_on_variables_y_1y_2y_3_initially_only_integer_values_of_-1_and_1_could_be_assigned_to_them_integer_program_next_constraints_are_relaxed_allowing_variables_to_take_any_real_value_between_-1_and_1_finally_it_is_permitted_for_them_to_be_unit_vectors_in_a_high-dimensional_space_here_3_dimensions_the_hyperplane_in_the_last_picture_would_be_used_for_rounding_the_variables_at_the_end_this_hyperplane_can_be_randomly_selected_and_truth_values_for_variables_y_1y_2y_3_are_determined_based_on_which_side_of_the_hyperplane_they_land_after_continuous_optimization_center_b3_interpretation_and_rounding_the_sdp_solver_optimizes_the_numbers_in_the_matrix_y_but_using_the_factorization_we_can_visualize_what_happens_with_the_vectors_mathbfy_1_the_process_starts_with_random_unit_vectors_that_are_continuously_updated_to_maximize_the_objective_function_if_we_fix_the_position_of_the_vector_mathbfy_0_corresponding_to_the_value_true_we_would_see_that_the_vectors_of_variables_that_will_be_set_to_true_in_the_final_assignment_get_closer_to_the_vector_mathbfy_0_while_the_vectors_mathbfy_j_of_variables_that_will_be_set_to_false_move_away_from_it_so_that_the_inner_product_langle_mathbfy_0mathbfy_jrangle_is_close_to_-_1_if_the_formula_is_satisfiable_the_objective_function_drives_the_vectors_to_form_two_wellseparated_clusters_however_if_only_a_few_clauses_can_be_satisfied_simultaneously_the_vectors_would_end_up_being_scattered_a_simple_way_to_round_the_resulting_vectors_mathbfy_1ldots_mathbfy_n_and_get_the_assignment_for_the_original_boolean_variables_is_to_compute_the_inner_product_langle_mathbfy_0mathbfy_irangle_and_assign_the_value_according",
      "4_experimental_setup_41_data_representation_and_graph_structure_boolean_formulas_in_cnf_form_can_be_naturally_represented_as_bipartite_graphs_where_clauses_and_variables_or_literals_form_two_distinct_sets_of_nodes_in_this_work_we_explore_two_different_graph_representations_literal-_clause_graph_lcg_in_the_literal-_clause_graph_representation_each_literal_both_positive_and_negative_polarity_of_a_variable_is_represented_as_a_separate_node_for_a_formula_with_n_variables_this_results_in_2n_literal_nodes_each_literal_node_is_connected_to_all_clause_nodes_containing_that_literal_formally_for_a_cnf_formula_phi_with_variables_x_1ldots_x_n_and_clauses_c_1ldots_c_m_we_construct_a_bipartite_graph_g_lc_lcup_ce_where_-_l_l_1ldots_l_nbarl_1ldots_barl_n_is_the_set_of_literal_nodes_-_c_c_1ldots_c_m_is_the_set_of_clause_nodes_-_l_ic_jin_e_if_and_only_if_literal_l_i_appears_in_clause_c_j_variable-_clause_graph_vcg_in_the_variable-_clause_graph_representation_each_variable_rather_than_each_literal_is_represented_as_a_node_for_a_formula_with_n_variables_this_results_in_exactly_n_variable_nodes_each_variable_node_is_connected_to_all_clause_nodes_containing_either_the_positive_or_negative_literal_of_that_variable_to_retain_information_about_the_polarity_of_literals_we_assign_edge_features_p_ijin_-_11_to_each_edge_x_ic_j_where_p_ij_1_if_the_positive_literal_x_i_appears_in_clause_c_j_and_p_ij_-_1_if_the_negative_literal_overlinex_i_appears_in_clause_c_j_formally_we_construct_a_bipartite_graph_g_vc_vcup_cep_where_-_v_x_1ldots_x_n_is_the_set_of_variable_nodes-_c_c_1ldots_c_m_is_the_set_of_clause_nodes-_x_ic_jin_e_if_and_only_if_variable_x_i_appears_in_clause_c_j_in_either_polarity-_peto_-11_maps_each_edge_to_its_corresponding_polarity_both_graph_representations_capture_the_structure_of_the_boolean_formula_but_they_differ_in_how_they_handle_variable_polarity_the_literal-_clause_graph_explicitly_represents_both_polarities_as_separate_nodes_which_increases_the_number_of_nodes_but_simplifies_the_message_passing_process_of_the_gnn_the_variable-_clause_graph_is_more_compact_but_requires_handling_polarity_information_through_edge_features_for_the_gnns_we_use_the_variable-_clause_graph_representation_is_more_computationally_efficient_than_the_literal-_clause_graph_reducing_both_memory_requirements_and_processing_time_this_efficiency_comes_from_having_half_as_many_variable_nodes_compared_to_literal_nodes_and_avoiding_an_expensive_operation_during_message_passing_as_will_be_described_in_section_42_in_our_experiments_we_compare_both_representations_together_with_different_message_passing_operations_and_different_training_regimes_42_architecture_variants_our_gnn_architecture_variants_are_derived_from_the_neuro_sat_architecture_selsam_et_al_2018_which_demonstrated_the_possibility_of_using_gnns_for_sat_solving_the_main_advantage_of_this_architecture_is_that_it_is_recurrent_and_therefore_the_number_of_message_passing_iterations_is_theoretically_not_limited_this_is_not_the_case_for_the_non-_recurrent_alternatives_with_fixed_number_of_layers_we_will_demonstrate_the_usefulness_of_this_feature_in_section_53"
    ]
  },
  "960a3cc47a619e0ae16407fb45a58435": {
    "file_path": "converted_papers/2312.11547_A_unified_pre-training_and_adaptation_framework_fo.md",
    "title": "A unified pre training and adaptation framework fo",
    "file_hash": "ee97354951138f550a65b88197e9ec05",
    "parsed_at": "2025-12-08T15:13:54.064841",
    "section_count": 5,
    "section_keys": [
      "a_unified_pre_training_and_adaptation_framework_fo",
      "introduction",
      "35_inference_with_local_search_method_the_last_step_of_our_model_is_to_search_the_feasible_solutions_for_co_problems_with_constraint_conditions_by_using_the_predictions_generated_by_our_framework_we_can_also_get_close_to_optimal_solutions_with_a_search_algorithm_for_max-_cut_problems_without_hard_clauses_our_model_can_directly_infer_the_truth_assignment_and_obtain_the_target_values_for_problems_with_hard_clauses_we_introduce_a_heuristic_local_search_method_to_discretely_obtain_feasible_solutions_that_satisfy_constraints_and_obtain_the_target_values_local_search_is_a_typical_search_method_for_discrete_optimization_we_adopt_a_2-_improvement_local_search_algorithm_4950_that_iterates_over_all_nodes_in_the_graph_and_attempts_to_replace_a_1-_labeled_node_v_i_with_two_1-_labeled_nodes_v_j_and_v_k_for_the_mis_problem_v_j_and_v_k_must_be_the_neighbors_of_v_i_these_two_nodes_are_1-_tight_and_are_not_connected_here_a_node_is_1-_tight_if_exactly_one_of_its_neighbors_is_1-_labeled_in_other_words_v_i_is_the_only_1-_labeled_neighbor_of_v_j_and_v_k_in_the_graph_notice_that_this_local_search_algorithm_can_find_a_valid_2-_improvement_in_oe_time_if_it_exists_we_introduce_this_method_with_fixed_steps_to_find_a_feasible_solution_and_evaluate_the_performance_of_our_model_36_overall_algorithms_most_of_the_previous_works_focus_on_modeling_specific_co_problems_in_contrast_our_method_captures_the_common_knowledge_of_cos_and_designs_a_unified_learning_method_to_utilize_the_knowledge_to_solve_cos_to_summarize_our_framework_has_three_essential_processes_the_problem_transfer_process_leverages_max-_sat_to_bridge_various_cos_the_cos_are_converted_to_max-_sat_with_a_general_form_that_can_capture_logical_information_within_co_problems_from_the_perspective_of_data_transformation_the_graphs_in_cos_are_firstly_converted_into_clauses_and_then_converted_to_bipartite_graphs_the_overall_algorithm_for_problem_transfer_is_listed_in_algorithm_1_algorithm_1_problem_transfer_via_max-_sat_input_a_co_graph_mathcalg_mathcalvmathcale_objective_function_max_fs_constraint_condition_g_isleqslant_b_ib_iin_omega_output_a_bipartite_graph_tildemathcalg_tildemathcalv_xtildemathcalv_ctildemathcale_from_a_graph_to_max-_sat_clauses_1_generate_soft_clauses_c_s_with_objective_function_max_fs_2_generate_hard_clauses_c_h_with_constraint_condition_g_isleqslant_b_ib_iin_omega_from_max-_sat_clauses_to_a_bipartite_graph_3_construct_nodes_tildenu_x_and_tildenu_c_for_each_variable_v_i_and_clause_c_j_4_construct_edges_e_v_ic_jprimein_tildemathcaleforall_v_iin_c_j_the_pre-_training_process_uses_samples_from_max-_sat_to_learn_generalizable_features_that_can_benefit_all_cos_the_networks_used_for_pre-_training_include_the_mlp_feature_extraction_backbones_based_on_gnns_and_the_classification_network_the_overall_algorithm_for_pre-_training_is_listed_in_algorithm_2_the_fine-_tuning_process_uses_samples_from_max-_sat_and_target_co_to_build_a_domain_adaptation_architecture_based_on_the_pre-_trained_network_the_fine-_tuning_network_introduces_an_additional_discriminator_for_domain_classification_which_can_learn_domain-_invariant_features_to_further_improve_the_generalizability_of_features_the_overall_algorithm_for_fine-_tuning_is_listed_in_algorithm_3_4_experimental_results_in_this_section_we_evaluated_the_effectiveness_of_the_proposed_framework_for_three_co_problems_on_graphs_ie_max-_cut_mis_and_mds_generally_we_aimed_to_answer_three_essential_questions_by_the_experiments_q1_can_we_leverage_max-_sat_to_learn_transferable_and_generalizable_features_that_can_improve_the_ability_to_solve_different_co_problems_on_graphs_q2_how_can_we_incorporate_max-_sat",
      "1_introduction_combinatorial_optimization_co_is_a_multidisciplinary_field_ranging_from_optimization_to_artificial_intelligence_the_goal_of_co_is_to_find_optimal_solutions_from_finite_possible_options_where_exhaustive_enumeration_cannot_be_accomplished_graphs_are_the_most_common_research_objects_in_co_typical_co_problems_on_graphs_include_traveling_salesman_problems_tsp_1_max-_cut_problems_2_and_graph_coloring_problems_3_solving_co_on_graphs_can_benefit_many_real-_world_applications_such_as_transportation_logistics_4_and_telecommunications_5_solving_cos_on_graphs_is_extremely_difficult_due_to_their_discrete_property_and_high_computational_costs_for_many_decades_researchers_have_made_many_efforts_to_develop_various_algorithms_to_solve_cos_on_graphs_for_example_branch_and_bound_6_is_a_traditional_exact_algorithm_that_gradually_divides_the_solution_space_into_subspaces_and_uses_bounds_to_prune_possible_solutions_that_exceed_the_bounds_unfortunately_these_traditional_exact_algorithms_encounter_excessive_computational_burdens_as_the_scales_of_problems_increase_7_recently_deep_learning_has_developed_as_an_effective_tool_for_cos_inspiring_the_emergence_of_the_category_of_learning-_based_methods_for_solving_cos_particularly_graph_neural_networks_gnns_8-_10_are_advanced_deep_learning_models_designed_for_graph-_structure_data_which_have_been_successfully_applied_to_cos_on_graphs_11-_13_in_general_the_graphs_in_co_problems_are_first_fed_into_gnns_so_that_the_high-_level_features_can_be_extracted_then_the_predictions_generated_from_features_serve_as_feasible_solutions_or_can_be_further_used_to_guide_the_searching_process_of_the_heuristic_learning_process_14_15_learning-_based_methods_extract_more_useful_knowledge_to_reduce_the_search_space_which_can_be_used_to_effectively_and_efficiently_solve_cos_on_graphs_besides_the_data-_driven_manner_is_more_adaptive_to_real-_world_problems_when_real_data_is_available_however_current_gnn_models_for_co_problems_still_face_several_challenges_first_directly_modeling_co_with_graphs_is_intuitive_but_not_necessarily_accurate_although_graphs_can_capture_relationships_between_objects_they_ignore_the_mathematical_logicality_and_properties_inside_cos_therefore_constructing_learning_methods_over_original_graphs_often_results_in_insufficient_problem-_solving_abilities_second_current_methods_are_usually_designed_to_solve_a_single_co_problem_on_graphs_nevertheless_different_cos_on_graphs_may_share_the_same_graphs_and_leverage_similar_gnn_backbones_for_feature_extraction_it_is_quite_beneficial_to_explore_whether_the_transferability_of_knowledge_exists_among_different_problems_moreover_for_real-_world_problems_on_graphs_the_data_collected_for_training_gnns_is_often_limited_it_is_essential_to_leverage_data_from_other_problems_that_may_have_common_knowledge_to_improve_the_generalization_ability_of_gnns_to_address_the_above_issues_we_propose_in_this_paper_a_unified_pre-_training_and_adaptation_framework_for_cos_on_graphs_the_core_idea_is_to_develop_a_generalizable_and_transferable_gnn_framework_from_which_all_cos_can_be_benefited_to_achieve_this_goal_it_is_necessary_to_find_out_the_common_knowledge_and_properties_of_all_co_problems_on_graphs_that_can_be_extracted_by_gnns_and_used_for_different_problems_we_notice_that_using_graphs_to_formulate_co_omits_logical_information_that_is_usually_more_general_and_common_for_all_problems_inspired_by_this_observation_we_propose_to_leverage_the_maximum_satisfiability_problems_max-_sat_to_bridge_different_co_problems_on_graphs_max-_sat_is_a_classic_problem_containing_propositional_logic_formulas_that_can_be_used_to_describe_additional_logic_information_beyond_simple_relations_on_graphs_with_this_unified_representation_we_further_adopt_a_pre-_training_process_and_a_fine-_tuning_process_based_on_domain_adaptation_to_extract_general_features_and_utilize_the_transferability_between_different_problems_our_framework_can_be_adaptive_to_all_co_problems_that_can_be_transferred_to_max-_sat_problems_and_is_also_suitable_for_various_gnn_architectures_which_exhibit_superior_flexibility_and_versatility_to_be_concrete_we_first_use_the_max-_sat_problem_as_an_intermediate_problem_by_which_we_can_transform_graphs_from_different_cos_to_clauses_and_formulas_we_can_construct_bipartite_graphs_from_these_clauses_in_max-_sat_that_not_only_capture_logical_information_but_also_can_be_adaptive_to_different_cos_on_graphs_in_the_training_stage_we_can_generate_abundant_clauses_from_certain_max-_sat_problems_without_considering_the_original_co_and_transform_them_into_bipartite_graphs_for_pre-_training_due_to_the_generic_nature_of_max-_sat_the_pre-_trained_model_is_equipped_with_better_initialization_then_we",
      "33_learning_with_bipartite_graph_attention_networks_in_this_subsection_we_first_introduce_the_backbone_network_that_is_used_to_extract_features_from_bipartite_graphs_traditional_gnns_for_feature_extractions_from_graphs_mainly_follow_a_message-_passing_scheme_beginarrayr_l_mathbfa_vl_mathrmaggmathbfx_umid_uin_mathcalnv_mathbfx_vl_1_mathrmcommathbfa_vlmathbfx_vl_endarray_quad_38_where_u_and_v_denote_the_nodes_mathbfa_v_denotes_the_feature_vector_accumulated_from_neighbor_nodes_mathbfx_vl_denotes_the_features_at_layer_l_and_mathbfx_v0_is_the_initial_attributes_mathrmaggcdot_and_mathrmcomcdot_are_the_aggregation_and_combination_functions_for_message_passing_and_feature_updating_and_mathcalnv_denotes_the_set_of_neighbors_for_v_the_above_process_is_only_suitable_for_graphs_with_one_type_of_nodes_recall_that_a_bipartite_graph_contains_two_sets_of_nodes_that_represent_variables_and_clauses_and_a_set_of_edges_that_connect_nodes_from_these_two_sets_due_to_the_different_properties_of_nodes_the_traditional_message-_passing_scheme_in_gnns_cannot_be_directly_used_to_extract_features_we_particularly_design_a_bipartite_gnn_with_attentive_message-_passing_schemes_for_an_input_bipartite_graph_tildemathcalg_tildenu_xtildenu_ctildemathcale_the_attributed_matrices_of_variables_and_clauses_are_denoted_as_mathbfcin_mathbbrmtimes_d_and_mathbfxin_mathbbrntimes_d_where_m_and_n_denote_the_numbers_of_clauses_and_variables_and_d_is_the_dimension_of_attributes_for_symbolic_convenience_we_use_v_xi_and_v_ci_to_denote_nodes_of_variables_and_clauses_in_the_bipartite_graph_the_variable_and_clause_of_these_nodes_are_denoted_as_x_i_and_c_i_and_their_attributed_vectors_are_denoted_as_mathbfx_i_and_mathbfc_i_accordingly_since_the_initial_bipartite_graph_does_not_contain_attributed_information_the_attributed_matrices_require_initialization_there_are_different_strategies_for_initialization_eg_uniform_distribution_normal_distribution_and_all-_one_matrices_since_no_significant_influences_have_been_observed_in_our_experiments_we_simply_initialize_attributed_matrices_as_all-_one_matrices_mathbfx_ini_and_mathbfc_ini_the_message-_passing_process_of_bipartite_gnns_consists_of_two_steps_in_each_iteration_the_clause-_wise_aggregation_step_and_the_variable-_wise_aggregation_step_first_clause-_wise_aggregation_updates_the_feature_of_the_clause_node_by_aggregating_the_features_from_the_variable_nodes_to_further_discriminate_the_importance_of_different_neighbors_the_attention_mechanism_is_introduced_to_learn_features_during_the_aggregation_given_a_clause_c_i_and_its_neighbor_variable_x_j_the_layer-_wise_aggregation_from_x_j_to_c_i_through_attention_is_represented_as_beginarrayr_l_alpha_x_jto_c_il_fracexpleftlanglemathbfw_qlmathbfc_ilmathbfw_klmathbfx_jlranglerightsum_v_xkinmathcalnv_ciexpleftlanglemathbfw_qlmathbfc_ilmathbfw_klmathbfx_klrangleright_mathbfc_il_1_mathrmmlpleftmathbfc_ilsum_v_xjinmathcalnv_cialpha_x_jto_c_ilmathbfw_vlmathbfx_jlright_endarray_quad_311_where_alpha_x_jto_c_i_denotes_the_attention_score_between_clause_c_i_and_variable_x_j_mathbfw_qlin_mathbbrd_mathbfw_klin_mathbbrd_and_mathbfw_vlin_mathbbrd_are_learnable_parameters_of_weights_mathbfc_il_and_mathbfx_jl_are_features_of_i_-_th_clause_and_j_-_th_variable_at_layer_l_mathcalnv_ci_is_the_set_of_neighbor_variable_nodes_for_clause_c_i_langle_cdot_cdot_rangle_is_the_dot_product_operation_mathbfc_il_1_is_the_updated_features_for_clause_c_i_and_mlp_denotes_the_multi-_layer_perceptron_second_the_variable_feature_can_be_updated_through_variable-_wise_aggregation_from_the_features_of_clauses_that_contain_the_variable_similarly_for_variable_x_i_the_layer-_wise_aggregation_from_c_j_to_x_i_can_be_denoted_as_beginarrayr_l_alpha_c_jto_x_il_fracexpleftlangletildemathbfw_qlmathbfx_iltildemathbfw_klmathbfc_jlranglerightsum_v_ckinmathcalnv_xiexpleftlangletildemathbfw_qlmathbfx_iltildemathbfw_klmathbfc_klrangleright_mathbfx_il_1_mathrmmlpleftmathbfx_ilsum_v_cjinmathcalnv_xialpha_c_jto_x_iltildemathbfw_vlmathbfc_jlright_endarray_quad_312"
    ]
  },
  "c0d56776510218968a38f9f4a9d4e0e9": {
    "file_path": "converted_papers/2510.15583_Attn-JGNN_Attention_Enhanced_Join-Graph_Neural_Net.md",
    "title": "Attn JGNN Attention Enhanced Join Graph Neural Net",
    "file_hash": "4fea7bcbb66006f174cc73f0e270adf7",
    "parsed_at": "2025-12-08T15:13:54.065356",
    "section_count": 4,
    "section_keys": [
      "attn_jgnn_attention_enhanced_join_graph_neural_net",
      "introduction",
      "42_evaluation_baselines_following_bpnn_and_nsnet_we_use_the_1_root_mean_square_error_rmse_between_the_estimated_log_countings_and_ground_truth_as_our_evaluation_metrics_we_compare_attn-_jgnn_the_neural_baseline_bpnn_and_nsnet_and_two_state-_of-_the-_art_approximate_model_counting_solvers_approx_mc3_and_f2_1_for_approx_mc3_and_f2_we_set_a_time_limit_of_5000_seconds_on_each_instance_43_main_results_centerfig_3_a_is_rmse_between_estimated_log_countings_and_ground_truth_for_each_solver_on_the_bird_benchmarkb_is_scatter_plot_comparing_the_estimated_log_countings_against_the_ground_truth_for_each_solver_on_the_bird_benchmark_center_as_shown_in_figure_3a_attn-_jgnn_can_estimate_tighter_counts_than_nsnet_bpnn_and_f2_in_all_categories_of_the_bird_benchmark_attn-_jgnn_estimates_are_almost_three_times_more_accurate_than_f2_and_bpnn_however_attn-_jgnn_cannot_compete_with_approx_mc3_figure_3b_shows_the_scatter_plot_the_estimated_logarithmic_count_is_compared_to_the_ground_truth_for_each_solver_on_the_bird_benchmark_when_the_ground_truth_is_less_than_e100_attn-_jgnn_and_approx_mc3_can_provide_more_accurate_estimates_than_nsnet_f2_and_bpnn_in_most_cases_approx_mc3_is_unable_to_complete_in_5000_seconds_when_the_ground_truth_count_exceeds_e100_attn-_jgnn_can_still_give_a_close_approximation_when_the_ground_truth_count_exceeds_e1000_this_demonstrates_the_effectiveness_of_attn-_jgnn_in_solving_difficult_and_large_cases_the_solution_speed_of_attn-_jgnn_without_using_the_attention_mechanism_is_same_order_of_magnitude_as_that_of_nsnet_and_its_effect_is_still_better_than_that_of_nsnet_this_further_indicates_that_the_reasoning_ability_of_the_ijgp_algorithm_is_superior_to_that_of_bp",
      "5_related_works_since_sat_was_proven_to_be_a_p-_complete_problem_developing_efficient_solutions_for_sat_with_limited_computational_resources_has_become_a_key_research_focus_traditional_model_counting_methods_are_categorized_into_two_groups_based_on_the_required_accuracy_of_results_exact_counting_and_approximate_counting_recent_advances_have_also_introduced_data-_driven_neural_network_approaches_which_leverage_learning_capabilities_to_address_sats_inherent_complexity_exact_counting_methods_prioritize_absolute_correctness_of_results_making_them_suitable_for_scenarios_with_small_variable_scales_or_specialized_formula_structures_they_can_be_further_divided_into_search-_based_and_dynamic_programming_dp-_based_approaches_depending_on_their_core_reasoning_mechanisms_search-_based_methods_typically_extend_the_davis-_putnam-_logemann-_loveland_dpll_algorithman_iterative_search_procedure_for_propositional_satisfiabilityto_count_satisfying_assignments_while_these_methods_guarantee_exact_results_their_scalability_is_sometimes_limited_due_to_exponential_time_complexity_in_the_worst_case_well_known_tools_in_the_search-_based_category_includes_c2d_10_sharp_sat_32_d4_19_ganak_29_exact_mc_20_panini_22_etc_dp-_based_exact_counters_avoid_brute-_force_search_by_decomposing_the_formula_into_subproblems_and_solving_them_recursively_two_representative_methods_are_addmc_13_and_dpmc_14_approximate_counting_methods_trade_off_result_accuracy_for_polynomial-_time_complexity_addressing_the_scalability_gap_of_exact_methods_for_large-_scale_cnf_formulas_the_most_mainstream_approaches_in_this_category_are_hash-_based_approximate_counters_which_rely_on_randomization_to_estimate_model_counts_without_exhaustive_enumeration_the_core_idea_of_hash-_based_methods_is_to_partition_the_solution_space_all_variable_assignments_into_disjoint_uniformly_sized_cells_using_random_hash_functions_the_total_number_of_models_is_then_estimated_by_1_randomly_selecting_a_cell_2_exactly_counting_the_number_of_satisfying_assignments_within_that_cell_and_3_scaling_the_count_by_the_total_number_of_cells_a_pioneering_and_widely_used_solver_in_this_area_is_approx_mc_6_and_its_subsequent_optimizations_7_31_30_36_approx_mc_introduces_random_xor_constraints_to_partition_the_solution_spaceeach_xor_constraint_defines_a_hash_function_that_groups_assignments_into_cells_it_provides_provable_approximation_guarantees_by_controlling_the_number_of_xor_constraints_and_the_number_of_sampled_cells_however_approx_mcs_performance_heavily_depends_on_the_efficiency_of_its_underlying_sat_solver_used_to_count_assignments_in_sampled_cells_and_requires_careful_engineering_for_state_management_and_solver_interaction_while_approx_mc_provide_guaranteed_approximation_there_are_also_some_efficient_approximate_model_counters_without_guarantee_such_as_sts_15_sats_17_and_partial_kc_21_with_the_rise_of_deep_learning_data-_driven_neural_network_approaches_have_emerged_as_a_new_paradigm_for_sat_leveraging_graph_neural_networks_gnns_and_message-_passing_architectures_to_learn_patterns_from_formula_structures_these_methods_do_not_rely_on_handcrafted_heuristics_making_them_more_adaptable_to_diverse_formula_distributions_early_works_focused_on_predicting_satis"
    ]
  },
  "fc136d6364c6e0fa586846bd1bdc6408": {
    "file_path": "converted_papers/2309.16941_G4SATBench_Benchmarking_and_Advancing_SAT_Solving_.md",
    "title": "G4SATBench Benchmarking and Advancing SAT Solving",
    "file_hash": "e2530eb3451ae7048224effb4bb74839",
    "parsed_at": "2025-12-08T15:13:54.066244",
    "section_count": 4,
    "section_keys": [
      "g4satbench_benchmarking_and_advancing_sat_solving",
      "introduction",
      "72_conclusion_72_conclusion_in_this_work_we_present_g4satbench_a_benchmark_study_that_comprehensively_evaluates_gnn_models_in_sat_solving_g4satbench_offers_curated_synthetic_sat_datasets_sourced_from_various_domains_and_difficulty_levels_and_benchmarks_a_wide_range_of_gnn-_based_sat_solvers_under_diverse_settings_our_empirical_analysis_yields_valuable_insights_into_the_performances_of_gnn-_based_sat_solvers_and_further_provides_a_deeper_understanding_of_their_capabilities_and_limitations_we_hope_the_proposed_g4satbench_will_serve_as_a_solid_foundation_for_gnn-_based_sat_solving_and_inspire_future_research_in_this_exciting_field_acknowledgments_acknowledgments_this_work_was_supported_in_part_by_individual_discovery_grants_from_the_natural_sciences_and_engineering_research_council_of_canada_and_the_canada_cifar_ai_chair_program_references_saeed_amizadeh_sergiy_matusevych_and_markus_weimer_learning_to_solve_circuit-_sat_an_unsupervised_differentiable_approach_in_international_conference_on_learning_representations_iclr_2019a_saeed_amizadeh_sergiy_matusevych_and_markus_weimer_pdp_a_general_neural_framework_for_learning_constraint_satisfaction_solvers_ar_xiv_preprint_ar_xiv190301969_2019b_jimmy_lei_ba_jamie_ryan_kiros_and_geoffrey_e_hinton_layer_normalization_ar_xiv_preprint_ar_xiv160706450_2016_adrian_balint_and_andreas_fr\u00f6hlich_improving_stochastic_local_search_for_sat_with_a_new_probability_distribution_in_theory_and_applications_of_satisfiability_testing-_sat_2010_13th_international_conference_sat_2010_edinburgh_uk_july_11-_14_2010_proceedings_13_pp_10-_15_springer_2010_armin_biere_marijn_heule_and_hans_van_maaren_handbook_of_satisfiability_volume_185_ios_press_2009_b\u00e9la_bollob\u00e1s_and_paul_erd\u0151s_cliques_in_random_graphs_in_mathematical_proceedings_of_the_cambridge_philosophical_society_1976_benedikt_b\u00fcnz_and_matthew_lamm_graph_neural_networks_and_boolean_satisfiability_ar_xiv_preprint_ar_xiv170203592_2017_chris_cameron_rex_chen_jason_hartford_and_kevin_leyton-_brown_predicting_propositional_satisfiability_via_end-_to-_end_learning_in_aaai_conference_on_artificial_intelligence_aaai_2020_ting_chen_simon_kornblith_mohammad_norouzi_and_geoffrey_e_hinton_a_simple_framework_for_contrastive_learning_of_visual_representations_in_international_conference_on_machine_learning_icml_2020_ziliang_chen_and_zhanfu_yang_graph_neural_reasoning_may_fail_in_certifying_boolean_unsatisfiability_ar_xiv_preprint_ar_xiv190911588_2019_james_m_crawford_and_larry_d_auton_experimental_results_on_the_crossover_point_in_random_3-_sat_artificial_intelligence_1996_haonan_duan_pashootan_vaezipoor_max_b_paulus_yangjun_ruan_and_chris_j_maddison_augment_with_care_contrastive_learning_for_combinatorial_problems_in_international_conference_on_machine_learning_icml_2022_matthias_fey_and_jan_eric_lenssen_fast_graph_representation_learning_with_pytorch_geometric_ar_xiv_preprint_ar_xiv190302428_2019_abkfm_fleury_and_maximilian_heisinger_cadical_kissat_paracooba_plingeling_and_treengeling_entering_the_sat_competition_2020_sat_competition_2020",
      "a_datasets_generators_to_generate_high-_quality_sat_datasets_that_do_not_contain_trivial_instances_we_have_employed_a_rigorous_process_of_selecting_appropriate_parameters_for_each_cnf_generator_in_g4satbench_table_7_provides_detailed_information_about_the_generators_we_have_used_table_7_details_of_the_synthetic_generators_employed_in_g4satbench_tabletrtddatasettdtddescriptiontdtdparameterstdtdnotestdtrtrtdsrtdtdthe_sr_dataset_is_composed_of_pairs_of_satisfiable_and_unsatisfiable_formulas_with_the_only_difference_between_each_pair_being_the_polarity_of_a_single_literal_given_the_number_of_variables_n_the_synthetic_generator_iteratively_samples_k_1bernoullibgeometricg_vari-ables_uniformly_at_random_without_replacement_and_negates_each_one_with_independent_probability_50_to_build_a_clause_this_procedure_continues_until_the_generated_formula_is_unsatisfiable_the_satisfiable_instance_is_then_constructed_by_negating_the_first_literal_in_the_last_clause_of_the_unsatisfiable_onetdtdgeneral_b_03_g_04_easy_dataset_n_uniform10_40_medium_dataset_n_uniform40_200_hard_dataset_n_uniform200_400tdtdthe_sampling_parameters_are_the_same_as_the_original_paper_selsam_et_al_2019tdtrtrtd3-sattdtdthe_3-sat_dataset_comprises_cnf_formulas_at_the_phase_transition_where_the_proportion_of_generated_satisfiable_and_unsatisfiable_formu-_las_is_roughly_equal_given_the_number_of_variables_n_and_clauses_m_the_synthetic_generator_iteratively_samples_three_variables_and_their_polarities_uniformly_at_random_until_m_clauses_are_obtainedtdtdgeneral_m_4258n_5826n-23_easy_dataset_n_uniform10_40_medium_dataset_n_uniform40_200_hard_dataset_n_uniform200_300tdtdthe_parameter_m_is_the_same_as_the_paper_crawford_ampamp_auton_1996tdtrtrtdcatdtdthe_ca_dataset_contains_sat_instances_that_are_designed_to_mimic_the_community_structures_and_modularity_features_found_in_real-world_industrial_instances_given_variable_number_n_clause_number_m_clause_size_k_community_number_c_and_modularity_q_the_synthetic_generator_iteratively_selects_k_literals_in_the_same_community_uni-_formly_at_random_with_probability_p_q_1c_and_selects_k_literals_in_the_distinct_community_uniformly_at_random_with_probability_1-p_to_build_a_clause_and_repeat_for_m_times_to_construct_a_cnf_formulatdtdgeneral_m_uniform13n_15n_k_uniform4_5_c_uniform3_10_q_uniform07_09_easy_dataset_n_uniform10_40_medium_dataset_n_uniform40_200_hard_dataset_n_uniform200_400tdtdthe_parameters_are_selected_based_on_the_experiments_in_the_original_paper_gir\u00e1ldez-cru_ampamp_levy_2015_and_our_study_to_ensure_that_the_generated_sat_instances_have_a_balance_of_satisfiability_and_unsatisfiabilitytdtrtrtdpstdtdps_dataset_encompasses_sat_instances_with_a_power-law_distribution_in_the_number_of_variable_occurrences_popularity_and_good_class-_tering_between_them_similarity_given_variable_number_n_clause_number_m_and_average_clause_size_k_the_synthetic_generator_first_as-_signs_random_angles_\u03b8i_\u03b8j_0_2\u03c0_to_each_variable_i_and_each_clause_j_and_then_randomly_samples_variable_i_in_clause_j_with_the_proba-_bility_p_11i\u03b2j\u03b2irt_here_\u03b8ij_\u03c0-\u03c0-\u03b8i-\u03b8j_is_the_angle_between_variable_i_and_clause_j_the_exponent_parameters_\u03b2_and_\u03b2_control_the_power-law_distribution_of_variable_occurrences_and_clause_size_respectively_the_temperature_parameter_t_controls_the_sharpness_of_the_probability_distribution_while_r_is_an_approximate_normalization_constant_that_ensures_the_average_number_of_selected_edges_is_kmtdtdgeneral_m_uniform6n_8n_k_uniform4_5_\u03b2_uniform0_1_\u03b2_1_c_uniform3_10_t_uniform075_15_easy_dataset_n_uniform10_40_medium_dataset_n_uniform40_200_hard_dataset_n_uniform200_300tdtdthe_parameters_are_selected_based_on_the_experiments_in_the_original_paper_gir\u00e1ldez-cru_ampamp_levy_2017_and_our_study_to_ensure_that_the_generated_sat_instances_have_a_balance_of_satisfiability_and_unsatisfiabilitytdtrtrtdk-cliquetdtdthe_k-clique_dataset_includes_sat_instances_that_encode_the_k-clique_problem_which_involves_determining_whether_there_exists_a_clique_ie_a_subset_of_vertices_that_are_all_adjacent_to_each_other_with_v_vertices_in_a_given_graph_given_the_number_of_cliques_k_the_synthetic_generator_produces_an_erd\u0151s-r\u00e9nyi_graph_with_v_vertices_and_a_given_edge_probability_p_and_then_transforms_the_corresponding_k-clique_problem_into_a_sat_instancetdtdgeneral_p_vk-1v_easy_dataset_v_uniform5_15_k_uniform3_4_medium_dataset_v_uniform15_20_k_uniform3_5_hard_dataset_v_uniform20_25_k_uniform4_6tdtdthe_parameter_p_is_selected_based_on_the_paper_bollob\u00e1s_ampamp_erd\u0151s_1976_making_the_expected_number_of_k-cliques_in_the_generated_graph_equals_1tdtrtrtdk-domsettdtdthe_k-domset_dataset_contains_sat_instances_that_encode_the_k-dominating_set_problem_this_problem_is_to_determine_whether_there_exists_a_dominating_set_ie_a_subset_of_vertices_such_that_every_vertex_in_the_graph_is_either_in_the_subset_or_adjacent_to_a_vertex_in_the_sub-_set_with_at_most_k_vertices_in_a_given_graph_given_the_domination_number_k_the_synthetic_generator_produces_an_erd\u0151s-r\u00e9nyi_graph_with_v_vertices_and_a_given_edge_probability_p_and_then_transforms_the_corresponding_k-dominating_set_problem_into_a_sat_instancetdtdgeneral_p_1-1-vk-1vk_easy_dataset_v_uniform5_15_k_uniform2_3_medium_dataset_v_uniform15_20_k_uniform3_5_hard_dataset_v_uniform20_25_k_uniform4_6tdtdthe_parameter_p_is_selected_based_on_the_paper_wieland_ampamp_godbole_2001_making_the_expected_number_of_domination_set_with_size_k_in_the_generated_graph_equals_1tdtrtrtdk-vercovtdtdthe_k-vercov_dataset_consists_of_sat_instances_that_encode_the_k-vercov_problem_ie_check_whether_there_exists_a_set_of_k_vertices_in_a_graph_such_that_every_edge_has_at_least_one_endpoint_in_this_set_given_the_vertex_cover_number_k_the_synthetic_generator_produces_a_complement_graph_of_an_erd\u0151s-r\u00e9nyi_graph_with_v_vertices_and_a_given_edge_probability_p_and_then_converts_the_corresponding_k-vercov_cover_problem_into_a_sat_instancetdtdgeneral_p_vk-1v_easy_dataset_v_uniform5_15_k_uniform3_5_medium_dataset_v_uniform10_20_k_uniform6_8_hard_dataset_v_uniform15_25_k_uniform9_10tdtdthe_generation_process_and_the_parameter_are_selected_based_on_the_relationship_between_k-vercov_cover_and_k-clique_problems_making_the_size_of_the_minimum_vertex_cover_in_the_generated_graph_around_ktdtrtable"
    ]
  },
  "46b65358d382a78f15182fe583348ec2": {
    "file_path": "converted_papers/2508.04235_Circuit-Aware_SAT_Solving_Guiding_CDCL_via_Conditi.md",
    "title": "Circuit Aware SAT Solving Guiding CDCL via Conditi",
    "file_hash": "5aba988cedd5c5f41ad366f4d38062ad",
    "parsed_at": "2025-12-08T15:13:54.067031",
    "section_count": 4,
    "section_keys": [
      "circuit_aware_sat_solving_guiding_cdcl_via_conditi",
      "introduction",
      "references_amaru_l_gaillardon_p-_e_testa_e_and_micheli_g_d_2019_the_epfl_combinational_benchmark_suite_in_24th_international_workshop_on_logic_synthesis_iwlsamizadeh_s_matusevych_s_and_weimer_m_2018_learning_to_solve_circuit-_sat_an_unsupervised_differentiable_approach_in_international_conference_on_learning_representationsarmin_b_tobias_f_katalin_f_mathias_f_nils_f_and_florian_p_2024_ca_di_ca_l_gimsatul_isa_sat_and_kissat_entering_the_sat_competition_2024_proc_of_sat_competition_8-_10_audemard_g_and_simon_l_2018_on_the_glucose_sat_solver_international_journal_on_artificial_intelligence_tools_2701_1840001_brayton_r_k_and_mishchenko_a_2010_abc_an_academic_industrial-_strength_verification_tool_in_international_conference_on_computer_aided_verificationchanseok_o_2015_between_sat_and_unsat_the_fundamental_difference_in_cdcl_sat_in_international_conference_on_theory_and_applications_of_satisfiability_testing_307-_323_springerdavidson_s_1999_itc99_benchmark_circuits_-_preliminary_results_in_international_test_conference_1999_proceedings_ieee_cat_no99ch37034_1125-_1125_e\u00e9n_n_mishchenko_a_and_s\u00f6rensson_n_2007_applying_logic_synthesis_for_speeding_up_sat_in_theory_and_applications_of_satisfiability_testing_-_sat_2007_272-_286_springerfang_w_li_w_liu_s_lu_y_zhang_h_and_xie_z_2025a_net_tag_a_multimodal_rtl-_and-_layout-_aligned_netlist_foundation_model_via_text-_attributed_graph_ar_xiv_preprint_ar_xiv250409260_fang_w_liu_s_wang_j_and_xie_z_2025b_circuitfusion_multimodal_circuit_representation_learning_for_agile_chip_design_ar_xiv_preprint_ar_xiv250502168_fleury_a_and_heisinger_m_2020_cadical_kissat_paracooba_plungeling_and_treengeling_entering_the_sat_competition_2020_sat_competition_2020_50_goldberg_e_prasad_m_and_brayton_r_2001_using_sat_for_combinational_equivalence_checking_design_automation_and_test_in_europe_design_automation_and_test_in_europekingma_d_and_ba_j_2014_adam_a_method_for_stochastic_optimization_ar_xiv_learning_ar_xiv_learningliu_j_zhai_j_zhao_m_lin_z_yu_b_and_shi_c_2024_polar_gate_breaking_the_functionality_representation_bottleneck_of_and-_inverter_graph_neural_network_in_2024_ieeeacm_international_conference_on_computer-_aided_design_iccadlu_f_wang_l-_c_cheng_k-_t_and_huang_r-_y_2003_a_circuit_sat_solver_with_signal_correlation_guided_learning_in_2003_design_automation_and_test_in_europe_conference_and_exhibition_892-_897_marques-_silva_j_lynce_i_and_malik_s_2021_conflict-_driven_clause_learning_sat_solvers_in_handbook_of_satisfiability_133-_182_ios_press_open_core_t_1999_open_core_httpsopencoresorgqian_y_chen_z_zhang_x_and_cai_s_2025_x-_sat_an_efficient_circuit-_based_sat_solver_in_2025_62nd_acmieee_design_automation_conference_dac_ieeeshi_z_li_z_ma_c_zhou_y_zheng_z_liu_j_pan_h_zhou_l_li_k_zhu_j_yan_l_he_z_xue_c_jiang_w_yang_f_sun_g_yang_x_chen_g_shi_c_chu_z_yang_j_and_xu_q_2025a_forge_eda_towards_verifiable_and_customizable_circuit_benchmarks_ar_xiv_preprint_ar_xiv250502016_shi_z_ma_c_zheng_z_zhou_l_pan_h_jiang_w_yang_f_yang_x_chu_z_and_xu_q_2025b_deep_cell_multiview_representation_learning_for_post-_mapping_netlists_ar_xiv_preprint_ar_xiv250206816_shi_z_pan_h_khan_s_li_m_liu_y_huang_j_zhen_h-_l_yuan_m_chu_z_and_xu_q_2023_deepgate2_functionality-_aware_circuit_representation_learning_in_2023_ieeeacm_international_conference_on_computer_aided_design_iccad_1-_9_ieeeshi_z_tang_t_zhu_j_sadaf_k_zhen_h-_l_yuan_m_chu_z_and_xu_q_2025c_logic_optimization_meets_sat_a_novel_framework_for_circuit-_sat_solving_in_2025_62nd_acmieee_design_automation_conference_dac_ieeeshi_z_zheng_z_khan_s_zhong_j_li_m_and_xu_q_2024_deep_gate3_towards_scalable_circuit_representation_learning_ar_xiv_preprint_ar_xiv240711095_stephan_p_brayton_r_and_sangiovanni-_vincentelli_a_1996_combinational_test_generation_using_satisfiability_ieee_transactions_on_computer-_aided_design_of_integrated_circuits_and_systems_1167-_1176_vaswani_a_shazeer_n_parmar_n_uszkoreit_j_jones_l_gomez_a_n_kaiser_l_u_and_polosukhin_i_2017_attention_is_all_you_need_in_guyon_i_luxburg_u_v_bengio_s_wallach_h_fergus_r_vishwanathan_s_and_garnett_r_eds_advances_in_neural_information_processing_systems_volume_30_curran_associates_incwang_z_bai_c_he_z_zhang_g_xu_q_ho_t-_y_huang_y_and_yu_b_2024_fgnn2_a_powerful_pre-_training_framework_for_learning_the_logic_functionality_of_circuits_ieee_transactions_on_computer-_aided_design_of_integrated_circuits_and_systemswang_z_bai_c_he_z_zhang_g_xu_q_ho_t-_y_yu_b_and_huang_y_2022_functionality_matters_in_netlist_representation_learning_in_proceedings_of_the_59th_acmieee_design_automation_conference_61-_66_wu_c-_a_lin_t-_h_lee_c-_c_and_huang_c-_y_2007_qute_sat_a_robust_circuit-_based_sat_solver_for_complex_circuit_structure_in_2007_design_automation_test_in_europe_conference_exhibition_1-_6_wu_h_zheng_h_pu_y_and_yu_b_2025_circuit_representation_learning_with_masked_gate_modeling_and_verilog-_aig_alignment_ar_xiv_preprint_ar_xiv250212732_zheng_z_huang_s_zhong_j_shi_z_dai_g_xu_n_and_xu_q_2025_deep_gate4_efficient_and_effective_representation_learning_for_circuit_design_at_scale_ar_xiv_preprint_ar_xiv250201681",
      "a1_model_details_model_architecture_we_adopt_the_deepgate4_framework_zheng_et_al_2025_where_each_gate_is_represented_by_structural_and_functional_embeddings_and_updated_via_selfattention_vaswani_et_al_2017_for_each_gate_type_gin_mathrmandmathrmnotmathrmdiv_we_implement_type-_specific_aggregation_and_update_functions_not_the_aggregator_directly_propagates_the_single_input_and_applies_a_logical_negation_and_virtual_and_the_aggregation_function_captures_conjunction_semantics_where_attention_weights_reflect_the_relative_importance_of_inputs-_for_example_giving_higher_weight_to_controlling_inputs_that_can_determine_the_output_value_virtual_div_in_addition_to_a_two-_input_aggregator_we_account_for_the_asymmetric_roles_of_the_inputs_numerator_a_joint_vs_denominator_c_by_incorporating_positional_encodings_to_distinguish_them_during_message_passing_this_unified_framework_enables_the_model_to_reason_jointly_over_logical_and_probabilistic_structures_level-_by-_level_propagation_let_mathcallv_denote_the_topological_level_of_gate_v_for_every_level_ell_from_primary_inputs_pi_to_primary_outputs_po_the_model_performs_beginarrayr_l_h_vsleftarrow_mathrmagg_gsbigh_usmid_uin_pvmathcallu_ell_-1_big_h_vfleftarrow_mathrmagg_gfbigh_ufh_usmid_uin_pvmathcallu_ell_-1_big_endarray_quad_a1_where_pv_denotes_the_set_of_predecessor_nodes_of_v_a2_effectiveness_of_div_gate_the_div_gate_explicitly_models_conditional_probability_in_the_circuit_graph_via_the_following_equation_pamid_c_fracpawedge_cpc_quad_a3_while_this_formulation_is_convenient_the_division_operation_can_amplify_errors_significantly_when_pc_is_very_small_always_smaller_than_1_as_it_represents_the_logic_probability_of_node_c_to_illustrate_this_behavior_we_analyze_two_representative_cases_using_ac97_ctrlaig_as_a_case_study_case_1_extremely_small_pc_in_this_scenario_when_the_conditional_node_rarely_evaluates_to_1_pc_001_even_minor_absolute_errors_in_pawedge_c_translate_into_large_relative_errors_in_pamid_c_table_a1_presents_the_ground-_truth_and_predicted_probabilities_for_pc_and_a_subset_of_20_target_nodes_a_the_prediction_error_for_pc_is_moderate_hatp_c_000323_yet_the_resulting_conditional_probabilities_are_significantly_distorted_due_to_error_amplification_case_2_moderate_pc_when_pc_is_in_a_mid-_range_eg_pc_03_the_division_operation_becomes_relatively_numerically_stable_however_it_still_amplifies_errors_in_pamid_c_to_an_extent_that_is_practically_unacceptable_table_a2_compares_aggregate_losses_showing_that_the_overall_absolute_error_remains_greater_than_01_in_contrast_our_method_incorporates_the_div_gate_during_the_dataset_preparation_stage_the_probability_labels_for_the_div_gate_are_computed_as_the_division_of_the_probabilities_of_its_two_input_nodes_which_are_tagged_with_distinct_positional_markers_by_learning_this_behavior_as_a_standard_gate_type_the_model_directly_internalizes_the_division_operation_eliminating_the_need_to_explicitly_compute_the_quotient_of_joint_and_marginal_probabilities_a3_pattern-based_dataset_to_bootstrap_training_we_hope_to_generate_pattern-_based_traces_that_approximate_full_truth_tables_given_a_circuit_with_m_primary_inputs_pis_the_complete_truth_table_has_2m_rowsimpractical_beyond_mapprox_20_instead_we_conduct_simulation_with_20000_patterns_and_uniformly_sample_100_random_patterns_per_circuit_per_epoch_see_table_a3_and_record_the_resulting_logic_probability_values_for_every_node_tabletrtdpattern_idtdtdpi1tdtdpi2tdtdpi3tdtdtdtdpimtdtdnode_utdtdnode_vtdtrtrtd1tdtd0tdtd1tdtd0tdtdtdtd1tdtd0tdtd0tdtrtrtd2tdtd1tdtd0tdtd0tdtdtdtd0tdtd1tdtd0tdtrtrtd3tdtd0tdtd0tdtd1tdtdtdtd1tdtd0tdtd1tdtrtrtdtdtdtdtdtdtdtdtdtdtdtdtdtdtdtdtrtrtd19997tdtd0tdtd1tdtd1tdtdtdtd0tdtd1tdtd1tdtrtrtd19998tdtd0tdtd1tdtd1tdtdtdtd0tdtd0tdtd1tdtrtrtd19999tdtd1tdtd0tdtd1tdtdtdtd1tdtd1tdtd0tdtrtrtd20000tdtd0tdtd0tdtd0tdtdtdtd0tdtd0tdtd1tdtrtable_table_a3_excerpt_of_a_truth_table_each_training_epoch_picks_a_fresh_random_subset_of_100_rows_ensuring_the_model_learns_to_interpolate_between_unseen_patterns_for_each_node_v_we_compute_hatp_mathrmrandv_frac1100sum_i_1100mathbf1bigv_1mathrminrowibig_quad_a4_which_serves_as_a_supervisory_signal_encouraging_the_gnn_to_capture_fine-_grained_functional_behavior_because_the_100-_row_subset_changes_every_epoch_the_network_is_exposed_to_thousands_of_distinct_local_views_yielding_better_generalization_than_a_single_large_simulation_a4_workload-based_dataset_while_random_patterns_provide_breadth_they_fail_to_capture_realistic_activity_biases_we_therefore_introduce_workload-_based_simulations_where_each_primary_input_pi_is_independently_assigned_a_probability_value_rho_sampled_from_the_set_01_02_ldots_09_a_toy_circuit_c_a_wedge_b_fig_a1_is_used_to_illustrate_the_necessity"
    ]
  },
  "9dfe8a7b63cb43b47cb5e188d19e214f": {
    "file_path": "converted_papers/2504.11885_HyperSAT_Unsupervised_Hypergraph_Neural_Networks_f.md",
    "title": "HyperSAT Unsupervised Hypergraph Neural Networks f",
    "file_hash": "d4efb790728256fae73dd327fb34f618",
    "parsed_at": "2025-12-08T15:13:54.067713",
    "section_count": 4,
    "section_keys": [
      "hypersat_unsupervised_hypergraph_neural_networks_f",
      "introduction",
      "31_hypergraph_modeling_given_a_weighted_max_sat_instance_phi_mathcalxmathcalcmathbfw_we_construct_the_hypergraph_mathcalg_mathcalvmathcalemathbfw_as_follows_for_the_construction_of_the_vertex_set_nu_each_variable_x_iin_mathcalx_is_represented_by_two_nodes_v_i_and_v_n_i_which_correspond_to_the_positive_and_negative_literals_of_the_variable_x_i_ie_x_i_and_neg_x_i_respectively_for_the_construction_of_the_hyperedge_set_mathcale_each_clause_c_jin_mathcalc_is_represented_by_a_hyperedge_e_jin_mathcale_which_connects_the_nodes_corresponding_to_the_literals_in_c_j_specifically_if_clause_c_j_consists_of_the_literals_l_j1l_j2ldots_l_jk_j_then_the_corresponding_hyperedge_e_j_connects_the_nodes_corresponding_to_these_literals_for_the_construction_of_the_weight_matrix_mathbfw_the_weight_of_each_hyperedge_is_equal_to_the_weight_of_the_corresponding_clause_ie_w_jj_w_j_through_the_above_modeling_process_the_weighted_max_sat_instance_can_be_uniquely_represented_by_a_hypergraph_where_each_literal_is_a_node_and_each_clause_is_a_hyperedge_connecting_the_literals_involved_the_weight_of_each_hyperedge_is_the_weight_of_the_corresponding_clause_in_the_instance_figure_2_gives_an_illustration_of_the_hypergraph_modeling_process_centerfigure_2_hypergraph_modeling_of_a_weighted_max_sat_instance_neg_x_1lor_x_2lor_neg_x_3land_x_1lor_x_2land_neg_x_2lor_x_3lor_x_4land_neg_x_1lor_x_3lor_neg_x_4_with_8_literals_4_clauses_and_weights_w_1w_2w_3w_4_center_in_contrast_to_existing_methods_that_represent_conjunctive_normal_form_cnf_formulas_as_factor_graphs_guo_et_al_2023_which_limit_their_ability_to_model_relationships_beyond_pairwise_connections_we_construct_weighted_max_sat_instances_as_hypergraphs_these_hypergraphs_can_encode_higher-_order_variable_dependencies_through_their_degree-_free_hyperedges_and_thus_offer_more_powerful_representational_capacity_and_enable_more_efficient_handling_of_higher-_order_relationships_in_combinatorial_optimization_problems_in_particular_we_treat_literals_rather_than_variables_as_nodes_which_addresses_a_major_issue_present_in_hyp_op_heydaribeni_et_al_2024_since_the_logical_relationships_between_positive_and_negative_literals_are_central_to_the_weighted_max_sat_problem_it_is_essential_to_treat_them_as_distinct_nodes_rather_than_merging_them_into_a_single_node_in_addition_we_encode_the_weights_of_the_clauses_in_the_weighted_max_sat_problem_as_the_weights_of_the_corresponding_hyperedges_in_the_hypergraph_32_neural_network_architecture_the_neural_network_architecture_of_our_proposed_hyper_sat_comprises_the_hyper_gcn_a_transformer_module_with_the_cross-_attention_mechanism_and_a_softmax_layer_321_hypergraph_convolutional_networks_in_hyper_sat_we_introduce_a_t_-_layer_hyper_gcn_for_message_passing_among_different_nodes_the_operation_at_the_l_-_th_layer_of_the_hyper_gcn_is_formally_expressed_as_follows_beginarrayrpmbll_1prime_sigma_leftpmbd_v-frac12widetildepmbqpmbd_v-frac12pmbllpmbrlright_endarray_quad_2_in_this_formulation_the_matrix_pmbll_1prime_represents_the_output_of_the_l_-_th_layer_pmbd_v_is_the_diagonal_matrix_of_the_vertex_degrees_in_the_hypergraph_pmbllin_mathbbr2ntimes_d_l_is_the_matrix_of_node_representations_at_the_l_-_th_layer_where_d_l_is_the_dimension_of_the_l_-_th_layer_node_representations_pmbrlin_mathbbrd_ltimes_d_l_1_is_the_l_-_th_layer_learnable_weight_matrix_and_widetildepmbq_is_given_by_widetildepmbq_pmb_hwidetildepmbd_e-1pmb_htop_-_mathrmdiagpmb_hwidetildepmbd_e-1pmb_htop_quad_3_where_pmbh_is_the_hypergraph_incidence_matrix_and_widetildepmbd_e_pmbd_e_-_pmbi_specifically_sigma_denotes_the_nonlinear_activation_function_and_pmbl0_is_a_learnable_input_embedding_of_hyper_gcn_compared_to_the_updating_rule_in_eq_1_in_traditional_hgnn_our_hyper_gcn_focuses_the_convolutional_layers_computation_more_on_the_influence_of_adjacent_nodes_by_removing_the_diagonal_elements_of_widetildepmbq_this_adjustment_allows_the_representation_updates_of_each_node_to_better_align_with_the_higher-_order_relationships_of_the_adjacency_structure_322_cross-attention_mechanism_the_core_of_the_weighted_max_sat_problem_lies_in_the_logical_constraints_among_variables_positive_and_negative_literal_nodes_eg_x_and_neg_x_are_logically_mutually_exclusive_and_strongly_correlated_and_their_relationships_directly_reflect_the_underlying_structural_characteristics_of_the_problem",
      "42_analytical_experiment_we_first_evaluate_the_performance_of_unsupervised_methods_hyper_sat_and_hyp_op_with_a_focus_on_their_convergence_we_evaluate_the_convergence_using_the_primary_task_loss_mathcall_mathrmtask_from_eq_8_which_represents_the_sum_of_the_weights_of_unsatisfied_clauses_we_illustrate_the_evolution_curves_of_loss_for_hyper_sat_and_hyp_op_on_the_uuf250-_1065_dataset_in_figure_3_as_an_example_all_models_can_converge_within_300_epochs_moreover_the_loss_of_hyper_sat_is_around_52_while_the_loss_of_hyp_op_is_around_139_we_can_observe_that_hyper_sat_achieves_better_performance_than_hyp_op_specifically_hyper_sat_decreases_the_loss_more_quickly_and_achieves_a_lower_loss_value_the_experimental_results_demonstrate_that_hyper_sat_can_be_used_in_learning_to_solve_weighted_max_sat_problems_centerfigure_3_the_evolution_of_loss_for_hyper_sat_and_hyp_op_during_an_inference_process_of_300_epochs_on_the_uuf250-1065_dataset_center_43_result_we_evaluate_the_performance_of_hyper_sat_against_baseline_algorithms_hyp_op_and_liu_et_al_2023_on_various_datasets_the_primary_evaluation_metric_is_the_average_weighted_sum_of_unsatisfied_clauses_the_experiments_are_conducted_on_datasets_with_the_number_of_variables_ranging_from_100_to_250_and_the_number_of_clauses_varying_between_430_and_1065_the_results_are_shown_in_table_2_as_presented_in_the_table_hyper_sat_consistently_achieves_lower_values_for_the_average_weighted_sum_of_unsatisfied_clauses_compared_to_the_baseline_algorithms_liu_et_al_2023_and_hyp_op_across_multiple_datasets_for_example_on_the_uf100-_430_dataset_hyper_sat_significantly_outperforms_both_baselines_with_a_result_of_1564_this_result_represents_a_substantial_improvement_over_the_results_of_liu_et_al_2023_3248_and_hyp_op_9915_this_trend_is_observed_across_all_datasets_where_hyper_sat_always_exceeds_the_table_2_the_average_weighted_sum_of_unsatisfied_clauses_of_weighted_max_sat_problems_tabletrtddatasettdtdliu_et_al_2023tdtdhypoptdtdhypersattdtrtrtduf100-430tdtd3248tdtd9915tdtd1564tdtrtrtduuf100-430tdtd4165tdtd10244tdtd2046tdtrtrtduf200-860tdtd6738tdtd15846tdtd2898tdtrtrtduuf200-860tdtd8168tdtd17134tdtd3555tdtrtrtduf250-1065tdtd7906tdtd17060tdtd3324tdtrtrtduuf250-1065tdtd10004tdtd18239tdtd4164tdtrtable_performance_of_the_baselines_the_average_weight_of_unsatisfied_clauses_is_reduced_by_approximately_50_compared_to_liu_et_al_2023_and_over_80_compared_to_hyp_op_on_the_uf200-_860_and_uuf200-_860_datasets_hyper_sat_achieves_reductions_of_5699_and_5648_compared_to_liu_et_al_2023_respectively_and_reductions_of_8171_and_7925_compared_to_hyp_op_similarly_on_the_uf250-_1065_and_uuf250-_1065_datasets_hyper_sat_continues_to_outperform_the_baselines_the_reductions_compared_to_liu_et_al_2023_are_5796_and_5838_respectively_while_the_reductions_compared_to_hyp_op_are_8052_and_7717_importantly_even_with_the_larger_datasets_hyper_sat_maintains_or_even_enhances_its_efficacy_this_demonstrates_that_hyper_sat_scales_well_and_performs_better_these_results_show_that_hyper_sat_consistently_provides_substantial_improvements_over_both_baseline_algorithms_across_a_range_of_datasets_including_those_with_larger_problem_sizes_as_a_result_it_validates_its_robustness_and_effectiveness_in_reducing_the_weighted_sum_of_unsatisfied_clauses_44_ablation_study_we_conduct_an_ablation_study_to_evaluate_the_contribution_of_each_key_component_in_our_proposed_model_by_systematically_removing_components_we_analyze_their_impact_on_performance_and_highlight_the_importance_of_each_component_the_experiments_are_designed_to_isolate_the_impact_of_the_following_components_i_the_hypergraph_modeling_of_literal_nodes_rather_than_variable_nodes_ii_the_transformer_module_with_the_cross-_attention_mechanism_iii_the_shared_representation_constraint_loss_the_results_of_the_ablation_study_are_shown_in_table_3_effect_of_hypergraph_modeling_of_literal_nodes_the_performance_drops_by_5277_when_the_hypergraph_modeling_of_variable_nodes_is_used_instead_of_literal_nodes_the_results_demonstrate_the_importance_and_superiority_of_modeling_the_weighted_max_sat_instance_as_a_hypergraph_with_literal_nodes_effect_of_transformer_with_cross-_attention_we_disable_the_transformer_module_with_the_cross-_attention_mechanism_to_assess_its_importance_without_this_module_the_model_experiences_a_performance_drop_of_1154_this_signifi"
    ]
  },
  "38a6100f8a1976a22dbac38a66b0421e": {
    "file_path": "converted_papers/2405.11024_GraSS_Combining_Graph_Neural_Networks_with_Expert_.md",
    "title": "GraSS Combining Graph Neural Networks with Expert",
    "file_hash": "359cb3c7af3c728b1971eb9516abf595",
    "parsed_at": "2025-12-08T15:13:54.068164",
    "section_count": 2,
    "section_keys": [
      "grass_combining_graph_neural_networks_with_expert",
      "introduction"
    ]
  },
  "ac3f9f5343377be2d869bdfaf7a87622": {
    "file_path": "converted_papers/2508.21513_On_the_Hardness_of_Learning_GNN-based_SAT_Solvers_.md",
    "title": "On the Hardness of Learning GNN based SAT Solvers",
    "file_hash": "0560f3043e660ca3ea1094fd724f45a3",
    "parsed_at": "2025-12-08T15:13:54.069429",
    "section_count": 7,
    "section_keys": [
      "on_the_hardness_of_learning_gnn_based_sat_solvers",
      "introduction",
      "acknowledgements_acknowledgements_geri_skenderi_is_funded_by_the_european_union_through_the_next_generation_eu_-_miur_prin_pnrr_2022_grant_p20229pbzr_the_views_and_opinions_expressed_are_however_those_of_the_authors_only_and_do_not_necessarily_reflect_those_of_the_european_union_or_the_european_research_council_executive_agency_neither_the_european_union_nor_the_granting_authority_can_be_held_responsible_references_1_uri_alon_and_eran_yahav_on_the_bottleneck_of_graph_neural_networks_and_its_practical_implications_in_international_conference_on_learning_representations_2021_1_3_2_maria_chiara_angelini_and_federico_ricci-_tersenghi_monte_carlo_algorithms_are_very_effective_in_finding_the_largest_independent_set_in_sparse_random_graphs_physical_review_e_1001_013302_2019_1_5_3_alvaro_arroyo_alessio_gravina_benjamin_gutteridge_federico_barbero_claudio_gallicchio_xiaowen_dong_michael_bronstein_and_pierre_vandergheynst_on_vanishing_gradients_oversmoothing_and_over-_squashing_in_gnns_bridging_recurrent_and_graph_learning_ar_xiv_preprint_ar_xiv250210818_2025_8_4_jeff_bezanson_alan_edelman_stefan_karpinski_and_viral_b_shah_julia_a_fresh_approach_to_numerical_computing_siam_review_59165-_98_2017_url_httpsdoiorg101137141000671_7_5_bhaswar_b_bhattacharya_and_sumit_mukherjee_exact_and_asymptotic_results_on_coarse_ricci_curvature_of_graphs_discrete_mathematics_338123-_42_2015_4_13_15_19_6_armin_biere_marijn_heule_and_hans_van_maaren_handbook_of_satisfiability_volume_185_ios_press_2009_1_3_7_lucas_bordeaux_youssef_hamadi_and_lintao_zhang_propositional_satisfiability_and_constraint_programming_a_comparative_survey_acm_computing_surveys_csur_38412-_es_2006_1_8_a_braunstein_m_mezard_and_r_zecchina_survey_propagation_an_algorithm_for_satisfiability_random_structures_algorithms_272201-_226_2005_doi_httpsdoiorg101002rsa20057_url_httpsonlinelibrarywileycomdoiabs101002rsa20057_1_9_michael_m_bronstein_joan_bruna_yann_le_cun_arthur_szlam_and_pierre_vandergheynst_geometric_deep_learning_going_beyond_euclidean_data_ieee_signal_processing_magazine_34418-_42_2017_3_10_quentin_cappart_didier_chetelat_elias_b_khalil_andrea_lodi_christopher_morris_and_petar_veli\u010dkovi\u0107_combinatorial_optimization_and_reasoning_with_graph_neural_networks_journal_of_machine_learning_research_241301-_61_2023_1_11_ben_chamberlain_james_rowbottom_maria_i_gorinova_michael_bronstein_stefan_webb_and_emanuele_rossi_grand_graph_neural_diffusion_in_international_conference_on_machine_learning_pages_1407-_1418_pmlr_2021_9_12_wenjing_chang_and_wenlong_liu_sat-_gatv2_a_dynamic_attention-_based_graph_neural_network_for_solving_boolean_satisfiability_problem_electronics_143423_2025_1_13_stephen_a_cook_the_complexity_of_theorem-_proving_procedures_in_proceedings_of_the_third_annual_acm_symposium_on_theory_of_computing_stoc_71_page_151-_158_new_york_ny_usa_1971_association_for_computing_machinery_isbn_9781450374644_doi_101145800157805047_url_httpsdoiorg101145800157805047_1_14_gabriele_corso_hannes_stark_stefanie_jegelka_tommi_jaakkola_and_regina_barzilay_graph_neural_networks_nature_reviews_methods_primers_4117_2024_3_15_william_falcon_and_the_py_torch_lightning_team_py_torch_lightning_march_2019_url_httpsgithubcomlightning-_ailightning_7_16_lukas_fesser_and_melanie_weber_effective_structural_encodings_via_local_curvature_profiles_in_international_conference_on_learning_representations_2024_18_17_matthias_fey_and_jan_eric_lenssen_fast_graph_representation_learning_with_pytorch_geometric_ar_xiv_preprint_ar_xiv190302428_2019_7",
      "a_appendix_in_what_follows_we_provide_supplementary_material_that_complements_the_main_manuscript_the_appendix_is_organized_as_follows_1_appendix_a1_begins_with_background_on_graph_ricci_curvature_where_we_review_both_the_ollivier_and_balanced_forman_discretizations_in_order_to_give_the_reader_an_intuitive_and_formal_foundation_for_subsequent_results_2_in_appendix_a2_we_provide_detailed_proofs_of_the_propositions_and_theorems_discussed_in_the_manuscript_3_appendix_a3_describes_the_test-time_graph_rewiring_procedure_used_in_our_experiments_including_a_full_presentation_of_the_stochastic_curvature-guided_algorithm_4_in_appendix_a4_we_outline_preliminary_ideas_and_implementations_for_curvature-aware_solvers_along_with_empirical_results_that_illustrate_their_potential_5_finally_appendix_a5_contains_additional_plots_and_visualizations_that_further_clarify_the_relationship_between_curvature_problem_hardness_and_solver_behavior_a1_ricci_curvature_of_graphs_in_this_section_we_provide_for_the_sake_of_completeness_both_definitions_and_intuitive_explanations_of_the_two_types_of_graph_ricci_curvature_rc_mentioned_in_this_paper_the_definitions_are_taken_from_bhattacharya_and_mukherjee_5_and_topping_et_al_47_respectively_we_simply_report_them_here_in_a_synthesized_manner_we_kindly_refer_the_reader_to_the_aforementioned_works_for_more_details_a11_ollivier_ricci_curvature_oc_the_formulation_of_ollivier_39_aligns_with_the_intuition_from_differential_geometry_edges_with_negative_curvature_act_as_structural_bottlenecks_separating_dense_regions_and_limiting_smooth_information_propagation_edges_with_positive_curvature_in_the_other_hand_facilitate_smooth_information_propagation_and_are_indicators_of_community_structure_graph_ollivier-_ricci_curvature_oc_is_very_well_studied_and_has_being_linked_to_properties_of_graph_lapacians_and_mixing_times_of_markov_chain_monte_carlo_mcmc_methods_22_42_the_intuitive_idea_of_this_discretization_is_to_directly_implement_the_idea_from_differential_geometry_we_use_a_ratio_between_the_amount_of_mass_moved_around_of_an_edge_neighborhood_with_the_shortest_path_distance_ie_the_graph_geodesic_let_us_formalize_this_concept_for_two_probability_measures_mu_1mu_2_on_a_metric_space_xd_the_the_wasserstein_distance_between_them_is_defined_as_w_1mu_1mu_2_inf_nu_in_mmu_1mu_2int_xtimes_xdxymathrmdnu_xy_quad_12_where_mmu_1mu_2_is_the_collection_of_probability_measures_on_xtimes_x_with_marginals_mu_1_and_mu_2_the_wasserstein_distance_is_the_result_of_the_solution_to_a_famous_problem_called_optimal_transport_43_intuitively_this_distance_measures_the_optimal_cost_to_move_one_pile_of_sand_to_another_one_with_the_same_mass_let_a_metric_measure_space_xdm_be_a_metric_space_xd_with_a_collection_of_probability_measures_m_m_xxin_x_indexed_by_the_points_of_x_the_coarse_ricci_curvature_of_a_metric_measure_space_is_defined_as_follows_definition_a1_ollivier_39_on_any_metric_measure_space_xdm_for_any_two_distinct_points_xyin_x_the_coarse_ricci_curvature_of_xdm_of_xy_is_defined_as_kappa_xycoloneqq_1_-_fracw_1m_xm_ydxy_quad_13_extending_this_definition_to_graphs_requires_some_additional_steps_consider_a_locally_finite_and_possibly_weighted_simple_graph_g_ve_where_each_edge_ijin_e_is_assigned_a_positive_weight_w_ij_w_ji_the_graph_is_equipped_with_the_standard_shortest_path_graph_distance_d_g_that_is_for_ijin_v_d_gij_is_the_length_of_the_shortest_path_in_g_connecting_nodes_i_and_j_for_iin_v",
      "a2_proofs_for_the_sake_of_clarity_and_exposition_we_report_here_both_the_formal_statements_alongside_their_respective_proofs_proposition_31_let_isim_j_be_an_edge_from_the_literal-_clause_graph_lcg_representation_g_of_a_random_k-_sat_problem_with_n_variables_and_m_clauses_with_degree_distributions_given_by_equations_6_and_3_then_mathbbe_isim_jkappa_ijto_0_and_mathbbe_isim_jr_i_cijto_0_as_alpha_fracmnto_0_proof_as_alpha_to_0_the_expected_value_of_the_literal_degree_becomes_lim_alpha_to_0mathbbe_d_isim_pd_id_i_lim_alpha_to_0fraclambda_elambdaelambda_-_1_lim_alpha_to_0elambdacdot_lim_alpha_to_0fraclambdaelambda_-_1_1cdot_lim_alpha_to_0frac1fracelambda_-_1lambda_1_quad_17_where_the_last_equality_is_obtained_due_to_the_fact_that_beginarrayrlim_alpha_to_0lambda_lim_alpha_to_0frac12alpha_k_0_endarray_and_the_limit_formula_beginarrayrlim_xto_0fracax_-_1x_ln_leftaright_endarray_given_the_average_degree_of_the_literals_that_act_as_an_endpoint_of_at_least_one_edge_is_1_in_this_limiting_case_by_definition_of_the_lower_bound_in_equation_8_we_obtain_that_beginarrayrlim_alpha_to_0mathbbe_isim_jbarr_ij_0_endarray_as_a_lower_bound_barr_ij_satisfies_the_following_inequality_5_47_-2_barr_ijleq_r_i_cijleq_kappa_ijleq_0_quad_18_given_that_both_limits_and_expectations_preserve_weak_inequalities_by_sandwiching_we_obtain_beginarrayr_l_underset_alpha_to_0limmathbbe_isim_jbarr_ij_0leq_underset_alpha_to_0limmathbbe_isim_jr_i_cijleq_underset_alpha_to_0limmathbbe_isim_jkappa_ijleq_0_underset_alpha_to_0limmathbbe_isim_jkappa_ij_underset_alpha_to_0limmathbbe_isim_jr_i_cij_0_endarray_quad_20_proposition_32_let_isim_j_be_an_edge_from_the_lcg_representation_g_of_a_random_k-_sat_problem_with_n_variables_and_m_clauses_with_degree_distributions_given_by_equations_6_and_3_then_mathbbe_isim_jbarr_ijto_frac2k_-_2_as_alpha_fracmnto_infty_proof_as_alpha_to_infty_the_expected_value_of_the_literal_degree_becomes_lim_alpha_to_inftymathbbe_d_isim_pd_id_i_lim_alpha_to_inftyfraclambda_elambdaelambda_-_1_lim_alpha_to_inftyfrac1frac1lambda_-_frac1lambda_elambda_infty_quad_21_therefore_we_can_consider_the_lower_bound_as_consisting_only_of_beginarrayrunderliner_ij_frac2d_i_frac2d_j_-_2_endarray_ignoring_the_uninteresting_case_when_k_1_given_that_the_expected_value_is_a_linear_operation_we_obtain_that_lim_alpha_to_inftymathbbe_isim_jbarr_ij_frac2lim_alpha_to_inftymathbbe_d_isim_pd_id_i_frac2k_-2_frac2k_-2_quad_22_theorem_31_let_isim_j_be_an_edge_from_the_lcg_representation_g_of_a_random_k-_sat_problem_with_n_variables_and_m_clauses_with_degree_distributions_given_by_equations_6_and_3_the_bfc_at_isim_j_is_bounded_from_above_by_the_quantity_barr_ijcoloneqq_frac2d_i_frac2d_j_-2_fracd_i_k_-_2max_k_-_1m_-_1_max_d_id_j_quad_23_furthermore_as_beginarrayralpha_fracmnto_infty_mathbbe_isim_jbarr_ijto_frac2k_-_2_endarray_and_therefore_the_average_bfc_over_the_edges_of_g_converges_to_mathbbe_isim_jr_i_cijto_frac2k_-_2",
      "22_graph_neural_networks_gnns_are_a_subclass_of_neural_networks_nns_that_can_learn_a_representation_of_graph_data_by_locally_aggregating_information20_45_the_main_goal_of_the_architecture_is_to_implement_inductive_biases_natural_to_graph_data_9_an_example_of_such_a_property_is_learning_graph-_level_functions_invariant_to_the_nodes_ordering_consider_for_simplicity_an_unweighted_and_undirected_graph_g_with_n_nodes_represented_by_a_symmetric_binary_adjacency_matrix_ain_01ntimes_n_this_setting_can_be_easily_extended_to_deal_with_more_general_connectivity_structures_14_by_associating_a_node_feature_matrix_xin_mathbbrntimes_d_to_the_graph_we_can_describe_a_gnns_as_a_convolution_of_the_graph_signal_with_a_as_the_shift_operator_a_generalization_of_this_concept_can_be_obtained_by_considering_the_message-_passing_framework_20_x_ik_thetakleftx_ik_-_1bigoplus_jin_mathcalniphikleftx_ik_-_1x_jk_-_1e_j_irightright_quad_1_where_x_ik_denotes_node_features_of_node_x_i_at_layer_k_e_j_i_the_optional_edge_features_from_node_j_to_node_i_mathcalncdot_the_set_of_1-_hop_neighbor_nodes_bigoplus_a_differentiable_permutation_invariant_function_eg_sum_mean_and_phi_theta_denote_differentiable_and_optionally_nonlinear_functions_such_as_multi-_layer_perceptrons_mlps_a_cnf_formula_can_be_easily_translated_into_a_bipartite_graph_6_which_can_then_be_fed_into_a_gnn-_based_solver_the_particular_bipartition_we_consider_in_this_work_is_detailed_later_in_section_3_the_application_of_the_above_message-_passing_scheme_for_sat_problems_can_be_done_by_applying_equation1_to_the_clause_and_literal_partitions_28_let_i_be_a_literal_node_and_j_be_a_clause_node_then_beginarrayr_l_h_jk_theta_cklefth_jk_-_1bigoplus_iin_mathcalnjleftleftphi_iklefth_ik_-_1rightrightrightright_h_ik_theta_lklefth_ik_-_1bigoplus_jin_mathcalnileftleftphi_cklefth_jk_-_1rightrightrightright_endarray_quad_2_where_the_subscripts_c_and_l_refer_to_nns_specialized_on_the_clause_and_literal_partitions_respectively_23_ricci_curvature_of_graphs_in_riemannian_geometry_rc_quantifies_the_local_deviation_of_a_manifold_mathcalm_from_flat_euclidean_space_as_a_result_of_the_metric_defined_on_mathcalm_intuitively_rc_captures_how_the_neighborhoods_of_two_adjacent_points_relate_when_moving_from_one_point_to_the_other_on_smooth_manifolds_it_compares_how_a_small_ball_of_mass_around_a_point_is_distorted_when_transported_along_a_geodesic_to_a_neighboring_point_extending_this_notion_to_more_general_structures_such_as_metric_spaces_or_combinatorial_complexes_has_been_an_extremely_active_area_of_mathematical_research_with_the_works_of_ollivier_39_and_forman_18_standing_out_in_the_case_of_graphs_we_ask_ourselves_how_local_connectivity_either_concentrates_or_disperses_ollivier_39_implements_this_idea_by_comparing_probability_mass_on_local_neighborhoods_ie_a_random_walk_distribution_on_the_endpoints_of_an_edge_given_these_two_distributions_one_can_compare_the_ratio_between_their_wasserstein_and_shortest_path_distance_serving_as_a_direct_and_discrete_analogue_of_geodesic_transport_see_appendix_a11_for_more_details_the_definition_of_forman_18_relies_heavily_on_topology_and_thus_it_takes_a_combinatorial_form_essentially_given_a_cell_complex_the_curvature_of_a_p-_cell_depends_only_on_the_topological_structures_between_the_cell_and_its_neighbors_this_makes_forman-_ricci_curvature_fc_simpler_to_compute_numerically_since_it_can_avoid_the_optimization_of_the_optimal_transport_problem_that_arises_in_ollivier-_ricci_curvature_oc_curvature_given_that_rc_is_directly_related_to_the_structure_of_local_neighborhoods_it_has_emerged_as_a_powerful_way_of_theoretically_analyzing_limitations_of_gnns_in_a_seminal_paper_topping_et_al_47_provide_both_a_balanced_version_of_the_fc_curvature_and_show_that_the_oversquashing_problem_1_can_be_directly_connected_to_edges_with_high_negative_curvature_this_definition_namely_the_bfc_is_central_to_this_paper_therefore_please_consult_appendix_a12_for_the_definition_and_additional_details_nguyen_et_al_38_have_shown_that_similar_results_can_be_derived_using_the_oc_curvature_it_is_worth_noting_that_these_notions_of_curvature_are_naturally_correlated_with_one_another_as_shown_empirically_in_a_multitude_of_complex_networks_by_samal_et_al_44",
      "3_curvature_of_random_k-sat_problems_and_its_relationship_with_gnns_setting_and_notation_we_consider_random_k_-_sat_problems_with_n_variables_and_m_clauses_with_alpha_m_n_and_knmin_mathbbn_these_problems_are_represented_through_a_simple_bipartite_graph_g_ve_where_the_node_set_is_a_literal-_clause_bipartition_v_lcup_c_with_lcap_c_emptyset_and_l_2nc_m_the_edge_set_takes_the_form_e_ijin_vtimes_visim_jiin_ljin_c_where_isim_j_indicates_a_connection_between_nodes_given_vin_v_we_denote_its_degree_by_d_v_finally_we_denote_the_expected_value_of_the_random_variable_x_with_probability_distribution_mathrmp_by_mathbbe_px_and_mathbbe_psim_px_the_expectation_over_samples_drawn_from_p_unless_noted_otherwise_when_we_refer_to_the_expected_value_in_simulations_we_imply_its_estimate_via_the_sample_mean_statistic_data_model_our_bipartite_formulation_is_a_simplification_of_the_input_graphs_considered_in_many_gnn-_based_solvers_40_46_recent_literature_28_refers_to_this_data_structure_as_a_literal-_clause_graph_lcg_following_an_erd\u0151sr\u00e9nyi-_like_procedure_each_clause_is_assigned_k_literals_independently_at_random_with_probability_p_assuming_that_all_literals_are_equally_likely_to_appear_in_a_given_clause_we_obtain_the_following_degree_distributions_beginarraycpd_j_h_delta_h_-_k_pd_i_h_binommh_ph1_-_pm_-_h_endarray_quad_3_where_delta_cdot_represents_the_dirac_delta_function_binomcdot_the_binomial_coefficient_and_pcoloneqq_frack2n_in_the_limit_mnto_infty_we_can_approximate_the_binomial_form_of_the_literal_degree_distribution_with_a_poisson_distribution_pd_i_h_fraclambdahe-lambdah_quad_5_with_lambda_m_p_textstyle_frac12alpha_k_we_will_be_interested_in_using_this_approximation_to_calculate_properties_of_the_graph_rc_which_is_an_edge_level_property_therefore_the_case_d_i_0_should_be_truncated_this_fact_leads_us_to_propose_an_alternative_probability_mass_function_based_on_zero-_truncated_poisson_distribution_the_for_the_literal_degrees_pd_i_h_pd_i_hhgeq_1_fracpd_i_h1_-_pd_i_0_fraclambdahhelambda_-_1_quad_6_characterizing_average_curvature_we_will_now_proceed_by_showing_that_the_above_bipartite_representation_of_sat_problems_has_significant_implications_on_the_average_rc_most_importantly_we_will_precisely_characterize_the_average_behavior_of_a_specific_definition_curvature_namely_the_bfc_which_in_turn_has_strong_implications_on_oversquashing_in_gnns_the_proofs_of_all_statements_are_deferred_to_appendix_a2_we_start_from_an_established_lower_bound_of_the_oc_definition_31_oc_lower_bound_in_bipartite_graphs_5_23_for_any_edge_isim_j_in_a_simple_unweighted_bipartite_graph_g_we_have_that_that_underlinekappaij_-2cdot_max_01_-_frac1d_i_-frac1d_j_leq_kappa_ijleq_0_quad_7_where_kappa_is_the_oc_the_above_lower_bound_can_be_redefined_by_alternatively_stating_the_condition_encoded_inside_the_maximum_function_in_equation_7_underlinerij_left_beginarrayl_l0_mathrmifmind_id_j_1_frac2d_i_frac2d_j_-2_mathrmotherwise_endarray_right_quad_8_this_alternative_definition_shows_that_equation_8_is_also_a_lower_bound_for_the_bfc_st_underlinerijleq_r_i_cij_this_statement_is_a_direct_consequence_of_the_definition_of_the_bfc_and_a_proof_is_given_inside_the_proof_of_theorem_31_furthermore_we_also_have_that_-_2_r_i_cijleq_kappa_ijleq_0_which_is_a_direct_consequence_of_the_facts_that_kappa_ijgeq_r_i_cij_47_and_equation_7_starting_from_these_statements_we_can_proceed_to_show_that_the_average_behavior_of_the_graph_rc_is_naturally_dictated_by_average_degree_of_the_literals_given_that_d_j_k_always_holds_the_literal_degree_distribution_is_a_zero-_truncated_poisson_distribution_from_which_the_average_degree_of_a_literal_i_can_be_calculated_as_mathbbe_d_isim_pd_id_i_fraclambda_elambdaelambda_-_1_quad_9"
    ]
  },
  "87ad0dd8785bdc058db3d41ee492de21": {
    "file_path": "converted_papers/2111.07568_Can_Graph_Neural_Networks_Learn_to_Solve_MaxSAT_Pr.md",
    "title": "Can Graph Neural Networks Learn to Solve MaxSAT Pr",
    "file_hash": "1c92a851d286d8b6a2297017b85751ee",
    "parsed_at": "2025-12-08T15:13:54.069748",
    "section_count": 2,
    "section_keys": [
      "can_graph_neural_networks_learn_to_solve_maxsat_pr",
      "introduction"
    ]
  },
  "08d3123d0d72b2e7090c85f5894a4d99": {
    "file_path": "converted_papers/2305.16373_DeepGate2_Functionality-Aware_Circuit_Representati.md",
    "title": "DeepGate2 Functionality Aware Circuit Representati",
    "file_hash": "95381edae411ffbc28e74b3bd3df7a6a",
    "parsed_at": "2025-12-08T15:13:54.070584",
    "section_count": 4,
    "section_keys": [
      "deepgate2_functionality_aware_circuit_representati",
      "introduction",
      "c_comparison_with_other_models_on_logic_equivalence_gates_identification_this_section_compares_the_functionality-_aware_accuracy_as_defined_in_section_iv-_a2_of_deep_gate2_with_that_of_two_other_models_deep_gate_7_and_fgnn_8_the_deep_gate_7_model_treats_the_logic_probability_as_supervision_since_it_contains_the_statistical_information_of_truth_table_the_fgnn_8_is_trained_to_differentiate_between_logic_equivalent_and_inequivalent_circuits_using_contrastive_learning_table_ii_presents_the_performance_of_three_models_on_the_task_of_logic_equivalence_gates_identification_firstly_our_proposed_approach_outperforms_the_other_two_models_on_all_circuits_with_an_average_f1-_score_of_09434_while_deep_gate_and_fgnn_only_achieve_f1-_score_06778_and_04402_respectively_for_instance_in_circuit_d7_our_proposed_functionality-_aware_circuit_learning_approach_achieves_an_f1-_score_of_09831_and_accurately_identifies_9915_of_logic_equivalence_gate_pairs_with_a_precision_of_9748_indicating_a_low_false_positive_rate_in_contrast_deep_gate_only_achieves_an_f1-_score_of_06778_while_fgnn_fails_on_most_of_the_pairs_secondly_although_deep_gate_has_an_average_recall_of_9146_its_precision_is_only_5400_indicating_a_large_number_of_false_positive_identifications_this_is_because_deep_gate_can_only_identify_logic_equivalent_pairs_by_predicting_logic_probability_which_leads_to_incorrect_identification_of_gate_pairs_with_similar_logic_probability_according_to_our_further_experiment_in_8083_of_false_positive_pairs_the_model_incorrectly_identifies_gate_pairs_with_similar_logic_probability_as_functionally_equivalent_thirdly_fgnn_achieves_the_lowest_performance_among_the_other_models_with_only_04402_f1-_score_the_poor_performance_of_fgnn_is_attributed_to_the_lack_of_effective_supervision_while_fgcn_learns_to_identify_logic_equivalence_circuits_generated_by_perturbing_local_structures_slightly_the_model_tends_to_consider_circuits_with_similar_structures_to_have_the_same_functionality_however_in_the_validation_dataset_and_practical_applications_two_circuits_may_have_the_same_function_even_if_their_topological_structures_are_extremely_different_therefore_the_self-_supervised_approach_limits_the_effectiveness_of_fgnn_in_identifying_logic_equivalence_gates_d_effectiveness_of_pi_encoding_strategy_to_demonstrate_the_effectiveness_of_our_proposed_pi_encoding_pie_strategy_we_trained_another_model_without_assigning_unique_identifications_for_pis_which_we_refer_to_as_wo_pie_the_results_are_presented_in_table_iii_which_show_that_disabling_the_pie_reduces_the_f1-_score_of_identifying_logic_equivalence_gates_from_09434_to_07541_resulting_in_an_average_reduction_of_2007_such_reduction_can_be_attributed_to_the_fact_that_as_demonstrated_as_the_failure_case_in_section_ii_and_fig_1_the_one-_round_gnn_model_without_the_pie_strategy_cannot_model_the_structural_information_of_the_circuit_more_specifically_the_accuracy_of_the_reconvergence_structure_identification_task_with_w_pie_model_is_9322_while_the_wo_model_only_achieve_7456_the_functionality_of_logic_gate_is_affected_by_both_functionality_of_fan-_in_gates_and_whether_there_is_reconvergence_between_its_fan-_in_gates_once_the_reconvergence_structure_cannot_be_accurately_identified_node_functionality_cannot_be_modeled_accurately_e_effectiveness_of_training_strategies_to_investigate_the_effectiveness_of_our_multi-_stage_training_strategy_we_train_another_model_noted_as_wo_multi-_stage_model_with_all_loss_functions_in_only_one_stage_instead_of_adding_the_functionality-_aware_loss_function_in_the_second_stage_the_original_model_with_multiple_stages_training_strategy_is_noted_as_w_multi-_stage_model_the_w_multi-_stage_model_learn_to_predict_the_logic_probability_and_structural_correlation_in_the_first_stage_and_learn_the_more_difficult_task_which_predicts_the_functionality_in_the_second_stage_the_results_are_shown_in_table_iv_where_the_model_w_multi-_stage_achieves_an_f1-_score_of_09434_on_average_and_the_model_wo_multi-_stage_achieves_only_07137_we_analyze_the_reason_as_follows_the_cost_of_comparing_each_pair_of_logic_gates_in_the_task_of_learning_functionality_is_extremely_high_which_is_proportional_to_the_square_of_the_circuit_size_we_limit_the_dataset_and_train_the_model_to_learn_functional_similarity_only_among_pairs_with_similar_logic_probability_which_is_a_necessary_condition_for_functional_equivalence_therefore_without_the_staged_multi-_stage_strategy_be_effectively_supervised_with_the_simplified_dataset_leading_to_poor_performance_in_learning_functionality_as_shown_in_table_v_the_differences_between_the_two_models_in_the_loss_values_for_predicting_logic_probability_l_prob_and_identifying_reconvergence_structures_l_rc_are_not_significant_indicating_that_they_perform_similarly_in_these_two_tasks_however_compared_to_the_wo_multi-_stage_model_the_w_multi-_stage_model_performs_better_in_learning_functionality_with_l_func_00594_which_is_5147_smaller_than_that_of_wo_multi-_stage_model_however_the_w_multi-_stage_model_outperforms_the_model_wo_multi-_stage_in_learning_functionality_task_with_a_significantly_lower_l_func_value_of_00594_which_is_5147_smaller_than_that_of_the_latter_v_downstream_tasks_in_this_section_we_combine_our_deep_gate2_with_the_open-_source_eda_tools_and_apply_our_model_to_practical_eda_tasks_logic_synthesis_and_boolean_satisfiability_sat_solving_the_logic_synthesis_tools_aim_to_identify_logic_equivalence_gates_as_quickly_as_possible_in_section_v-_a_our_proposed_functionality-_aware_circuit_learning_model_provides_guidance_to_the_logic_synthesis_tool_about_the_logic_similarity_additionally_in_section_v-_b_we_apply_the_learnt_functional_similarity_in_sat_solving_where_the_variables_with_dissimilar_functionality_are_assigned_the_same_decision_value_this_approach_efficiently_shrinks_the_search_space_by_enabling_solvers_to_encounter_more_constraints_a_logic_synthesis_this_subsection_shows_the_effectiveness_of_our_proposed_functionality-_aware_circuit_learning_framework_in_sat-_sweeping_28_a_common_technique_of_logic_synthesis_fig_5_illustrates_the_components_of_a_typical_ecosystem_for_sat-_sweeping_engine_also_called_sat_sweeper_where_including_equivalence_class_ec_manager_sat-_sweeping_manager_simulator_and_sat_solver_all_computations_are_coordinated_by_the_sat-_sweeping_manager_29_the_sat_sweeper_starts_by_computing_candidate_ecs_using_several_rounds_of_initial_simulation_and_storing_ecs_into_ec_manager_in_the_next_step_the_sat-_sweeping_manager_selects_two_gates_within_an_ec_and_then_calls_the_sat_solver_to_check_whether_they_are_equivalent_if_so_the_ec_manager_merges_these_two_gates_otherwise_sat_solver_will_return_a_satisfiable_assignment_as_a_counterexample_for_incremental_simulation_to_refine_the_candidate_ecs_to_the_best_of_our_knowledge_most_sat-_sweeping_managers_select_ec_only_based_on_the_circuit_structure_without_efficient_heuristic_strategy_considering_the_functionality_of_candidate_gates_we_will_introduce_the_functional_information_into_sat-_sweeping_manager_to_further_improve_efficiency_1_experiment_settings_we_combine_our_deep_gate2_into_sat_sweeper_to_guide_ec_selection_to_be_specific_the_updated_manager_sorts_all_candidate_equivalence_classes_by_computing_the_cosine_similarity_of_their_embeddings_unlike_traditional_sat_sweepers_our",
      "references_1_j_chen_j_kuang_g_zhao_d_j-_h_huang_and_e_f_young_pros_a_plug-_in_for_routability_optimization_applied_in_the_state-_of-_the-_art_commercial_eda_tool_using_deep_learning_in_proceedings_of_the_39th_international_conference_on_computer-_aided_design_2020_pp_1-_8_2_a_mirhoseini_a_goldie_m_yazgan_j_w_jiang_e_songhori_s_wang_y-_j_lee_e_johnson_o_pathak_a_nazi_et_al_a_graph_placement_methodology_for_fast_chip_design_nature_vol_594_no_7862_pp_207-_212_2021_3_w_l_neto_m_austin_s_temple_l_amaru_x_tang_and_p-_e_gaillardon_losaic_a_logic_synthesis_framework_driven_by_artificial_intelligence_in_2019_ieeeacm_international_conference_on_computer-_aided_design_iccad_ieee_2019_pp_1-_6_4_w_haaswijk_e_collins_b_seguin_m_soeken_f_kaplan_s_susstrunk_and_g_de_micheli_deep_learning_for_logic_optimization_algorithms_in_2018_ieee_international_symposium_on_circuits_and_systems_iscas_ieee_2018_pp_1-_4_5_z_shi_m_li_s_khan_l_wang_n_wang_y_huang_and_q_xu_deeptpi_test_point_insertion_with_deep_reinforcement_learning_in_2022_ieee_international_test_conference_itc_ieee_2022_pp_194-_203_6_j_huang_h-_l_zhen_n_wang_h_mao_m_yuan_and_y_huang_neural_fault_analysis_for_sat-_based_atpg_in_2022_ieee_international_test_conference_itc_ieee_2022_pp_36-_45_7_m_li_s_khan_z_shi_n_wang_h_yu_and_q_xu_deepgate_learning_neural_representations_of_logic_gates_in_proceedings_of_the_59th_acmieee_design_automation_conference_2022_pp_667-_672_8_z_wang_c_bai_z_he_g_zhang_q_xu_t-_y_ho_b_yu_and_y_huang_functionality_matters_in_netlist_representation_learning_in_proceedings_of_the_59th_acmieee_design_automation_conference_2022_pp_61-_66_9_k_zhu_h_chen_w_j_turner_g_f_kokai_p-_h_wei_d_z_pan_and_h_ren_tag_learning_circuit_spatial_embedding_from_layouts_in_proceedings_of_the_41st_ieeeacm_international_conference_on_computer-_aided_design_2022_pp_1-_9_10_y_lai_y_mu_and_p_luo_maskplace_fast_chip_placement_via_reinforced_visual_representation_learning_ar_xiv_preprint_ar_xiv221113382_2022_11_a_fayyazi_s_shababi_p_nuzzo_s_nazarian_and_m_pedram_deep_learning-_based_circuit_recognition_using_sparse_mapping_and_level-_dependent_decaying_sum_circuit_representations_in_2019_design_automation_test_in_europe_conference_exhibition_date_ieee_2019_pp_638-_641_12_z_he_z_wang_c_bail_h_yang_and_b_yu_graph_learning-_based_arithmetic_block_identification_in_2021_ieeeacm_international_conference_on_computer_aided_design_iccad_ieee_2021_pp_1-_8_13_m_li_z_shi_q_lai_s_khan_and_q_xu_deepsat_an_eda-_driven_learning_framework_for_sat_ar_xiv_preprint_ar_xiv220513745_2022_14_a_mishchenko_s_chatterjee_r_jiang_and_r_k_brayton_fraigs_a_unifying_representation_for_logic_synthesis_and_verification_erl_technical_report_tech_rep_2005_15_s_d_queue_cadical_at_the_sat_race_2019_sat_race_2019_p_8_2019_16_t_brown_b_mann_n_ryder_m_subbiah_j_d_kaplan_p_dhariwal_a_neelakantan_p_shyam_g_sastry_a_askell_et_al_language_models_are_few-_shot_learners_advances_in_neural_information_processing_systems_vol_33_pp_1877-_1901_2020_17_j_devlin_m-_w_chang_k_lee_and_k_toutanova_bert_pre-_training_of_deep_bidirectional_transformers_for_language_understanding_ar_xiv_preprint_ar_xiv181004805_2018_18_z_wu_y_xiong_s_x_yu_and_d_lin_unsupervised_feature_learning_via_non-_parametric_instance_discrimination_in_proceedings_of_the_ieee_conference_on_computer_vision_and_pattern_recognition_2018_pp_3733-_3742_19_e_hoffer_and_n_ailon_deep_metric_learning_using_triplet_network_in_similarity-_based_pattern_recognition_third_international_workshop_simbad_2015_copenhagen_denmark_october_12-_14_2015_proceedings_3_springer_2015_pp_84-_92_20_a_vaswani_n_shazeer_n_parmar_j_uszkoreit_l_jones_a_n_gomez_l_kaiser_and_i_polosukhin_attention_is_all_you_need_advances_in_neural_information_processing_systems_vol_30_2017_21_y_bengio_j_louradour_r_collobert_and_j_weston_curriculum_learning_in_proceedings_of_the_26th_annual_international_conference_on_machine_learning_2009_pp_41-_48_22_s_ruder_an_overview_of_multi-_task_learning_in_deep_neural_networks_ar_xiv_preprint_ar_xiv170605098_2017_23_s_davidson_characteristics_of_the_itc99_benchmark_circuits_in_itsw_1999_24_c_albrecht_iwls_2005_benchmarks_in_iwls_2005_25_l_amar\u00fa_p-_e_gaillardon_and_g_de_micheli_the_epfl_combinational_benchmark_suite_in_iwls_no_conf_2015_26_o_team_opencores_httpsopencoresorg_27_d_p_kingma_and_j_ba_adam_a_method_for_stochastic_optimization_ar_xiv_preprint_ar_xiv14126980_2014_28_a_kuehlmann_v_paruthi_f_krohm_and_m_k_ganai_robust_boolean_reasoning_for_equivalence_checking_and_functional_property_verification_ieee_transactions_on_computer-_aided_design_of_integrated_circuits_and_systems_vol_21_no_12_pp_1377-_1394_2002_29_a_mishchenko_and_r_brayton_integrating_an_aig_package_simulator_and_sat_solver_in_international_workshop_on_logic_and_synthesis_iwls_2018_pp_11-_16_30_b_l_synthesis_and_v_group_abc_a_system_for_sequential_synthesis_and_verification_httpwwwcadeecsberkeleyedualanmiabc_2023_31_e_i_goldberg_m_r_prasad_and_r_k_brayton_using_sat_for_combinational_equivalence_checking_in_proceedings_design_automation_and_test_in_europe_conference_and_exhibition_2001_ieee_2001_pp_114-_121_32_k_l_mc_millan_interpolation_and_sat-_based_model_checking_in_computer_aided_verification_15th_international_conference_cav_2003_boulder_co_usa_july_8-_12_2003_proceedings_15_springer_2003_pp_1-_13_33_k_yang_k-_t_cheng_and_l-_c_wang_trangen_a_sat-_based_atpg_for_path-_oriented_transition_faults_in_asp-_dac_2004_asia_and_south_pacific_design_automation_conference_2004_ieee_cat_no_04ex753_ieee_2004_pp_92-_97_34_f_lu_l-_c_wang_k-_t_cheng_and_r-_y_huang_a_circuit_sat_solver_with_signal_correlation_guided_learning_in_2003_design_automation_and_test_in_europe_conference_and_exhibition_ieee_2003_pp_892-_897_35_g_audemard_and_l_simon_glucose_a_solver_that_predicts_learnt_clauses_quality_sat_competition_pp_7-_8_2009_36_on_the_glucose_sat_solver_international_journal_on_artificial_intelligence_tools_vol_27_no_01_p_1840001_2018_37_g_s_tseitin_on_the_complexity_of_derivation_in_propositional_calculus_automation_of_reasoning_2_classical_papers_on_computational_logic_1967-_1970_pp_466-_483_1983_38_j_marques-_silva_i_lynce_and_s_malik_conflict-_driven_clause_learning_sat_solvers_in_handbook_of_satisfiability_ios_press_2021_pp_133-_182"
    ]
  },
  "9d9cf73ce3ea598240dec3d0490f78a4": {
    "file_path": "converted_papers/2505.16053_Learning_from_Algorithm_Feedback_One-Shot_SAT_Solv.md",
    "title": "Learning from Algorithm Feedback One Shot SAT Solv",
    "file_hash": "b7fc41bcfe655b0a10f31b7850477ae1",
    "parsed_at": "2025-12-08T15:13:54.072295",
    "section_count": 9,
    "section_keys": [
      "learning_from_algorithm_feedback_one_shot_sat_solv",
      "introduction",
      "references_audemard_g_and_simon_l_2009_predicting_learnt_clauses_quality_in_modern_sat_solvers_in_ijcai_volume_9_pages_399-_404_citeseer_audemard_g_and_simon_l_2017_glucose_and_syrup_in_the_sat17_proceedings_of_sat_competition_pages_16-_17_biere_a_faller_t_fazekas_k_fleury_m_froleyks_n_and_pollitt_f_2024_ca_di_ca_l_20_in_gurfinkel_a_and_ganesh_v_editors_computer_aided_verification_-_36th_international_conference_cav_2024_montreal_qc_canada_july_24-_27_2024_proceedings_part_i_volume_14681_of_lecture_notes_in_computer_science_pages_133-_152_springer_biere_a_heule_m_and_van_maaren_h_editors_2021_handbook_of_satisfiability_volume_185_ios_press_2nd_edition_cameron_c_hartford_j_lundy_t_truong_t_milligan_a_chen_r_and_leyton-_brown_k_2024_unsat_solver_synthesis_via_monte_carlo_forest_search_in_international_conference_on_the_integration_of_constraint_programming_artificial_intelligence_and_operations_research_pages_170-_189_springer_cappart_q_chetelat_d_khalil_e_b_lodi_a_morris_c_and_veli\u010dkovi\u0107_p_2021_combinatorial_optimization_and_reasoning_with_graph_neural_networks_in_zhou_z-_h_editor_proceedings_of_the_thirtieth_international_joint_conference_on_artificial_intelligence_ijcai-_21_pages_4348-_4355_international_joint_conferences_on_artificial_intelligence_organization_survey_track_christiano_p_f_leike_j_brown_t_martic_m_legg_s_and_amodei_d_2017_deep_reinforcement_learning_from_human_preferences_in_guyon_i_luxburg_u_v_bengio_s_wallach_h_fergus_r_vishwanathan_s_and_garnett_r_editors_advances_in_neural_information_processing_systems_volume_30_curran_associates_inc_courtois_n_oneil_s_and_quisquater_j-_j_2009_practical_algebraic_attacks_on_the_hitag2_stream_cipher_volume_5735_pages_167-_176_crawford_j_m_and_auton_l_d_1996_experimental_results_on_the_crossover_point_in_random_3-_sat_artificial_intelligence_811-_231-_57_e\u00e9n_n_and_s\u00f6rensson_n_2003_an_extensible_sat-_solver_in_international_conference_on_theory_and_applications_of_satisfiability_testing_pages_502-_518_springer_guo_d_yang_d_zhang_h_song_j_zhang_r_xu_r_zhu_q_ma_s_wang_p_bi_x_et_al_2025_deepseek-_r1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning_ar_xiv_preprint_ar_xiv250112948_han_j_m_2020_learning_cubing_heuristics_for_sat_from_drat_proofs_heule_m_dufour_m_van_zwieten_j_and_van_maaren_h_2005_march_eq_implementing_additional_reasoning_into_an_efficient_look-_ahead_sat_solver_in_theory_and_applications_of_satisfiability_testing_7th_international_conference_sat_2004_vancouver_bc_canada_may_10-_13_2004_revised_selected_papers_7_pages_345-_359_springer_heule_m_j_and_van_maaren_h_2006_march_dl_adding_adaptive_heuristics_and_a_new_branching_strategy_journal_on_satisfiability_boolean_modelling_and_computation_21-_447-_59_khalil_e_dai_h_zhang_y_dilkina_b_and_song_l_2017_learning_combinatorial_optimization_algorithms_over_graphs_advances_in_neural_information_processing_systems_30_khalil_e_b_morris_c_and_lodi_a_2022_mip-_gnn_a_data-_driven_framework_for_guiding_combinatorial_solvers_in_proceedings_of_the_aaai_conference_on_artificial_intelligence_volume_36_pages_10219-_10227_kullmann_o_2021_fundaments_of_branching_heuristics_in_handbook_of_satisfiability_pages_351-_390_ios_press",
      "a_method_details_a1_graph_representation_and_architecture_we_represent_a_formula_phi_as_a_graph_gphi_vphiephi_this_is_a_standard_literal-_clause_graph_used_in_prior_work_such_as_neuro_sat_selsam_et_al_2019_formally_the_vertices_of_this_graph_vphi_mathrmlitphi_cup_mathrmclsphi_are_the_literals_and_clauses_of_phi_the_edges_ephi_e_lcphi_cup_e_llphi_connect_literals_with_the_clauses_they_occur_in_and_with_the_opposing_literal_of_the_same_variable_beginarrayr_l_e_l_lphi_xneg_xmid_xin_mathcalxphi_e_l_cphi_bigcup_cin_mathrmclsphicellmid_ell_in_c_endarray_quad_12_a2_message-passing_neural_network_for_each_vertex_vin_operatorname_litphicup_operatorname_clsphi_we_obtain_an_initial_embedding_h0vin_mathbbrd_h0v_mathbfenclog_deg_v_1_quad_13_here_d_is_the_latent_embedding_dimension_of_the_model_and_enc_is_a_trainable_2-_layer_mlp_that_is_applied_to_the_log-_normalized_degree_of_v_the_gnn_model_then_stacks_lin_mathbbn_message_passing_layers_for_tin_1ldots_l_the_t_-_th_layer_takes_as_input_the_previous_embedding_ht_-_1_and_outputs_a_refined_embedding_ht_by_performing_a_message_pass_this_message_pass_is_split_into_two_phases_first_each_clause_cin_mathrmcls_aggregates_information_from_its_associated_literals_ht_1c_mathbfu_mathrmclslefthtcbigoplus_ell_in_chellellright_quad_14_here_mathbfu_mathrmcls_is_a_trainable_mlp_and_bigoplus_is_an_order-_invariant_aggregation_throughout_all_experiments_we_use_element-_wise_mean_for_aggregation_in_the_second_phase_each_literal_ell_in_operatorname_lit_aggregates_the_updated_embeddings_from_the_clauses_it_occurs_in_ht_1ell_mathbfu_mathrmlitlefthellellhell-ellbigoplus_cell_in_cht_1cright_quad_15_here_mathbfu_mathrmlit_is_another_trainable_mlp_that_additionally_also_takes_the_embedding_of_the_opposing_literal_-_ell_as_input_this_model_architecture_is_conceptually_similar_to_that_of_neuro_sat_one_major_difference_is_that_we_use_a_more_standard_fixed-_depth_feed-_forward_gnn_instead_of_a_recurrent_model_note_that_all_mlps_used_in_our_model_have_two_layers_and_the_hidden_layer_is_always_si_lu-_activated_and_has_hidden_dimension_2d_the_final_output_is_a_variable_embedding_ymathrmvarphito_mathbbr2_which_is_obtained_by_concatenating_the_two_literal_embeddings_associated_with_each_variable_x_and_then_applying_a_final_2-_layer_mlp_dec_yx_mathbfdechlxhlneg_x_quad_16_note_that_we_choose_dec_as_a_2-_layer_mlp_with_input_dimension_2d_hidden_dimension_2d_and_output_dimension_2_no_activation_is_applied_to_the_output_and_the_weights_and_biases_of_the_final_layer_of_dec_are_initialized_as_zeros_this_ensures_that_at_the_beginning_of_training_the_initial_gnn_n_theta_0_assigns_mu_x_0_and_rho_x_0_to_all_variables_we_found_this_to_be_a_stable_configuration_for_initializing_training_in_particular_mu_x_0_ensures_that_the_log-_normally_distributed_weight_policy_pi_theta_0wx_has_a_mode_of_approximately_1_for_all_variables_while_rho_x_0_ensures_that_the_polarity_of_each_variable_is_initially_distributed_uniformly",
      "a3_sat_solver_details_glucose_glucose_audemard_and_simon_2009_is_a_popular_cdcl_solver_based_on_minisat_e\u00e9n_and_s\u00f6rensson_2003_our_modification_is_based_on_glucose_421_audemard_and_simon_2017_3_like_many_other_cdcl_solvers_glucose_uses_the_variable_state_independent_decaying_sum_vsids_heuristic_for_branching_each_variable_x_is_assigned_an_activity_score_activity_x_that_reflects_its_involvement_in_conflicts_when_a_conflict_occurs_the_activity_scores_of_variables_involved_are_increased_by_a_constant_delta_ie_mathrmactivityxleftarrow_mathrmactivityx_delta_quad_17_periodically_all_activity_scores_are_multiplied_by_a_decay_factor_beta_where_0_beta_1_mathrmactivityxleftarrow_beta_cdot_mathrmactivityx_quad_18_the_activity_then_effectively_serves_as_the_score_function_from_algorithm_2_note_that_in_practice_cdcl_solvers_commonly_use_exponential_vsids_evsids_which_is_a_variation_that_yields_identical_decisions_but_avoids_a_costly_loop_over_all_variables_to_compute_equation_18_rather_than_decaying_the_activity_the_increment_delta_is_instead_scaled_up_delta_leftarrow_frac1betadelta_quad_19_the_cumulative_values_of_the_activity_scores_then_yield_the_same_decisions_to_incorporate_our_variable_weights_w_into_this_process_we_simply_modify_equation_17_by_scaling_the_increment_with_the_variable_weight_mathrmactivityxleftarrow_mathrmactivityx_wxcdot_delta_quad_20_this_ensures_that_the_total_activity_score_of_each_variable_is_scaled_by_a_factor_of_wx_at_each_step_of_the_search_while_still_preventing_loops_over_all_variables_we_found_that_the_runtime_overhead_of_the_additional_multiplication_in_equation_20_is_negligible_we_use_the_provided_polarities_px_to_initialize_the_polarity_or_phase_of_each_variable_note_that_we_leave_phase_saving_on_so_this_initial_polarity_may_be_overwritten_by_the_solver_in_later_search_steps_we_run_all_experiments_without_randomized_decisions_rnd-_freq_0_we_further_set_the_parameter_k_01_to_minimize_solver_restarts_which_we_found_to_improve_performance_on_the_three_instance_distributions_considered_in_our_experiments_apart_from_this_we_use_the_default_parameters_of_glucose_march_march_heule_et_al_2005_heule_and_van_maaren_2006_is_a_dpll-_based_solver_that_uses_a_branching_heuristic_based_on_look-_ahead_biere_et_al_2021_4_it_is_among_the_best-_known_solvers_for_purely_random_sat_instances_look-_ahead_branching_heuristics_estimate_how_each_variables_selection_as_a_branching_variable_would_affect_the_instance_in_march_the_scoring_function_scorex_essentially_quantifies_how_many_new_binary_clauses_would_occur_if_x_is_picked_for_branching_in_the_current_search_step_computing_this_score_is_relatively_expensive_when_compared_to_activity-_based_approaches_and_look-_ahead_solvers_usually_make_fewer_decisions_per_time_to_decrease_the_cost_of_each_branching_step_march_first_applies_a_pre-_selection_step_before_each_branching_decision_where_a_reduced_set_of_candidate_variables_is_selected_according_to_a_second_scoring_function_score-_preselect_x_this_score_aims_to_approximate_the_expected_look-_ahead_score_but_is_cheaper_to_compute_in_the_modified_solver_we_also_apply_the_variable_weight_w_in_pre-_selection_ie_the_weighted_scores_wx_cdot_textscore-_preselect_x_are_used_to_select_the_candidate_variables_the_ratio_of_pre-_selected_candidates_is_fixed_at_10_the_same_weights_w_are_then_applied_again_to_the_actual_look-_ahead_scores_to_obtain_the_branching_variable_afterwards_we_use_the_given_polarities_p_in_each_branching_to_determine_the_sign_of_the_branching_literal_aside_from_these_changes_we_run_march_in_its_default_configuration",
      "b_experiment_details_b1_data_table_2_provides_full_dataset_statistics_for_all_data_distributions_and_splits_in_the_following_we_provide_further_details_on_how_each_instance_distribution_is_generated_random_3sat_uniformly_random_3sat_instances_are_commonly_used_to_benchmark_sat_solvers_here_each_clause_is_sampled_by_choosing_three_distinct_variables_uniformly_at_random_and_negating_each_with_a_probability_of_50_hard_instances_are_known_to_occur_when_the_number_of_clauses_is_around_m_4258n_5826n-frac23_where_n_is_the_number_of_variables_crawford_and_auton_1996_this_is_approximately_the_critical_density_where_the_instances_transition_from_sat_to_unsat_we_define_3mathrmsatn_as_the_distribution_of_uniformly_random_3sat_instances_with_n_variables_and_left4258n_5826n-frac23right_clauses_for_training_we_use_20k_instances_sampled_from_3sat200_which_are_filtered_such_that_exactly_10k_instances_are_sat_and_unsat_respectively_our_test_sets_contain_larger_instances_with_nin_300350400_where_we_sample_200_instances_for_each_size_n_graph_coloring_combinatorial_problems_on_graphs_are_commonly_solved_by_reducing_them_to_boolean_sat_instances_here_we_consider_the_problem_of_finding_a_3-_coloring_for_erd\u0151s-_r\u00e9nyi_graphs_we_define_3mathrmcoln_as_the_distribution_of_sat_problems_that_are_obtained_by_sampling_an_erd\u0151s-_r\u00e9nyi_graph_with_n_vertices_and_then_encoding_the_problem_of_deciding_3-_colorability_as_a_sat_instance_we_set_the_edge_probability_such_that_the_expected_vertex_degree_is_467_which_is_approximately_the_critical_density_for_3-_colorability_where_hard_instances_commonly_occur_zdeborov\u00e1_and_kr\u017eakala_2007_we_train_on_20k_instances_sampled_from_3mathrmcol300_again_these_are_filtered_such_that_exactly_10k_instances_are_sat_and_unsat_respectively_our_test_sets_consist_of_larger_problems_with_nin_400500600_cryptographic_hard_structured_sat_problems_commonly_arise_in_the_context_of_cryptoanalysis_for_example_for_sat-_based_decryption_attacks_soos_et_al_2009_to_generate_data_in_this_domain_we_use_grain-_of-_salt_soos_2010_to_generate_sat_instances_for_decrypting_stream_ciphers_we_define_crypto_n_as_the_distribution_of_sat_instances_generated_for_decrypting_the_hi_tag2_cipher_courtois_et_al_2009_with_n_given_help_bits_we_use_the_recommended_generation_parameters_-_outputs_56_-_base-_shift_8_-_karnaugh_8_note_that_these_instances_are_harder_for_smaller_values_of_n_and_are_mostly_unsat_we_train_on_20k_instances_from_crypto22_and_test_on_harder_problems_with_nin_201510_for_each_of_these_three_instance_classes_we_formally_define_the_corresponding_training_distribution_omega_from_equation_6_as_the_uniform_distribution_over_the_set_of_training_instances_table_2_dataset_statistics_tabletrtd_rowspan2distributiontdtd_rowspan2splittdtd_rowspan2numbertdtd_rowspan2sattdtd_rowspan2unsattdtd_colspan3var\u03c6tdtd_colspan3cls\u03c6tdtrtrtdmeantdtdmintdtdmaxtdtdmeantdtdmintdtdmaxtdtrtrtd0tdtd3sat200tdtdtraintdtd20000tdtd10000tdtd10000tdtd20000tdtd200tdtd200tdtd85300tdtd853tdtrtrtd1tdtd3sat200tdtdvaltdtd200tdtd100tdtd100tdtd20000tdtd200tdtd200tdtd85300tdtd853tdtrtrtd2tdtd3sat300tdtdtesttdtd200tdtd103tdtd97tdtd30000tdtd300tdtd300tdtd127800tdtd1278tdtrtrtd3tdtd3sat350tdtdtesttdtd200tdtd108tdtd92tdtd35000tdtd350tdtd350tdtd149100tdtd1491tdtrtrtd4tdtd3sat400tdtdtesttdtd200tdtd89tdtd111tdtd40000tdtd400tdtd400tdtd170400tdtd1704tdtrtrtd5tdtd3col300tdtdtraintdtd20000tdtd10000tdtd10000tdtd90000tdtd900tdtd900tdtd328475tdtd3009tdtrtrtd6tdtd3col300tdtdvaltdtd200tdtd100tdtd100tdtd90000tdtd900tdtd900tdtd328863tdtd3042tdtrtrtd7tdtd3col400tdtdtesttdtd200tdtd77tdtd123tdtd120000tdtd1200tdtd1200tdtd439231tdtd4144tdtrtrtd8tdtd3col500tdtdtesttdtd200tdtd91tdtd108tdtd150000tdtd1500tdtd1500tdtd548856tdtd5216tdtrtrtd9tdtd3col600tdtdtesttdtd200tdtd87tdtd113tdtd180000tdtd1800tdtd1800tdtd659724tdtd6306tdtrtrtd10tdtdcrypto22tdtdtraintdtd20000tdtd0tdtd20000tdtd52941tdtd518tdtd544tdtd842071tdtd7669tdtrtrtd11tdtdcrypto22tdtdvaltdtd200tdtd0tdtd200tdtd52924tdtd523tdtd537tdtd841341tdtd7937tdtrtrtd12tdtdcrypto20tdtdtesttdtd100tdtd0tdtd100tdtd53343tdtd526tdtd544tdtd876757tdtd8182tdtrtrtd13tdtdcrypto15tdtdtesttdtd100tdtd0tdtd100tdtd54289tdtd537tdtd552tdtd962204tdtd9129tdtrtrtd14tdtdcrypto10tdtdtesttdtd100tdtd0tdtd100tdtd55099tdtd544tdtd568tdtd1049763tdtd9947tdtrtable",
      "b2_hyperparameters_table_3_provides_an_overview_of_all_rlaf_training_runs_from_our_main_experiments_we_tuned_the_learning_rate_in_eta_in_00001_000005_000001_and_schedule_it_to_warm_up_over_the_first_5_grpo_iterations_after_warm_up_the_the_learning_rate_stays_constant_throughout_training_the_clip_ratio_was_tuned_in_epsilon_in_01_02_and_the_kl-_penalty_beta_in_01_10_all_other_hyperparameters_were_given_constant_default_values_which_we_found_to_be_stable_based_on_preliminary_experiments_table_3_hyperparameters_tabletrtdtdtd3sattdtdglucosebr3coltdtdcryptotdtd3sattdtdmarchbr3coltdtdcryptotdtrtrtdktdtd2000tdtd2000tdtd2000tdtd2000tdtd2000tdtd2000tdtrtrtdmtdtd40tdtd40tdtd40tdtd40tdtd40tdtd40tdtrtrtdntdtd100tdtd100tdtd100tdtd100tdtd100tdtd100tdtrtrtdstdtd50tdtd50tdtd50tdtd50tdtd50tdtd50tdtrtrtd\u03c3wtdtd01tdtd01tdtd01tdtd01tdtd01tdtd01tdtrtrtdclip_ratio_\u03b5tdtd02tdtd02tdtd02tdtd01tdtd02tdtd02tdtrtrtdkl-penalty_\u03b2tdtd01tdtd10tdtd01tdtd10tdtd01tdtd01tdtrtrtdbatch_sizetdtd20tdtd20tdtd20tdtd20tdtd20tdtd20tdtrtrtdlearning_rate_\u03b7tdtd00001tdtd000005tdtd000005tdtd000005tdtd000001tdtd00001tdtrtrtdweight_decaytdtd00tdtd00tdtd00tdtd00tdtd00tdtd00tdtrtrtdhidden_dim_dtdtd256tdtd256tdtd256tdtd256tdtd256tdtd256tdtrtrtdmodel_depth_ltdtd10tdtd10tdtd10tdtd10tdtd10tdtd10tdtrtable_b3_training_figure_6_provides_the_learning_curves_for_the_6_rlaf-_trained_models_in_our_main_experiments_for_all_models_the_cost_decreases_throughout_training_we_found_that_training_with_the_march_base_solver_tends_to_yield_noisier_training_particularly_on_3sat_instances_where_the_policy_does_not_improve_further_after_700_grpo_iterations_exploring_effective_strategies_for_reducing_this_noise_remains_future_work_nonetheless_we_are_able_to_learn_guidance_policies_that_decrease_the_solver_cost_of_both_base_solvers_on_all_three_problem_instances_centerfigure_6_grpo_training_curves_of_the_rlaf_models_from_our_main_experiment_we_plot_the_mean_number_of_decisions_on_the_validation_set_against_the_grpo_iteration_center",
      "b4_supervised_unsat-core_and_backbone_prediction_for_unsat-_core_and_backbone_prediction_in_section_32_we_train_supervised_gnn_models_that_are_identical_in_architecture_and_size_to_the_models_used_for_our_rlaf-_based_policies_the_used_hyperparameters_are_specified_in_table_4_in_the_following_we_provide_a_detailed_description_of_how_these_models_are_trained_and_evaluated_unsat-core_selsam_and_bj\u00f8rner_2019_propose_to_train_supervised_models_that_predict_the_unsat-_core_membership_of_variables_and_then_use_the_model_prediction_to_guide_branching_heuristics_following_their_methodology_we_phrase_the_task_of_predicting_whether_or_not_a_variable_occurs_in_an_unsat-_core_as_a_variable-_level_binary_classification_task_and_train_a_gnn_for_this_problem_in_a_supervised_manner_using_a_standard_cross-_entropy_loss_the_ground-_truth_on_training_and_validation_instances_is_computed_by_extracting_the_cores_from_drat_unsat_proofs_generated_by_the_ca_di_ca_l_biere_et_al_2024_solver_note_that_these_cores_are_not_minimal_as_computing_such_would_not_be_feasible_as_discussed_in_section_32_the_extracted_cores_on_our_unsatisfiable_3sat_training_instances_contain_all_variables_for_almost_all_instances_and_are_therefore_not_a_meaningful_training_target_we_therefore_only_train_unsat-_core_prediction_models_for_3col_and_crypto_we_train_a_separate_model_for_each_distribution_and_restrict_training_to_the_unsatisfiable_instances_note_that_selsam_and_bj\u00f8rner_2019_integrate_their_prediction_by_periodically_resetting_the_vsids_scores_of_the_guided_cdcl-_solver_to_prediction_logits_of_the_gnn_this_requires_careful_tuning_of_the_reset_frequency_it_is_also_specific_to_solvers_based_on_the_vsids_heuristic_and_would_for_example_not_be_applicable_to_the_march_solver_furthermore_in_later_ablation_experiments_selsam_and_bj\u00f8rner_2019_report_that_the_performance_improvement_obtained_with_a_trained_gnn_is_barely_distinguishable_from_when_an_untrained_randomly_initialized_model_is_used_further_questioning_the_effectiveness_of_guiding_solvers_with_this_strategy_to_facilitate_a_direct_and_fair_comparison_with_rlaf-_trained_policies_we_instead_combine_the_unsat-_core_predictions_with_our_own_solver_guidance_based_on_multiplicative_weights_for_a_variable_x_let_p_mathrmcorex_be_the_predicted_probability_of_x_being_in_an_unsat-_core_according_to_the_trained_gnn_model_then_we_transform_these_probabilities_to_variable_weights_through_the_following_transformation_wx_1_alpha_cdot_p_mathrmcorex_quad_21_here_alpha_geq_0_is_a_parameter_that_determines_how_the_variable_weight_scales_with_the_raw_model_predictions_for_this_experiment_we_found_weights_of_wx_geq_1_to_perform_better_hence_the_offset_of_1_in_equation_21_the_value_of_alpha_is_tuned_on_the_corresponding_validation_dataset_in_the_range_10-_4_10-_3_10-_2_10-_1_100_101_102_103_104_we_tune_alpha_separately_for_both_glucose_and_march_the_polarities_are_simply_set_to_px_1_as_the_prediction_of_unsat-_core_membership_has_no_clear_implication_for_the_sign_of_the_branching_literal_using_this_methodology_we_found_that_the_unsat-_core_predictions_can_significantly_accelerate_both_base_solvers_although_by_a_smaller_margin_than_rlaf-_trained_policies_backbone_wang_et_al_2024_suggests_using_the_backbone_membership_of_literals_as_a_supervised_training_target_and_then_setting_variable_polarities_using_the_model_predictions_we_follow_their_methodology_and_train_a_gnn_on_the_literal-_level_binary_classification_task_using_cross-_entropy_loss_as_discussed_in_section_32_we_only_train_a_model_for_the_3sat_instances_and_only_use_the_satisfiable_problems_for_training_the_backbone_of_coloring_problems_is_always_empty_due_to_the_permutation_symmetry_of_the_colors_and_some_distributions_such_as_crypto_predominantly_consist_of_unsat_instances_when_evaluating_we_set_the_polarity_of_a_variable_x_as_px_0_if_p_mathrmbackbone-alpha_p_mathrmbackbonex_and_px_1_otherwise_here_p_mathrmbackboneell_is_the_predicted_probability_of_literal_ell_belonging_to_the_backbone_we_further_assign_variable_weights_wx_under_the_assumption_that_correctly_assigning_backbone_literals_in_early_search_steps_positively_affects_the_runtime_to_this_end_we_apply_the_transformation_from_equation_21_to_the_mean_backbone_probability_overlinep_mathrmbackbonex_05p_mathrmbackbone-alpha_p_mathrmbackbonex_to_obtain_a_weight_for_each_variable_again_we_tune_the_transformation_parameter_alpha_for_both_base_solvers_on_the_validation_set",
      "algorithm_1_dpll_solver_1_input_formula_phi_2_function_solve_phi_3_simplify_formula_4_phi_leftarrow_mathrmunit_-_propagationphi_5_phi_leftarrow_mathrmpure_-_literal_-_eliminationphi_6_7_if_phi_emptyset_return_sat_8_if_emptyset_in_phi_return_unsat_9_10_decide_next_branching_variable_11_ell_leftarrow_mathrmpick_-_literalphi_12_return_mathrmsolvephi_land_ell_vee_mathrmsolvephi_land_neg_ell_13_end_function_algorithm_2_decision_heuristic_1_input_formula_phi_2_function_pick-_literal_phi_3_hatxleftarrow_mathrmargmax_xmathrmscorex_4_return_hatx_if_pick-_sign_hatx_else_neg_hatx_5_end_function_algorithm_3_guided_decision_heuristic_1_input_formula_phi_parameters_mathcalw_w_p_2_function_pick-_literal-_guided_phi_mathcalw_3_hatx_leftarrow_mathrmargmax_x_wx_cdot_mathrmscorex_4_return_hatx_if_phatx_1_else_neg_hatx_5_end_function_figure_1_dpll_sat_solver_and_branching_heuristics_algorithm_1_a_dpll_sat_solver_performs_backtracking_search_to_solve_a_given_cnf_formula_phi_at_each_search_step_the_formula_is_simplified_through_unit_propagation_and_pure_literal_elimination_before_selecting_the_next_branching_literal_algorithm_2_branching_heuristics_are_often_implemented_by_choosing_the_variable_that_maximizes_some_hand-_crafted_scoring_function_algorithm_3_we_propose_to_extend_existing_branching_heuristics_by_incorporating_given_variable_weights_into_the_branching_decisions_that_scale_the_associated_score_of_each_variable_we_additionally_choose_the_sign_of_each_literal_according_to_a_provided_polarity_heuristics_further_related_work_is_proposed_by_han_2020_who_accelerate_cube-_and-_conquer_solvers_with_supervised_learning_liu_et_al_2024_who_suggest_improving_clause_deletion_heuristics_in_cdcl_sat_solvers_with_gnns_and_zhai_and_ge_2025_who_use_rl_to_speed_up_parallelized_divide-_and-_conquer_solvers_2_rlaf-guided_sat_solvers_21_guided_branching_heuristics_we_modify_existing_sat_solvers_to_incorporate_external_variable_weights_into_their_branching_heuristic_let_some_base_sat_solver_be_given_we_assume_that_this_solver_is_a_dpll-_derived_backtracking_search_algorithm_we_further_assume_that_the_branching_heuristic_is_implemented_by_first_selecting_a_variable_hatx_mathrmargmax_x_mathrmscorex_that_maximizes_some_variable_scoring_function_score_before_picking_a_literal_sign_according_to_some_secondary_heuristic_as_illustrated_in_algorithm_2_many_existing_branching_heuristics_such_as_vsids_and_look-_ahead_heuristics_fit_the_generic_algorithm_pattern_while_relying_on_different_definitions_of_variable_scores_note_that_these_scores_usually_depend_on_the_current_partial_assignment_of_the_search_as_well_as_information_extracted_in_previous_search_steps_such_as_encountered_conflicts_we_can_modify_this_decision_heuristic_to_incorporate_additional_variable_weights_w_mathrmvarphi_to_mathbbr_0_for_the_given_input_formula_phi_hatx_mathrmargmax_x_wx_cdot_mathrmscorex_quad_1_these_weights_are_passed_to_the_modified_solver_as_additional_input_and_modulate_its_branching_heuristic_by_scaling_the_variable-_wise_scores_in_this_manner_we_can_inject_prior_knowledge_of_variable_importance_into_the_solvers_branching_decisions_without_sacrificing_its_original_heuristic_naturally_choosing_a_useful_variable_weighting_w_by_hand_is_difficult_instead_our_focus_is_on_learning_to_infer_effective_variable_weights_from_the_input_formulas_structure_using_a_deep_neural_network_in_addition_to_these_weights_we_may_also_specify_a_mapping_p_mathrmvarphi_to_0_1_that_assigns_a_polarity_px_to_each_variable_x_when_x_is_chosen_as_a_decision_variable_the_polarity_determines_which_value_is_assigned_to_x_first_specifying_polarities_for_variables_is_a_common_function_for_modern_sat_solvers_and_well-_chosen_values_can_have_a_significant_impact_on_run_time_especially_on_satisfiable_instances_in_this_work_we_will_infer_variable-_wise_polarities_alongside_the_variable_weights_w_with_a_learned_gnn_model_overall_the_modified_solver_mathrmsolvephi_mathcalw_takes_as_input_a_cnf_formula_phi_as_well_as_a_variable_parameterization_mathcalw_w_p_that_assigns_a_weight_wx_in_mathbbr_0_and_polarity_px_in_0_1_to_each_variable_x_in_mathrmvarphi"
    ]
  },
  "a9cb544ef31b581fdf9860eab55512bf": {
    "file_path": "converted_papers/2309.11452_Using_deep_learning_to_construct_stochastic_local_.md",
    "title": "Using deep learning to construct stochastic local",
    "file_hash": "0bcf12539bb1820fb2726549ae1104ec",
    "parsed_at": "2025-12-08T15:13:54.073141",
    "section_count": 6,
    "section_keys": [
      "using_deep_learning_to_construct_stochastic_local",
      "introduction",
      "5_conclusion_and_future_work_in_this_work_we_have_presented_an_approach_to_use_deep_learning_to_construct_sls-_based_sat_solvers_the_main_contribution_is_that_we_design_the_algorithms_and_loss_functions_in_such_a_way_that_the_training_process_is_guaranteed_by_theoretical_results_to_in_expectation_produce_a_stronger_solver_the_hope_is_that_with_this_approach_we_can_create_solvers_for_sat_and_constraint_optimization_more_generally_that_are_highly_performant_on_for_specific_applications_while_also_being_provably_robust_in_terms_of_their_average_performance_we_have_empirically_investigated_this_promise_on_a_random_3-_sat_dataset_with_promising_results_however_the_practical_potential_of_our_approach_remains_largely_unchartered_as_more_thorough_experiments_on_larger_and_application-_oriented_datasets_more_research_on_suitable_deep_learning_architectures_as_well_as_the_application_to_more_mature_and_sophisticated_sls_solvers_needs_to_be_carried_out_since_the_resulting_solvers_are_currently_far_from_being_competitive_with_state_of_the_art_solvers_we_leave_these_experiments_for_future_research_and_hope_that_the_preliminary_results_presented_in_this_paper_nevertheless_establish_the_promise_of_the_idea_acknowledgments_pb_thanks_mahdi_manesh_for_stimulating_discussions_both_authors_thank_jens_eisert_for_introducing_them_to_the_results_of_the_moser-_tardos_algorithm_and_related_works_which_instilled_the_idea_for_this_work_they_thank_porsche_digital_gmb_h_for_the_possibility_to_work_on_this_research_project_mk_thanks_the_bmbf_hybrid_and_the_bmwk_eni_qm_a_for_their_support_references_1_stephen_a_cook_the_complexity_of_theorem-_proving_procedures_in_proceedings_of_the_third_annual_acm_symposium_on_theory_of_computing_stoc_71_pages_151-_158_new_york_ny_usa_1971_association_for_computing_machinery_2_l_a_levin_problems_of_information_transmission_1973_3_wenxuan_guo_junchi_yan_hui-_ling_zhen_xijun_li_mingxuan_yuan_and_yaohui_jin_machine_learning_methods_in_solving_the_boolean_satisfiability_problem_2022_4_benedikt_bunz_and_matthew_lamm_graph_neural_networks_and_boolean_satisfiability_2017_5_daniel_selsam_matthew_lamm_benedikt_bunz_percy_liang_leonardo_de_moura_and_david_l_dill_learning_a_sat_solver_from_single-_bit_supervision_2019_6_emils_ozolins_karlis_freivalds_andis_draguns_eliza_gaile_ronalds_zakovskis_and_sergejs_kozlovics_goal-_aware_neural_sat_solver_in_2022_international_joint_conference_on_neural_networks_ijcnn_ieee_jul_2022_7_daniel_selsam_and_nikolaj_bjorner_guiding_high-_performance_sat_solvers_with_unsat-_core_predictions_2019_8_sebastian_jaszczur_michal_\u0142uszczyk_and_henryk_michalewski_neural_heuristics_for_sat_solving_2020_9_jesse_michael_han_enhancing_sat_solvers_with_glue_variable_predictions_2020_10_emre_yolcu_and_barnabas_poczos_learning_local_search_heuristics_for_boolean_satisfiability_in_h_wallach_h_larochelle_a_beygelzimer_f_dalch\u00e9-_buc_e_fox_and_r_garnett_editors_advances_in_neural_information_processing_systems_volume_32_curran_associates_inc_2019_11_wenjie_zhang_zeyu_sun_qihao_zhu_ge_li_shaowei_cai_yingfei_xiong_and_lu_zhang_nlocal_sat_boosting_local_search_with_solution_prediction_in_proceedings_of_the_twenty-_ninth_international_joint_conference_on_artificial_intelligence_international_joint_conferences_on_artificial_intelligence_organization_jul_2020_12_robin_a_moser_a_constructive_proof_of_the_lovasz_local_lemma_2008_13_robin_a_moser_and_g\u00e1bor_tardos_a_constructive_proof_of_the_general_lovasz_local_lemma_2009_14_david_g_harris_and_aravind_srinivasan_algorithmic_and_enumerative_aspects_of_the_moser-_tardos_distribution_2017_15_dimitris_achlioptas_fotis_iliopoulos_and_alistair_sinclair_beyond_the_lovasz_local_lemma_point_to_set_correlations_and_their_algorithmic_applications_2020",
      "a_appendix_a1_evaluating_the_loss_in_practice_in_sec24_of_the_main_text_we_motivated_the_goal_to_find_an_oracle_factory_that_minimises_the_total_loss_12_however_in_practice_we_dont_have_access_to_mathcalp_as_is_common_practice_we_instead_estimate_the_loss_using_samples_from_a_training_set_mathcalx_x_iphi_i_i_1n_of_n_instances_that_we_assume_to_be_sampled_independently_from_mathcalp_here_x_i_xij_j_1n_i_describes_a_set_of_unique_n_i_assignments_for_the_problem_instance_phi_i_for_each_assignment_xij_let_eij_denote_the_number_of_clauses_it_violates_for_every_instance_we_can_calculate_ll_loss_by_first_explicitly_constructing_the_dependency_graph_mathcald_f_thetaphiphi_this_is_relatively_simple_in_our_case_because_due_to_the_simple_bernoulli_structure_of_our_output_oracles_we_know_that_the_neighbourhood_of_a_given_clause_node_j_is_simply_the_set_of_clause_nodes_whose_corresponding_variable_sets_intersect_with_that_of_c_j_ie_gamma_jcoloneqq_jprimein_mjprimeneq_jvc_jprimecap_vc_jneq_emptyset_given_a_batch_bsubset_mathcalx_of_instances_we_estimate_the_ll_loss_by_calculating_the_z_-_norm_of_the_whole_loss_vector_creating_the_dependency_graph_of_the_batch_as_the_union_of_the_dependency_graphs_of_batch_elements_regarding_the_gibbs_loss_we_follow_ref_17_and_generate_for_a_given_batch_b_an_estimator_hatl_g_i_b_b_stheta_-sum_iin_bsum_jn_iomega_i_jlog_leftp_f_thetaphixijright_quad_20_with_the_gibbs_weights_omega_i_j_fracexpleft-betaphixijrightsum_k_1n_iexpleft-betaphixikright_quad_21_a2_detailed_plots_for_the_experiments_fig_8_and_fig_9_provide_more_detail_of_the_experiments_we_provide_them_in_this_technical_appendix_for_the_interested_reader",
      "algorithm_3_a_single_in_layer_1_for_kin_1dots_e_do_2_mathbfe_kprimeleftarrow_etaeleftmathbfe_kmathbfn_r_kmathbfn_s_kright_3_end_for_4_for_iin_1dots_n_do_5_let_e_iprime_leftmathbfe_kprimer_ks_kright_r_k_i_k_1e_6_overlinemathbfe_iprimeleftarrow_rhoelefte_iprimeright_7_mathbfn_iprimeleftarrow_etanleftoverlinemathbfe_iprimemathbfn_iright_8_end_for_9_let_nprime_mathbfnprime_i_1n_10_let_eprime_leftmathbfe_kprimer_ks_kright_k_1e_11_return_eprimenprime_25_graph_neural_networks_as_oracle_factories_in_principle_many_different_ml_models_qualify_as_parametrized_oracle_factories_all_that_is_required_to_train_an_ml_model_with_our_loss_is_for_it_to_return_both_an_oracle_factory_f_theta_as_well_as_the_parametrized_functional_mu_theta_in_order_to_evaluate_the_lll_loss_here_we_follow_the_common_strategy_to_use_a_gnn_as_a_deep_learning_model_in_particular_we_use_an_interaction_network_16_24_like_all_other_gnns_this_type_of_network_maps_graphs_to_graphs_consider_a_graph_g_ne_with_a_set_of_nodes_n_mathbfn_i_i_1n_and_a_set_of_edges_e_leftmathbfe_kr_ks_kright_k_1e_where_mathbfn_iin_mathbbrd_n_are_feature_vectors_of_the_nodes_for_some_d_nin_mathbbn_mathbfe_kin_mathbbrd_e_are_feature_vectors_of_the_edges_for_some_d_ein_mathbbn_and_r_ks_kin_n_are_indices_of_the_receiver_and_source_nodes_for_the_k_-_th_edge_respectively_a_single_layer_of_the_interaction_network_takes_g_as_input_and_updates_both_edge_and_node_features_based_on_a_message_passing_algorithm_whose_pseudocode_is_shown_as_algorithm_3_this_algorithm_requires_the_specification_of_a_node_update_function_etan_an_edge_update_function_etae_as_well_as_an_edge_aggregation_function_rhoe_note_that_depending_on_the_update_and_aggregation_functions_the_dimensions_of_the_features_in_gprime_might_be_different_from_those_of_g_here_we_use_as_update_functions_etan_and_etav_simple_neural_networks_of_the_form_eta_l_ncirc_r_lcirc_f_l_d_ucirc_r_lcirc_f_l_d_u_-_1circ_ldots_circ_f_l_d_1_quad_14_where_l_n_is_a_layer_normalisation_layer_25_r_l_is_a_rectified_linear_unit_layer_26_and_f_l_d_is_a_fully_connected_layer_from_an_input_feature_dimension_to_an_output_dimension_d_hence_specifying_an_update_function_in_our_case_requires_a_list_d_t_t_1u_of_layer_dimensions_one_for_each_of_the_u_layers_as_an_edge_aggregation_function_we_simply_use_summation_over_the_elements_of_e_iprime_a_full_application_of_an_in_consists_of_the_consecutive_application_of_several_single_in_layers_followed_by_a_fully_connected_layer_f_l_1_to_ensure_that_outgoing_features_across_nodes_and_edges_are_one-_dimensional_that_is_for_an_in_consisting_of_l_layers_the_output_of_the_in_is_i_n_thetag_f_l_1circ_i_n_theta_lcirc_i_n_theta_l_-_1circ_ldots_i_n_theta_1g_quad_15_here_theta_r_denotes_the_parameter_vector_for_the_r_-_th_layer_whose_values_specify_the_entries_of_the_fully_connected_layers_in_its_update_functions_we_have_theta_theta_f_ltheta_ltheta_l_-_1ldots_theta_1_where_theta_f_l_are_the_parameters_in_the_last_layer_251_representing_sat_instances_as_graphs_since_gnns_map_graphs_to_graphs_in_order_to_feed_a_sat_instance_into_a_graph_network_we_need_to_represent_it_as_a_graph_fortunately_there_exist_various_simple_graph_representations_of_boolean_formulas_we_use_the_common_lcg_representation3_let_phi_be_a_sat_formula_in_cnf_form_with_n_variables_v_i_i_1n_and_m_clauses_c_j_j_1m_we_introduce_a_directed_tripartite_graph_g_ne_consisting_of_one_set_n_c_of_m_nodes_one_per_constraint_as_well_as_two_sets_n_v_and_n_v-_of_n_nodes_each_one_per_variable_the_initial_node_embeddings_mathbfn_are_one-_hot_encodings_for_which_of_these_three_sets_a_given_node_belongs_to_the_edge_set_e_further_consists_of_two_groups_the_first_group_connects_the_i_-_th_node_in_n_v_with_the_i_-_th_node_in_n_v-_for_iin_n_the_second_group_connects_for_each_clause_c_jin_phi_and_every_variable_v_iin_vc_j_the_j_-_th_node_in_v_c_with_the_i_-_th_node_in_n_v_and_also_for_every_variable_v_iin_v-_c_j_the_j_-_th_node_in",
      "4_results_we_compare_three_different_variants_of_both_for_the_mt-_algorithm_and_for_walk_sat_namely_-_the_original_version_that_uses_the_uniform_oracle-_a_hybrid_version_that_uses_the_trained_oracle_just_for_initialization_and_then_switches_to_uniform_updating_and-_the_full_oracle-based_algorithm_that_uses_the_oracle_for_both_initialization_and_updating_in_table_1_we_provide_a_summary_of_the_results_we_find_that_for_both_solvers_the_full_oracle-_based_algorithm_outperforms_both_others_across_the_board_moreover_the_walk_sat_algorithm_dominates_the_mt_algorithm_on_each_of_the_variants_a_particularly_dramatic_improvement_is_seen_in_terms_of_the_median_steps_required_where_the_boosted_mt_algorithm_is_faster_by_a_factor_of_more_than_8_while_the_boosted_walk_sat_provides_a_speed-_up_by_a_factor_of_around_5_this_indicates_that_the_boosted_solvers_are_especially_powerful_when_solving_relatively_simple_instances_a_less_dramatic_but_still_significant_improvement_is_seen_in_the_mean_number_of_steps_where_we_see_an_average_decrease_of_35_across_solvers_centerfigure_4_benchmark_of_three_different_variants_of_the_oracle-based_mt_algorithm_namely_the_uniform_version_the_hybrid_version_and_the_full_version_left_number_of_steps_needed_on_average_for_instances_of_the_evaluation_data_set_for_given_alpha_center_number_of_clauses_violated_on_average_as_a_function_of_the_number_of_steps_the_algorithm_has_taken_right_ratio_of_solved_instances_as_a_function_of_the_number_of_steps_the_algorithm_has_taken_center_centerfigure_5_benchmark_of_three_different_variants_of_the_oracle-based_walk_sat_using_the_same_plots_as_in_fig_4_center"
    ]
  },
  "276c92a9f75ec42d28cf72510257deda": {
    "file_path": "converted_papers/1909.11588_Graph_Neural_Reasoning_May_Fail_in_Certifying_Bool.md",
    "title": "Graph Neural Reasoning May Fail in Certifying Bool",
    "file_hash": "c319366c22f3ae4b45a7477306ef2fc3",
    "parsed_at": "2025-12-08T15:13:54.073548",
    "section_count": 3,
    "section_keys": [
      "graph_neural_reasoning_may_fail_in_certifying_bool",
      "introduction",
      "3_certifying_unsat_by_gnns_may_fail_although_existing_researches_showed_that_gnn_can_learn_a_well-_performed_solver_for_satisfiability_problems_gnn-_based_sat_solvers_actually_have_terrible_performances_in_predicting_unsatisfiability_with_high_confidence_12_in_a_sat_formula_if_the_formula_does_not_have_a_small_unsatisfiable_core_minimal_number_of_clauses_that_is_enough_to_cause_unsatisfiability_in_fact_some_previous_work_1_even_completely_removed_unsatisfiable_formulas_from_the_training_dataset_since_they_slowed_down_the_whole_training_process_the_difficulty_in_proving_unsatisfiability_is_understandable_since_constructing_a_proof_of_unsatisfiability_demands_a_complete_reasoning_in_the_search_space_which_is_more_complex_than_constructing_a_proof_of_satisfiability_that_only_requires_a_witness_traditionally_it_relies_on_the_recursive_decision_procedures_that_either_traverse_all_possible_assignments_to_construct_the_proof_dpll_4_or_generate_extra_constraints_from_assignment_trials_that_lead_to_conflicts_until_some_of_the_constraints_contradict_each_other_cdcl_13_the_line_of_recursive_algorithms_include_some_operation_branches_that_reconfigure_the_bipartite_graph_behind_the_cnf_in_each_step_while_they_search_in_the_terms_of_a_graph_that_may_iteratively_change_eg_dpll_perhaps_miserably_their_recursive_processes_can_not_be_simulated_by_gnns_observation_31_given_a_recursive_algorithm_that_iteratively_reconfigures_the_graph_gnns_in_eq2_can_not_simulate_this_recursive_process_proof_associating_the_aggregate_and_combine_functions_in_eq_2_we_obtain_the_iterative_update_rule_for_the_embedding_of_a_literal_v_beginarrayr_l_h_vk_mathrmcombine_lklefth_vk_-_1h_v_-_vk_-_1mathrmaggregate_lklefth_psi_vk_-_1psi_vin_phi_rightright_quad_mathrmupdate_lklefth_vk_-_1h_v_-_vk_-_1h_psi_vk_-_1psi_vin_phi_right_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_quad_stvin_l_endarray_quad_3_towards_this_principle_we_observe_that_the_embedding_update_of_v_in_the_current_stage_relies_on_the_last-_stage_embeddings_of_v_and_its_negation_neg_v_and_the_embeddings_of_all_the_clauses_that_include_v_in_a_cnf_formula_psi_v_in_phi_the_literal_v_neg_v_and_the_clauses_containing_v_are_consistent_over_iterations_hence_if_the_update_function_eq_3_is_consistent_over_the_iterations_in_eq2_ie_forall_k_in_mathbbn__mathrmupdate_lk_mathrmupdate_l_where_mathrmupdate_l_means_the_update_for_literal_embedding_gnns_derived_from_eq_3_receive_a_fixed_graph_generated_by_a_cnf_formula_as_input_however_if_a_recursive_algorithm_iteratively_changes_the_graph_that_represents_a_cnf_formula_it_implies_that_there_must_be_a_clause_that_was_changed_or_eliminated_after_this_iteration_since_clauses_are_permutation-_invariant_in_a_cnf_formula_accordingly_there_must_be_a_literal_embedding_whose_update_process_depends_on_a_clause_different_from_the_previous_iteration_it_contradicts_the_literal_embedding_update_function_learned_by_eq_3_with_forall_k_in_mathbbn__mathrmupdate_lk_mathrmupdate_l_hence_the_message-_passing_in_gnns_could_not_resemble_the_procedures_in_the_complete_sat-_solvers_in_fact_gnns_are_rather_similar_to_learning_a_subfamily_of_incomplete_sat_solvers_gsat_walk_sat_11_which_randomly_assign_variables_and_stochastically_search_for_local_witnesses_observation_32_gnns_in_eq_2_may_simulate_the_local_search_in_walk_sat_proof_recall_the_iterative_update_routine_of_walk_sat_starting_by_assigning_a_random_value_to_each_literal_variable_in_a_formula_it_randomly_chooses_an_unsatisfied_clause_in_the_formula_and_flips_the_value_of_a_boolean_variable_within_that_clause_such_process_is_repeated_till_the_literal_assignment_satisfies_all_clauses_in_the_formula_here_we_construct_the_optimal_aggregation_and_combine_functions_derived_from_eq_2_which_are_designed_to_simulate_the_procedure_of_walk_sat_in_this_way_if_the_aggregation_and_combine_functions_in_eq_2_approximate_these_optimal_aggregation_and_combine_functions_the_gnn_may_simulate_the_local_search_in_walk_sat_given_a_universe_of_literals_in_logical_reasoning_we_first_initiate_the_embeddings_of_them_and_their_negation_thus_forall_v_in_l_random_value_of_h_v0_and_h_neg_v0_are_initiated_this_assignment_can_be_treated_as_the_boolean_value_that_belong_to_different_literals_which_have_been_mapped_from_a_binary_vector_into_a_real-_value_embedding_space_about_the_literals_we_also_randomly_initiate_the_clause_embeddings_h_psi_v0_for_reasoning_each_formula_that_contains_the_clause_psi_v_here_we_define_the_optimal"
    ]
  },
  "247555210734f65824867d1d2260c5c0": {
    "file_path": "converted_papers/2408.15418_Understanding_GNNs_for_Boolean_Satisfiability_thro.md",
    "title": "Understanding GNNs for Boolean Satisfiability thro",
    "file_hash": "82caa28bb8d27acc35a708108e7b4bb0",
    "parsed_at": "2025-12-08T15:13:54.074584",
    "section_count": 6,
    "section_keys": [
      "understanding_gnns_for_boolean_satisfiability_thro",
      "introduction",
      "s_supplementary_material_s1_derivation_of_the_sdp_relaxation_for_max-2-sat_here_we_provide_further_details_about_the_definition_of_sdp_relaxation_for_max-_2-_sat_the_goal_is_to_write_an_objective_function_for_2-_cnf_formulae_which_consist_of_clauses_c_1ldots_c_k_over_variables_x_1ldots_x_n_with_at_most_two_literals_per_clause_for_each_boolean_variable_x_i_where_iin_12ldots_n_a_new_variable_y_iin_-_11_is_first_instantiated_and_one_additional_variable_y_0in_-_11_is_introduced_the_additional_variable_is_introduced_to_unambiguously_assign_the_truth_value_in_the_original_problem_from_values_of_relaxed_problem_it_is_not_possible_to_just_assign_true_false_to_x_i_if_y_i_10_because_quadratic_terms_cannot_distinguish_between_y_icdot_y_j_and_-_y_icdot_-_y_j_instead_the_truth_value_of_x_i_is_assigned_by_comparing_y_i_with_y_0colon_x_i_is_true_if_and_only_if_y_i_y_0_otherwise_it_is_false_the_assignment_is_therefore_invariant_to_negating_all_variables_to_determine_the_value_of_a_formula_we_sum_the_value_of_its_clauses_c_which_are_given_by_the_value_function_vc_here_are_examples_of_the_value_function_for_3_different_clauses_vx_i_frac1_y_0cdot_y_i2_vneg_x_i_1_-_vx_i_frac1_-_y_0cdot_y_i2_vx_ivee_neg_x_j_1_-_vneg_x_iwedge_x_j_qquad_1_-_frac1_-_y_0cdot_y_i2frac1_y_0cdot_y_j2_qquad_frac14_1_y_0cdot_y_i_frac14_1_-_y_0cdot_y_j_frac14_1_y_icdot_y_j_by_summing_over_all_clauses_c_in_in_the_boolean_formula_the_following_integer_quadratic_program_for_max-_2-_sat_is_obtained_mathrmmaximizequad_sum_cin_cvc_mathrmsubjecttoquad_y_iin_-11_mathrmforalliin_01ldots_n_this_can_be_rewritten_by_collecting_coefficients_of_y_icdot_y_j_for_ijin_01ldots_n_and_putting_them_symmetrically_into_a_n_1times_n_1_coefficient_matrix_w_the_terms_y_icdot_y_j_can_be_collected_in_a_matrix_y_with_same_dimensions_as_w_the_elements_y_ij_correspond_to_y_icdot_y_j_for_ijin_01ldots_n_both_matrices_are_symmetric_hence_the_sum_of_all_elements_in_their_element-_wise_product_which_is_the_objective_function_can_be_compactly_expressed_by_using_trace_operation_this_leads_to_the_following_version_of_the_same_integer_program_mathrmmaximizequad_mathrmtrw_y_mathrmsubjecttoquad_y_ijin_-11_mathrmforallijin_01ldots_n_ineq_j_so_far_no_relaxation_has_been_made_to_make_the_discrete_program_continuous_the_value_of_the_variables_y_i_is_allowed_to_be_any_real_number_between_-_1_and_1_after_solving_a_quadratic_program_with_this_relaxation_rounding_can_be_used_to_obtain_a_value_from_-_11_semi-_definite_programming_goes_further_and_allows_variables_to_be_n_1_-_dimensional_unit_vectors_y_0ldots_y_nlongrightarrow_y_0ldots_y_n_schematically_depicted_in_figure_5_this_directly_leads_to_the_relaxation_used_in_the_main_part_of_this_study_our_aim_was_to_show_that_solving_sdp_relaxation_by_optimization_and_rounding_by_separating_the_high-_dimensional_vectors_closely_resembles_the_behavior_of_gnn",
      "s5_analysis_of_the_evolution_of_literal_embeddings_to_reinforce_the_assertion_regarding_the_relationship_between_neuro_sat_and_the_sdp_relaxation_for_max_sat_we_tested_whether_the_evolution_of_literal_embeddings_in_neuro_sat_actually_corresponds_to_an_optimization_process_that_tries_to_maximize_the_sdp_objective_we_first_sampled_several_hundred_2-_cnf_formulas_and_obtained_the_sdp_objective_function_for_each_of_them_using_the_expressions_mentioned_in_appendix_s1_the_objective_function_is_a_linear_function_of_the_gram_matrix_y_corresponding_to_the_inner_products_between_the_unit_vectors_representing_the_lifted_variables_and_one_vector_mathbfy_0_representing_the_value_true_an_sdp_solver_optimizes_the_matrix_y_while_adhering_to_specified_constraints_which_ensure_that_the_matrix_can_be_obtained_as_a_gram_matrix_for_some_set_of_unit_vectors_to_observe_the_behavior_of_the_same_objective_function_with_literal_embeddings_from_neuro_sat_we_need_to_compute_this_gram_matrix_y_after_each_mp_iteration_of_the_gnn_if_the_evolution_of_these_embeddings_would_correspond_to_an_optimization_process_maximizing_the_objective_then_we_should_observe_an_increase_in_this_objective_after_each_mp_step_when_computing_the_matrix_y_from_the_neuro_sat_embeddings_two_details_must_be_taken_into_account_first_only_the_positive_literals_are_taken_into_account_as_the_objective_function_automatically_assumes_that_the_embeddings_of_negative_literals_are_obtained_by_negation_of_the_positive_ones_second_neuro_sat_does_not_explicitly_represent_the_embedding_mathbfy_0_representing_the_value_true_therefore_we_estimate_it_by_averaging_all_literals_that_are_assigned_to_true_in_the_extracted_solution_before_computing_the_matrix_y_we_also_center_all_vectors_to_0_and_normalize_them_to_unit_vectors_for_each_iteration_t_of_the_gnn_we_obtain_the_matrix_yt_ltltt_where_lt_represents_the_matrix_of_centered_and_normalized_positive_literal_embeddings_with_the_estimated_vector_mathbfy_0_also_normalized_and_centered_concatenated_as_its_first_row_in_figure_7_we_show_how_the_objective_function_changes_after_each_iteration_t_for_10_randomly_selected_instances_we_also_include_the_objective_value_obtained_with_a_sdp_solver_as_a_reference_centerfigure_7_a_plot_showing_how_the_sdp_objective_value_computed_from_the_neuro_sat_embeddings_in_blue_changes_after_each_iteration_of_mp_the_horizontal_red_line_represents_the_value_of_the_same_objective_obtained_with_an_sdp_solver_center_figure_7_shows_that_the_evolution_of_literal_embeddings_corresponds_to_an_increase_in_the_objective_value_of_sdp_it_is_also_visible_that_there_is_a_gap_between_the_highest_value_achieved_and_the_objective_value_obtained_with_an_sdp_solver_in_figure_8_a_we_plot_a_histogram_of_these_gaps_computed_for_all_generated_problems_we_hypothesized_that_the_gap_may_be_partially_caused_by_the_inappropriate_choice_of_the_vector_mathbfy_0_therefore_we_took_the_matrix_yt_from_the_last_step_of_mp_t_40_and_further_optimized_it_using_a_gradient-_based_sdp_solver_implemented_in_py_torch_this_closed_the_gaps_mentioned_above_in_most_instances_as_visible_in_figure_8_b_in_figure_9_we_show_how_the_entries_in_the_matrix_yt_change_after_further_optimization_for_a_random_formula_as_can_be_seen_the_largest_change_in_values_occurs_in_the_first_row_and_in_the_first_column_which_correspond_to_inner_products_of_each_literal_embedding_with_the_vector_mathbfy_0_this_supports_our_hypothesis_that_if_we_would_be_able_to_pick_the_vector_mathbfy_0_in_a_more_optimal_way_the_gaps_in_figure_7_would_be_smaller",
      "s6_training_with_the_sdp_objective_function_to_further_support_the_connection_to_sdp-_based_approximation_algorithms_we_tried_to_train_the_gnn_with_a_loss_function_that_is_minimized_when_the_maximum_number_of_clauses_is_satisfied_unlike_the_experiments_in_section_s5_here_we_focus_on_general_max-_sat_for_which_we_came_up_with_the_following_multilinear_objective_function_given_a_set_of_variables_x_1x_2ldots_x_n_x_associated_to_each_boolean_variable_one_special_variable_x_0_and_a_set_of_clauses_c_1c_2ldots_c_m_c_where_each_clause_c_i_consists_of_multiple_literals_variables_with_polarity_the_objective_function_vc_for_an_integer-_valued_problem_can_be_defined_as_vc_sum_cin_cleftprod_lin_cfrac1_-_sgnlcdot_x_lcdot_x_02right_where_operatorname_sgnl_is_a_variable_polarity_in_clause_c_and_evaluates_to_1_for_a_positive_occurrence_of_the_variable_l_and_-_1_for_negative_and_x_lin_x_can_take_the_value_-_1_or_1_the_product_for_a_clause_c_is_0_if_at_least_for_one_of_the_variables_x_l_in_the_clause_sgnlcdot_x_lcdot_x_0_1_which_is_the_case_when_this_clause_is_satisfied_to_use_this_as_a_loss_function_for_the_supervision_of_neuro_sat_we_lift_the_variables_x_0x_1x_2ldots_x_n_to_be_unit_vectors_in_a_high-_dimensional_space_the_objective_is_to_minimize_the_following_differentiable_expression_by_optimizing_these_unit_vectors_vc_sum_cin_cleftprod_lin_cfrac1_-_sgnlx_lcdot_x_02right_the_scalar_product_of_the_unit_vectors_x_lcdot_x_0_is_a_real_number_between_-_1_and_1_the_vector_x_0_is_sampled_randomly_as_a_unit_vector_and_is_kept_fixed_whereas_other_variable_vectors_are_sampled_randomly_fed_into_neuro_sat_as_initial_embeddings_for_positive_literals_updated_by_mp_iterations_and_normalized_after_each_update_we_supervise_the_negative_literals_with_the_same_objective_with_the_exception_that_the_operatorname_sgnl_returns_-_1_for_positive_and_1_for_a_negative_literal_occurrence_to_extract_the_assignment_of_individual_variables_we_compute_an_inner_product_with_the_vector_x_0_representing_the_value_true_and_assign_the_variable_to_true_if_it_is_positive_and_to_false_in_the_other_case_once_we_have_the_assignment_we_can_classify_the_formula_as_satunsat_by_checking_whether_the_solution_satisfies_the_formula_the_trained_model_accurately_classifies_only_sim_73_of_all_problems_vs_sim_85_in_the_case_of_the_neuro_sat_trained_by_the_original_loss_when_optimizing_the_embeddings_wrt_this_objective_directly_with_autograd_the_accuracy_was_sim_65_on_the_other_hand_when_trained_with_this_objective_function_the_model_starts_to_quickly_improve_even_when_trained_only_on_the_formulas_of_the_largest_size_ie_40_variables_without_a_curriculum_this_suggests_that_a_possible_combination_of_loss_functions_one_that_tries_to_maximize_the_number_of_satisfied_formulas_and_one_that_penalizes_the_model_for_incorrect_classification_may_be_beneficial_we_leave_the_investigation_of_this_idea_for_future_work_s7_results_for_different_numbers_of_decimation_steps_in_table_2_we_show_the_effect_of_running_the_decimation_process_multiple_times_on_our_test_sets_the_decimation_process_did_not_result_in_any_further_improvement_when_repeated_more_than_twice_the_hyperparameters_are_the_same_as_for_the_experiments_in_table_1_with_16_random_init_samples_per_formula_in_the_first_pass_and_1_random_init_sample_for_each_subsequent_pass_we_note_that_we_did_not_try_to_optimize_the_decimation_threshold_which_could_lead_to_further_improvements_table_2_this_table_shows_a_number_of_problems_solved_after_subsequent_application_of_the_decimation_procedure_tabletrtdproblem_typetdtdsat_problemstdtdfirst_passtdtdsecond_passtdtdthird_passtdtrtrtdsr40tdtd5000tdtd4442_888_tdtd274_54_tdtd35_07_tdtrtrtdlatin_squares_9x9tdtd200tdtd186_93_tdtd14_7_tdtd4_2_tdtrtrtdlatin_squares_8x8tdtd200tdtd196_98_tdtd1_05tdtd0_0_tdtrtrtdlogical_circuitstdtd344tdtd319_927_tdtd0_0_tdtd0_0_tdtrtrtdsudoku_9x9tdtd200tdtd83_46_tdtd11_55_tdtd3_15_tdtrtable_s8_results_for_belief_propagation_here_we_report_the_results_of_belief_propagation_algorithm_applied_on_the_same_problems_as_reported_in_the_main_paper_neuro_sat_results_are_without_resampling_and_decimation_copied_from_table_1_belief_propagation_is_run_once_for_a_maximum_of_1000_iterations",
      "52_qualitative_results_521_evolution_of_literal_embeddings_to_further_support_our_claim_about_the_connection_to_sdp_we_tested_whether_the_evolution_of_literal_embeddings_in_neuro_sat_actually_corresponds_to_an_optimization_process_that_maximizes_the_sdp_objective_23_the_test_was_performed_on_several_hundred_2-_cnf_random_formulas_since_their_sdp_formulation_is_simple_to_state_an_sdp_solver_optimizes_the_matrix_y_whose_entries_could_be_interpreted_as_dot_products_of_vectors_corresponding_to_boolean_variables_to_observe_the_behavior_of_the_sdp_objective_function_on_the_evolving_literal_embeddings_from_neuro_sat_we_need_to_compute_matrix_y_after_each_mp_iteration_of_the_gnn_for_each_iteration_t_of_the_gnn_we_obtain_the_matrix_yt_ltltt_where_lt_represents_the_matrix_of_centered_and_normalized_embeddings_of_positive_literals_the_first_row_of_lt_corresponds_to_the_vector_y_0_representing_the_value_true_and_therefore_is_not_present_in_the_mp_graph_for_neuro_sat_thus_we_therefore_set_y_0_to_the_average_true_vector_described_in_section_4_in_figure_4_we_show_how_the_objective_function_changes_after_each_iteration_t_for_3_randomly_selected_instances_we_also_include_the_objective_value_obtained_with_an_sdp_solver_as_a_reference_as_can_be_seen_the_centerfigure_4_a_plot_showing_how_the_sdp_objective_value_computed_from_the_neuro_sat_embeddings_in_blue_changes_after_each_iteration_of_mp_the_horizontal_red_line_represents_the_value_of_the_same_objective_obtained_with_an_sdp_solver_center_522_training_directly_with_the_max-_sat_sdp_objective_function_to_outline_one_possible_future_work_direction_we_tried_to_train_the_gnn_with_a_loss_function_that_is_minimized_when_the_maximum_number_of_clauses_is_satisfied_given_a_set_of_variables_x_1x_2ldots_x_n_x_corresponding_to_each_variable_in_the_boolean_formula_one_special_variable_x_0_corresponding_to_value_true_and_a_set_of_clauses_c_1c_2ldots_c_m_c_where_each_clause_c_i_is_a_set_of_literals_variables_with_a_polarity_the_objective_function_vc_for_an_integer-_valued_problem_can_be_defined_as_vc_sum_cin_cleftprod_lin_cfrac1_-_sgnlcdot_x_lcdot_x_02right_operatorname_sgnl_is_a_variable_polarity_in_clause_c_and_evaluates_to_1_for_a_positive_occurrence_of_the_variable_x_l_and_to_-_1_for_a_negative_occurrence_the_variable_x_lin_x_can_take_the_value_-_1_or_1_corresponding_to_false_and_true_if_we_set_x_0_to_1_then_the_product_for_a_clause_c_will_yield_0_if_at_least_for_one_of_the_variables_x_l_in_the_clause_sgnlcdot_x_lcdot_x_0_1_which_is_the_case_when_this_clause_is_satisfied_to_use_this_as_a_loss_function_for_the_supervision_of_neuro_sat_we_lift_boolean_variables_to_be_unit_vectors_y_0y_1y_2ldots_y_n_in_a_high-_dimensional_space_the_objective_is_to_minimize_the_following_differentiable_expression_by_optimizing_these_unit_vectors_vc_sum_cin_cleftprod_lin_cfrac1_-_sgnly_lcdot_y_02right_the_scalar_product_of_the_unit_vectors_y_lcdot_y_0_is_a_real_number_between_-_1_and_1_the_vector_y_0_is_sampled_randomly_as_a_unit_vector_and_is_kept_fixed_whereas_other_variable_vectors_are_sampled_randomly_fed_into_neuro_sat_as_initial_embeddings_for_positive_literals_updated_by_mp_iterations_and_normalized_after_each_update_we_supervise_the_negative_literals_with_the_same_objective_with_the_exception_that_the_operatorname_sgnl_returns_-_1_for_positive_and_1_for_a_negative_literal_occurrence_in_order_to_compare_the_network_trained_with_the_sdp_max-_sat_objective_with_neuro_sat_trained_with_classification_loss_we_need_a_boolean_variable_assignment_to_extract_the_assignment_of_individual_variables_we_compute_an_inner_product_with_the_vector_y_0_representing_the_value_true_and_assign_the_variable_to_true_if_it_is_positive_and_to_false_in_the_other_case_once_we_have_the_assignment_we_can_classify_the_formula_as_satunsat_by_checking_whether_the_solution_satisfies_the_formula_the_model_trained_with_max-_sat_sdp_objective_accurately_classifies_only_sim_73_of_all_problems_vs_sim_85_in_the_case_of_the_neuro_sat_trained_by_the_classification_loss_on_the_other_hand_when_trained_with_this_objective_function_the_model_starts_to_quickly_improve_even_when_trained_only_on_the_formulas_of_the_largest_size_ie_40_variables_without_a_curriculum_this_suggests_that_a_possible_combination_of_loss_functions_one_that_tries_to_maximize_the_number_of_satisfied_formulas_and_one_that_penalizes_the_model_for_incorrect_classification_may_be_beneficial_we_leave_the_investigation_of_this_idea_for_future_work_53_quantitative_results_531_training_convergence_with_the_curriculum_to_demonstrate_the_effectiveness_of_the_proposed_curriculum_we_compare_the_training_process_with_two_baselines_the_first_is_the_publicly_available_implementation_of_neuro_sat_4_and_the_second_is_our_model_without_curriculum_we_stop_training_each_model_once_it_reaches_the_validation_accuracy_reported_in_the_original_paper_85_as_visible_in_figure_3_our_model_with_the_curriculum_reaches_this_accuracy_in_approximately_30_minutes_while_the_other_two_baselines_need_to_be_trained_for_several_hours_all_models_were_trained_on_1_gpu_nvidia_a100_532_sampling_and_decimation_in_table_1_we_show_the_increase_in_accuracy_due_to_the_enhancements_described_in_section_4_together_with_the_results_on_randomly_generated_problems_we_also_show_results_on_three_different_structured_problems_whose_details_are_described_in_the_supplementary_material_s32_the_results_show_a_noticeable_increase_in_the_number_of_solved_problems_for_both_enhancements_sampling_and_decimation_for_decimation_we_use_only_two_passes_which_means_that_if_the_first_application_of_the"
    ]
  },
  "d718c5c662baa3c007cdb0ed1cad77db": {
    "file_path": "converted_papers/2403.03517_IB-Net_Initial_Branch_Network_for_Variable_Decisio.md",
    "title": "IB Net Initial Branch Network for Variable Decisio",
    "file_hash": "e1646f5856c03384a47aa7b5c2af2595",
    "parsed_at": "2025-12-08T15:13:54.075073",
    "section_count": 3,
    "section_keys": [
      "ib_net_initial_branch_network_for_variable_decisio",
      "introduction",
      "references_amizadeh_et_al_2019_saeed_amizadeh_sergiy_matusevych_and_markus_weimer_learning_to_solve_circuit-_sat_an_unsupervised_differentiable_approach_in_7th_international_conference_on_learning_representations_iclr_2019_new_orleans_la_usa_may_6-_9_2019_open_reviewnet_2019_audemard_and_simon_2018_gilles_audemard_and_laurent_simon_on_the_glucose_sat_solver_international_journal_on_artificial_intelligence_tools_ijiat_2711-_25_2018_balunovic_et_al_2018_mislav_balunovic_pavol_bielik_and_martin_vechev_learning_to_solve_smt_formulas_in_s_bengio_h_wallach_h_larochelle_k_grauman_n_cesa-_bianchi_and_r_garnett_editors_advances_in_neural_information_processing_systems_volume_31_curran_associates_inc_2018_balyo_et_al_2022_tomas_balyo_marijn_jh_heule_markus_iser_matti_jarvisalo_and_martin_suda_editors_proceedings_of_sat_competition_2022_solver_and_benchmark_descriptions_department_of_computer_science_series_of_publications_b_department_of_computer_science_university_of_helsinki_finland_2022_biere_and_fleury_2022_armin_biere_and_mathias_fleury_gimsatul_isa_sat_and_kissat_entering_the_sat_competition_2022_in_tomas_balyo_marijn_heule_markus_iser_matti_jarvisalo_and_martin_suda_editors_proc_of_sat_competition_2022_-_solver_and_benchmark_descriptions_volume_b-_2022-_1_of_department_of_computer_science_series_of_publications_b_pages_10-_11_university_of_helsinki_2022_biere_2019_armin_biere_cadical_at_the_sat_race_2019_2019_cook_1971_stephen_a_cook_the_complexity_of_theorem-_proving_procedures_in_proceedings_of_the_third_annual_acm_symposium_on_theory_of_computing_stoc_71_page_151-_158_new_york_ny_usa_1971_association_for_computing_machinery_huang_et_al_junhua_huang_hui-_ling_zhen_naixing_wang_mingxuan_yuan_hui_mao_yu_huang_and_jiping_tao_accelerate_sat-_based_atpg_via_preprocessing_and_new_conflict_management_heuristics_in_asp-_dac_2022_huang_et_al_2022_zeren_huang_kerong_wang_furui_liu_hui-_ling_zhen_weinan_zhang_mingxuan_yuan_jianye_hao_yong_yu_and_jun_wang_learning_to_select_cuts_for_efficient_mixed-_integer_programming_pattern_recognition_123108353_2022_kasi_and_sarma_2013_bakhtiar_khan_kasi_and_anita_sarma_cassandra_proactive_conflict_minimization_through_optimized_task_scheduling_in_icse_2023_pages_732-_741_2013_leino_2010_k_rustan_m_leino_dafny_an_automatic_program_verifier_for_functional_correctness_in_edmund_m_clarke_and_andrei_voronkov_editors_logic_for_programming_artificial_intelligence_and_reasoning_pages_348-_370_berlin_heidelberg_2010_springer_berlin_heidelberg_li_et_al_2018_zhuwen_li_qifeng_chen_and_vladlen_koltun_combinatorial_optimization_with_graph_convolutional_networks_and_guided_tree_search_in_proceedings_of_the_32nd_international_conference_on_neural_information_processing_systems_nips18_page_537-_546_red_hook_ny_usa_2018_curran_associates_inclin_et_al_2020_tsung-_yi_lin_priya_goyal_ross_girshick_kaiming_he_and_piotr_dollar_focal_loss_for_dense_object_detection_ieee_transactions_on_pattern_analysis_and_machine_intelligence_422318-_327_2020_lopera_et_al_2021_daniela_sanchez_lopera_lorenzo_servadei_gamze_naz_kiprit_souvik_hazra_robert_wille_and_wolfgang_ecker_a_survey_of_graph_neural_networks_for_electronic_design_automation_in_2021_acmieee_3rd_workshop_on_machine_learning_for_cad_mlcad_pages_1-_6_2021_selsam_and_bj\u00f8rner_2019_daniel_selsam_and_nikolaj_bj\u00f8rner_guiding_high-_performance_sat_solvers_with_unsat-_core_predictions_in_mikol\u00e1\u0161_janota_and_in\u00e9s_lynce_editors_theory_and_applications_of_satisfiability_testing_-_sat_2019_pages_336-_353_cham_2019_springer_international_publishingselsam_et_al_2019_daniel_selsam_matthew_lamm_benedikt_b\u00fcnz_percy_liang_leonardo_de_moura_and_david_l_dill_learning_a_sat_solver_from_single-_bit_supervision_in_iclr_2019_open_reviewnet_2019_s\u00f6rensson_and_e\u00e9n_2005_niklas_s\u00f6rensson_and_niklas_e\u00e9n_minisat_v113_-_a_sat_solver_with_conflict-_clause_minimization_2005_tseitin_1983_g_s_tseitin_on_the_complexity_of_derivation_in_propositional_calculus_pages_466-_483_springer_berlin_heidelberg_berlin_heidelberg_1983_wetzler_et_al_2014_nathan_wetzler_marijn_j_h_heule_and_warren_a_hunt_drat-_trim_efficient_checking_and_trimming_using_expressive_clausal_proofs_in_carsten_sinz_and_uwe_egly_editors_theory_and_applications_of_satisfiability_testing_-_sat_2014_pages_422-_429_cham_2014_springer_international_publishingxu_et_al_2018_hong_xu_sven_koenig_and_t_k_satish_kumar_towards_effective_deep_learning_for_constraint_satisfaction_problems_in_john_hooker_editor_principles_and_practice_of_constraint_programming_pages_588-_597_cham_2018_springer_international_publishingzhang_et_al_2021_wenjie_zhang_zeyu_sun_qihao_zhu_ge_li_shaowei_cai_yingfei_xiong_and_lu_zhang_nicolasat_boosting_local_search_with_solution_prediction_in_proceedings_of_the_twenty-_ninth_international_joint_conference_on_artificial_intelligence_ijcai_2020_2021_zhao_et_al_2021_yunxiang_zhao_jianzhong_qi_qingwei_liu_and_rui_zhang_wgcn_graph_convolutional_networks_with_weighted_structural_features_in_proceedings_of_the_44th_international_acm_sigir_conference_on_research_and_development_in_information_retrieval_sigir_21_page_624-_633_new_york_ny_usa_2021_association_for_computing_machinery"
    ]
  },
  "85db9e5aac62b5b1758102f95edbd8ee": {
    "file_path": "converted_papers/2110.14053_NeuroBack_Improving_CDCL_SAT_Solving_using_Graph_N.md",
    "title": "NeuroBack Improving CDCL SAT Solving using Graph N",
    "file_hash": "4b6ed5c3a4516daf84e551aa35e16012",
    "parsed_at": "2025-12-08T15:13:54.075907",
    "section_count": 5,
    "section_keys": [
      "neuroback_improving_cdcl_sat_solving_using_graph_n",
      "introduction",
      "9_acknowledgment_9_acknowledgmentwe_would_like_to_thank_the_anonymous_reviewers_for_their_valuable_feedback_this_work_was_supported_by_a_grant_from_the_army_research_office_accomplished_under_cooperative_agreement_number_w911nf-_19-_2-_0333_the_views_and_conclusions_contained_in_this_document_are_those_of_the_authors_and_should_not_be_interpreted_as_representing_the_official_policies_either_expressed_or_implied_of_the_army_research_office_or_the_us_government_the_us_government_is_authorized_to_reproduce_and_distribute_reprints_for_government_purposes_notwithstanding_any_copyright_notation_herein_this_work_was_also_supported_in_part_by_ace_one_of_the_seven_centers_in_jump_20_a_semiconductor_research_corporation_src_program_sponsored_by_darpa_and_by_the_intel_rare_center_10_appendix_101_additional_experimental_results_101_additional_experimental_resultsthe_scatter_plots_is_another_commonly_used_plot_in_the_sat_community_for_comparing_the_solving_effectiveness_of_two_solvers_on_each_problem_fig_5_shows_the_scatter_plots_of_neuro_back-_kissat_and_its_two_baseline_solvers_default-_kissat_and_random-_kissat_it_is_evident_that_more_dots_are_present_in_the_lower_triangular_area_indicating_that_there_are_more_problems_on_which_neuro_back-_kissat_outperforms_both_default-_kissat_and_random-_kissat_specifically_neuro_back-_kissat_outperforms_default-_kissat_on_43_and_40_additional_problems_in_satcomp-_2022_and_satcomp-_2023_respectively_reducing_solving_time_by_117_and_36_seconds_per_problem_similarly_neuro_back-_kissat_outperforms_random-_kissat_in_satcomp-_2022_and_satcomp-_2023_on_22_and_29_more_problems_respectively_leading_to_a_reduction_in_solving_time_of_98_and_246_seconds_per_problem_102_performance_on_solved_sat_and_unsat_problems_upon_detailed_analysis_for_661_problems_from_both_satcomp-_2022_and_satcomp-_2023_testing_sets_there_are_194_unsat_problems_and_216_sat_problems_that_are_solved_by_either_default-_kissat_or_neuro_back-_kissat_for_the_194_solved_unsat_problems_neuro_back-_kissat_outperformed_default-_kissat_in_121_cases_624_while_default-_kissat_outperformed_neuro_back-_kissat_in_only_61_problems_314_for_the_216_solved_sat_problems_neuro_back-_kissat_outperformed_default-_kissat_in_110_problems_509_while_default-_kissat_outperformed_neuro_back-_kissat_in_87_problems_403_while_neuro_back-_kissat_showed_a_higher_improvement_rate_in_unsat_problems_compared_to_sat_ones_624_vs_509_the_extent_of_improvement_was_more_significant_in_sat_problems_on_average_neuro_back-_kissat_enhanced_the_performance_of_sat_problems_by_532_compared_to_an_average_improvement_of_only_146_in_unsat_problems_these_trends_were_similarly_observed_when_comparing_neuro_back-_kissat_with_random-_kissat_the_experimental_results_highlight_two_key_aspects_first_they_demonstrate_that_neuro_backs_predicted_variable_phases_can_enhance_the_efficiency_in_solving_unsat_problems_our_explanation_is_that_neuro_backs_phase_predictions_can_aid_in_directing_the_search_towards_the_unsatisfiable_part_of_the_search_space_while_neuro_back_cannot_satisfy_all_components_of_a_given_sat_problem_it_may_predict_phases_that_satisfy_certain_components_thereby_allowing_the_solver_to_concentrate_on_the_unsat_part_furthermore_in_modern_sat_solvers_such_as_default-_kissat_biere_fleury_2020_an_assignment_that_falsifies_the_fewer_clauses_is_often_preferred_in_the_searching_loop_allowing_the_solver_to_specifically_target_the_unsat_portions_of_the_clause_set_consequently_the_phases_predicted_by_neuro_back_can_facilitate_identifying_an_assignment_that_reduces_clause_falsification_thereby_enhancing_solving_unsat_problems_second_the_experimental_results_also_show_that_neuro_back_achieves_a_more_pronounced_improvement_in_solving_sat_problems_than_in_solving_unsat_problems_this_distinction_stems_from_the_inherent_nature_of_these_problems_in_sat_problems_a_complete_satisfying_assignment_exists_where_each_variable_is_assigned_a_phase_that_leads_to_a_solution_conversely_in_unsat_problems_only_partial_satisfying_assignments_exist_with_phases_assigned_to_just_a_subset_of_variables_consequently_the_phases_predicted_by_neuro_back_have_a_generally_greater_impact_in_resolving_sat_problems_this_is_because_for_these_problems_the_predicted_phases_can_contribute_directly_to_finding_a_satisfying",
      "references_model_counting_competition_2020_url_2020_httpsmccompetitionorg2021mc_descriptionhtml_model_counting_competition_2021_url_2021_httpsmccompetitionorg2021mc_descriptionhtml_model_counting_competition_2022_url_2022_httpsmccompetitionorg2022mc_descriptionhtml_sat_competition_2022_httpssatcompetitiongithubio2022_2022_accessed_2023-_08-_10_sat_competition_2023_httpssatcompetitiongithubio2023_2023_accessed_2023-_11-_23_tasniem_al-_yahya_mohamed_el_bachir_abdelkrim_menai_and_hassan_mathkour_boosting_the_performance_of_cdc1-_based_sat_solvers_by_exploiting_backbones_and_backdoors_algorithms_159_302_2022_gilles_audemard_and_laurent_simon_on_the_glucose_sat_solver_international_journal_on_artificial_intelligence_tools_27011840001_2018_jimmy_lei_ba_jamie_ryan_kiros_and_geoffrey_e_hinton_layer_normalization_ar_xiv_preprint_ar_xiv160706450_2016_peter_w_battaglia_jessica_b_hamrick_victor_bapst_alvaro_sanchez-_gonzalez_vinicius_zambaldi_mateusz_malinowski_andrea_tacchetti_david_raposo_adam_santoro_ryan_faulkner_et_al_relational_inductive_biases_deep_learning_and_graph_networks_ar_xiv_preprint_ar_xiv180601261_2018_armin_biere_and_mathias_fleury_chasing_target_phases_in_workshop_on_the_pragmatics_of_sat_2020_armin_biere_and_mathias_fleury_gimsatul_isa_sat_and_kissat_entering_the_sat_competition_2022_in_tomas_balyo_marijn_heule_markus_iser_matti_j\u00e4rvisalo_and_martin_suda_eds_proc_of_sat_competition_2022_-_solver_and_benchmark_descriptions_volume_b-_2022-_1_of_department_of_computer_science_series_of_publications_b_pp_10-_11_university_of_helsinki_2022_armin_biere_mathias_fleury_and_maximilian_heisinger_cadical_kissat_paracooba_entering_the_sat_competition_2021_2021_url_httpsapisemanticscholarorgcorpus_id_238996423_armin_biere_nils_froleyks_and_wenxi_wang_cadiback_extracting_backbones_with_cadical_in_26th_international_conference_on_theory_and_applications_of_satisfiability_testing_sat_2023_schloss_dagstuhl-_leibniz-_zentrum_f\u00fcr_informatik_2023_deli_chen_yankai_lin_wei_li_peng_li_jie_zhou_and_xu_sun_measuring_and_relieving_the_oversmoothing_problem_for_graph_neural_networks_from_the_topological_view_in_proceedings_of_the_aaai_conference_on_artificial_intelligence_volume_34_pp_3438-_3445_2020_martin_davis_george_logemann_and_donald_loveland_a_machine_program_for_theorem-_proving_communications_of_the_acm_57394-_397_1962_alexey_dosovitskiy_lucas_beyer_alexander_kolesnikov_dirk_weissenborn_xiaohua_zhai_thomas_unterthiner_mostafa_dehghani_matthias_minderer_georg_heigold_sylvain_gelly_et_al_an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale_ar_xiv_preprint_ar_xiv201011929_2020_niklas_e\u00e9n_and_niklas_s\u00f6rensson_an_extensible_sat-_solver_in_international_conference_on_theory_and_applications_of_satisfiability_testing_pp_502-_518_springer_2003_matthias_fey_and_jan_e_lenssen_fast_graph_representation_learning_with_py_torch_geometric_in_iclr_workshop_on_representation_learning_on_graphs_and_manifolds_2019",
      "421_gnn_model_design_the_sizes_of_converted_graphs_representing_practical_sat_formulas_usually_with_millions_of_variables_and_clauses_are_typically_substantial_to_enable_effective_training_within_the_constraints_of_limited_gpu_memory_it_is_essential_for_our_model_to_be_both_compact_and_robust_our_gnn_model_design_is_inspired_by_the_robust_graph_transformer_architecture_graph_trans_wu_et_al_2021_however_in_our_particular_sat_application_context_graph_trans_exhibits_two_limitations_both_arising_from_the_global_self-_attention_mechanism_within_its_transformer_subnet_first_the_mechanism_does_not_explicitly_integrate_the_topological_graph_structure_information_when_determining_attention_scores_however_such_information_is_essential_in_characterizing_a_sat_formula_second_the_global_self-_attention_mechanism_computes_attention_scores_for_all_possible_node_pairs_leading_to_quadratic_memory_complexity_with_respect_to_the_number_of_nodes_in_the_graph_this_is_obviously_infeasible_for_tackling_the_large-_scale_sat_formulas_in_our_task_to_overcome_the_limitations_we_introduce_a_novel_transformer_subnet_that_both_distinctly_harnesses_topological_structure_information_and_significantly_enhances_memory_efficiency_it_combines_graph_self-_attention_gsa_and_local_self-_attention_lsa_replacing_the_original_transformers_global_self-_attention_instead_of_computing_attention_scores_for_all_node_pairs_as_global_self-_attention_gsa_calculates_attention_scores_solely_for_directly_connected_node_pairs_leveraging_information_of_edges_and_edge_weights_this_not_only_explicitly_incorporates_the_topological_structure_information_of_the_graph_but_also_reduces_the_memory_complexity_to_linear_in_terms_of_the_number_of_edges_in_the_graph_additionally_to_further_reduce_the_memory_complexity_lsa_segments_each_node_embedding_into_multiple_node_patches_and_computes_attention_scores_for_each_pair_of_node_patches_the_results_in_a_linear_memory_complexity_in_terms_of_the_number_of_nodes_in_the_graph_figure_3_illustrates_the_design_of_our_gnn_model_architecture_it_consists_of_three_main_components_a_gnn_subnet_with_l_stacked_gnn_blocks_a_transformer_subnet_with_m_gsa_transformer_blocks_and_n_lsa_transformer_blocks_and_a_ffn_layer_for_node_classification_within_each_gnn_block_a_gnn_layer_is_preceded_by_a_normalization_layer_with_a_skip_connection_bridging_the_two_the_transformer_block_is_crafted_to_accelerate_training_on_a_significant_collection_of_large-_scale_graphs_inspired_by_the_recent_vision_transformer_architecture_vi_t-_22b_padlewski_djolonga_2023_each_transformer_block_integrates_a_normalization_layer_succeeded_by_both_an_ffn_layer_and_a_gsalsa_layer_that_operate_concurrently_to_optimize_training_efficiency_422_implementation_the_current_implementation_of_neuro_back_model_utilizes_ginconv_fey_lenssen_2023b_xu_et_al_2018_to_build_the_gnn_layer_for_its_proficiency_in_distinguishing_non-_isomorphic_graph_structures_however_ginconv_lacks_the_capability_to_encode_edge_weight_information_to_address_this_we_employ_three_ginconv_layers_each_corresponding_to_a_distinct_edge_weight_in_our_graph_representation_each_ginconv_layer_exclusively_performs_message_passing_for_edges_with_its_corresponding_weight_the_node_embeddings_from_these_three_ginconv_layers_are_finally_aggregated_as_the_output_of_the_gnn_layer_gatconv_layers_fey_lenssen_2023a_velickovic_et_al_2017_is_utilized_to_built_the_gsa_transformer_block_the_patch_encoder_in_the_vi_t_transformer_dosovitskiy_et_al_2020_is_applied_to_construct_the_lsa_transformer_block_layer_norm_ba_et_al_2016_is_employed_as_our_normalization_layer_to_avoid_potential_over-_smoothing_issues_as_instructed_in_chen_et_al_2020_the_number_of_blocks_in_the_gnn_subnet_is_set_to_the_maximum_diameter_of_the_graph_representation_ie_l_4_to_ensure_the_accuracy_of_our_model_while_taking_into_account_our_limited_gpu_memory_the_number_of_both_gsa_and_lsa_blocks_are_set_to_three_ie_m_3_and_n_3_additionally_ffns_within_the_transformer_blocks_contain_no_hidden_layers_while_the_final_ffn_utilized_for_node_classification_is_structured_to_include_one_hidden_layer_the_model_is_implemented_using_py_torch_paszke_et_al_2019_and_py_torch_geometric_fey_lenssen_2019_423_model_pre-training_and_fine-tuning_the_neuro_back_model_undergoes_a_two-_stage_training_process_initially_it_is_pre-_trained_on_an_extensive_and_diverse_dataset_gathered_from_various_sources_this_pre-_training_equips_it_with_the_fundamental_knowledge_to_classify_the_backbone_variable_phases_across_a_broad_spectrum_of_cnf_formulas_subsequently_this_pre-_trained_model_is_refined_or_fine-_tuned_on_a_smaller_domain-_specific_dataset_this_fine-_tuning_process_enhances_the_models_proficiency_in_classifying_backbone_variable"
    ]
  }
}