# Can Graph Neural Networks Learn to Solve MaxSAT Pr


## Introduction


at least \(|C| / 2\) clauses, which implies an approximation ratio of \(1 / 2\) . ## Frameworks of Models The frameworks of MS- NSFG and MS- ESFG are shown below. The embeddings of literals and clauses in the \(k\) - th layer are denoted as \(L^{(k)}\) and \(C^{(k)}\) , respectively. \(d\) is the dimension of embeddings, and \(T\) is the number of GNN layers. Algorithm 2: MS- NSFG Input: An NSFG \(G = \langle V_{L}, V_{C}, E \rangle\) Output: The predicted assignments of literals \(\Phi\) 1: Build the adjacency matrix \(M\) of graph \(G\) 2: Initialize \(L^{(0)} \in \mathbb{R}^{|V_{L}| \times d} \sim U(0, 1)\) 3: Initialize \(C^{(0)} \in \mathbb{R}^{|V_{C}| \times d} \sim U(0, 1)\) 4: for \(k\) from 1 to \(T\) do 5: \(C^{(k)} \leftarrow \mathrm{UPDC}(C^{(k - 1)}, M \cdot \mathrm{AGG}_{\mathrm{L}}(L^{(k - 1)}))\) 6: \(L^{(k)} \leftarrow \mathrm{UPDL}(L^{(k - 1)}, M^{\top} \cdot \mathrm{AGG}_{\mathrm{C}}(C^{(k - 1)}))\) 7: end for 8: \(L_{pred} \in \mathbb{R}^{|V_{L}|} \leftarrow \mathrm{PRED}_{\mathrm{L}}(L^{(T)})\) 9: \(\Phi \leftarrow \mathrm{Round}(\mathrm{Sigmoid}(L_{pred}))\) 10: return \(\Phi\) Algorithm 3: MS- ESFG Input: An ESFG \(G = \langle V_{L}, V_{C}, E^{+}, E^{- } \rangle\) Output: The predicted assignments of literals \(\Phi\) 1: Build the adjacency matrices \(M^{+}, M^{- }\) of graph \(G\) 2: Initialize \(L^{(0)} \in \mathbb{R}^{|V_{L}| \times d} \sim U(0, 1)\) 3: Initialize \(C^{(0)} \in \mathbb{R}^{|V_{C}| \times d} \sim U(0, 1)\) 4: for \(k\) from 1 to \(T\) do 5: \(C^{(k)} \leftarrow \mathrm{UPDC}(C^{(k - 1)}, M^{+} \cdot \mathrm{AGG}_{\mathrm{L}}^{+}(L^{(k - 1)}) + M^{- } \cdot \mathrm{AGG}_{\mathrm{L}}^{-}(L^{(k - 1)}))\) 6: \(L^{(k)} \leftarrow \mathrm{UPDL}(L^{(k - 1)}, (M^{+})^{\top} \cdot \mathrm{AGG}_{\mathrm{C}}^{+}(C^{(k - 1)}) + (M^{- })^{\top} \cdot \mathrm{AGG}_{\mathrm{C}}^{-}(C^{(k - 1)}))\) 7: end for 8: \(L_{pred} \in \mathbb{R}^{|V_{L}|} \leftarrow \mathrm{PRED}_{\mathrm{L}}(L^{(T)})\) 9: \(\Phi \leftarrow \mathrm{Round}(\mathrm{Sigmoid}(L_{pred}))\) 10: return \(\Phi\)

literals) is in the complexity class P, the Max2SAT problem is still NP- hard (Garey, Johnson, and Stockmeyer 1976), so even generating near- optimal solution for Max2SAT is not trivial. Moreover, despite GNNs have shown their effectiveness on SAT problem through experiments, the theoretical analysis about why they can work is still absent so far. Recently, there have been some theoretical research about learning to solve combinatorial problems with GNNs. (Xu et al. 2019) proves that the discriminative power of many popular GNN variants, such as Graph Convolutional Networks (GCN) and Graph SAGE, is not sufficient for the graph isomorphism problem. (Xu et al. 2020) proposes the algorithmic alignment theory to explain what reasoning tasks GNNs can learn well, and shows GNNs can align with dynamic programming algorithms. So, many classical reasoning tasks could be solved relatively well with a GNN model, such as visual question answering and shortest paths. However, it also raises an issue: as a polynomial- time procedure, any GNN model cannot align with an exact algorithm for NP- hard problems unless \(\mathrm{P} = \mathrm{NP}\) , whereas some approximation algorithms may be used to analyze the capability of GNNs to solve such problems. (Sato, Yamada, and Kashima 2019) has employed a class of distributed local algorithms to prove that GNNs can learn to solve some combinatorial optimization problems, such as minimum dominating set and minimum vertex cover, with some approximation ratios. These theoretical contributions provide a basis for us to further study the capability of GNNs on the Max SAT problem. In this paper, we investigate the capability of GNNs in learning to solve Max SAT problem both from theoretical and practical perspectives. Inspired from the algorithmic alignment theory, we design a distributed local algorithm for Max SAT, which is guaranteed to have an approximation ratio of \(1 / 2\) . As the algorithm aligns well with a single- layer GNN, it can provide a theoretical explanation of the effect that a simple GNN can learn to solve Max SAT problem to some extent. More importantly, for the first time, this work leverages GNNs to learn the solution of Max SAT problem. Specifically, we build two typical GNN models, MS- NSFG and MS- ESFG, based on summarizing the common forms proposed for SAT, to investigate the capability of GNNs from the practical perspective. We have trained and tested both models on several random Max SAT datasets in different settings. The experimental results demonstrate that both models have achieved pretty high accuracy on the testing sets, as well as the satisfactory generalization to larger and more difficult problems. This implies that GNNs are expected to be promising alternatives to improve the state- of- the- art solvers. We are hopeful that the results in this paper can provide preliminary knowledge about the capability of GNNs to solve Max SAT problem, and become a basis for more theoretical and practical research in the future. The contributions can be summarized as follows: - We build two typical GNN models to solve Max SAT problem, which is the first work to predict the solution of Max SAT problem with GNNs in an end-to-end fashion. <center>Figure 1: The general framework of applying GNNs to solve SAT and Max SAT problems. The CNF formula is firstly transformed to a bipartite factor graph, then each node is assigned an initial embedding. The embeddings are updated iteratively through the message passing process. Finally, a classifier decodes the assignment of each variable from its embedding. </center> - We analyze the capability of GNNs to solve Max SAT problem theoretically, and prove that even a single-layer GNN can achieve the approximation ratio of \(1 / 2\) .- We evaluate the GNN models on several datasets of random Max SAT instances with different distributions, and show that GNNs can achieve good performance and generalization on this task. ## Preliminaries In this section, we firstly give the definition of SAT and Max SAT problems, and then give an introduction to graph neural networks. ## SAT and Max SAT Given a set of Boolean variables \(\{x_{1}, x_{2}, \ldots , x_{n}\}\) , a literal is either a variable \(x_{i}\) or its negation \(\neg x_{i}\) . A clause is a disjunction of literals, such as \(C_{1} \coloneqq x_{1} \vee \neg x_{2} \vee x_{3}\) . A propositional logic formula in conjunctive normal form (CNF) is a conjunction of clauses, such as \(F \coloneqq C_{1} \wedge C_{2} \wedge C_{3}\) . For a CNF formula \(F\) , the Boolean Satisfiability (SAT) problem is to find a truth assignment for each variable, such that all the clauses are satisfied (i.e., at least one literal in each clause is True). And the Maximum Satisfiability (Max SAT) problem is to find a truth assignment for each variable, such that the number of satisfied clauses is maximized. SAT and Max SAT have long been proved to be NP- hard, which means there is no polynomial- time algorithm to solve them unless \(\mathrm{P} = \mathrm{NP}\) . Specifically, if every clause in \(F\) contains exactly \(k\) literals, the problem is called '\(k\) SAT' or 'Max\(k\) SAT'. There is a close relationship between Max SAT and SAT. Firstly, Max SAT can be seen as an optimization version of SAT, which can provide more information when handling over- constrained problems. Besides, many solving techniques that are effective in SAT have also been adapted

to Max SAT, such as lazy data structures and variable selection heuristics. Accordingly, SAT can also benefit from the progress of Max SAT. ## Graph Neural Networks Graph neural networks (GNNs) are a family of neural network architectures that operate on graphs, which have shown great power on many tasks across domains, such as protein interface prediction, recommendation systems and traffic prediction (Zhou et al. 2020). For a graph \(G = \langle V,E\rangle\) where \(V\) is a set of nodes and \(E\subseteq V\times V\) is a set of edges, GNN models accept \(G\) as input, and represent each node \(v\) as an embedding vector \(h_v^{(0)}\) . Most popular GNN models follow the message passing process that updates the embedding of a node by aggregating the information of its neighbors iteratively. The operation of the \(k\) - th iteration (layer) of GNNs can be formalized as \[\begin{array}{r l} & {s_{v}^{(k)} = \mathrm{AGG}^{(k)}(\{h_{v}^{(k - 1)}|u\in \mathcal{N}(v)\} ,}\\ & {h_{v}^{(k)} = \mathrm{UPD}^{(k)}(h_{v}^{(k - 1)},s_{v}^{(k)}),} \end{array} \quad (1)\] where \(h_v^{(k)}\) is the embedding vector of node \(v\) after the \(k\) - th iteration. In the aggregating (messaging) step, a message \(s_v^{(k)}\) is generated for each node \(v\) by collecting the embeddings from its neighbors \(\mathcal{N}(v)\) . Next, in the updating (combining) step, the embedding of each node is updated combined with the message generated above. After \(T\) iterations, the final embedding \(h_v^{(T)}\) for each node \(v\) is obtained. If the task GNNs need to handle is at the graph level such as graph classification, an embedding vector of the entire graph should be generated from that of all the nodes: \[h_{G} = \mathrm{READOUT}(\{h_{u}^{(T)}|u\in V\}). \quad (2)\] The final embeddings can be decoded into the outputs by a learnable function such as multi- layer perceptron (MLP), whether for node- level or graph- level tasks. The modern GNN variants have made different choices of aggregating, updating and readout functions, such as concatenation, summation, max- pooling and mean- pooling. A more comprehensive review of GNNs can be found in (Wu et al. 2021). ## GNN Models for Max SAT As described in Section 1, there have been several attempts that learn to solve SAT problem with GNNs in recent years. <center>Figure 2: Two kinds of factor graphs to represent the CNF formula \((x_{1}\lor x_{2}\lor x_{3})\land (x_{1}\lor \neg x_{3})\land (\neg x_{1}\lor \neg x_{2}\lor \neg x_{3})\) with 3 variables and 3 clauses. </center> The pipeline of such work generally consists of three parts. Firstly, a CNF formula is transformed to a graph through some rules. Next, a GNN variant is employed that maps the graph representation to the labels, e.g., the satisfiability of a problem or the assignment of a variable. Finally, the prediction results are further analyzed and utilized after the training converges. Intuitively, they can almost be transfered to work on Max SAT problem without much modification, since these two problems have the same form of input. Factor graph is a common representation for CNF formulas, which is a bipartite structure to represent the relationship between literals and clauses. There are mainly two kinds of factor graphs that have appeared in the previous work. The first one is node- splitting factor graph (NSFG), which splits the two literals \((x_{i},\neg x_{i})\) corresponding to a variable \(x_{i}\) into two nodes. NSFG is used by many work such as (Selsam et al. 2019) and (Zhang et al. 2020). The other one is edge- splitting factor graph (ESFG), which establishes two types of edges, connected the clauses with positive and negative literals separately. (Yolcu and Póczos 2019) uses ESFG with a pair of biadjacency matrices. An example that represents a CNF formula with these two kinds of factor graphs is illustrated in Figure 2. The models generally follow the message passing process of GNNs. Considering the bipartite structure of graph, in each layer the process is divided in two directions executed in sequence. For NSFG, the operation of the \(k\) - th layer of GNNs can be formalized as \[\begin{array}{r l} & {C_{j}^{(k)} = \mathrm{UPD}_{C}(C_{j}^{(k - 1)},\mathrm{AGG}_{L}(L_{i}^{(k - 1)}|(i,j)\in E)),}\\ & {(L_{i}^{(k)},\widetilde{L}_{i}^{(k)}) = \mathrm{UPD}_{L}(L_{i}^{(k - 1)},\widetilde{L}_{i}^{(k - 1)},}\\ & {\qquad \mathrm{AGG}_{C}(C_{j}^{(k - 1)}|(i,j)\in E)),} \end{array} \quad (3)\] where \(L_{i}^{(k)},C_{j}^{(k)}\) is the embedding of literal \(i\) and clause \(j\) in the \(k\) - th layer. The message is firstly collected from the literals in clause \(j\) , and the embedding of \(j\) is updated by that of the last layer and the message. Next, the embedding of literal \(i\) is updated in almost the same way, except that the embedding of literal \(i\) is updated with that of its negation together (denoted by \(\widetilde{L}_{i}^{(k)}\) ). This operation maintains the consistency of positive and negative literals of the same variable. For ESFG, there are two types of edges, which can be denoted as \(E^{+}\) and \(E^{- }\) . If a positive literal \(i_{1}\) appears in clause \(j\) , we have \((i_{1},j)\in E^{+}\) , and similarly have \((i_{2},j)\in E^{- }\) if \(i_{2}\) is a negative literal. The operation of the \(k\) - th layer of GNNs can be formalized as \[\begin{array}{r l} & {C_{j}^{(k)} = \mathrm{UPD}_{C}(C_{j}^{(k - 1)},\mathrm{AGG}_{L}^{+}(L_{i}^{(k - 1)}|(i,j)\in E^{+}),}\\ & {\qquad \mathrm{AGG}_{L}^{-}(L_{i}^{(k - 1)}|(i,j)\in E^{- })),}\\ & {L_{i}^{(k)} = \mathrm{UPD}_{L}(L_{i}^{(k - 1)},\mathrm{AGG}_{C}^{+}(C_{j}^{(k - 1)}|(i,j)\in E^{+}),}\\ & {\qquad \mathrm{AGG}_{C}^{-}(C_{j}^{(k - 1)}|(i,j)\in E^{- })).} \end{array} \quad (4)\] Finally, for both NSFG and ESFG, a binary classifier is applied to map the embedding of each variable to its assignment. We use the binary cross entropy (BCE) as loss function, which can be written as \[B C E(y,p) = -(y\log (p) + (1 - y)\log (1 - p)), \quad (5)\]

where \(p \in [0,1]\) is the predicted probability of a variable being assigned True, and \(y\) is the binary label from an optimal solution. By averaging the loss of each variable, we obtain the loss of a problem instance, which should be minimized. We will investigate the performance of these two models through experiments in Section 5. ## Theoretical Analysis With the results and prospects revealed in practice, GNNs are considered to be a suitable choice in learning to solve SAT problem from benchmarks. However, the knowledge about why these models would work from a theoretical perspective is still limited so far. In this section, we are committed to explaining the possible mechanism of GNNs to solve Max SAT problem. More specifically, we prove that a single- layer GNN can achieve an approximation ratio of \(1 / 2\) with the help of a distributed local algorithm. ## Algorithmic Alignment Our theoretical analysis framework is inspired by the algorithmic alignment theory proposed by (Xu et al. 2020), which provides a new point of view to understand the reasoning capability of neural networks. Formally, suppose a neural network \(\mathcal{N}\) with \(n\) modules \(\mathcal{N}_i\) , if by replacing each \(\mathcal{N}_i\) with a function \(f_i\) it can simulate, and \(f_1, \ldots , f_n\) generate a reasoning function \(g\) , we say \(\mathcal{N}\) aligns with \(g\) . As shown in that paper, even if different models such as MLPs, deep sets and GNNs have the same expressive power theoretically, GNNs tend to achieve better performance and generalization stably in the experiments. This can be explained by the fact that the computational structure of GNNs aligns well with the dynamic programming (DP) algorithms. Therefore, compared with other models, each component of GNNs only needs to approximate a simpler function, which means the algorithmic alignment can improve the sample complexity. ## Distributed Local Algorithm According to the algorithmic alignment theory, it is possible to reasonably infer the capability that a GNN model can achieve with the help of an algorithm aligned with its computational structure. However, this also raises an issue: as a polynomial- time procedure, any GNN model cannot align with an exact algorithm for NP- hard problems such as Max SAT, under the assumption that \(\mathrm{P} \neq \mathrm{NP}\) . In this case, we turn to employ a weaker approximation algorithm to analyze the capability of GNNs. Although there has been a lot of research on the approximation algorithms of Max SAT (Vazirani 2001), it is regrettable that most of them cannot align with the structure of GNNs in an intuitive way. In fact, there is a class of algorithms called distributed local algorithm (DLA) (Elkin 2004), which have been found to be a good choice to align with GNNs for combinatorial problems. DLA assumes a distributed computing system that there are a set of nodes in a graph, where any two nodes do not know the existence of each other at first. The algorithm then runs in a constant number of synchronous communication rounds. In each round, a node performs local computation, and sends one message to its neighboring nodes, while Algorithm 1: A distributed local algorithm for Max SAT Input: The set of literals \(L\) , the set of clauses \(C\) Output: The assignments of literals \(\Phi\) 1: Set up a factor graph such as NSFG for \(L\) and \(C\) . 2: \(W(L_i) \leftarrow 0\) for each \(L_i \in L\) . 3: \(S(C_j) \leftarrow \{\}\) for each \(C_j \in C\) . 4: \(S(C_j) \leftarrow S(C_j) \cup \{L_i\}\) for each edge \((L_i, C_j)\) in the factor graph. 5: for each \(C_j \in C\) do 6: \(L^* \leftarrow\) Pick a literal from \(S(C_j)\) . 7: \(W(L^*) \leftarrow W(L^*) + 1\) . 8: end for 9: for each \(L_i \in L\) do 10: if \(W(L_i) > = W(\bar{L}_i)\) then 11: \(\Phi (L_i) \leftarrow \mathrm{True}\) . 12: else 13: \(\Phi (L_i) \leftarrow \mathrm{False}\) . 14: end if 15: end for 16: return \(\Phi\) receiving one from each of them. Finally, each node computes the output in terms of the information it holds. DLAs have been widely used in many applications, such as designing sublinear- time algorithms (Parnas and Ron 2007) and controlling wireless sensor networks (Kubisch et al. 2003). ## Analyzing GNNs for Max SAT Most DLAs are designed for solving combinatorial problems in graph theory. (Sato, Yamada, and Kashima 2019) has employed DLAs to clarify the approximation ratios of GNNs for several NP- hard problems on graphs such as minimum dominating set and minimum vertex cover. This also encourages us to analyze the capability of GNNs to solve Max SAT problem with the help of a DLA. As the DLA for Max SAT has not been studied in the literature, we design an algorithm that aligns well with the message passing process described in Section 3, and the pseudo code is shown in Algorithm 1. This algorithm accepts the description of a Max SAT problem instance as input, and returns the assignment of literals, while the value of objective (i.e., the number of satisfied clauses) is easy to compute from it. Following the rules of DLA, the literals and clauses do not have any information about the problem initially. \(W\) and \(S\) correspond to the states of literals and clauses, respectively. In line 4, the message is passed from literals to clauses, so that each clause is aware of the literals it contains, and stores the information into \(S\) . Next, in lines 5- 8, the message is generated by clauses and sent to literals. Finally, the assignment of each literal is decoded from \(W\) by a greedy- like method. It can be seen that the algorithm only carries out one round of communication. It is interesting to know whether a single- layer GNN may also have some capabilities to solve the Max SAT problem theoretically. This can be analyzed by aligning it with the algorithm. Theorem 1. There exists a single- layer GNN to solve the

Max SAT problem, which is guaranteed to have an approximation ratio of \(1 / 2\) . Proof. We give a proof sketch here because of the space limitation, and the full proof is available in Appendix. Firstly, we show that there exists a single- layer GNN model that can align with the proposed distributed local algorithm (Algorithm 1), so that every part of the algorithm can be effectively approximated by a component of GNN. This implies that the GNN model can achieve the same performance with the algorithm when solving Max SAT problem. Next, we prove that it is a \(1 / 2\) - approximation algorithm. The picking operation of literal (line 6) is equivalent to transforming the original problem into another, where each clause only contains one literal. Given a solution, the number of satisfied clauses of the original problem is at least as large as that of the new one, so we get the approximation ratio by analyzing the new Max1SAT problem. It is not hard to find that the algorithm has a guarantee that at least half of the clauses are satisfied for any Max1SAT instance. \(\square\) The proposed DLA can be considered as a candidate explanation for why a single- layer GNN model works and generalizes on the Max SAT instances. The methodologies and results may also serve as a basis for future work to make theoretical progress on this task, such as explaining the improvement of capability as the number of GNN layers increases in the experiments and finding a tighter bound. ## Experimental Evaluation Although the GNN models have shown their capability on many reasoning tasks such as SAT problem, the experimental evidence of whether GNNs can learn to solve Max SAT problem is still under exploration. In order to demonstrate the capability of GNNs on this task, we firstly build two models, using NSFG and ESFG separately. After constructing the datasets with a commonly used generator for random Max SAT instances, the GNN models are trained and tested in different settings, so that we can obtain a comprehensive understanding of their capabilities. The experimental results show that GNNs have attractive potential in learning to solve Max SAT problem with good performance and generalization. ## Building the Models In Section 3, we have described the general GNN frameworks that can learn to solve Max SAT problem. Based on the successful early attempts, we build two GNN models that accept a CNF formula as input, and output the assignment of literals. We abbreviate them to MS- NSFG and MS- ESFG, because they use NSFG and ESFG as the graph representation, respectively. The structures of models follow those in Eq. (3) and (4). Here we only describe the implementation of some key components, and the complete frameworks are shown in Appendix. Aggregating functions. The implementation of aggregating functions: \(\mathrm{AGG_L}\) , \(\mathrm{AGG_C}\) in MS- NSFG, and \(\mathrm{AGG_L}^+\) , \(\mathrm{AGG_L}^-\) , \(\mathrm{AGG_C}^+\) , \(\mathrm{AGG_C}^-\) in MS- ESFG, consists of two steps. First, an MLP module maps the embedding of each node to a message vector. After that, the messages from relevant nodes are summed up to get the final output. For example, the function \(\mathrm{AGG_L}\) in MS- NSFG which generates the message from literals and sends it to clause \(j\) , can be formalized as \(\sum_{(i,j)\in E} \mathrm{MLP}(L_i)\) , where \(L_i\) is the embedding of literal \(i\) . Updating functions. The updating functions: \(\mathrm{UPD_L}\) and \(\mathrm{UPD_C}\) in both MS- NSFG and MS- ESFG can be implemented by the LSTM module (Hochreiter and Schmidhuber 1997). For example, to implement the function \(\mathrm{UPD_C}\) , the embedding of clause \(j\) (denoted as \(C_j\) ) is taken as the hidden state, and the message generated from aggregation is taken as the input of LSTM. ## Data Generation For data- driven approaches, a large number of labeled instances are necessary. So, the dataset should be easy to solve by off- the- shelf Max SAT solvers to ensure the size of dataset, meanwhile it should also have the ability to produce larger instances in the same distribution to test the generalization. As a result, we construct the datasets of random Max SAT instances by running a generator proposed by (Mitchell, Selman, and Levesque 1992), which is also used to provide benchmarks for the random track of Max SAT competitions<sup>1</sup>. The generator can produce CNF formulas with three parameters: the number of literals in each clause \(k\) , the number of variables \(n\) , and the number of clauses \(m\) . In order to better observe the performance of GNN models, we generate multiple datasets with different distributions, which are listed in Table 1. For R2 (60, 600), R2 (60, 800) and R3 (30, 300), we generate 20K instances, and divide them into training set, validation set and testing set according to the ratio of 8:1:1. For R2 (80, 800) and R3 (50, 500), we only generate 2K instances as testing sets. Then we call Max HS (Bacchus 2020), a state- of- the- art Max SAT solver, to find an optimal solution for each instance as the labels of variables. The average time Max HS spends to solve the instances in each dataset is also presented, so that we can know about their difficulty. Table 1: The parameters and difficulty of the datasets. <table><tr><td>Dataset</td><td>k</td><td>n</td><td>m</td><td>Time(s)</td></tr><tr><td>R2 (60, 600)</td><td>2</td><td>60</td><td>600</td><td>4.01</td></tr><tr><td>R2 (60, 800)</td><td>2</td><td>60</td><td>800</td><td>18.21</td></tr><tr><td>R2 (80, 800)</td><td>2</td><td>80</td><td>800</td><td>105.19</td></tr><tr><td>R3 (30, 300)</td><td>3</td><td>30</td><td>300</td><td>1.92</td></tr><tr><td>R3 (50, 500)</td><td>3</td><td>50</td><td>500</td><td>216.26</td></tr></table> ## Implementation Details We implement the models MS- NSFG and MS- ESFG in Python, and examine their practical performance to solve Max SAT problem<sup>2</sup>. The models are trained by the Adam optimizer (Kingma and Ba 2015). All the experiments are run

ning on a machine with Intel Core i7- 8700 CPU (3.20GHz) and NVIDIA Tesla V100 GPU. For reproducibility, we also summarize the setting of hyper- parameters as follows. In our configuration, the dimension of embeddings and messages \(d = 128\) , and the learning rate is \(2 \times 10^{- 5}\) with a weight decay of \(10^{- 10}\) . Unless otherwise specified, the number of GNN layers \(T = 20\) . The instances are fed into models in batches, with each batch containing 20K nodes. ## Accuracy of Models We firstly evaluate the performance of both GNN models, including their convergence and the quality of predicted solution. The accuracy of prediction is reported with two values in the following paragraphs. The first one is the gap to optimal objective, which is the distance between the predicted objective and the corresponding optimal value. The predicted objective can be computed by counting the satisfied clauses given the predict solution. The other is the accuracy of assignments, which is the percentage of correctly classified variables, i.e., the assignment of a variable from the predicted solution is the same as its label. Generally, the gap to optimal objective could be a better indicator, because the optimal solution of a Max SAT problem may not be unique. We have trained MS- NSFG and MS- ESFG separately on three different datasets: R2 (60, 600), R2 (60, 800) and R3 (30, 300), and illustrate the evolution curves of accuracy throughout training on R2 (60, 600) in Figure 3 as an example. All the models can converge within 150 epochs, and achieve pretty good performance. The average gaps to optimal objectives are less than 2 clauses for all the models, with the approximation ratio \(>99.5\%\) . Besides, the accuracy of assignments is around \(92\%\) (Max2SAT) and \(83\%\) (Max3SAT). There is no significant difference between the two models, while MS- ESFG performs slightly better than MS- NSFG. The experimental results show that both GNN models can be used in learning to solve Max SAT problem. <center>Figure 3: The evolution of accuracy of MS-NSFG and MS-ESFG during a training process of 150 epochs on the dataset R2 (60, 600). </center> ## Influence of GNN Layers According to the theoretical analysis, the number of GNN layers determines the information that each node can receive from its neighborhood, thereby affecting the accuracy of prediction. We examine this phenomenon from the experimental perspective by training MS- NSFG and MS- ESFG on the three datasets separately, with different hyper- parameter \(T\) from 1 to 30. The changes of accuracy on testing sets are illustrated in Figure 4. It can be seen that when \(T = 1\) , the capabilities of both models are weak, where the predicted objective is far from the optimal value, and the accuracy of assignments is not ideal. However, the effectiveness of both models has been significantly improved when \(T = 5\) . If we increase \(T\) to 20, the number of clauses satisfied by the predicted solution is only one less than the optimal value on average. We have continued to increase \(T\) to 60, but no obvious improvement occurred. This indicates that increasing the number of layers - within an appropriate range - will improve the capability of GNN models. <center>Figure 4: The accuracy of MS-NSFG and MS-ESFG trained with different number of GNN layers. </center> ## Generalizing to Other Distributions Generalization is an important factor in evaluating the possibility to apply GNN models to solve those harder problems. We make the predictions by MS- NSFG and MS- ESFG on the testing sets with different distributions from the training sets, and the results are shown in Table 1 and 2, respectively. The first column is the dataset used for training the model, and the first row is the testing set. For each pair of training and testing datasets, we report the gap to optimal objective together with the approximation ratio in the brackets above, and the accuracy of assignments below. Here, the approximation ratio is defined as the ratio of predicted and optimal objectives. Here we mainly focus on three kinds of generalization. The first is generalizing to the datasets with different clause- variable proportion. From the results, both models trained on R2 (60, 600) and tested on R2 (60, 800) (or

Table 2: The accuracy of MS-NSFG on different combinations of training and testing sets. <table><tr><td>Train \ Test</td><td>R2 (60, 600)</td><td>R2 (60, 600)</td><td>R2 (60, 800)</td><td>R3 (30, 300)</td><td>R2 (80, 800)</td><td>R3 (50, 500)</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.86 (99.8%)</td><td>1.42 (99.7%)</td><td>3.86 (98.6%)</td><td>1.15 (99.8%)</td><td>6.77 (98.5%)</td><td></td></tr><tr><td>91.9%</td><td>91.9%</td><td>76.1%</td><td>92.0%</td><td>75.4%</td><td></td></tr><tr><td rowspan="2">R3 (30, 300)</td><td>1.19 (99.7%)</td><td>1.17 (99.8%)</td><td>4.59 (98.4%)</td><td>1.50 (99.7%)</td><td>7.96 (98.3%)</td><td></td></tr><tr><td>91.7%</td><td>92.4%</td><td>75.8%</td><td>91.6%</td><td>75.1%</td><td></td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>20.58 (96.0%)</td><td>29.10 (95.7%)</td><td>1.37 (99.5%)</td><td>27.92 (95.9%)</td><td>2.16 (99.5%)</td><td></td></tr><tr><td>78.1%</td><td>76.0%</td><td>83.3%</td><td>78.0%</td><td>82.2%</td><td></td></tr></table> Table 3: The accuracy of MS-ESFG on different combinations of training and testing sets. <table><tr><td>Train \ Test</td><td>R2 (60, 600)</td><td>R2 (60, 800)</td><td>R3 (30, 300)</td><td>R2 (80, 800)</td><td>R3 (50, 500)</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.54 (99.8%)</td><td>1.12 (99.8%)</td><td>5.98 (97.9%)</td><td>0.69 (99.9%)</td><td>9.92 (97.9%)</td></tr><tr><td>92.3%</td><td>92.2%</td><td>74.5%</td><td>92.2%</td><td>73.9%</td></tr><tr><td rowspan="2">R2 (60, 800)</td><td>0.48 (99.9%)</td><td>0.78 (99.8%)</td><td>6.13 (97.8%)</td><td>0.62 (99.9%)</td><td>10.24 (97.8%)</td></tr><tr><td>92.3%</td><td>92.8%</td><td>74.7%</td><td>92.2%</td><td>74.2%</td></tr><tr><td rowspan="2">R3 (30, 300)</td><td>14.83 (97.1%)</td><td>16.44 (97.5%)</td><td>1.32 (99.5%)</td><td>20.22 (97.0%)</td><td>1.98 (99.5%)</td></tr><tr><td>78.8%</td><td>79.5%</td><td>83.5%</td><td>78.6%</td><td>82.5%</td></tr></table> vice versa) can maintain almost the same prediction accuracy. The second is generalizing to larger and more difficult problems. We use two testing sets, R2 (80, 800) and R3 (50, 500), where the number of variables is larger than those appeared in the training sets, as well as more difficult to solve by Max HS. It can be found that both models trained on Max2SAT datasets can generalize to work on R2 (80, 800) with a satisfactory accuracy. This also holds when the training set is R3 (30, 300) and the testing set is R3 (50, 500), which implies that GNN models are expected to be promising alternatives to help solve those difficult Max SAT problems. The last is generalizing to other datasets with different parameter \(k\) . For example, the model is trained on Max2SAT but tested on Max3SAT problems. The results show that both models have limitations under this condition, since they cannot achieve an accuracy of assignments \(>80\%\) on every pair of training and testing sets, and the gap to optimal objective is not close enough, especially when trained on R3 (30, 300) and tested on Max2SAT datasets. ## Related Work Although the mainstream approaches for solving combinatorial problems, not limited to SAT or Max SAT, are based on the search algorithms from symbolism, there have always been attempts trying to tackle these problems through data- driven techniques. A class of research is to integrate machine learning model in the traditional search framework, which has made progress on a number of problems such as mixed integer programming (MIP) (Khalil et al. 2016), satisfiability modulo theories (SMT) (Balunovic, Bielik, and Vechev 2018) and quantified boolean formulas (QBF) (Lederman et al. 2020). Here, we work on building end- to- end models which do not need the aid of search algorithm. The earliest research work can be traced back to the Hopfield network (Hopfield and Tank 1985) to solve TSP problem. Recently, many variants of neural networks have been proposed, which directly learn to solve combinatorial problems. (Vinyals, Fortunato, and Jaitly 2015) introduces Pointer Net, a sequential model that performs well on solving TSP and convex hull problems. (Khalil et al. 2017) uses the com bination of reinforcement learning and graph embedding, and learns greedy- like strategies for minimum vertex cover, maximum cut and TSP problems. With the development of graph neural networks, there have been a series of work that uses GNN models to solve combinatorial problems. An important reason is that many of such problems are directly defined on graph, and in general the relation between variables and constraints can be naturally represented as a bipartite graph. Except for the mentioned Neuro SAT (Selsam et al. 2019) and its improvement, there have been some efforts learning to solve TSP (Prates et al. 2019), pseudo- Boolean (Liu et al. 2020) and graph coloring (Lemos et al. 2019) problems with GNN- based models. The results indicate that GNNs have become increasingly appealing alternatives in solving combinatorial problems. Moreover, there are also some well- organized literature reviews on this subject, such as (Bengio, Lodi, and Prouvost 2021), (Cappart et al. 2021) and (Lamb et al. 2020). ## Conclusion and Future Work Graph neural networks (GNNs) have been considered as a promising technique that can learn to solve combinatorial problems in the data- driven fashion, especially for the Boolean Satisfiability (SAT) problem. In this paper, we further study the quality of solution predicted by GNNs in learning to solve Maximum Satisfiability (Max SAT) problem, both from theoretical and practical perspectives. Based on the graph construction methods in the previous work, we build two kinds of GNN models, MS- NSFG and MS- ESFG, which can predict the solution of Max SAT problem. The models are trained and tested on randomly generated benchmarks with different distributions. The experimental results show that both models have achieved pretty high accuracy, and also satisfactory generalization to larger and more difficult instances. In addition, this paper is the first that attempts to present an explanation of the capability of GNNs to solve Max SAT problem from a theoretical point of view. On the basis of algorithmic alignment theory, we prove that even a single- layer GNN model can solve the Max SAT problem with an approximation ratio of \(1 / 2\) . We hope the results in this paper can inspire future work from multiple perspectives. A promising direction is to integrate the GNN models into a powerful search framework to handle more difficult problems in the specific domain, such as weighted and partial Max SAT problems. It is also interesting to further analyze the theoretical capability of multilayer GNNs to achieve better approximation. ## References Ansótegui, C.; Bonet, M. L.; and Levy, J. 2013. SAT- based Max SAT algorithms. Artif. Intell., 196: 77- 105. Bacchus, F. 2020. Max HS in the 2020 Max Sat Evaluation. Max SAT Evaluation 2020, 19. Balunovic, M.; Bielik, P.; and Vechev, M. T. 2018. Learning to Solve SMT Formulas. In Bengio, S.; Wallach, H. M.; Larochelle, H.; Grauman, K.; Cesa- Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing

Systems 31: Annual Conference on Neural Information Processing Systems 2018, Neur IPS 2018, December 3- 8, 2018, Montréal, Canada, 10338- 10349. Bengio, Y.; Lodi, A.; and Prouvost, A. 2021. Machine learning for combinatorial optimization: A methodological tour d'horizon. Eur. J. Oper. Res., 290(2): 405- 421. Biere, A.; Heule, M.; van Maaren, H.; and Walsh, T., eds. 2009. Handbook of Satisfiability, volume 185 of Frontiers in Artificial Intelligence and Applications. IOS Press. ISBN 978- 1- 58603- 929- 5. Cai, S.; and Zhang, X. 2021. Deep Cooperation of CDCL and Local Search for SAT. In Li, C.; and Manyà, F., eds., Theory and Applications of Satisfiability Testing - SAT 2021 - 24th International Conference, Barcelona, Spain, July 5- 9, 2021, Proceedings, volume 12831 of Lecture Notes in Computer Science, 64- 81. Springer. Cameron, C.; Chen, R.; Hartford, J. S.; and Leyton- Brown, K. 2020. Predicting Propositional Satisfiability via End- to- End Learning. In The Thirty- Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty- Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7- 12, 2020, 3324- 3331. AAAI Press. Cappart, Q.; Chetelat, D.; Khalil, E. B.; Lodi, A.; Morris, C.; and Velickovic, P. 2021. Combinatorial Optimization and Reasoning with Graph Neural Networks. In Zhou, Z., ed., Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19- 27 August 2021, 4348- 4355. ijcai.org. Cook, S. A. 1971. The Complexity of Theorem- Proving Procedures. In Harrison, M. A.; Banerji, R. B.; and Ullman, J. D., eds., Proceedings of the 3rd Annual ACM Symposium on Theory of Computing, May 3- 5, 1971, Shaker Heights, Ohio, USA, 151- 158. ACM. Elkin, M. 2004. Distributed approximation: a survey. SIGACT News, 35(4): 40- 57. Garey, M. R.; Johnson, D. S.; and Stockmeyer, L. J. 1976. Some Simplified NP- Complete Graph Problems. Theor. Comput. Sci., 1(3): 237- 267. Hochreiter, S.; and Schmidhuber, J. 1997. Long Short- Term Memory. Neural Comput., 9(8): 1735- 1780. Hopfield, J. J.; and Tank, D. W. 1985. "Neural" computation of decisions in optimization problems. Biological cybernetics, 52(3): 141- 152. Khalil, E. B.; Bodic, P. L.; Song, L.; Nemhauser, G. L.; and Dilkina, B. 2016. Learning to Branch in Mixed Integer Programming. In Schuurmans, D.; and Wellman, M. P., eds., Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12- 17, 2016, Phoenix, Arizona, USA, 724- 731. AAAI Press. Khalil, E. B.; Dai, H.; Zhang, Y.; Dilkina, B.; and Song, L. 2017. Learning Combinatorial Optimization Algorithms over Graphs. In Guyon, I.; von Luxburg, U.; Bengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S. V. N.; and Garnett, R., eds., Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4- 9, 2017, Long Beach, CA, USA, 6348- 6358. Kingma, D. P.; and Ba, J. 2015. Adam: A Method for Stochastic Optimization. In Bengio, Y.; and Le Cun, Y., eds., 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7- 9, 2015, Conference Track Proceedings. Kubisch, M.; Karl, H.; Wolisz, A.; Zhong, L. C.; and Rabaey, J. M. 2003. Distributed algorithms for transmission power control in wireless sensor networks. In 2003 IEEE Wireless Communications and Networking, WCNC 2003, New Orleans, LA, USA, 16- 20 March, 2003, 558- 563. IEEE. Lamb, L. C.; d'Avila Garcez, A. S.; Gori, M.; Prates, M. O. R.; Avelar, P. H. C.; and Vardi, M. Y. 2020. Graph Neural Networks Meet Neural- Symbolic Computing: A Survey and Perspective. In Bessiere, C., ed., Proceedings of the Twenty- Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, 4877- 4884. ijcai.org. Lederman, G.; Rabe, M. N.; Seshia, S.; and Lee, E. A. 2020. Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26- 30, 2020. Open Review.net. Lemos, H.; Prates, M. O. R.; Avelar, P. H. C.; and Lamb, L. C. 2019. Graph Colouring Meets Deep Learning: Effective Graph Neural Network Models for Combinatorial Problems. In 31st IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2019, Portland, OR, USA, November 4- 6, 2019, 879- 885. IEEE. Liu, M.; Zhang, F.; Huang, P.; Niu, S.; Ma, F.; and Zhang, J. 2020. Learning the Satisfiability of Pseudo- Boolean Problem with Graph Neural Networks. In Simonis, H., ed., Principles and Practice of Constraint Programming - 26th International Conference, CP 2020, Louvain- la- Neuve, Belgium, September 7- 11, 2020, Proceedings, volume 12333 of Lecture Notes in Computer Science, 885- 898. Springer. Mitchell, D. G.; Selman, B.; and Levesque, H. J. 1992. Hard and Easy Distributions of SAT Problems. In Swartout, W. R., ed., Proceedings of the 10th National Conference on Artificial Intelligence, San Jose, CA, USA, July 12- 16, 1992, 459- 465. AAAI Press / The MIT Press. Parnas, M.; and Ron, D. 2007. Approximating the minimum vertex cover in sublinear time and a connection to distributed algorithms. Theor. Comput. Sci., 381(1- 3): 183- 196. Prates, M. O. R.; Avelar, P. H. C.; Lemos, H.; Lamb, L. C.; and Vardi, M. Y. 2019. Learning to Solve NP- Complete Problems: A Graph Neural Network for Decision TSP. In The Thirty- Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty- First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, 4731- 4738. AAAI Press. Sato, R.; Yamada, M.; and Kashima, H. 2019. Approximation Ratios of Graph Neural Networks for Combinatorial Problems. In Wallach, H. M.; Larochelle, H.; Beygelzimer,

A.; d'Aliche- Buc, F.; Fox, E. B.; and Garnett, R., eds., Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, Neur IPS 2019, December 8- 14, 2019, Vancouver, BC, Canada, 4083- 4092. Selsam, D.; and Bjorner, N. 2019. Guiding High Performance SAT Solvers with Unsat- Core Predictions. In Janota, M.; and Lynce, I., eds., Theory and Applications of Satisfiability Testing - SAT 2019 - 22nd International Conference, SAT 2019, Lisbon, Portugal, July 9- 12, 2019, Proceedings, volume 11628 of Lecture Notes in Computer Science, 336- 353. Springer. Selsam, D.; Lamm, M.; Bunz, B.; Liang, P.; de Moura, L.; and Dill, D. L. 2019. Learning a SAT Solver from Single- Bit Supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6- 9, 2019. Open Review.net. Vazirani, V. V. 2001. Approximation algorithms. Springer. ISBN 978- 3- 540- 65367- 7. Vinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer Networks. In Cortes, C.; Lawrence, N. D.; Lee, D. D.; Sugiyama, M.; and Garnett, R., eds., Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7- 12, 2015, Montreal, Quebec, Canada, 2692- 2700. Wu, Z.; Pan, S.; Chen, F.; Long, G.; Zhang, C.; and Yu, P. S. 2021. A Comprehensive Survey on Graph Neural Networks. IEEE Trans. Neural Networks Learn. Syst., 32(1): 4- 24. Xu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2019. How Powerful are Graph Neural Networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6- 9, 2019. Open Review.net. Xu, K.; Li, J.; Zhang, M.; Du, S. S.; Kawarabayashi, K.; and Jegelka, S. 2020. What Can Neural Networks Reason About? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26- 30, 2020. Open Review.net. Yolcu, E.; and Poczos, B. 2019. Learning Local Search Heuristics for Boolean Satisfiability. In Wallach, H. M.; Larochelle, H.; Beygelzimer, A.; d'Aliche- Buc, F.; Fox, E. B.; and Garnett, R., eds., Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, Neur IPS 2019, December 8- 14, 2019, Vancouver, BC, Canada, 7990- 8001. Zhang, W.; Sun, Z.; Zhu, Q.; Li, G.; Cai, S.; Xiong, Y.; and Zhang, L. 2020. NLocal SAT: Boosting Local Search with Solution Prediction. In Bessiere, C., ed., Proceedings of the Twenty- Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, 1177- 1183. ijcai.org. Zhou, J.; Cui, G.; Hu, S.; Zhang, Z.; Yang, C.; Liu, Z.; Wang, L.; Li, C.; and Sun, M. 2020. Graph neural networks: A review of methods and applications. AI Open, 1: 57- 81. ## Appendix ## Proof of Theorem 1 A CNF formula \(\phi\) can be represented as a pair \((L, C)\) , where \(L\) is the set of literals, and \(C\) is the set of clauses. Let \(\mathcal{A}\) be the proposed distributed local algorithm for Max SAT problem. First of all, we present the following two lemmas: Lemma 2. Given the algorithm \(\mathcal{A}\) , there exists a single- layer graph neural network \(\mathcal{N}\) , such that for any input \(\phi = (L, C)\) , \(\mathcal{A}(\phi) = \mathcal{N}(\phi)\) holds. Lemma 3. The algorithm \(\mathcal{A}\) has an approximation ratio of \(1 / 2\) for any input \(\phi = (L, C)\) . If these lemmas hold, we have found a single- layer GNN \(\mathcal{N}\) that achieves the same performance as \(\mathcal{A}\) when solving Max SAT problem, which is guaranteed to have a \(1 / 2\) - approximation. As a consequence, Theorem 1 holds. Proof of Lemma 2. We use a single- layer GNN \(\mathcal{N}\) to align with the algorithm \(\mathcal{A}\) . Let \(L\) be a finite set of literals, and \(d = |L|\) . We assume that the embedding of each clause is a 0- 1 vector of length \(d\) , which represents a set of literals, and the embedding of each literal \(L_{i}\) is composed of two values: \(W(L_{i})\) and \(W(\widetilde{L}_{i})\) . Then, we construct some key components of \(\mathcal{N}\) to align with the operations in \(\mathcal{A}\) . Consider the operation \(S(C_{j}) \leftarrow S(C_{j}) \cup \{L_{i}\}\) (line 4). As \(S(C_{j})\) and \(\{L_{i}\}\) are sets of literals, this set union operation is equivalent to a logical AND function of two 0- 1 vectors, which is apparently linearly separable. So there exists a learnable function \(f_{1}: \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}\) to simulate the operation exactly. Consider the operation of picking a literal \(L^{*}\) from \(S(C_{j})\) , and \(W(L^{*}) \leftarrow W(L^{*}) + 1\) (lines 6- 7). According to the universal approximation theorem, there exists a learnable function \(f_{2}: \mathbb{R} \times \mathbb{R}^{d} \rightarrow \mathbb{R}\) to approximate this operation with arbitrarily small error \(\epsilon\) . Next, we show the error will not break the exact simulation of the assignment operation (lines 10- 14). When \(W(L_{i}) \neq W(\widetilde{L}_{i})\) , the condition \(W(L_{i}) \geq W(\widetilde{L}_{i})\) still holds if \(\epsilon < 0.5\) , since the elements in \(W\) are integers. Besides, when \(W(L_{i}) = W(\widetilde{L}_{i})\) , either \(L_{i}\) or \(\widetilde{L}_{i}\) can be assigned True. So there exists an \(f_{2}\) with \(\epsilon < 0.5\) to simulate the operations exactly. The alignment and simulation of other parts are straightforward. As shown above, each component of \(\mathcal{N}\) can align with an operation in \(\mathcal{A}\) . Therefore, for any input \(\phi = (L, C)\) , their outputs must be equal, i.e., \(\mathcal{A}(\phi) = \mathcal{N}(\phi)\) . \(\square\) Proof of Lemma 3. In the algorithm \(\mathcal{A}\) , the picking operation of literal (line 6) transforms the original Max SAT problem \(P\) into a new Max1SAT problem \(P'\) , where \(W(L_{i})\) counts the number of occurrences of literal \(L_{i}\) in \(P'\) . Given a solution of \(P'\) , the number of satisfied clauses of \(P\) is at least as large as that of \(P'\) . Then, we consider the assignment operation of literal (lines 9- 15) in \(\mathcal{A}\) . For a literal \(L_{i}\) appearing in \(P'\) , according to the condition \(W(L_{i}) \geq W(\widetilde{L}_{i})\) , the satisfied clauses are no less than the rejected ones if \(L_{i}\) is assigned True. Therefore, a solution produced by \(\mathcal{A}\) satisfies