# One Model Any CSP Graph Neural Networks as Fast Gl


## Introduction


## A Method Details Here, we will provide a formal definition of our architecture and training procedure. We also give information on model selection and hyperparameters and discuss some implementation details. ## A.1 Architecture Let us formalize the architecture of our policy GNN \(\pi_{\theta}\) . Recall that the main hyperparameters of \(\pi_{\theta}\) are the latent dimension \(d\in \mathbb{N}\) and the aggregation function \(\bigoplus\) which we either choose as an element- wise SUM, MEAN or MAX function. Our GNN is then composed of the following trainable components: A GRU- Cell \(\mathbf{G}:\mathbb{R}^{d}\times \mathbb{R}^{d}\to \mathbb{R}^{d}\) and its trainable initial state \(\mathbf{h}\in \mathbb{R}^{d}\) . This cell is used to update the recurrent value states. A value encoder \(\mathrm{MLP}\mathbf{E}:\mathbb{R}^{d + 1}\to \mathbb{R}^{d}\) which merges the information of the recurrent state and the binary label of each value. Two linear perceptrons \(\mathbf{M}_{\mathcal{V}},\mathbf{M}_{\mathcal{E}}:\mathbb{R}^{d}\to \mathbb{R}^{2d}\) . These functions are used to generate the messages that are sent from values to constraints and from constraints to values, respectively. Three MLPs \(\mathbf{U}_{\mathcal{V}},\mathbf{U}_{\mathcal{E}},\mathbf{U}_{\mathcal{X}}:\mathbb{R}^{d}\to \mathbb{R}^{d}\) for combining aggregated messages for values, constraints and variables, respectively. The output MLP \(\mathbf{O}:\mathbb{R}^{d}\to \mathbb{R}\) which generates the logit scores for each value before we apply the domain- wise softmax. The combined trainable weights of these functions form the parameter vector \(\theta\) . The MLPs \(\mathbf{E},\mathbf{U}_{\mathcal{V}},\mathbf{U}_{\mathcal{E}},\mathbf{U}_{\mathcal{X}}\) and \(\mathbf{O}\) all have two layers. The hidden layer is Re LU- activated and has dimension \(d\) while the second layer is linear. We also note that \(\mathbf{E},\mathbf{M}_{\mathcal{V}},\mathbf{M}_{\mathcal{E}},\mathbf{U}_{\mathcal{V}},\mathbf{U}_{\mathcal{E}}\) and \(\mathbf{U}_{\mathcal{X}}\) each apply Layer Norm to their output, which we found to significantly improve convergence during training. In iteration \(t\) we associate a recurrent state \(h^{(t)}(\nu)\in \mathbb{R}^{d}\) with each value \(\nu \in \mathcal{V}\) . These states are passed on from the previous iteration \(t - 1\) and initialized as \(h^{(0)}(\nu) = \mathbf{h}\) . \(\pi_{\theta}\) then performs the following message passing procedure in each iteration \(t\) : First, each value \(\nu \in \mathcal{V}\) generates a latent state \(x^{(t)}(\nu)\) by applying the encoder \(\mathbf{E}\) to its recurrent state and its binary label: \[x^{(t)}(\nu) = \mathbf{E}\Big(\big[h^{(t - 1)}(\nu),L_{V}^{(t - 1)}(\nu)\big]\Big) \quad (5)\] Here, [...] denotes concatenation of vectors. The latent state is then used to generate two messages for each value by applying the message generation MLP \(\mathbf{M}_{\mathcal{V}}\) : \[m^{(t)}(\nu ,0),m^{(t)}(\nu ,1) = \mathbf{M}_{\mathcal{V}}\big(x^{(t)}(\nu)\big) \quad (6)\] Note that the output of \(\mathbf{M}_{\mathcal{V}}\) has dimension \(2d\) and is the stack of both \(d\) - dimensional messages. The message \(m^{(t)}(\nu ,i)\) is send along all constraint edges \((C,\nu)\) with label \(L_{E}(C,\nu) =\) \(i\) . Hence, the edge labels are incorporated by generating different messages for each label. The constraints aggregate these messages and process the result with their message generation function \(\mathbf{M}_{\mathcal{E}}\) : \[\begin{array}{r l} & {y^{(t)}(C) = \bigoplus_{v\in \mathcal{N}(C)}m^{(t)}\big(\nu ,L_{E}(C,\nu)\big)}\\ & {m^{(t)}(C,0),m^{(t)}(C,1) = \mathbf{M}_{\mathcal{E}}\big(y^{(t)}(C)\big)} \end{array} \quad (7)\] These messages are then aggregated by the values that combine the information with their latent state \(x\) by applying the update MLP \(\mathbf{U}_{\mathcal{V}}\) : \[y^{(t)}(\nu) = \bigoplus_{C\in \mathcal{N}(\nu)\cap \mathcal{C}}m^{(t)}(C,L_{E}(C,\nu)) \quad (9)\] \[z^{(t)}(\nu) = \mathbf{U}_{\mathcal{V}}\big(x^{(t)}(\nu) + y^{(t)}(\nu)\big) + x^{(t)}(\nu) \quad (10)\] Note that we added a residual connection around \(\mathbf{U}_{\mathcal{V}}\) for better gradient flow. In the next phase of our message passing procedure values exchange messages with their respective variables. To this end, each variable \(X\in \mathcal{X}\) pools the latent states of their respective values and applies \(\mathbf{U}_{\mathcal{X}}\) to obtain a variable- level latent representation \(z^{(t)}(X)\) : \[z^{(t)}(X) = \mathbf{U}_{\mathcal{X}}\Big(\bigoplus_{\nu \in D_{X}}z^{(t)}(\nu)\Big) \quad (11)\] This representation is send back to each value \(\nu \in \mathcal{V}_{X}\) of \(X\) , where it is combined with the value- level latent state by a simple addition. Note that this final message pass needs no aggregation as every value is connected to exactly one variable. The result is used as input to the GRU- Cell \(\mathbf{G}\) , which updates the recurrent states of the values: \[h^{(t)}(\nu) = \mathbf{G}\Big(h^{(t - 1)}(\nu),z^{(t)}(\nu) + z^{(t)}(X)\Big) \quad (12)\] Finally, \(\pi_{\theta}\) computes a soft assignment \(\phi^{(t)}\) for \(\mathcal{I}\) . To this end, the MLP \(\mathbf{O}\) maps the new recurrent state of each value \(\nu \in \mathcal{V}_{X}\) of each variable \(X\) to a scalar real number \(o^{(t)}(\nu) = \mathbf{O}(h^{(t)}(\nu))\) . We can then apply the softmax function within each domain to produce a soft value assignment: \[\phi^{(t)}(\nu) = \frac{\exp\left(o^{(t)}(\nu)\right)}{\sum_{\nu^{\prime}\in\mathcal{V}_{X}}\exp\left(o^{(t)}(\nu^{\prime})\right)} \quad (13)\] Figure 5 provides a visual representation of our message passing procedure. We also provide the forward pass of ANYCSP as pseudocode in Algorithm 1. Figure 6 visualizes a run of a trained ANYCSP model on a 2- coloring problem for a grid graph. ## A.2 Training Let us formalize how we apply REINFORCE when training an ANYCSP model. Recall that our action space is extremely large as we choose one assignment from the set of all possible assignments in each step. We can handle this action space efficiently because we model probability distributions over this space as soft assignments from which a new value is sampled independently for every variable. The probability with which a hard assignment \(\alpha\) is sampled from a soft assignment \(\phi\) is therefore given by \[\mathbf{P}(\alpha |\phi) = \prod_{X\in \mathcal{X}}\phi (\alpha (X)). \quad (14)\]

<center>Figure 5: Illustration of the message passing scheme in our policy GNN \(\pi_{\theta}\) . The process is performed once in each iteration \(t\) . (1) Values pass messages to constraints. (2) Constraints pass messages to values. (3) Values pass messages to variables. (4) Variables pass messages to values. (5) Values predict a new soft assignment. </center> Note that sampling one assignment \(\alpha \sim \phi\) and computing its probability according to 14 are both efficient operations and are highly parallelizable. These are the only operation we need on our action space for training and testing. In each training step, we independently draw a batch of training instances from \(\Omega\) . For each such instance \(\mathcal{I}\) , we first run \(\mathrm{ANYCSP}\) for \(T\) steps to generate sequences of soft assignments \(\phi_{\theta} = \phi_{\theta}^{(1)},\ldots ,\phi_{\theta}^{(T)}\) and hard assignments \(\alpha = \alpha^{(1)},\ldots ,\alpha^{(T)}\) . Note that we added \(\theta\) as a subscript to the soft assignments to indicate that the parameters in \(\theta\) have a partial derivative with respect to the probabilities stored in \(\phi_{\theta}^{(t)}\) . We first define \(G_{t}\) as the discounted future reward after step \(t\) : \[G_{t} = \sum_{k = t}^{T}\lambda^{k - t}r^{(k)} \quad (15)\] Here, \(\lambda \in (0,1]\) is a discount factor that we usually choose as \(\lambda = 0.75\) . The purpose of the discount factor is to encourage the policy to earn rewards quickly. Our objective is to find parameters \(\theta\) that maximize the discounted reward over the whole search: \[J(\theta)\coloneqq \underset {\alpha \sim \pi_{\theta}(\mathcal{I})}{\mathbf{E}}\left[\sum_{t = 1}^{T}\lambda^{t - 1}r^{(t)}\right] \quad (16)\] REINFORCE (Williams 1992) enables us to estimate the policy gradient as follows: \[\begin{array}{l}{\nabla_{\theta}J(\theta) = \nabla_{\theta}\sum_{t = 1}^{T}G_{t}\log \mathbf{P}(\alpha^{(t)}|\phi_{\theta}^{(t)})}\\ {= \nabla_{\theta}\sum_{t = 1}^{T}\left(G_{t}\sum_{X\in \mathcal{X}}\log \left(\phi_{\theta}^{(t)}(\alpha^{(t)}(X)) + \epsilon\right)\right)} \end{array} \quad (18)\] Equation 18 applies Equation 14 and adds a small \(\epsilon = 10^{- 5}\) for numerical stability. These policy gradients are averaged over all instances in the batch and then used for one step of gradient ascent (or rather descent with \(- \nabla_{\theta}J(\theta)\) ) in the Adam optimizer. Note that we sample a single trace for each instance in the current batch. The process is repeated in each training step. Algorithm 2 provides our overall training procedure as pseudocode. This training procedure is simply the standard REINFORCE algorithm applied to our Markov Decision Process. We do not use a baseline or critic network. We initially expected this simple algorithm to be unable to estimate a useful policy gradient given the unusually large size of our action space and hard nature of our learning problem. Contrary to this expectation REINFORCE is able to train \(\mathrm{ANYCSP}\) effectively. While more sophisticated RL algorithms have been proposed to address training with large action spaces they are apparently not essential for training with exponentially large action spaces in the context of CSP heuristics. ## A.3 Hyperparameters and Model Selection Before training (and hyperparameter tuning) we sample fixed validation datasets of 200 instances from the given distribution of CSP instances. We usually modify the distribution to yield larger instances than those used for training. This favors the selection of models that generalize well to larger instances, which is almost always desirable. Exact details on how the validation distribution differs from the training distribution in each experiment are provided for in Section B. During validation we perform \(T_{\mathrm{val}} = 200\) search iterations on each validation instance. The metric used for selection is the number of unsatisfied constraints in the best solution averaged over all validation instances. To save compute resources we perform only 100K training steps with each hyperparameter configuration and only perform the full 500K steps of the training with the best configuration. The aggregation function \(\bigoplus \in \{\mathrm{SUM},\mathrm{MEAN},\mathrm{MAX}\}\) is a key hyperparameter. The choice of \(\bigoplus\) is critical for performance, as we observed MAX aggregation to consistently perform best on decision problems but poorly on maximization tasks. The hidden dimension is set to \(d = 128\) . We also validated some models with \(d = 64\) but larger models seem to be more capable. We did not increase \(d\) further to avoid memory bottlenecks. We tuned the discount factor \(\lambda \in \{0.5,0.75,0.9,0.99\}\) and found the value of 0.75 to yield the best results in all of our experiments. The learning rate is initialized as \(\mathrm{lr} = 5\cdot 10^{- 6}\) and decays linearly throughout training to a final value of \(\mathrm{lr} = 5\cdot 10^{- 7}\) . All model train with a batch size of 25. We also considered larger batch sizes of 50 and 100 without improvement. Table 5 specifies the final configuration used in each experiment. ## A.4 Design Constraints and Bottlenecks The primary bottleneck of \(\mathrm{ANYCSP}\) is GPU memory. More specifically, the maximum instance size that can be pro

Table 5: Selected Hyperparameters for each considered CSP <table><tr><td></td><td>MODEL RB</td><td>k-COL</td><td>3-SAT</td><td>Max-k-SAT</td><td>MAXCUT</td></tr><tr><td>d</td><td>128</td><td>128</td><td>128</td><td>128</td><td>128</td></tr><tr><td>⊕</td><td>MAX</td><td>MAX</td><td>MAX</td><td>MEAN</td><td>SUM</td></tr><tr><td>λ</td><td>0.75</td><td>0.75</td><td>0.75</td><td>0.75</td><td>0.75</td></tr><tr><td>Ttrain</td><td>40</td><td>40</td><td>40</td><td>40</td><td>40</td></tr><tr><td>batch size</td><td>25</td><td>25</td><td>25</td><td>25</td><td>25</td></tr><tr><td>lr</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td><td>5·10-6</td></tr></table> cessed is usually determined by the memory required for the message passes between values and constraints. Let \(\mathcal{F} = (\mathcal{X},\mathcal{C},\mathcal{D})\) be a CSP instance with constraints of arity \(k\) and domains of uniform size \(\ell\) . Then the constraint value graph will contain \(|\mathcal{C}|\cdot k\cdot \ell\) constraint edges. Constraint edges are represented with a sparse matrix. For each non- zero entry of the sparse matrix (edge), we store the row (outgoing node) and the column (incoming node). Thus the space complexity of storing constraint edges is \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell)\) . During message passing from values to constraints, each value generates two messages with length d. Those messages are stacked along the first dimension, resulting in a dense matrix with \(2\cdot |\mathcal{X}|\cdot \ell \cdot d\) entries. To pass messages, we use sparse dense matrix multiplication (SPMM) between the sparse matrix of the edges and the dense matrix of the messages generated from values. Alternatively, one can also use the scatter operation from the Py Torch Scatter library, but the scatter operation requires the construction of an intermediate tensor stacking the messages send along each edge. This allocates extra memory of size \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell \cdot d)\) . In contrast, SPMM only allocates memory for the result of the aggregation, but no intermediate tensor is build. As a result, the space complexity of the SPMM operation is \(\mathcal{O}(|\mathcal{C}|\cdot d)\) for all aggregation types. Combining all of the terms, we get a space complexity of \(\mathcal{O}(|\mathcal{C}|\cdot k\cdot \ell +|\mathcal{X}|\cdot \ell \cdot d + |\mathcal{C}|\cdot d)\) for the message passing from values to constraints. The message passing from constraints to values uses the same operations that are used for values to constraints, therefore, the space complexity is the same for both directions. Py Torch Sparse supports generalized SPMM only in CSR format \(^6\) . Their implementation requires the expensive conversion of the sparse matrix from COO to CSR. The adjacency matrix between the stack of generated messages and the constraints is re- wired in every iteration \(t\) according to the new edge labels. Converting a new large matrix into CSR format in every step would be prohibitively expensive. To avoid that, we implement generalized sparse dense matrix multiplication in COO format with CUDA. With our newly implemented function we can pass messages memory efficiently and faster. We also experimented with more advanced attention- based aggregation, namely GAT (Veličković et al. 2017). However, it did not improve the performance but made the construction of large, intermediate edge- level tensors in the message passes unavoidable. Due to this, we restrict our focus on the three basic aggregations of element- wise SUM, MEAN and MAX. ## A.5 Relabeling Constraint Value Graphs One critical requirement for ANYCSP is a fast subroutine for recomputing the edge labels \(L_{E}\) given the newly sampled assignment \(\alpha^{(t)}\) in step \(t\) . Our implementation of this relabeling procedure is based entirely on Py Torch and is GPU accelerated to maximize performance. Here, we will briefly discuss how this implementation works. Let \(\mathcal{F} = (\mathcal{X},\mathcal{D},\mathcal{C})\) be a CSP instances and let \(\alpha\) be the newly sampled assignment for which we have to compute the edge labels \(L_{E}\) . We first compute the node labels \(L_{V}\) which are a simple binary encoding of \(\alpha\) . Let \(C \in \mathcal{C}\) be some constraint with scope \(s^{C} = (X_{1}, \ldots , X_{k})\) , relation \(R^{C} \in \mathcal{D}(X_{1}) \times \dots \times \mathcal{D}(X_{k})\) and arity \(k\) . For each tuple \(r \in R^{C}\) we compute a score that counts how many of the values occurring in \(t\) are currently chosen by \(\alpha\) : \[s(r) = \sum_{i = 1}^{k}L_{V}(r_{i}) \quad (19)\] For each value \(\boldsymbol {v}\in \mathcal{V}_{X_{i}}\) of each variable \(X_{i}\) in the scope of \(C\) we then compute the maximum of \(s(t)\) over all tuples \(r\in R^{C}\) with \(v\in r\) .. \[m(C,\boldsymbol {v}) = \max_{r\in R^{C},\boldsymbol {v}\in r}s(r) \quad (20)\] For each value \(\boldsymbol{v}\) we observe that \(m(C,\boldsymbol {v}) - L_{V}(\boldsymbol {v}) = k - 1\) if and only if there exists some tuple \(r\in R^{C}\) with \(\boldsymbol {v}\in r\) such that for all \(\boldsymbol{v}^{\prime}\in r,\boldsymbol{v}^{\prime}\neq \boldsymbol{v}\) we already have \(\boldsymbol{v}^{\prime}\in \alpha\) . This is equivalent to our original definition of \(L_{E}\) and we can use this case distinction to obtain the edge labels: \[L_{E}(C,\boldsymbol {v}) = \left\{ \begin{array}{ll}1 & \mathrm{if} m(C,\boldsymbol {v}) - L_{V}(\boldsymbol {v}) + 1 = k,\\ 0 & \mathrm{otherwise}. \end{array} \right. \quad (21)\] Our implementation maintains edge lists which connect values and constraint edges to their respective tuples across all constraints. With these edge lists, Equations 19 and 20 are simply scatter operations and Equation 21 is carried out with standard torch arithmetic. These functions are fully based on the GPU and allow us to rapidly update all edge labels in parallel. We can optimize this further by allowing relations to be specified in terms of the disallowed tuples (conflicts), rather than the allowed ones. In this case, we can carry out the exact same procedure, except that we swap the labels in the case distinction of Equation 21. This optimization is very

useful for many problems. SAT in particular has constraints that all forbid exactly one tuple. It is therefore more efficient to work with this one forbidden tuple rather than the \(2^{k} - 1\) allowed tuples. The procedure described here is designed for extension constraints, i.e. constraints where the relations are defined explicitly with lists of allowed or disallowed tuples. Many applications of CSPs require constraints for which this is infeasible. In this case, the relations can only be specified implicitly through intensions. A common example are arithmetic inequalities over discrete numerical domains. To address this issue, our implementation does provide support for two additional classes of constraints: 1. Linear (in)-equalities over numerical domains 2. "All-Different"-constraint over many variables These two types of constraints are commonly found in many CSPs but are hard to express explicitly. We can still compute the edge labels efficiently on the GPU with similar tricks used for the aforementioned extension constraints. Since these are not needed to reproduce the results of our main experiments we refer to our source code for further details. Note that our implementation allows all three supported constraint types to be freely mixed within each instance. ## A.6 Expressiveness A well- known theoretical result on Graph Neural Networks is their correspondence to the Weisfeiler- Lehman isomorphism test. More specifically, standard message- passing GNNs can not distinguish more structures than the 1- dimensional Weisfeiler- Lehman test. For node- level tasks this prohibits a GNN to map two different nodes with identical \(n\) - hop subtrees to different outputs after \(n\) message passes. For some graph structures, such as regular graphs, this makes certain combinatorial tasks, such as graph coloring, fundamentally impossible with standard GNNs. Crucially, ANYCSP is not limited by 1- WL. Our policy GNN \(\pi_{\theta}\) has access to randomness which has been proven to strengthen GNN expressiveness beyond the WL- hierarchy (Abboud et al. 2021; Sato, Yamada, and Kashima 2021). In each iteration \(\pi_{\theta}\) predicts a soft assignment. From this we sample a hard assignment and pass it back to the GNN as a binary pattern. This process can be understood as giving the GNN oracle access to randomness in every iteration. Soft assignments are not just a way of outputting a new assignment but also provide the means with which \(\pi_{\theta}\) interacts with randomness. The GNN can learn to predict soft assignments with high variance to break symmetries through random sampling. Empirically this is also demonstrated in the MAXCUT experiment. The graphs \(G48\) and \(G49\) are both 4- regular toroidal graphs. ANYCSP computes optimal cuts for both graphs, which correspond to conflict- free 2- colorings. This could not be achieved by any function limited by 1- WL.

Algorithm 1: Forward Pass of \(\pi_{\theta}\) . All inner for-loops are parallelized. Input: CSP instance \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{C})\) , Number of steps \(T\in \mathbb{N}\) Output: Soft Assignments \(\phi = \phi^{(1)},\ldots ,\phi^{(T)}\) , Assignments \(\alpha = \alpha^{(1)},\ldots ,\alpha^{(T)}\) , Rewards \(\pmb {r} = r^{(1)},\ldots ,r^{(T)}\) 1: for \(X\in \mathcal{X}\) do 2: \(\alpha^{(0)}(X)\sim \mathcal{D}(X)\) 3: end for 4: \(L_{V}^{(0)},L_{E}^{(0)}\leftarrow \mathrm{LABEL}(\mathcal{I},\alpha^{(0)})\) 5: \(q^{(1)}\leftarrow Q_{\mathcal{I}}(\alpha^{(0)})\) 6: for \(v\in \mathcal{V}\) do 7: \(h^{(0)}(v)\leftarrow \mathbf{h}\) 8: end for 9: for \(t\in \{1,\ldots ,T\}\) do 10: for \(v\in \mathcal{V}\) do 11: \(x^{(t)}(v)\leftarrow \mathbf{E}\left(\left[h^{(t - 1)}(v),L_{V}^{(t - 1)}(v)\right]\right)\) 12: \(m^{(t)}(v,0),m^{(t)}(v,1)\leftarrow \mathbf{M}_{\mathcal{V}}\left(x^{(t)}(v)\right)\) 13: end for 14: for \(C\in \mathcal{C}\) do 15: \(y^{(t)}(C) = \bigoplus_{v\in \mathcal{V}(C)}m^{(t)}\left(v,L_{E}(C,v)\right)\) 16: \(m^{(t)}(C,0),m^{(t)}(C,1) = \mathbf{M}_{\mathcal{C}}\left(y^{(t)}(C)\right)\) 17: end for 18: for \(v\in \mathcal{V}\) do 19: \(y^{(t)}(v) = \bigoplus_{C\in \mathcal{V}(v)\cap \mathcal{C}}m^{(t)}(C,L_{E}(C,v))\) 20: \(z^{(t)}(v) = \mathbf{U}_{\mathcal{V}}\left(x^{(t)}(v) + y^{(t)}(v)\right) + x^{(t)}(v)\) 21: end for 22: for \(X\in \mathcal{X}\) do 23: \(z^{(t)}(X) = \mathbf{U}_{\mathcal{X}}\left(\bigoplus_{v\in D_{X}}z^{(t)}(v)\right)\) 24: end for 25: for \(v\in \mathcal{V}\) do 26: \(h^{(t)}(v)\leftarrow \mathbf{G}\left(h^{(t - 1)}(v),z^{(t)}(v) + z^{(t)}(X)\right)\) 27: \(o^{(t)}(v)\leftarrow \mathbf{O}(h^{(t)}(v))\) 28: end for 29: for \(v\in \mathcal{V}\) do 30: \(\phi^{(t)}(v)\leftarrow \frac{\exp\left(o^{(t)}(v)\right)}{\sum_{v^{\prime}\in \mathcal{V}(X_{v})}\exp\left(o^{(t)}(v^{\prime})\right)}\) 31: end for 32: \(\alpha^{(t)}\sim \phi^{(t)}\) 33: \(L_{V}^{(t)},L_{E}^{(t)}\leftarrow \mathrm{LABEL}(\mathcal{I},\alpha^{(t)})\) 34: \(r^{(t)}\leftarrow \max \{Q_{\mathcal{I}}(\alpha^{(t)}) - q^{(t)},0\}\) 35: \(q^{(t + 1)}\leftarrow \max \{q^{(t)},Q_{\mathcal{I}}(\alpha^{(t)})\}\) 36: end for 37: return \(\theta\) , \(\alpha\) , \(r\) Sample initial assignment uniformly. Get vertex + edge labels. Init. best prior quality. \(\mathbf{h}\) is the learned initial state. Values generate latent state. Values generate two messages. Constraints receive messages. Constraints generate messages. Values receive messages from constraints. Values receive messages. Variables receive states from values. Update recurrent states. Values predict scores. Apply softmax within each domain Sample next assignment. Relabel graph. Get Reward. Update best prior quality.

Algorithm 2: Training ANYCSP Input: Initial parameters \(\theta\) , training distribution \(\Omega\) , train_steps \(\in \mathbb{N}\) , batch_size \(\in \mathbb{N}\) , \(T_{\mathrm{train}} \in \mathbb{N}\) , \(\mathrm{lr} > 0\) , \(\lambda \in (0,1]\) Output: Trained parameters \(\theta\) 1: for \(s \in \{1, \ldots , \mathrm{train\_steps}\}\) do 2: for \(i \in \{1, \ldots , \mathrm{batch\_size}\}\) do 3: \(\mathcal{F} \sim \Omega\) 4: \(\phi_{\theta}, \alpha , \mathbf{r} \leftarrow \pi_{\theta}(\mathcal{F}, T_{\mathrm{train}})\) 5: for \(t \in \{1, \ldots , T_{\mathrm{train}}\}\) do 6: \(G_{t} \leftarrow \sum_{k = t}^{T} \lambda^{k - t} r^{(k)}\) 7: end for 8: \(\nabla_{\theta} J_{i} \leftarrow \nabla_{\theta} \sum_{t = 1}^{T} \left(G_{t} \sum_{X \in \mathcal{X}} \log \left(\phi_{\theta}^{(t)} (\alpha^{(t)} (X)) + \epsilon\right)\right)\) 9: end for 10: \(\theta \leftarrow \theta + \frac{\mathrm{lr}}{\mathrm{batch\_size}} \sum_{i} \nabla_{\theta} J_{i}\) 11: end for 12: return \(\theta\) \(\triangleright\) This loop is parallel across all \(i\) . \(\triangleright\) Sample training instance. \(\triangleright\) Apply policy network. Policy gradient for \(i\) - th instance in batch. \(\triangleright\) Average gradients and ascent. <center>Figure 6: A 2-coloring for a grid graph with size \(12 \times 12\) found by ANYCSP. Conflicting edges are shown in red. </center>

## B Experiment Details In this section, we will provide additional details on our experimental setup baselines. We also provide detailed instancelevel results for our graph coloring and MAXCUT experiments, since these use structured benchmark instances. In Table 10 we provide an overview of all external software used in our experiments. ## B.1 MODEL RB The MODEL RB defines an easy way to generate theoretically hard random CSP instances by randomly choosing a number of disallowed tuples of a fixed arity. A class of random CSP instances of model RB is denoted \(\mathrm{RB}(k,n,\alpha ,r,p)\) where each instance consists of \(n\geq 2\) variables with domain size \(d = n^{\alpha}\) for \(a > 0\) . Each instance has \(m = r n\ln n\) constraints for \(r > 0\) of arity \(k\geq 2\) , with each constraint disallowing \(t = p d^{k}\) randomly selected tuples. Note that the selection of scopes and tuples is performed with repetition. This is due to the fact that the number of repeated constraints and tuples are asymptotically smaller than the total number of constraints and tuples and thus can be neglected. The hardest of the MODEL RB instances occur around the critical value \(p_{cr} = 1 - e^{- \alpha /r}\) of \(p\) (Xu and Li 2003). Data Our training data consists of randomly generated MODEL RB instances with 30 variables and arity 2. We randomly select \(d\in (n^{1 / k},2n^{1 / k}]\) and \(m\in\) \([n\log_{k}d,2n\log_{k}d]\) and generate instances with \(p = 0,9p_{cr}\) slightly smaller than the critical values of \(p\) to increase the number of satisfiable instances seen during training. To generate one instance we build \(m\) constraints, each by randomly selecting scope of \(k\) distinct variables with repetition and then randomly selecting with repetition a relation of \(t\) distinct disallowed tuples. Our validation data contains 200 instances sampled from the exact same distribution. The test dataset is obtained from the XCSP project (Audemard et al. 2020) and contains 50 satisfiable MODEL RB instances with 50 variables, each with domain size 22 and about 500 constraints of arity 2. More specifically, we use all instances of the Random- RB- 2- 50- 23f dataset as our test data. Baselines We used three state- of- the- art CSP- solvers from the XCSP Competition as baselines: Picat (Zhou 2022), ACE (Lecoutre 2022), and Co So Co (Audemard 2018). Picat is a SAT- based solver and the winner of the most recent XCSP Competition (Audemard et al. 2020). ACE and Co So Co are based on constraint propagation. We include Co So Co because it demonstrated very strong performance specifically on binary MODEL RB instances in previous CSP Competitions. Indeed, it also is the best performing baseline in our experiment. ## B.2 Vertex Coloring A CSP instance of \(k\) - COL with the input Graph \(G = (V,E)\) has a variable \(x_{v}\) for each vertex \(v\in G(V)\) , the domain \(\mathcal{D} = \{1,\dots,k\}\) for each variable, and a constraint \(C = ((x_{v},x_{u}),R_{\neq}^{k})\) for each edge \(vu\in G(E)\) . Here, the Relation \(R_{\neq}^{k} = \{(i,j)|1\leq i,j\leq k;i\neq j\}\) implies the color inequality of connected nodes. We consider the decision problem of \(k\) - COL. That is, we provide the number of colors \(k\) as part of the input instance and ask whether or not a conflict- free \(k\) - coloring exists for the given graph. If ANYCSP fails at this task, then it produces a coloring with unsatisfied constraints. We do point out that not all of our baselines use this setup. The greedy heuristic and DSATUR are constructive and yield solutions that are always conflict- free but may have a sub- optimal number of colors. Hybrid EA initially constructs a (sub- optimal) conflict- free coloring and then iteratively attempts to lower the number of colors through tabu search and evolutionary optimization. For all of these methods, we can measure whether or not they produce a conflict- free solution with the optimal number of colors within a given timeout. However, we should keep these differences in mind during a comparison. Data To generate training graphs we mix the following 3 distributions uniformly: - Erdős-Rényi graphs with \(n = 50\) vertices and edge probability \(p \sim U[0.1, 0.3]\) - Barabási-Albert graphs with \(n = 50\) vertices and parameter \(m \sim U[2, 10]\) - Random geometric graphs with \(n = 50\) vertices distributed uniformly at random in a 2-dimensional \(1 \times 1\) square. The edge threshold radius is drawn uniformly from \(r \sim U[0.15, 0.3]\) . For each graph \(G\) drawn from this distribution we then choose a number of colors \(k \in [3, 10]\) as follows: We first apply a linear time greedy coloring heuristic as implemented by Network X (Hagberg, Swart, and S Chult 2008) to color the graph without conflict. If the greedy heuristic required \(k'\) colors for \(G\) , then we pose the problem of coloring \(G\) with \(k\) colors as the training CSP instance, where \(k\) is chosen as: \[k = \max \{3,\min \{10,k' - 1\} \} \quad (22)\] Intuitively, ANYCSP has to color each graph with 1 color less than the greedy heuristic. Some of these instances are unsatisfiable, which is not a problem for our reward scheme and training procedure. We found this simple method to be very effective at quickly generating graph coloring instances around the threshold of satisfiability with minimal fine- tuning. The 200 validation instances are generated with the same parameters and procedure, except that we increase the number of vertices for all three graph types to \(n = 200\) . Baselines RUNCSP was trained on the same data distribution used in its experiments on structured coloring instances (in the Appendix of Tönshoff et al. (2021)). We use a Py Torch implementation of RUNCSP and train each model for a total of 100K steps to ensure convergence. Recall that RUNCSP requires us to fix one \(k\) before training and we train one model for each \(k \in \{4, \ldots , 9\}\) . We consider a graph solved by RUNCSP if the model trained for the graph's chromatic number is able to find a conflict- free coloring. To evaluate the CSP solvers Picat and Co So Co in this experiment we reduce each coloring instance to a CSP instance

in the XCSP3 format. As for ANYCSP, we model the decision variant of graph coloring and fix the known chromatic number of each graph as the domain size. The solvers then have to find a satisfiable assignment for these instances within the 20- minute timeout. For Hybrid EA we optimize the recombination strategy and number of tabu search steps. We choose "n Point" for recombination and a factor of 64 for the number of tabu search steps per cycle. The greedy algorithm and DSATUR each run in their default configuration. Table 8 and 9 contain detailed instance- level results of all compared methods for \(\mathrm{COL}_{< 10}\) and \(\mathrm{COL}_{\geq 10}\) , respectively. ## B.3 MAXCUT In our MAXCUT experiment, we only consider instances with positive edge weights. In this case, MAXCUT is identical to maximum 2- Colorability. Let \(G = (V,E)\) be an input graph. We can model the MAXCUT problem for \(G\) as a CSP instance by using the same reduction we use for vertex coloring in Section B.2 but with the number of colors fixed at \(k = 2\) . Data Our training distribution \(\Omega_{\mathrm{MCUT}}\) consists if Erdős- Rényi graphs with \(n = 100\) vertices and an edge probability sampled uniformly from \(p\in [0.05,0.3]\) . Our validation data uses the same distribution for \(p\) but with \(n = 500\) vertices. Baselines Our classical baselines are a constructive greedy algorithm and a well- known approximation algorithm based on SDP (Goemans and Williamson 1995). For both, we use the implementation of (Mehta 2019) and we run SDP with a 3- hour timeout. We were unable to obtain results for SDP for graphs with over 1000 vertices within this timeout. We consider three neural approaches as baselines: RUNCSP (Tönshoff et al. 2021), ECORD (Barrett et al. 2020) and ECO- DQN (Barrett, Parsonson, and Laterre 2022). RUNCSP is also trained on \(\Omega_{\mathrm{MCUT}}\) . We train ECO- DQN and ECORD on the same data distributions used by Barrett, Parsonson, and Laterre (2022) in their Gset experiments. More specifically, ECO- DQN and ECORD train on Erdős- Rényi graphs with 200 and 500 vertices, respectively. Both methods were validated with the "ER500" validation dataset from their own experiments, which contains Erdős- Rényi graphs with 500 vertices. Note that we select a single model for each method. Barrett, Parsonson, and Laterre (2022) suggest selecting different models with different validation datasets modeled after each group of graphs in the Gset test data. We do not adopt this procedure and select a single model using Erdős- Rényi graphs for validation. The goal of our setup is to test the generalization of one model to new sizes and structures not seen during training and validation. We also experimented with training and validating ECO- DQN and ECORD on our data distributions but found the data chosen by Barrett, Parsonson, and Laterre (2022) to be better for their methods. Table 7 provides extended results of the MAXCUT experiment across all used Gset graphs. ## B.4 3-SAT Modeling a Boolean CNF formula \(f\) as a CSP instance \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{E})\) is straightforward. The set of variables \(\mathcal{X}\) in \(\mathcal{I}\) is simply the set of variables in \(f\) and all domains are given by \(\mathcal{D}(X) = \{0,1\}\) . For each clause \(c\) in \(f\) we add one constraint \(C\) to \(\mathcal{I}\) with the same scope of variables as \(c\) and the relation \(R^{C} = \{0,1\}^{k}\backslash \{t_{c}\}\) . Here, \(k\) is the arity of \(c\) and \(t_{c}\in \{0,1\}^{k}\) is the one combination of values that does not satisfy \(c\) . Data We train on the distribution \(\Omega_{\mathrm{3SAT}}\) of random uniform 3SAT instances with 100 variables and a clause/variable ratio sampled uniformly between 4 and 5. This density is roughly where the threshold of satisfiability for random 3- SAT is. Many of these instances are unsatisfiable. For validation, we use formulas with 200 variables and the same density distribution. Baselines PDP is also trained on \(\Omega_{\mathrm{3SAT}}\) . We used the default PDP configuration for all other options. RLSAT can not be trained on our distribution, since its reward expects all training instances to be satisfiable. Furthermore, their training procedure is relatively sensitive and requires a Curriculum Learning to train well. We, therefore, use the curriculum for 3- SAT provided by the authors of RLSAT. Training is performed on a sequence of data sets with increasing variable counts 5, 10, 25, 50 and finally 100. This training data is generated by their generators. We also evaluate the conventional algorithms Walk SAT and prob SAT. For Walk SAT we tuned the "walk probability" and "noise" parameters but found the default configuration to perform best. We do run prob SAT in its default configuration as well since it has internal heuristics that choose parameters based on the arity of the instance. We run all methods for 10K steps on each instance. We adopt the experimental setting from RLSAT and run each method 10 times on every instance and take the best attempt as the output. PDP is deterministic and we run it only once. ## B.5 MAX-k-SAT For MAX- \(k\) - SAT we can use the same reduction from Boolean CNF formulas to CSPs that we use for 3- SAT. The considered formulas are simply denser. Data Our training distribution \(\Omega_{\mathrm{MSAT}}\) contains \(k\) - CNF formulas with 100 variables and an arity of \(k\in \{3,4\}\) . The clause/variable ratio is sampled uniformly from [5, 8] and [10, 16] for \(k = 3\) and \(k = 4\) , respectively. We validate on formulas with 200 variables and identical density distribution. No 5- CNF formulas are used for training and model selection. Baselines In this experiment, we use Max Walk SAT, a version of the classical Walk SAT algorithm optimized for MAXSAT. The noise parameter is tuned on our validation data to a value of \(10^{- 3}\) . The implementations of CCLS and SATLike do not have command line options and run in their default configurations. ## B.6 Cross-Comparison of Trained Models Recall that every ANYCSP model can take any CSP instance as input. We train on specific distributions of CSPs to obtain problem specific heuristics. However, a model trained on graph coloring instances can still process 3- SAT formulas

Table 6: Cross-Comparison of training distributions on different test datasets. <table><tr><td>Ω</td><td>RB50</td><td>COL&amp;lt;10</td><td>Gset800</td><td>SL250</td><td>MAX-5-CNF</td></tr><tr><td>ΩRB</td><td>42</td><td>50</td><td>655.56</td><td>98</td><td>6192.18</td></tr><tr><td>ΩCOL</td><td>15</td><td>50</td><td>868.22</td><td>96</td><td>5076.16</td></tr><tr><td>ΩMCUT</td><td>0</td><td>0</td><td>1.22</td><td>0</td><td>9048.64</td></tr><tr><td>ΩSAT</td><td>0</td><td>19</td><td>1213.11</td><td>99</td><td>5001.72</td></tr><tr><td>ΩMSAT</td><td>0</td><td>15</td><td>1217.67</td><td>66</td><td>1103.14</td></tr></table> and vice versa. Naturally, we expect each model to perform best on the distribution it is trained on, but the universality of our architecture does raise interesting questions of how well models trained on one CSP perform on an entirely different CSP. In this section we aim to study this transferability of learned heuristics across different CSPs. Table 6 compares all models from our main experiments on each other's test data. For each test dataset, we use the same evaluation metric as the original experiments. First of all, each model does indeed achieve the best results on the test data of the CSP used for training. However, the degree to which each model generalizes to other problems varies substantially. There seems to be a significant compatibility between MODEL RB and graph coloring. The model trained on MODEL RB instances solves all coloring instances in \(\mathrm{COL}_{< 10}\) while the model trained on graph coloring problems solves 15 MODEL RB benchmarks instances. Both of these models also perform very well on decision 3- SAT instances. Curiously, they even outperform the model trained for MAX- \(k\) - SAT on decision 3- SAT. The training distributions \(\Omega_{3\mathrm{SAT}}\) and \(\Omega_{\mathrm{MSAT}}\) are closely related and one would expected our MAX- \(k\) - SAT model to do well on 3- SAT as well. While it does solve 66 of the 100 instances, it does not come close to the 96 and 98 instances solved by the policies trained on \(\Omega_{\mathrm{COL}}\) and \(\Omega_{\mathrm{RB}}\) , respectively. This surprising observation can most likely be attributed to the choice of the aggregation function. Like the model trained on \(\Omega_{3\mathrm{SAT}}\) , the MODEL RB and graph coloring policies both use MAX- aggregation. The MAX- \(k\) - SAT policy uses MEAN- aggregation. In this comparison, choosing the right aggregation function appears to be at least as important as training on similar data. A similar observation also holds for the MAXCUT model. It performs as well as random guessing on all problems other than MAX- CUT. Note that this model uses SUM- aggregation since this performed best in validation for the MAXCUT problem. However, SUM- aggregation is not very robust towards changes in the distribution of inputs. The learned functions are not able to handle larger domains, arities or degrees than those seen during training and the policy is highly specialized towards MAXCUT. On the other hand, no other model achieves competitive results on MAXCUT, indicating that this problem may require a higher degree of specialization. Overall, we can conclude that some of our trained models generalize well beyond their training distribution to entirely different CSPs. However, this is strongly dependent on the specific instance distributions and the robustness of the used aggregation functions.

Table 7: Extended MAXCUT results on Gset graphs. We provide the deviation from the best known cut size. <table><tr><td>GRAPH</td><td>|V|</td><td>|E|</td><td>GREEDY</td><td>SDP</td><td>RUN-CSP</td><td>ECO-DQN</td><td>ECORD</td><td>ANYCSP</td></tr><tr><td>G1</td><td>800</td><td>19176</td><td>675</td><td>346</td><td>242</td><td>90</td><td>0</td><td>0</td></tr><tr><td>G2</td><td>800</td><td>19176</td><td>570</td><td>343</td><td>226</td><td>103</td><td>0</td><td>1</td></tr><tr><td>G3</td><td>800</td><td>19176</td><td>607</td><td>326</td><td>207</td><td>85</td><td>10</td><td>1</td></tr><tr><td>G4</td><td>800</td><td>19176</td><td>587</td><td>330</td><td>222</td><td>55</td><td>0</td><td>0</td></tr><tr><td>G5</td><td>800</td><td>19176</td><td>593</td><td>344</td><td>225</td><td>67</td><td>0</td><td>1</td></tr><tr><td>G14</td><td>800</td><td>4694</td><td>164</td><td>145</td><td>132</td><td>46</td><td>15</td><td>3</td></tr><tr><td>G15</td><td>800</td><td>4661</td><td>185</td><td>126</td><td>146</td><td>46</td><td>16</td><td>0</td></tr><tr><td>G16</td><td>800</td><td>4672</td><td>158</td><td>128</td><td>141</td><td>45</td><td>20</td><td>3</td></tr><tr><td>G17</td><td>800</td><td>4667</td><td>164</td><td>121</td><td>132</td><td>49</td><td>17</td><td>0</td></tr><tr><td>G22</td><td>2000</td><td>19990</td><td>1019</td><td>-</td><td>365</td><td>157</td><td>33</td><td>6</td></tr><tr><td>G23</td><td>2000</td><td>19990</td><td>972</td><td>-</td><td>349</td><td>181</td><td>21</td><td>6</td></tr><tr><td>G24</td><td>2000</td><td>19990</td><td>965</td><td>-</td><td>350</td><td>177</td><td>39</td><td>9</td></tr><tr><td>G25</td><td>2000</td><td>19990</td><td>946</td><td>-</td><td>321</td><td>164</td><td>32</td><td>4</td></tr><tr><td>G26</td><td>2000</td><td>19990</td><td>1001</td><td>-</td><td>337</td><td>194</td><td>23</td><td>8</td></tr><tr><td>G35</td><td>2000</td><td>11778</td><td>442</td><td>-</td><td>371</td><td>134</td><td>46</td><td>19</td></tr><tr><td>G36</td><td>2000</td><td>11766</td><td>438</td><td>-</td><td>358</td><td>141</td><td>53</td><td>22</td></tr><tr><td>G37</td><td>2000</td><td>11785</td><td>415</td><td>-</td><td>379</td><td>146</td><td>54</td><td>22</td></tr><tr><td>G38</td><td>2000</td><td>11779</td><td>435</td><td>-</td><td>386</td><td>119</td><td>52</td><td>22</td></tr><tr><td>G43</td><td>1000</td><td>9990</td><td>473</td><td>213</td><td>156</td><td>66</td><td>0</td><td>0</td></tr><tr><td>G44</td><td>1000</td><td>9990</td><td>532</td><td>294</td><td>143</td><td>57</td><td>0</td><td>0</td></tr><tr><td>G45</td><td>1000</td><td>9990</td><td>463</td><td>329</td><td>135</td><td>52</td><td>0</td><td>1</td></tr><tr><td>G46</td><td>1000</td><td>9990</td><td>461</td><td>264</td><td>159</td><td>69</td><td>1</td><td>1</td></tr><tr><td>G47</td><td>1000</td><td>9990</td><td>455</td><td>289</td><td>158</td><td>33</td><td>6</td><td>1</td></tr><tr><td>G48</td><td>3000</td><td>6000</td><td>0</td><td>-</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>G49</td><td>3000</td><td>6000</td><td>0</td><td>-</td><td>0</td><td>72</td><td>0</td><td>0</td></tr><tr><td>G50</td><td>3000</td><td>6000</td><td>0</td><td>-</td><td>2</td><td>32</td><td>12</td><td>0</td></tr><tr><td>G51</td><td>1000</td><td>5909</td><td>196</td><td>152</td><td>175</td><td>49</td><td>6</td><td>4</td></tr><tr><td>G52</td><td>1000</td><td>5916</td><td>214</td><td>163</td><td>170</td><td>53</td><td>21</td><td>5</td></tr><tr><td>G53</td><td>1000</td><td>5914</td><td>208</td><td>167</td><td>191</td><td>50</td><td>20</td><td>5</td></tr><tr><td>G54</td><td>1000</td><td>5916</td><td>230</td><td>192</td><td>182</td><td>63</td><td>25</td><td>5</td></tr><tr><td>G55</td><td>5000</td><td>12498</td><td>1102</td><td>-</td><td>218</td><td>405</td><td>211</td><td>69</td></tr><tr><td>G58</td><td>5000</td><td>29570</td><td>1058</td><td>-</td><td>1014</td><td>560</td><td>223</td><td>56</td></tr><tr><td>G60</td><td>7000</td><td>17148</td><td>1562</td><td>-</td><td>279</td><td>836</td><td>367</td><td>71</td></tr><tr><td>G63</td><td>7000</td><td>41459</td><td>1384</td><td>-</td><td>1410</td><td>763</td><td>399</td><td>74</td></tr><tr><td>G70</td><td>10 000</td><td>9999</td><td>1088</td><td>-</td><td>285</td><td>758</td><td>290</td><td>143</td></tr></table>

eral CSP benchmark instances. We demonstrate that our method achieves a substantial increase in performance over prior GNN approaches and can compete with conventional algorithms. ANYCSP models trained on small random graph coloring problems are on par with state- of- the- art coloring heuristics on structured benchmark instances. On MAX- \(k\) - SAT, our method scales to test instances 100 times larger than the training data, where it finds better assignments than state- of- the- art conventional search heuristics despite performing 1000 times fewer search iterations. ## 2 Related Work For a comprehensive overview on applying GNNs to combinatorial problems, we refer to Cappart et al. (2021). In this paper, we are primarily interested in end- 2- end approaches which seek to directly predict approximate solutions for combinatorial problems with trainable neural networks. Early work in this area was done by Bello et al. (2016), who learned TSP heuristics with Pointer Networks (Vinyals, Fortunato, and Jaitly 2015) and policy gradient descent. Several extensions of these ideas have since been proposed based on attention (Kool, van Hoof, and Welling 2018) and GNNs (Joshi et al. 2020). Khalil et al. (2017) propose a general method for graph problems, such as MAXCUT or Minimum Vertex Cover. They model the expansion of partial solutions as a reinforcement learning task and train a GNN with Q- learning to iteratively construct approximate solutions. A related group of approaches models local modifications to complete solutions as actions of a reinforcement learning problem. A GNN is then trained as a local search heuristic that iteratively improves candidate solutions through local changes. Methods following this concept are RLSAT (Yolcu and Póczos 2019) for SAT, ECO- DQN (Barrett et al. 2020) for MAXCUT, LS- DQN (Yao, Cai, and Wang 2021) for graph partitioning problems and TSP as well as Bi Hyb (Wang et al. 2021) for graph problems based on selecting and modifying edges. Like conventional search heuristics, these architectures can be applied for any number of search iterations to refine the solution. A shared drawback on large instances is the relatively high computational cost of GNNs, which slows down the search substantially when compared to classical algorithms. ECORD (Barrett, Parsonson, and Laterre 2022) addresses this issue for MAXCUT by applying a GNN only once before the local search, which is carried out by a faster GRU- based architecture without costly message passes. We address the same problem, but not by iterating faster, but by allowing global modifications in each iteration. A fundamentally different approach considers soft relaxations of the underlying problems which can optimized directly with SGD. Examples of this concept are PDP (Amizadeh, Matusevych, and Weimer 2019) for SAT and RUNCSP (Tönshoff et al. 2021) for all binary CSPs with fixed constraint language. These architectures can predict completely new solutions in each iteration but the relaxed differentiable objectives used for training typically do not capture the full hardness of the discrete problem. ## 3 Preliminaries A CSP instance is a triple \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{C})\) , where \(\mathcal{X}\) is a finite set of variables, \(\mathcal{D}\) assigns to each variable \(X\in \mathcal{X}\) a finite set \(\mathcal{D}(X)\) , the domain of \(X\) , and \(\mathcal{C}\) is a set of constraints \(C = (s^C,R^C)\) , where for some \(k\geq 1\) , the scope \(s^C = (X_1,\ldots ,X_k)\in \mathcal{X}^k\) is a tuple of variables and \(R^C\subseteq \mathcal{D}(X_1)\times \ldots \times \mathcal{D}(X_k)\) is a \(k\) - ary relation over the corresponding domains. We always assume that the variables in the scope \(s^C\) of a constraint \(C\) are mutually distinct; we can easily transform an instance not satisfying this condition into one that does by adapting the relation \(R^C\) accordingly. Slightly abusing terminology, we call a pair \((X,d)\) where \(X\in \mathcal{X}\) and \(d\in \mathcal{D}(X)\) a value for variable \(X\) . For all \(X\in \mathcal{X}\) we let \(\mathcal{V}_X = \{X\} \times \mathcal{D}(X)\) be the set of all values for \(X\) , and we let \(\mathcal{V} = \bigcup_{X\in \mathcal{X}}\mathcal{V}_X\) be the set of all values. We usually denote values by \(\mathcal{U}\) . Working with these values instead of domain elements is convenient because the sets \(\mathcal{V}_X\) , for \(X\in \mathcal{X}\) , are mutually disjoint, whereas the domains \(\mathcal{D}(X)\) are not necessarily. An assignment for a CSP instance \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{C})\) is a mapping \(\alpha\) that assigns a domain element \(\alpha (X)\in \mathcal{D}(X)\) to each variable \(X\) . Alternatively, we may view an assignment as a subset \(\alpha \subseteq \mathcal{V}\) that contains exactly one value from each \(\mathcal{V}_X\) . Depending on the context, we use either view, and we synonymously write \(\alpha (X) = d\) or \((X,d)\in \alpha\) . An assignment \(\alpha\) satisfies a constraint \(C = ((X_1,\ldots ,X_k),R)\) (we write \(\alpha \models C\) ) if \((\alpha (X_1),\ldots ,\alpha (X_k))\in R\) , and \(\alpha\) satisfies \(\mathcal{I}\) , or is a solution to \(\mathcal{I}\) , if it satisfies all constraints in \(\mathcal{C}\) . The objective of a CSP is to decide if a given instance has a satisfying assignment and to find one if it does. To distinguish this problem from the maximization version introduced below, we sometimes speak of the decision version. Specific CSPs such as Boolean satisfiability or graph coloring problems are obtained by restricting the instances considered. We define the quality \(Q_{\mathcal{I}}(\alpha)\) of an assignment \(\alpha\) to be the fraction of constraints in \(\mathcal{C}\) satisfied by \(\alpha\) : \(Q_{\mathcal{I}}(\alpha) = |\{C|C\in \mathcal{C},\alpha \models C\} | / |\mathcal{C}|\) . An assignment \(\alpha\) is optimal if it maximizes \(Q_{\mathcal{I}}(\alpha)\) for the instance \(\mathcal{I}\) . The goal of the maximisation problem MAXCSP is to find an optimal assignment for a given instance. A soft assignment for a CSP instance \(\mathcal{I}\) is a mapping \(\phi :\mathcal{V}\to [0,1]\) such that \(\sum_{\nu \in \mathcal{V}_X}\phi (\nu) = 1\) for all \(X\in \mathcal{X}\) . We interpret the numbers \(\phi (\nu)\) as probabilities and say that an assignment \(\alpha\) is sampled from a soft assignment \(\phi\) (we write \(\alpha \sim \phi\) ) if for each variable \(X\in \mathcal{X}\) we independently draw a value \(\nu \in \mathcal{V}_X\) with probability \(\phi (\nu)\) . ## 4 Method With every CSP instance \(\mathcal{I} = (\mathcal{X},\mathcal{D},\mathcal{C})\) we associate a tripartite graph with vertex set \(\mathcal{X}\cup \mathcal{V}\cup \mathcal{C}\) , where \(\mathcal{V}\) is the set of values defined in the previous section, and two kinds of edges: variable edges \((X,\nu)\) for all \(X\in \mathcal{X}\) and \(\nu \in \mathcal{V}_X\) , and constraint edges \((C,\nu)\) for all \(C\in \mathcal{C}\) and \(\nu \in \mathcal{V}_X\) for some \(X\) in the scope of \(C\) . This graph representation is more or less standard; one slightly unusual feature is that we introduce edges from constraints directly to the values and not to the variables. This will be important in the next step, where information about

Table 8: Extended results on structured Vertex Coloring instances with chromatic number less then 10. <table><tr><td rowspan="2">Graph</td><td rowspan="2">\(|V|\)</td><td rowspan="2">\(|E|\)</td><td rowspan="2\(X(G)\)</td><td colspan="2">ANYCSP</td><td colspan="2">RUNCSP</td><td colspan="2">PICAT</td><td colspan="2">Co So Co</td><td colspan="2">HYBRIDEA</td><td colspan="2">DSATUR</td><td colspan="2">GREEDY</td></tr><tr><td>Solved</td><td>Time</td><td>Solved</td><td>Time</td><td>Solved</td><td>Time</td><td>Solved</td><td>Time</td><td>Solved</td><td>Cols</td><td>Time</td><td>Solved</td><td>Cols</td><td>Solved</td><td>Cols</td></tr><tr><td>1-Full Ins_3_col</td><td>30</td><td>100</td><td>4</td><td>True</td><td>0.52</td><td>True</td><td>0.13</td><td>True</td><td>0.09</td><td>True</td><td>0.07</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>False</td><td>5</td></tr><tr><td>1-Full Ins_4_col</td><td>93</td><td>593</td><td>5</td><td>True</td><td>0.52</td><td>False</td><td>TO</td><td>True</td><td>0.1</td><td>True</td><td>0.07</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>False</td><td>6</td></tr><tr><td>1-Full Ins_5_col</td><td>282</td><td>3247</td><td>6</td><td>True</td><td>0.53</td><td>False</td><td>TO</td><td>True</td><td>0.19</td><td>True</td><td>0.11</td><td>True</td><td>6</td><td>0</td><td>True</td><td>6</td><td>False</td><td>9</td></tr><tr><td>1-Insertions_4_col</td><td>67</td><td>232</td><td>5</td><td>True</td><td>0.51</td><td>True</td><td>0.13</td><td>True</td><td>0.08</td><td>True</td><td>0.01</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>True</td><td>5</td></tr><tr><td>1-Insertions_5_col</td><td>202</td><td>1227</td><td>6</td><td>True</td><td>0.52</td><td>True</td><td>0.21</td><td>True</td><td>0.11</td><td>True</td><td>0.12</td><td>True</td><td>6</td><td>0</td><td>True</td><td>6</td><td>True</td><td>6</td></tr><tr><td>1-Insertions_6_col</td><td>607</td><td>6337</td><td>7</td><td>True</td><td>0.52</td><td>True</td><td>3.42</td><td>True</td><td>0.35</td><td>True</td><td>0.09</td><td>True</td><td>7</td><td>0</td><td>True</td><td>7</td><td>False</td><td>8</td></tr><tr><td>2-Full Ins_3_col</td><td>52</td><td>201</td><td>5</td><td>True</td><td>0.51</td><td>True</td><td>0.13</td><td>True</td><td>0.08</td><td>True</td><td>0.01</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>True</td><td>5</td></tr><tr><td>2-Full Ins_4_col</td><td>212</td><td>1621</td><td>6</td><td>True</td><td>0.52</td><td>False</td><td>TO</td><td>True</td><td>0.16</td><td>True</td><td>0.03</td><td>True</td><td>6</td><td>0</td><td>True</td><td>6</td><td>True</td><td>6</td></tr><tr><td>2-Full Ins_5_col</td><td>852</td><td>12201</td><td>7</td><td>True</td><td>0.53</td><td>False</td><td>TO</td><td>True</td><td>0.73</td><td>True</td><td>0.19</td><td>True</td><td>7</td><td>10</td><td>True</td><td>7</td><td>False</td><td>10</td></tr><tr><td>2-Insertions_3_col</td><td>37</td><td>72</td><td>4</td><td>True</td><td>0.51</td><td>True</td><td>0.14</td><td>True</td><td>0.07</td><td>True</td><td>0.04</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>True</td><td>4</td></tr><tr><td>2-Insertions_4_col</td><td>149</td><td>541</td><td>5</td><td>True</td><td>0.52</td><td>True</td><td>0.15</td><td>True</td><td>0.08</td><td>True</td><td>0.02</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>False</td><td>6</td></tr><tr><td>2-Insertions_5_col</td><td>597</td><td>3936</td><td>6</td><td>True</td><td>0.52</td><td>True</td><td>0.84</td><td>True</td><td>0.17</td><td>True</td><td>0.06</td><td>True</td><td>6</td><td>0</td><td>True</td><td>6</td><td>False</td><td>8</td></tr><tr><td>3-Full Ins_3_col</td><td>80</td><td>346</td><td>6</td><td>True</td><td>0.51</td><td>True</td><td>0.15</td><td>True</td><td>0.08</td><td>True</td><td>0.01</td><td>True</td><td>6</td><td>0</td><td>True</td><td>6</td><td>False</td><td>7</td></tr><tr><td>3-Full Ins_4_col</td><td>405</td><td>3524</td><td>7</td><td>True</td><td>0.52</td><td>False</td><td>TO</td><td>True</td><td>0.17</td><td>True</td><td>0.05</td><td>True</td><td>7</td><td>0</td><td>True</td><td>7</td><td>False</td><td>8</td></tr><tr><td>3-Insertions_3_col</td><td>56</td><td>110</td><td>4</td><td>True</td><td>0.51</td><td>True</td><td>0.14</td><td>True</td><td>0.07</td><td>True</td><td>0.01</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>True</td><td>4</td></tr><tr><td>3-Insertions_4_col</td><td>281</td><td>1046</td><td>5</td><td>True</td><td>0.52</td><td>True</td><td>0.17</td><td>True</td><td>0.12</td><td>True</td><td>0.02</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>True</td><td>5</td></tr><tr><td>3-Insertions_5_col</td><td>1406</td><td>9695</td><td>6</td><td>True</td><td>0.52</td><td>True</td><td>7.52</td><td>True</td><td>0.52</td><td>True</td><td>0.25</td><td>True</td><td>6</td><td>20</td><td>True</td><td>6</td><td>False</td><td>8</td></tr><tr><td>4-Full Ins_3_col</td><td>114</td><td>541</td><td>7</td><td>True</td><td>0.52</td><td>True</td><td>0.26</td><td>True</td><td>0.12</td><td>True</td><td>0.01</td><td>True</td><td>7</td><td>0</td><td>True</td><td>7</td><td>True</td><td>7</td></tr><tr><td>4-Full Ins_4_col</td><td>690</td><td>6650</td><td>8</td><td>True</td><td>0.52</td><td>True</td><td>0.14</td><td>True</td><td>0.31</td><td>True</td><td>0.09</td><td>True</td><td>8</td><td>0</td><td>True</td><td>8</td><td>False</td><td>11</td></tr><tr><td>4-Insertions_3_col</td><td>79</td><td>156</td><td>4</td><td>True</td><td>0.51</td><td>True</td><td>0.14</td><td>True</td><td>0.06</td><td>True</td><td>0.01</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>False</td><td>5</td></tr><tr><td>4-Insertions_4_col</td><td>475</td><td>1795</td><td>5</td><td>True</td><td>0.52</td><td>True</td><td>0.21</td><td>True</td><td>0.09</td><td>True</td><td>0.03</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>False</td><td>6</td></tr><tr><td>5-Full Ins_3_col</td><td>154</td><td>792</td><td>8</td><td>True</td><td>0.52</td><td>False</td><td>TO</td><td>True</td><td>0.08</td><td>True</td><td>0.02</td><td>True</td><td>8</td><td>0</td><td>True</td><td>8</td><td>True</td><td>8</td></tr><tr><td>5-Full Ins_4_col</td><td>1085</td><td>11395</td><td>9</td><td>True</td><td>0.51</td><td>True</td><td>0.15</td><td>True</td><td>0.75</td><td>True</td><td>0.19</td><td>True</td><td>9</td><td>20</td><td>True</td><td>9</td><td>False</td><td>12</td></tr><tr><td>DSJC125.1</td><td>125</td><td>736</td><td>5</td><td>True</td><td>0.95</td><td>False</td><td>TO</td><td>True</td><td>0.13</td><td>True</td><td>0.05</td><td>True</td><td>5</td><td>0</td><td>False</td><td>6</td><td>False</td><td>9</td></tr><tr><td>DSJC250.1</td><td>250</td><td>3218</td><td>8</td><td>True</td><td>0.80</td><td>False</td><td>TO</td><td>True</td><td>0.13</td><td>True</td><td>0.07</td><td>True</td><td>8</td><td>30</td><td>True</td><td>11</td><td>False</td><td>13</td></tr><tr><td>ash331GIPA.1</td><td>662</td><td>4181</td><td>4</td><td>True</td><td>0.58</td><td>True</td><td>1.07</td><td>True</td><td>0.15</td><td>True</td><td>0.07</td><td>True</td><td>4</td><td>10</td><td>False</td><td>5</td><td>False</td><td>6</td></tr><tr><td>ash608GIPA.1</td><td>1216</td><td>7844</td><td>4</td><td>True</td><td>0.57</td><td>True</td><td>1.66</td><td>True</td><td>0.25</td><td>True</td><td>0.19</td><td>True</td><td>4</td><td>30</td><td>False</td><td>5</td><td>False</td><td>6</td></tr><tr><td>ash958GIPA.1</td><td>1916</td><td>12506</td><td>4</td><td>True</td><td>0.62</td><td>False</td><td>TO</td><td>True</td><td>0.45</td><td>True</td><td>0.24</td><td>True</td><td>4</td><td>110</td><td>False</td><td>6</td><td>False</td><td>6</td></tr><tr><td>games120.col</td><td>120</td><td>638</td><td>9</td><td>True</td><td>0.52</td><td>True</td><td>0.23</td><td>True</td><td>0.09</td><td>True</td><td>0.01</td><td>True</td><td>9</td><td>0</td><td>True</td><td>9</td><td>True</td><td>9</td></tr><tr><td>le450_5a.col</td><td>450</td><td>5714</td><td>5</td><td>True</td><td>32.04</td><td>False</td><td>TO</td><td>True</td><td>0.31</td><td>True</td><td>0.96</td><td>True</td><td>5</td><td>80</td><td>False</td><td>10</td><td>False</td><td>13</td></tr><tr><td>le450_5b.col</td><td>450</td><td>5734</td><td>5</td><td>True</td><td>6.64</td><td>False</td><td>TO</td><td>True</td><td>0.22</td><td>True</td><td>1.47</td><td>True</td><td>5</td><td>110</td><td>False</td><td>10</td><td>False</td><td>14</td></tr><tr><td>le450_5c.col</td><td>450</td><td>9803</td><td>5</td><td>True</td><td>4.02</td><td>True</td><td>1</td><td>True</td><td>0.32</td><td>True</td><td>0.68</td><td>True</td><td>5</td><td>40</td><td>False</td><td>11</td><td>False</td><td>17</td></tr><tr><td>miles250.col</td><td>450</td><td>9757</td><td>5</td><td>True</td><td>0.74</td><td>True</td><td>1.1</td><td>True</td><td>0.33</td><td>True</td><td>0.15</td><td>True</td><td>5</td><td>30</td><td>False</td><td>12</td><td>False</td><td>16</td></tr><tr><td>miles250.col</td><td>128</td><td>387</td><td>8</td><td>True</td><td>0.52</td><td>True</td><td>0.18</td><td>True</td><td>0.08</td><td>True</td><td>0.36</td><td>True</td><td>8</td><td>0</td><td>True</td><td>8</td><td>False</td><td>10</td></tr><tr><td>mug100_1.col</td><td>100</td><td>166</td><td>4</td><td>True</td><td>0.52</td><td>True</td><td>0.15</td><td>True</td><td>1.9</td><td>True</td><td>0.01</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>True</td><td>4</td></tr><tr><td>mug100_25.col</td><td>100</td><td>166</td><td>4</td><td>True</td><td>0.52</td><td>True</td><td>0.15</td><td>True</td><td>1.85</td><td>True</td><td>0.55</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>True</td><td>4</td></tr><tr><td>mug88_1.col</td><td>88</td><td>146</td><td>4</td><td>True</td><td>0.51</td><td>True</td><td>0.15</td><td>True</td><td>0.08</td><td>True</td><td>0.24</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>True</td><td>4</td></tr><tr><td>mug88_25.col</td><td>88</td><td>146</td><td>4</td><td>True</td><td>0.51</td><td>True</td><td>0.15</td><td>True</td><td>0.07</td><td>True</td><td>0.46</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>True</td><td>4</td></tr><tr><td>myciel3.col</td><td>11</td><td>20</td><td>4</td><td>True</td><td>0.50</td><td>True</td><td>0.14</td><td>True</td><td>0.08</td><td>True</td><td>0.79</td><td>True</td><td>4</td><td>0</td><td>True</td><td>4</td><td>True</td><td>4</td></tr><tr><td>myciel4.col</td><td>23</td><td>71</td><td>5</td><td>True</td><td>0.51</td><td>True</td><td>0.14</td><td>True</td><td>0.07</td><td>True</td><td>0.68</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>True</td><td>5</td></tr><tr><td>myciel5.col</td><td>47</td><td>236</td><td>6</td><td>True</td><td>0.51</td><td>True</td><td>0.14</td><td>True</td><td>0.07</td><td>True</td><td>0.23</td><td>True</td><td>6</td><td>0</td><td>True</td><td>6</td><td>True</td><td>6</td></tr><tr><td>myciel6.col</td><td>95</td><td>755</td><td>7</td><td>True</td><td>0.51</td><td>True</td><td>0.21</td><td>True</td><td>0.19</td><td>True</td><td>0.35</td><td>True</td><td>7</td><td>0</td><td>True</td><td>7</td><td>False</td><td>8</td></tr><tr><td>myciel7.col</td><td>191</td><td>2360</td><td>8</td><td>True</td><td>0.51</td><td>True</td><td>0.14</td><td>True</td><td>0.19</td><td>True</td><td>0.04</td><td>True</td><td>8</td><td>0</td><td>True</td><td>8</td><td>False</td><td>9</td></tr><tr><td>queen5_5.col</td><td>25</td><td>160</td><td>5</td><td>True</td><td>0.57</td><td>True</td><td>3.42</td><td>True</td><td>0.07</td><td>True</td><td>0.3</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>False</td><td>7</td></tr><tr><td>queen5_5.col</td><td>25</td><td>160</td><td>5</td><td>True</td><td>0.57</td><td>True</td><td>3.42</td><td>True</td><td>0.19</td><td>True</td><td>0.07</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>False</td><td>7</td></tr><tr><td>queen5_5.col</td><td>30</td><td>290</td><td>7</td><td>True</td><td>0.57</td><td>True</td><td>0.14</td><td>True</td><td>0.07</td><td>True</td><td>0.18</td><td>True</td><td>7</td><td>0</td><td>True</td><td>7</td><td>False</td><td>9</td></tr><tr><td>queen7_7.col</td><td>49</td><td>476</td><td>7</td><td>True</td><td>3.16</td><td>True</td><td>0.2</td><td>True</td><td>0.08</td><td>True</td><td>0.23</td><td>True</td><td>7</td><td>0</td><td>True</td><td>7</td><td>False</td><td>10</td></tr><tr><td>queen7_7.col</td><td>49</td><td>476</td><td>7</td><td>True</td><td>3.16</td><td>True</td><td>0.2</td><td>True</td><td>0.08</td><td>True</td><td>0.16</td><td>True</td><td>7</td><td>0</td><td>True</td><td>7</td><td>False</td><td>12</td></tr><tr><td>queen8_8.col</td><td>64</td><td>728</td><td>9</td><td>True</td><td>6.35</td><td>False</td><td>TO</td><td>True</td><td>0.12</td><td>True</td><td>1.18</td><td>True</td><td>9</td><td>0</td><td>True</td><td>9</td><td>False</td><td>13</td></tr><tr><td>r125.1.col</td><td>125</td><td>209</td><td>5</td><td>True</td><td>0.52</td><td>True</td><td>0.14</td><td>True</td><td>0.08</td><td>True</td><td>0.07</td><td>True</td><td>5</td><td>0</td><td>True</td><td>5</td><td>False</td><td>6</td></tr><tr><td>r250.1.col</td><td>250</td><td>867</td><td>8</td><td>True</td><td>0.53</td><td>False</td><td>TO</td><td>True</td><td>0.1</td><td>True</td><td>0.08</td><td>True</td><td>8</td><td>0</td><td>True</td><td>8</td><td>False</td><td>9</td></tr><tr><td>will199GIPA.col</td><td>701</td><td>6772</td><td>7</td><td>True</td><td>0.57</td><td>False</td><td>TO</td><td>True</td><td>0.26</td><td>True</td><td>0.1</td><td>True</td><td>7</td><td>0</td><td>True</td><td>7</td><td>False</td><td>12</td></tr></table>

Table 9: Extended results on structured Vertex Coloring instances with chromatic number at least 10. <table><tr><td rowspan="2">Graph</td><td rowspan="2">|V|</td><td rowspan="2">|E|</td><td rowspan="2">X(G)</td><td colspan="2">ANYCSP</td><td colspan="2">PICAT</td><td colspan="2">COSOCO</td><td colspan="2">HYBRIDEA</td><td colspan="2">DSATUR</td><td colspan="2">GREEDY</td></tr><tr><td>Solved</td><td>Time</td><td>Solved</td><td>Time</td><td>Solved</td><td>Time</td><td>Solved</td><td>Time</td><td>Solved</td><td>Time</td><td>Solved</td><td>Time</td><td></td></tr><tr><td>DSJC125.5.5</td><td>125</td><td>3891</td><td>17</td><td>True</td><td>186.92</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>True</td><td>17</td><td>400</td><td>False</td><td>22</td><td>False</td><td>25</td></tr><tr><td>DSJC125.9.5</td><td>125</td><td>6961</td><td>44</td><td>True</td><td>23.4</td><td>True</td><td>52.65</td><td>False</td><td>TO</td><td>True</td><td>44</td><td>40</td><td>False</td><td>51</td><td>False</td><td>56</td></tr><tr><td>DSJR500.1.5</td><td>500</td><td>3555</td><td>12</td><td>True</td><td>0.56</td><td>True</td><td>0.37</td><td>True</td><td>0.05</td><td>True</td><td>12</td><td>0</td><td>True</td><td>12</td><td>False</td><td>14</td></tr><tr><td>anna.ol</td><td>138</td><td>493</td><td>11</td><td>True</td><td>0.52</td><td>True</td><td>0.11</td><td>True</td><td>0.02</td><td>True</td><td>11</td><td>0</td><td>True</td><td>11</td><td>False</td><td>13</td></tr><tr><td>david.ol</td><td>87</td><td>406</td><td>11</td><td>True</td><td>0.51</td><td>True</td><td>0.1</td><td>True</td><td>0.02</td><td>True</td><td>11</td><td>0</td><td>True</td><td>11</td><td>False</td><td>12</td></tr><tr><td>flat300_28_ol</td><td>300</td><td>21695</td><td>28</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>31</td><td>TO</td><td>False</td><td>42</td><td>False</td><td>46</td></tr><tr><td>fpsol2.1.1.ol</td><td>496</td><td>11654</td><td>65</td><td>True</td><td>1.14</td><td>True</td><td>37.88</td><td>True</td><td>0.35</td><td>True</td><td>65</td><td>0</td><td>True</td><td>65</td><td>True</td><td>65</td></tr><tr><td>fpsol2.1.2.ol</td><td>451</td><td>8691</td><td>30</td><td>True</td><td>0.67</td><td>True</td><td>5.67</td><td>True</td><td>0.14</td><td>True</td><td>30</td><td>0</td><td>True</td><td>30</td><td>True</td><td>30</td></tr><tr><td>fpsol2.1.3.ol</td><td>425</td><td>8688</td><td>30</td><td>True</td><td>0.67</td><td>True</td><td>5.71</td><td>True</td><td>0.14</td><td>True</td><td>30</td><td>0</td><td>True</td><td>30</td><td>True</td><td>30</td></tr><tr><td>homer.ol</td><td>561</td><td>1628</td><td>13</td><td>True</td><td>0.51</td><td>True</td><td>0.17</td><td>False</td><td>TO</td><td>True</td><td>13</td><td>0</td><td>True</td><td>13</td><td>True</td><td>14</td></tr><tr><td>huck.ol</td><td>74</td><td>301</td><td>11</td><td>True</td><td>0.51</td><td>True</td><td>0.09</td><td>True</td><td>0.01</td><td>True</td><td>11</td><td>0</td><td>True</td><td>11</td><td>True</td><td>11</td></tr><tr><td>inithx.1.1.ol</td><td>864</td><td>18707</td><td>54</td><td>True</td><td>1.07</td><td>True</td><td>52.22</td><td>True</td><td>0.41</td><td>True</td><td>54</td><td>0</td><td>True</td><td>54</td><td>True</td><td>54</td></tr><tr><td>inithx.1.2.ol</td><td>645</td><td>13979</td><td>31</td><td>True</td><td>0.62</td><td>True</td><td>12.77</td><td>True</td><td>0.22</td><td>True</td><td>31</td><td>0</td><td>True</td><td>31</td><td>True</td><td>31</td></tr><tr><td>inithx.1.3.ol</td><td>621</td><td>13969</td><td>31</td><td>True</td><td>0.66</td><td>True</td><td>12.51</td><td>True</td><td>0.23</td><td>True</td><td>31</td><td>0</td><td>True</td><td>31</td><td>True</td><td>31</td></tr><tr><td>jean.ol</td><td>80</td><td>254</td><td>10</td><td>True</td><td>0.52</td><td>True</td><td>0.09</td><td>True</td><td>0.01</td><td>True</td><td>10</td><td>0</td><td>True</td><td>10</td><td>True</td><td>10</td></tr><tr><td>le450_15a.ol</td><td>450</td><td>8168</td><td>15</td><td>True</td><td>4.21</td><td>True</td><td>1.39</td><td>True</td><td>1078.32</td><td>True</td><td>15</td><td>2720</td><td>False</td><td>16</td><td>False</td><td>21</td></tr><tr><td>le450_15b.ol</td><td>450</td><td>8169</td><td>15</td><td>True</td><td>1.97</td><td>True</td><td>1.29</td><td>True</td><td>0.13</td><td>True</td><td>15</td><td>100</td><td>False</td><td>16</td><td>False</td><td>22</td></tr><tr><td>le450_15c.ol</td><td>450</td><td>16680</td><td>15</td><td>True</td><td>44.86</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>16</td><td>TO</td><td>False</td><td>24</td><td>False</td><td>29</td></tr><tr><td>le450_15d.ol</td><td>450</td><td>16750</td><td>15</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>True</td><td>15</td><td>3190</td><td>False</td><td>24</td><td>False</td><td>33</td></tr><tr><td>le450_25a.ol</td><td>450</td><td>8260</td><td>25</td><td>True</td><td>0.55</td><td>True</td><td>3.21</td><td>True</td><td>0.23</td><td>True</td><td>25</td><td>0</td><td>True</td><td>25</td><td>False</td><td>28</td></tr><tr><td>le450_25b.ol</td><td>450</td><td>8263</td><td>25</td><td>True</td><td>0.55</td><td>True</td><td>3.25</td><td>True</td><td>0.2</td><td>True</td><td>25</td><td>0</td><td>True</td><td>25</td><td>False</td><td>27</td></tr><tr><td>le450_25c.ol</td><td>450</td><td>17343</td><td>25</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>26</td><td>TO</td><td>False</td><td>29</td><td>False</td><td>34</td></tr><tr><td>le450_25d.ol</td><td>450</td><td>17425</td><td>25</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>26</td><td>10</td><td>False</td><td>28</td><td>False</td><td>38</td></tr><tr><td>miles1000.ol</td><td>128</td><td>3216</td><td>42</td><td>True</td><td>0.69</td><td>True</td><td>1.11</td><td>True</td><td>0.12</td><td>True</td><td>42</td><td>0</td><td>True</td><td>42</td><td>False</td><td>46</td></tr><tr><td>miles1500.ol</td><td>128</td><td>5198</td><td>73</td><td>True</td><td>0.72</td><td>True</td><td>10.02</td><td>True</td><td>0.27</td><td>True</td><td>73</td><td>0</td><td>True</td><td>73</td><td>False</td><td>75</td></tr><tr><td>miles500.ol</td><td>128</td><td>1170</td><td>20</td><td>True</td><td>0.52</td><td>True</td><td>0.19</td><td>True</td><td>0.02</td><td>True</td><td>20</td><td>0</td><td>True</td><td>20</td><td>False</td><td>22</td></tr><tr><td>miles750.ol</td><td>128</td><td>2113</td><td>31</td><td>True</td><td>0.71</td><td>True</td><td>0.39</td><td>True</td><td>0.08</td><td>True</td><td>31</td><td>0</td><td>True</td><td>31</td><td>False</td><td>33</td></tr><tr><td>mulsol.1.ol</td><td>197</td><td>3925</td><td>49</td><td>True</td><td>0.61</td><td>True</td><td>2.07</td><td>True</td><td>0.1</td><td>True</td><td>49</td><td>0</td><td>True</td><td>49</td><td>True</td><td>49</td></tr><tr><td>mulsol.1.2.ol</td><td>188</td><td>3885</td><td>31</td><td>True</td><td>0.58</td><td>True</td><td>0.9</td><td>True</td><td>0.17</td><td>True</td><td>31</td><td>0</td><td>True</td><td>31</td><td>True</td><td>31</td></tr><tr><td>mulsol.1.3.ol</td><td>184</td><td>3916</td><td>31</td><td>True</td><td>0.55</td><td>True</td><td>0.93</td><td>True</td><td>0.13</td><td>True</td><td>31</td><td>0</td><td>True</td><td>31</td><td>True</td><td>31</td></tr><tr><td>mulsol.1.4.ol</td><td>185</td><td>3946</td><td>31</td><td>True</td><td>0.57</td><td>True</td><td>0.92</td><td>True</td><td>0.07</td><td>True</td><td>31</td><td>0</td><td>True</td><td>31</td><td>True</td><td>31</td></tr><tr><td>mulsol.1.5.ol</td><td>186</td><td>3973</td><td>31</td><td>True</td><td>0.55</td><td>True</td><td>0.95</td><td>True</td><td>0.06</td><td>True</td><td>31</td><td>0</td><td>True</td><td>31</td><td>True</td><td>31</td></tr><tr><td>queen10_10.ol</td><td>100</td><td>2940</td><td>11</td><td>True</td><td>16.95</td><td>True</td><td>30.73</td><td>True</td><td>351.24</td><td>True</td><td>11</td><td>0</td><td>False</td><td>14</td><td>False</td><td>15</td></tr><tr><td>queen11_11.ol</td><td>121</td><td>3960</td><td>11</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>12</td><td>TO</td><td>False</td><td>15</td><td>False</td><td>16</td></tr><tr><td>queen12_12.ol</td><td>144</td><td>5192</td><td>12</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>13</td><td>TO</td><td>False</td><td>17</td><td>False</td><td>18</td></tr><tr><td>queen13_13.ol</td><td>169</td><td>6656</td><td>13</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>14</td><td>TO</td><td>False</td><td>18</td><td>False</td><td>20</td></tr><tr><td>queen14_14.ol</td><td>196</td><td>4186</td><td>14</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>15</td><td>TO</td><td>False</td><td>19</td><td>False</td><td>20</td></tr><tr><td>queen15_15.ol</td><td>225</td><td>5180</td><td>15</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>16</td><td>TO</td><td>False</td><td>22</td><td>False</td><td>22</td></tr><tr><td>queen16_16.ol</td><td>256</td><td>12640</td><td>16</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>TO</td><td>False</td><td>17</td><td>TO</td><td>False</td><td>22</td><td>False</td><td>23</td></tr><tr><td>queen8_12.ol</td><td>96</td><td>1368</td><td>12</td><td>True</td><td>0.6</td><td>True</td><td>0.14</td><td>True</td><td>0.93</td><td>True</td><td>12</td><td>0</td><td>False</td><td>13</td><td>False</td><td>16</td></tr><tr><td>queen9_9.ol</td><td>81</td><td>1056</td><td>10</td><td>True</td><td>4.97</td><td>True</td><td>0.21</td><td>True</td><td>47.81</td><td>True</td><td>10</td><td>0</td><td>False</td><td>13</td><td>False</td><td>14</td></tr><tr><td>r1000.1.ol</td><td>1000</td><td>14378</td><td>20</td><td>True</td><td>0.73</td><td>True</td><td>11.11</td><td>True</td><td>0.28</td><td>True</td><td>20</td><td>10</td><td>True</td><td>20</td><td>False</td><td>26</td></tr><tr><td>r125.1.ol</td><td>125</td><td>7501</td><td>46</td><td>True</td><td>0.73</td><td>True</td><td>14.05</td><td>True</td><td>1.14</td><td>True</td><td>46</td><td>0</td><td>True</td><td>46</td><td>False</td><td>51</td></tr><tr><td>r125.5.ol</td><td>125</td><td>3838</td><td>36</td><td>True</td><td>0.78</td><td>True</td><td>1.45</td><td>True</td><td>21.86</td><td>True</td><td>36</td><td>30</td><td>False</td><td>38</td><td>False</td><td>43</td></tr><tr><td>r250.5.ol</td><td>250</td><td>14849</td><td>65</td><td>True</td><td>36.41</td><td>True</td><td>56.35</td><td>True</td><td>TO</td><td>65</td><td>442170</td><td>True</td><td>65</td><td>False</td><td>78</td><td>False</td></tr><tr><td>school1.ol</td><td>385</td><td>19095</td><td>14</td><td>True</td><td>0.79</td><td>True</td><td>6.42</td><td>True</td><td>TO</td><td>True</td><td>14</td><td>0</td><td>False</td><td>17</td><td>False</td><td>44</td></tr><tr><td>school1_nsh.ol</td><td>352</td><td>14612</td><td>14</td><td>True</td><td>0.8</td><td>True</td><td>2.9</td><td>True</td><td>TO</td><td>True</td><td>14</td><td>20</td><td>False</td><td>27</td><td>False</td><td>38</td></tr><tr><td>zeroin.1.ol</td><td>211</td><td>4100</td><td>49</td><td>True</td><td>0.66</td><td>True</td><td>2.41</td><td>True</td><td>0.08</td><td>True</td><td>49</td><td>0</td><td>True</td><td>49</td><td>True</td><td>49</td></tr><tr><td>zeroin.1.2.ol</td><td>211</td><td>3541</td><td>30</td><td>True</td><td>0.56</td><td>True</td><td>0.8</td><td>True</td><td>0.06</td><td>True</td><td>30</td><td>0</td><td>True</td><td>30</td><td>True</td><td>30</td></tr><tr><td>zeroin.1.3.ol</td><td>206</td><td>3540</td><td>3</td><td>True</td><td>0.54</td><td>True</td><td>0.78</td><td>True</td><td>0.05</td><td>True</td><td>30</td><td>0</td><td>True</td><td>30</td><td>True</td><td>31</td></tr></table> Table 10: URLs and versions of used softwares. <table><tr><td>Software</td><td>Version</td><td>URL</td></tr><tr><td>PICAT</td><td>3.26</td><td>http://picat-lang.org/</td></tr><tr><td>ACE</td><td>2.0</td><td>https://github.com/xcsp3team/ace</td></tr><tr><td>Co So Co</td><td>1.12</td><td>https://www.cril.univ-artois.fr/XCSP18/files/cosoco-competition.tgz</td></tr><tr><td>Py CSP3</td><td>2.0</td><td>https://github.com/xcsp3team/pycsp3</td></tr><tr><td>RUNCSP</td><td>1.0</td><td>https://github.com/toenshoff/RUNCSP-Py Torch</td></tr><tr><td>Greedy + DSATUR + HYBRIDEA (COL)</td><td>2017</td><td>http://rhydlewis.eu/resources/g Col.zip</td></tr><tr><td>ECORD</td><td>commit:3463463</td><td>https://github.com/tomdbar/ecord</td></tr><tr><td>ECO-DQN</td><td>commit:134df73</td><td>https://github.com/tomdbar/eco-dqn/</td></tr><tr><td>Greedy + SDP (MAXCUT)</td><td>0.1.2</td><td>https://github.com/hermish/cvx-graph-algorithms</td></tr><tr><td>RLSAT</td><td>commit:292e55c</td><td>https://github.com/emreyolcu/sat</td></tr><tr><td>PDP</td><td>commit:1fca34d</td><td>https://github.com/microsoft/PDP-Solver</td></tr><tr><td>WALKSAT</td><td>56</td><td>https://gitlab.com/Henry Kautz/Walksat</td></tr><tr><td>PROBSAT</td><td>v2014</td><td>https://github.com/satcompetition/2018/blob/master/solvers/prob SAT.zip</td></tr><tr><td>MAXWALKSAT</td><td>2013</td><td>https://github.com/stechu/Maxwalksat</td></tr><tr><td>SATLIKE</td><td>3.0</td><td>http://lcs.ios.ac.cn/~caisw/Max SAT.html</td></tr><tr><td>CCLS</td><td>2015</td><td>http://lcs.ios.ac.cn/~caisw/Max SAT.html</td></tr></table>

## C Ablation We provide an empirical ablation study for two major design choices of ANYc SP: Firstly, we want to study the benefit of our exponentially sized action space when compared to a more conventional local search setting. Secondly, we aim to validate the reward scheme we constructed in Section 4.2. To this end, we will evaluate two modified versions of ANYc SP: 1. ANYc SPloc: A version of ANYc SP designed to be a local search heuristic. We modify \(\pi_{\theta}\) such that the softmax over the scores in \(o^{(t)}(\boldsymbol {v})\) is not performed separately within each domain but over all values in the disjoint union of domains \(\mathcal{V}\) .. \[\phi^{(t + 1)}(\boldsymbol {v}) = \frac{\exp\left(o^{(t)}(\boldsymbol{v})\right)}{\sum_{\boldsymbol{v}' \in \mathcal{V}}\exp\left(o^{(t)}(\boldsymbol{v}') \right)} \quad (23)\] The output \(\phi^{(t + 1)}\) of \(\pi_{\theta}\) in iteration \(t + 1\) is therefore not a soft assignment but a probability distribution over the disjoint union of domains. To obtain a new hard assignment \(\alpha^{(t + 1)}\) we sample a single value \(v\in \mathcal{C}\) from this distribution and set it as the value for its respective variable \(X_{v}\) with \(v\in \mathcal{V}_{X_{v}}\) \[\begin{array}{c}{\boldsymbol {v}\sim \phi^{(t + 1)}}\\ {\alpha^{(t + 1)} = \alpha^{(t)}[X_{v} = \boldsymbol {v}]} \end{array} \quad (25)\] All variables other than \(X_{v}\) remain unchanged in iteration \(t + 1\) . With this modification \(\mathrm{ANYCSP_{loc}}\) becomes a local search heuristic that only changes one variable at a time. The remaining architecture and training procedure are identical to ANYc SP, including the reward scheme. 2. ANYCSPqual: A version of ANYc SP trained by using the quality \(Q_{\mathcal{I}}(\alpha^{(t)})\) of the current assignment as a reward. For this configuration to train well, we found it helpful to use the quality of the initial assignment as a baseline: \[r^{(t)} = Q_{\mathcal{I}}(\alpha^{(t)}) - Q_{\mathcal{I}}(\alpha^{(0)}) \quad (26)\] Without the subtractive baseline, we found the training to be very unstable. Note that the baseline does not solve the fundamental problem of the reward scheme: A heuristic can not leave a local maximum without being immediately punished for doing so. Here, we will study how this proposed issue actually effects performance empirically. We perform our ablation experiments on the graph coloring, MAXCUT and MAX- \(k\) - SAT problems. For each problem, we train both modifications with the same training data and hyperparameters as ANYc SP. ## C.1 Results Tables 11, 12 and 13 contain the results of our ablation study for \(k\) - COL, MAXCUT and MAX- \(k\) - SAT, respectively. The metrics in each table are identical to those used in our main experiment. For graph coloring, \(\mathrm{ANYCSP_{qual}}\) performs significantly worse than the other two versions of our method. Compared to our main baselines it only outperforms RUNCSP Table 11: Ablation results on graph coloring. <table><tr><td>METHOD</td><td>COL&amp;lt;10</td><td>COL≥10</td></tr><tr><td>ANYCSPloc</td><td>49</td><td>37</td></tr><tr><td>ANYCSPqual</td><td>37</td><td>25</td></tr><tr><td>ANYCSP</td><td>50</td><td>40</td></tr></table> and the simple greedy approach. \(\mathrm{ANYCSP_{loc}}\) actually performs reasonably well, as it only solves four graphs less than ANYCSP across all 100 test instances. In this experiment the reward scheme seems to contribute more to the performance than the global search action space. However, only the combination of both in ANYCSP yields the best results. On the MAXCUT problem there is no clear hierarchy between \(\mathrm{ANYCSP_{loc}}\) and \(\mathrm{ANYCSP_{qual}}\) . However, both ablation versions perform significantly worse than ANYCSP. The same seems to hold on the MAX- \(k\) - SAT problem. The two modified versions yield similar results but perform far worse than ANYCSP. Figure 7 investigates the differences on the MAX- \(k\) - SAT problem further. We plot how the number of unsatisfied clauses in the best found solution evolves throughout the 60K search steps performed by ANYCSP in 20 minutes. The curves are averaged over all 50 instances in out MAX- 5- SAT test data. Both \(\mathrm{ANYCSP_{loc}}\) and \(\mathrm{ANYCSP_{qual}}\) are unable to converge to solutions as good as those found by ANYCSP, but for different reasons. \(\mathrm{ANYCSP_{loc}}\) converges slowly but steadily. Due to the slow convergence compared to ANYCSP, it is not able to find equivalent solutions in the same amount of time. \(\mathrm{ANYCSP_{qual}}\) initially converges as fast as ANYCSP. This is expected, since this version also performs global search and can refine the whole solution in parallel. However, it tapers of significantly earlier compared to ANYCSP and the solution quality remains virtually constant after 20K search steps. This is the expected problem our reward scheme intends to solve: During training, \(\mathrm{ANYCSP_{qual}}\) can not leave local maxima without being punished for doing so by the simple reward scheme. This inhibits exploration and encourages stagnation. After 60K steps, \(\mathrm{ANYCSP_{loc}}\) actually catches up to \(\mathrm{ANYCSP_{qual}}\) and both ablation versions yield similar results once the 20 minute timeout is reached. Overall, our experiments and ablation study suggests that our two main design choices are crucial to consistently obtaining strong search heuristics: 1. A global search space is necessary to refine the whole solution in parallel and speed up the search. Without this advantage, GNN-based heuristics can not compensate for their comparatively high computational cost. 2. A well-chosen reward scheme that encourages exploration is equally important. Without it, global search simply gets stuck faster than local search. A simple reward proportional to the quality is not suitable in this regard. ANYCSP combines these insights in one generic architecture for all CSPs.

<center>Figure 7: Number of unsat clauses in the best found solution on our MAX-5-SAT test instances. We plot the average over 50 instances as a function in the number of search steps \(t\) . We compare ANYCSP and its two ablation versions. </center> Table 12: Get results of out ablation models. <table><tr><td>METHOD</td><td>|V|=800</td><td>|V|=1K</td><td>|V|=2K</td><td>|V|≥3K</td></tr><tr><td>ANYCSPloc</td><td>14.22</td><td>26.00</td><td>84.00</td><td>197.50</td></tr><tr><td>ANYCSPqual</td><td>30.11</td><td>12.89</td><td>42.56</td><td>200.75</td></tr><tr><td>ANYCSP</td><td>1.22</td><td>2.44</td><td>13.11</td><td>51.63</td></tr></table> Table 13: Ablation results on Max-\(k\) -SAT instances. <table><tr><td>METHOD</td><td>3CNF</td><td>5CNF</td></tr><tr><td>ANYCSPloc</td><td>1697.08</td><td>1498.46</td></tr><tr><td>ANYCSPqual</td><td>1921.70</td><td>1471.00</td></tr><tr><td>ANYCSP</td><td>1537.46</td><td>1103.14</td></tr></table>

<center>Figure 1: Example of the constraint value graph \(G(\mathcal{F},\alpha)\) for a given CSP instance \(\mathcal{F}\) and an assignment \(\alpha\) . The graph contains vertices for the variables, values and constraints of \(\mathcal{F}\) . Each value is connected to its variable and labeled with the assignment \(\alpha\) . Each constraint is connected to the values of its variables. This edge set is labeled such that a label of 1 for edge \((C,\nu)\) states that choosing value \(\upsilon\) will satisfy the constraint \(C\) if no other variables involved in \(C\) change their values. </center> the constraint relations \(R^{C}\) is compactly encoded through a binary labeling of the constraint edges. For each assignment \(\alpha\) we introduce a vertex labeling \(L_{V}\) and an edge labeling \(L_{E}\) . The vertex labeling \(L_{V}\) is a binary encoding of \(\alpha\) , that is, \(L_{V}(\nu) = 1\) if \(\nu \in \alpha\) and \(L_{V}(\nu) = 0\) for each \(\nu \in \mathcal{V} \setminus \alpha\) . The edge labeling \(L_{E}\) encodes how changes to \(\alpha\) affect each constraint. For every constraint \(C \in \mathcal{C}\) and value \((X_{i}, d) \in \mathcal{V}_{X_{i}}\) of variable \(X_{i}\) in the scope \((X_{1}, \ldots , X_{k})\) of \(C\) we define the edge label to be \(L_{E}(C, \nu) = 1\) if \[(\alpha (X_{1}), \ldots , \alpha (X_{i - 1}), d, \alpha (X_{i + 1}), \ldots , \alpha (X_{k})) \in R^{C}.\] and \(L_{E}(C, \nu) = 0\) otherwise. Intuitively, the edge labels encode for each constraint edge \((C, \nu)\) whether or not choosing the value \(\nu\) for its variable would satisfy \(C\) under the condition that all other variables involved in \(C\) retain their current value in \(\alpha\) . We call the labeled graph \(G(\mathcal{F}, \alpha)\) obtained this way the constraint value graph of \(\mathcal{F}\) at \(\alpha\) . Figure 1 provides a visual example of our construction. ### 4.1 Architecture We construct a recurrent GNN \(\pi_{\theta}\) that maps constraint value graphs to soft assignments and serves as a trainable policy for our reinforcement- learning setup. Here, the real vector \(\theta\) contains the trainable parameters of \(\pi_{\theta}\) . The input of \(\pi_{\theta}\) in iteration \(t\) is the current graph \(G(\mathcal{F}, \alpha^{(t - 1)})\) and recurrent vertex states \(h^{(t - 1)}\) . The output is a new soft assignment \(\phi^{(t)}\) for \(\mathcal{F}\) as well as updated recurrent states: \[\phi^{(t)}, h^{(t)} = \pi_{\theta}\left(G(\mathcal{F}, \alpha^{(t - 1)}), h^{(t - 1)}\right) \quad (1)\] The next assignment \(\alpha^{(t)}\) can then be sampled from \(\phi^{(t)}\) before the process is repeated. Here, we will provide an overview of the GNN architecture while we give a detailed formal description in Appendix A. In a nutshell, our architecture is a recurrent heterogeneous GNN that uses distinct trainable functions for each of the three vertex types in the constraint value graph. The main hyperparameters of \(\pi_{\theta}\) are the latent dimension \(d \in \mathbb{N}\) and the aggregation function \(\bigoplus\) which we either choose as an element- wise SUM, MEAN or MAX function. As a rule of thumb, we found MAX- aggregation to perform best on decision problems while MEAN- aggregation seems more suitable for maximization tasks. This coincides with observations of Joshi et al. (2020). \(\pi_{\theta}\) associates a recurrent state \(h^{(t)}(\nu) \in \mathbb{R}^{d}\) with each value \(\nu \in \mathcal{V}\) and uses a GRU cell to update these states after each round of message passing. Variables and constraints do not have recurrent states. We did consider versions with stateful constraints and variables, but these did not perform better while being slower. All remaining functions for message generation and combination are parameterized by standard MLPs with at most one hidden layer. In each iteration \(t\) , \(\pi_{\theta}\) performs 4 directed message passes in the following order: (1) values to constraints, (2) constraints to values, (3) values to variables, (4) variables to values. The first two message passes incorporate the node and edge labels and enable the values to gather information about how changes to the current assignment effect each constraint. The final two message passes allow the values of each domain to negotiate the next variable assignment. Note that this procedure is carried out once in each search iteration \(t\) . As the recurrent states can carry aggregated information across search iterations we found a single round of message passes per iteration sufficient. Finally, \(\pi_{\theta}\) generates a new soft assignment \(\phi^{(t)}\) . To this end, each value \(\nu \in \mathcal{V}_{X}\) of each variable \(X\) predicts a scalar real number \(o^{(t)}(\nu) = \mathbf{O}(h^{(t)}(\nu))\) from its updated latent state with a shared MLP \(\mathbf{O}:\mathbb{R}^{d}\to \mathbb{R}\) . We can then apply the softmax function within each domain to produce a soft value assignment: \[\phi^{(t)}(\nu) = \frac{\exp\left(o^{(t)}(\nu)\right)}{\sum_{\nu^{\prime}\in\mathcal{V}_{X}}\exp\left(o^{(t)}(\nu^{\prime})\right)} \quad (2)\] This procedure leverages a major strength of our graph construction: By modeling values as vertices we can directly process arbitrary domains with one GNN. For larger domains, we simply add more value vertices to the graph. ### 4.2 Global Search as an RL Problem We deploy the policy GNN \(\pi_{\theta}\) as a trainable search heuristic. Note that a single GNN \(\pi_{\theta}\) can search for solutions on any given CSP instance. ANYCSP takes a CSP instance \(\mathcal{I}\) and a parameter \(T \in \mathbb{N}\) as input and outputs a sequence

<center>Figure 2: Illustration of a run of ANYCSF on a given CSP instance \(\mathcal{I}\) . We iteratively apply our policy GNN \(\pi_{\theta}\) to the constraint value graph \(G(\mathcal{I},\alpha^{(t - 1)})\) of \(\mathcal{I}\) and the current assignment \(\alpha^{(t - 1)}\) . From this we obtain a soft assignment \(\phi^{(t)}\) from which the next assignment \(\alpha^{(t)}\) is sampled freely with no restrictions to locality. </center> \(\alpha = \alpha^{(0)},\ldots ,\alpha^{(T)}\) of assignments for \(\mathcal{I}\) . The initial assignment \(\alpha^{(0)}\) is simply drawn uniformly at random. In each iteration \(1\leq t\leq T\) the policy GNN \(\pi_{\theta}\) is applied to the current constraint value graph \(G(\mathcal{I},\alpha^{(t - 1)})\) to generate a new soft assignment \(\phi^{(t)}\) . The next assignment \(\alpha^{(t)}\sim \phi^{(t)}\) is then sampled from the predicted soft assignment by drawing a new value \(\alpha^{(t)}(X)\) for all variables \(X\) independently and in parallel without imposing any restrictions on locality. Any number of variables may change their value in each iteration which makes our method a global search heuristic. This allows ANYCSF to modify different parts of the solution simultaneously to speed up the search. Figure 2 provides a visual illustration of the overall process. Formally, our action space is the set of all assignments for the input instance, one of which must be chosen as the next assignment in each iteration \(t\) . This set is extremely large for many CSPs, with up to \(10^{50}\) assignments to choose from for some of our training instances. Despite this, we found standard policy gradient descent algorithms to be effective and stable during training. Rewarding Iterative Improvements We devise a reward scheme that assigns a real- valued reward \(r^{(t)}\) to each generated assignment \(\alpha^{(t)}\) . A simple approach would be to use the quality \(Q_{\mathcal{I}}(\alpha^{(t)})\) as a reward. However, we found that models trained with this reward tend to get stuck in local maxima and have comparatively poor performance. Intuitively, this simple reward scheme immediately punishes the policy for stepping out of a local maximum causing stagnating behavior. We, therefore, choose a more sophisticated reward system that avoids this issue. First, we define the auxiliary variable \(q^{(t)} = \max_{t^{\prime}< t}Q_{\mathcal{I}}(\alpha^{(t^{\prime})})\) , which tracks the highest quality achieved before iteration \(t\) . We then define the reward in iteration \(t\) as follows: \[r^{(t)} = \left\{ \begin{array}{ll}0 & \mathrm{if} Q_{\mathcal{I}}(\alpha^{(t)}) \leq q^{(t)},\\ Q_{\mathcal{I}}(\alpha^{(t)}) - q^{(t)} & \mathrm{if} Q_{\mathcal{I}}(\alpha^{(t)}) > q^{(t)}. \end{array} \right. \quad (3)\] The policy earns a positive reward in iteration \(t\) if the new assignment \(\alpha^{(t)}\) satisfies more constraints than any assignment generated in the previous steps. In this case, the reward is the margin of improvement. Note that the reward is 0 in any step in which the new assignment is not an improvement over the previous best regardless of whether the quality of the solution is increasing or decreasing. This reward is designed to encourage \(\pi_{\theta}\) to yield iteratively improving assignments while being agnostic towards how the assignments change between improvements. Our reward is conceptually similar to that of ECO- DQN (Barrett et al. 2020). The main difference is that we do not add intermediate rewards for reaching local maxima. Inductively, we observe that the total reward over all iterations is given by \(\sum_{t = 1}^{T}r^{(t)} = q^{(T + 1)} - Q_{\mathcal{I}}(\alpha^{(0)})\) . For any input instance \(\mathcal{I}\) the total reward is maximal (relative to \(Q_{\mathcal{I}}(\alpha^{(0)})\) ) if and only if the highest achieved quality \(q^{(T + 1)}\) is the optimal quality for \(\mathcal{I}\) . In Appendix C we provide an ablation study where we compare our reward scheme to the simpler choice of using \(Q_{\mathcal{I}}(\alpha^{(t)})\) directly as a reward. Markov Decision Process For a given input \(\mathcal{I}\) we model the procedure described so far as a Markov Decision Process \(\mathcal{M}(\mathcal{I})\) which will allow us to deploy standard reinforcement learning methods for training: The state in iteration \(t\) is given by \(s^{(t)} = (\alpha^{(t)},q^{(t)})\) and contains the current assignment and highest quality achieved before step \(t\) . The initial assignment \(\alpha^{(0)}\) is drawn uniformly at random and \(q^{(0)} = 0\) . The space of actions \(\mathcal{A}\) is simply the set of all possible assignments for \(\mathcal{I}\) . The soft assignments produced by the policy \(\pi_{\theta}\) are distributions over this action space. After the next action is sampled from this distribution, the state transition of the MDP is deterministic and updates the state with the chosen assignment and its quality. The reward \(r^{(t)}\) at time \(t\) is defined as in Equation 3. Training During training, we assume that some data generating distribution \(\Omega\) of CSP instances are given. We aim to find a policy that performs well on this distribution of inputs. Ideally, we need to find the set of parameters \(\theta^{*}\) which maximizes the expected total reward if we first draw an instance from \(\Omega\) and then apply the model to it for \(T_{\mathrm{train}}\in \mathbb{N}\) steps: \[\theta^{*} = \arg \max_{\theta}\underset {\alpha \sim \pi_{\theta}(\mathcal{I})}{\mathbb{E}}\left[\sum_{t = 1}^{T_{\mathrm{train}}}\lambda^{t - 1}r^{(t)}\right] \quad (4)\] The discount factor \(\lambda \in (0,1]\) and the number of search iterations during training \(T_{\mathrm{train}}\) are both hyperparameters. Starting with randomly initialized parameters \(\theta\) , we utilize REINFORCE (Williams 1992) to train \(\pi_{\theta}\) with stochastic policy gradient ascent. REINFORCE is a natural choice for training ANYCSF since its complexity does not depend on the size of the action space \(\mathcal{A}\) . Soft assignments allow us to efficiently sample the next assignment \(\alpha \sim \phi\) and recover its probability \(\mathbf{P}(\alpha |\phi) = \prod_{X}\phi (\alpha (X))\) . These are the only operations on

<center>Figure 3: Survival plot for RB50. The x-axis gives the wall clock runtime in seconds. The y-axis counts the cumulative number of instances solved within a given time. </center> the action space required for REINFORCE. Note that we use vanilla REINFORCE without a baseline or critic network and we sample a single trajectory for every training instance. We found this simple version of the algorithm to be surprisingly robust and effective in our setting. Details on how the policy gradients are computed are provided in Appendix A. ### 4.3 Implementation and Hyperparameters We implement ANYCSP in Py Torch 3. The code for relabeling \(G(\mathcal{I},\alpha^{(t)})\) in each iteration \(t\) is also fully based on Py Torch and is GPU- compatible. We implement generalized sparse matrix multiplication in the COO format in CUDA. This helps to increase the memory efficiency and speed of the message passes between values and constraints. We plan to publish this extension as a stand- alone software package or merge it with Py Torch Sparse to make it accessible to the broader Graph Learning community. We choose a hidden dimension of \(d = 128\) for all experiments. We train with the Adam optimizer for 500K training steps with a batch size of 25. Training a model takes between 24 and 48 hours, depending on the data. During training, we set the upper number of iterations to \(T_{\mathrm{train}} = 40\) . During testing, we usually run ANYCSP with a timeout rather than a fixed upper number of iterations \(T\) . All hyperparameters are provided in Appendix A. For each training distribution \(\Omega\) we implement data loaders that sample new instances on- the- fly in each training step. With our hyperparameters we therefore train each model on 12.5 Million sampled training instances. We use fixed subsets of 200 instances sampled from each distribution before training as validation data. The exact generation procedures for each training distribution are provided in Appendix B. ## 5 Experiments We evaluate ANYCSP on a wide range of well- known CSPs: Boolean satisfiability (3- SAT) and its maximisation version Table 1: Results on structured Graph Coloring instances. We provide the number of instances solved with a 20 Minute timeout for both splits, each containing 50 instances with chromatic number less than 10 and at least 10, respectively. <table><tr><td>METHOD</td><td>COL&amp;lt;10</td><td>COL≥10</td></tr><tr><td>RUNCSP</td><td>33</td><td>-</td></tr><tr><td>Co So Co</td><td>49</td><td>33</td></tr><tr><td>PICAT</td><td>49</td><td>38</td></tr><tr><td>GREEDY</td><td>16</td><td>15</td></tr><tr><td>DSATUR</td><td>38</td><td>28</td></tr><tr><td>HYBRIDEA</td><td>50</td><td>40</td></tr><tr><td>ANYCSP</td><td>50</td><td>40</td></tr></table> (MAX- \(k\) - SAT for \(k = 3,4,5\) ), graph colorability ( \(k\) - COL), maximum cut (MAXCUT) as well as random CSPs (generated by the so- called MODEL RB). These problems are of high theoretical and practical importance and are commonly used to benchmark CSP heuristics. We train one ANYCSP model for each of these problems using randomly generated instances. Recall that the process of learning problem- specific heuristics with ANYCSP is purely data- driven as our architecture is generic and can take any CSP instance as input. We will compare the performance of ANYCSP to classical solvers and heuristics as well as previous neural approaches. When applicable, we also tune the configuration of the classical algorithms on our validation data to ensure a fair comparison. All neural approaches run with one NVIDIA Quadro RTX A6000 GPU with 48GB of memory. All classical approaches run on an Intel Xeon Platinum 8160 CPU (2.1 GHz) and 64GB of RAM. MODEL RB First, we evaluate ANYCSP on general CSP benchmark instances generated by the MODEL RB (Xu and Li 2003). Our training distribution \(\Omega_{\mathrm{RB}}\) consists of randomly generated MODEL RB instances with 30 variables and arity 2. The test dataset RB50 contains 50 satisfiable instances obtained from the XCSP project (Audemard et al. 2020). These instances each contain 50 variables, domains with 23 values and roughly 500 constraints. They are commonly used as part of the XCSP Competition to evaluate state- of- the- art CSP solvers. Note that the hardness of MODEL RB problems comes from the dense, random constraint relations chosen at the threshold of satisfiability and even instances with 50 variables are very challenging. We will compare ANYCSP to three state- of- the- art CSP solvers: Picat (Zhou 2022), ACE (Lecoutre 2022) and Co So Co (Audemard 2018). Picat is a SAT- based solver while ACE and Co So Co are based on constraint propagation. Picat in particular is the winner of the most recent XCSP Competition (Audemard, Lecoutre, and Lonca 2022). No prior neural baseline exists for this problem. Figure 3 provides a the results on the RB50 dataset. All algorithms run once on each instance with a 20 Minute timeout. ANYCSP solves the most instances by a substantial margin. The second strongest approach is the Co So Co solver which solves 34 instances in total, 8 less than ANYCSP. Within the timeout of 20 Minutes, ANYCSP will perform 500K search iterations. Recall that we set \(T_{\mathrm{train}} = 40\) . Therefore,

Table 2: MAx CUT results on Gset graphs. The graphs are grouped by their vertex counts and we provide the mean deviation from the best known cut size. <table><tr><td>METHOD</td><td>|V|=800</td><td>|V|=1K</td><td>|V|=2K</td><td>|V|≥3K</td></tr><tr><td>GREEDY</td><td>411.44</td><td>359.11</td><td>737.00</td><td>774.25</td></tr><tr><td>SDP</td><td>245.44</td><td>229.22</td><td>-</td><td>-</td></tr><tr><td>RUNCSP</td><td>185.89</td><td>156.56</td><td>357.33</td><td>401.00</td></tr><tr><td>ECO-DQN</td><td>65.11</td><td>54.67</td><td>157.00</td><td>428.25</td></tr><tr><td>ECORD</td><td>8.67</td><td>8.78</td><td>39.22</td><td>187.75</td></tr><tr><td>ANYCSP</td><td>1.22</td><td>2.44</td><td>13.11</td><td>51.63</td></tr></table> the learned policy generalizes to searches that are over 10K times longer than those seen during training. Graph Coloring We consider the problem of finding a conflict- free vertex coloring given a graph \(G\) and number of colors \(k\) . The corresponding CSP instance has variables for each vertex, domains containing the \(k\) colors and one binary " \(\neq\) "- constraint for each edge. We train on a distribution \(\Omega_{\mathrm{COL}}\) of graph coloring instances for random graphs with 50 vertices. We mix Erdős- Rényi, Barabási- Albert and random geometric graphs in equal parts. The number of colors is chosen to be in [3, 10]. As test instances we use 100 structured benchmark graphs with known chromatic number \(\mathcal{X}(G)\) . The instances are obtained from a collection of hard coloring instances commonly used to benchmark heuristics<sup>4</sup>. They are highly structured and come from a wide range of synthetic and real problems. We divide the test graphs into two sets with 50 graphs each: \(\mathrm{COL}_{< 10}\) contains graphs with \(\mathcal{X}(G) < 10\) and \(\mathrm{COL}_{> 10}\) contains graphs with \(\mathcal{X}(G) \geq 10\) . The graphs in \(\mathrm{COL}_{> 10}\) have up to 1K vertices, 19K edges and a chromatic number of up to 73. This experiment tests generalization to larger domains and more complex structures. We compare the performance to three problem specific heuristics: a simple greedy algorithm, the classic heuristic DSATUR (Brélaz 1979) and the state- of- the- art heuristic Hybrid EA (Galinier and Hao 1999), all implemented efficiently by Lewis et al. (2012); Lewis (2015). We also evaluate the best two CSP solvers from the MODEL RB experiment. The neural baseline RUNCSP is also tested on \(\mathrm{COL}_{< 10}\) . Unlike ANYCSP, RUNCSP requires us to fix a domain size before training. Therefore, we must train one RUNCSP model for each tested chromatic number \(4 \leq \mathcal{X}(G) \leq 9\) and omit testing on \(\mathrm{COL}_{> 10}\) . We use the same training data as Tönshoff et al. (2021) for their experiments on structured coloring benchmarks. Table 1 provides the number of solved \(k\) - COL instances from both splits. ANYCSP is on par with Hybrid EA which solves the most instances of all baselines. RUNCSP solves significantly fewer instances than ANYCSP on \(\mathrm{COL}_{< 10}\) and outperforms only the simple greedy approach. ANYCSP solves 40 out of the 50 instances in \(\mathrm{COL}_{> 10}\) . The optimally colored graphs include the largest instance with 73 colors. Since ANYCSP trains with 3 to 10 colors the trained model Table 3: Number of solved 3-SAT benchmark instances from SATLIB. For each number of variables there are 100 satisfiable test instances. <table><tr><td>METHOD</td><td>SL50</td><td>SL100</td><td>SL150</td><td>SL200</td><td>SL250</td></tr><tr><td>RLSAT</td><td>100</td><td>87</td><td>67</td><td>27</td><td>12</td></tr><tr><td>PDP</td><td>93</td><td>79</td><td>72</td><td>57</td><td>61</td></tr><tr><td>WALK SAT</td><td>100</td><td>100</td><td>97</td><td>93</td><td>87</td></tr><tr><td>PROB SAT</td><td>100</td><td>100</td><td>97</td><td>87</td><td>92</td></tr><tr><td>ANYCSP</td><td>100</td><td>100</td><td>100</td><td>97</td><td>99</td></tr></table> is able to generalize to significantly larger domains. MAXCUT For MAXCUT we train on the distribution \(\Omega_{\mathrm{MCUT}}\) of random unweighted Erdős- Rényi graphs with 100 vertices and an edge probability \(p \in [0.05, 0.3]\) . Our test data is Gset (Ye 2003), a collection of commonly used MAXCUT benchmarks of varying structure with 800 to 10K vertices. We evaluate three neural baselines: RUNCSP, ECO- DQN (Barrett et al. 2020) and ECORD (Barrett, Personson, and Laterre 2022). RUNCSP is also trained on \(\Omega_{\mathrm{MCUT}}\) . We train and validate ECO- DQN and ECORD models with the same data that Barrett, Personson, and Laterre (2022) used for their Gset experiments. We omit S2V- DQN (Khalil et al. 2017) since ECO- DQN and ECORD have been shown to yield substantially better cuts. We adopt the evaluation setup of ECORD and run the neural methods with 20 parallel runs and a timeout of 180s on all unweighted instances of Gset. The results of a standard greedy construction algorithm and the well- known SDP based approximation algorithm by Goemans and Williamson (1995) are also included as classical baselines. Both are implemented by Mehta (2019). SDP runs with a 3 hour timeout for graphs with up to 1K vertices. Table 2 provides results for Gset. We divide the test graphs into groups by the number of vertices (8- 9 graphs per group) and report the mean deviation from the best- known cuts obtained by Benlic and Hao (2013) for each method. ANYCSP outperforms all baselines across all graph sizes by a large margin. Recall that RUNCSP trains on a soft relaxation of MAXCUT while ECO- DQN and ECORD are both neural local search approaches. Neither concept matches the results of our global search approach trained with policy gradients. 3- SAT For 3- SAT we choose the training distribution \(\Omega_{\mathrm{3SAT}}\) as uniform random 3- SAT instances with 100 variables. The ratio of clauses to variables is drawn uniformly from the interval [4, 5]. For 3- SAT we test on commonly used benchmark instances for uniform 3- SAT from SATLIB<sup>5</sup>. The test set \(\mathrm{SLN}\) contains 100 instances with \(N \in \{50, 100, 150, 200, 250\}\) variables each. The density of these formulas is at the threshold of satisfiability. We evaluate two neural baselines: RLSAT (Yolcu and Póczos 2019) and PDP (Amizadeh, Matusevych, and Weimer 2019). PDP is also trained on \(\Omega_{\mathrm{3SAT}}\) . We train RLSAT with the curriculum learning dataset for 3- SAT provided by its authors, since its reward scheme is incompatible with our partially unsatisfi

Table 4: Results on Max- \(k\) -SAT instances with 10K variables. For each \(k\in \{3,4,5\}\) we provide the mean number of unsatisfied clauses over 50 random instances. <table><tr><td>METHOD</td><td>3CNF</td><td>4CNF</td><td>5CNF</td></tr><tr><td>WALKSAT</td><td>2145.28</td><td>1556.68</td><td>1685.10</td></tr><tr><td>CCLS</td><td>1567.24</td><td>1323.14</td><td>1315.96</td></tr><tr><td>SATLIKE</td><td>1595.86</td><td>1188.56</td><td>1152.88</td></tr><tr><td>ANYCSP</td><td>1537.46</td><td>1126.44</td><td>1103.14</td></tr></table> able training instances. We also adopt the experimental setup of RLSAT, which limits the evaluation run by the number of search steps instead of a timeout. The provided code for both PDP and RLSAT is comparatively slow and a timeout would compare implementation details rather than the capability of the learned algorithms. We also evaluate two conventional local search heuristics: The classical Walk SAT algorithm (Selman et al. 1993) based on random walks and a modern probabilistic approach called prob SAT (Balint and Schöning 2018). Like Yolcu and Póczos (2019), we apply stochastic boosting and run each method 10 times for 10K steps on every instance. PDP is deterministic and only applied once to each formula. Table 3 provides the number of solved instances for each tested size. All compared approaches do reasonably well on small instances with 50 variables. However, the performance of the two neural baselines drops significantly as the number of variables increases. ANYCSP does not suffer from this issue and even outperforms the classical local search algorithms on the three largest instance sizes considered here. MAX- \(k\) - SAT We train on the distribution \(\Omega_{\mathrm{MSAT}}\) of uniform random MAX- \(k\) - SAT instances with 100 variables and \(k\in \{3,4\}\) . Here, the clause/variable ratio is chosen from [5, 8] and [10, 16] for \(k = 3\) and \(k = 4\) , respectively. These formulas are denser than those of \(\Omega_{3\mathrm{SAT}}\) since we aim to train for the maximization task. Our test data for MAX- \(k\) - SAT consists of uniform random \(k\) - CNF formulas generated by us. For each \(k\in \{3,4,5\}\) we generate 50 instances with 10K variables each. The number of clauses is chosen as 75K for \(k = 3\) , 150K for \(k = 4\) and 300K for \(k = 5\) . These formulas are therefore 100 times larger than the training data and aim to test the generalization to significantly larger instances as well as unseen arities, since \(k = 5\) is not used for training. Neural baselines for SAT focus primarily on decision problems. For MAX- \(k\) - SAT we therefore compare ANYCSP only to conventional search heuristics: the classical (Max- )Walk SAT (Selman et al. 1993) and two state- of- the- art MAX- SAT local search heuristics CCLS (Luo et al. 2015) and SATLike (Cai and Lei 2020). Table 4 provides a comparison. We provide the mean number of unsatisfied clauses after processing each instance with a 20 Minute timeout. Remarkably, ANYCSP outperforms all classical baselines by a significant margin. We point out that the conventional search heuristics all perform over 100M search steps in the 20 Minute timeout. ANYCSP performs less than 100K steps on each instance in this experiment. The GNN cannot match the speed with <center>Figure 4: Detailed results for MAX-5-SAT. For each test instance and each method we plot the number of unsatisfied clauses in the best found solution against the search step in which it was found. </center> which classical algorithms iterate, even though it is accelerated by a GPU. Despite this, ANYCSP consistently finds the best solutions. Figure 4 evaluates this surprising observation further. We plot the number of unsatisfied clauses in the best found solution against the search step in which the solution was found (Steps to Opt.) for all methods and all instances of our MAX- 5- SAT test data. We also provide the results of a modified ANYCSP version (ANYCSP Local defined in Appendix C) that is only allowed to change one variable at a time and is therefore a local search heuristic. Note that the \(x\) - axis is logarithmic as there is a clear dichotomy separating neural and classical approaches: Compared to conventional heuristics ANYCSP performs roughly three orders of magnitude fewer search steps in the same amount of time. When restricted to local search, ANYCSP is unable to overcome this deficit and yields worse results than strong heuristics such as SATLike. However, when ANYCSP leverages global search to parallelize refinements across the whole instance it can find solutions in 100K steps that elude state- of- the- art local search heuristics after well over 100M iterations. ## 6 Conclusion We have introduced ANYCSP, a novel method for neural combinatorial optimization to learn heuristics for any CSP through a purely data- driven process. Our experiments demonstrate how the generic architecture of our method can learn effective search algorithms for a wide range of problems. We also observe that standard policy gradient descent methods like REINFORCE are capable of learning on an exponentially sized action space to obtain global search heuristics for NP- hard problems. This is a critical advantage when processing large problem instances. Directions for future work include widening the scope of the architecture even further: Weighted and partial CSPs are a natural extension of the CSP formalism and could be incorporated through node features and adjustments to the reward scheme. Variables with real- valued domains may be

another viable extension as policy gradient descent is also applicable to infinite continuous action spaces. ## References Abboud, R.; Ceylan, I. I.; Grohe, M.; and Lukasiewicz, T. 2021. The Surprising Power of Graph Neural Networks with Random Node Initialization. In Zhou, Z.- H., ed., Proceedings of the 30th International Joint Conference on Artificial Intelligence, 2112- 2118. Amizadeh, S.; Matusevych, S.; and Weimer, M. 2019. PDP: A general neural framework for learning constraint satisfaction solvers. ar Xiv preprint ar Xiv:1903.01969. Audemard, G. 2018. Co So Co 1.12. In XCSP3 Competition 2018 Proceedings, XCSP3 Competition, 78- 79. Audemard, G.; Boussemart, F.; Lecoutre, C.; Piette, C.; and Roussel, O. 2020. XCSP3 and its ecosystem. Constraints, 25(1): 47- 69. Audemard, G.; Lecoutre, C.; and Lonca, E., eds. 2022. XCSP3 Competition 2022 Proceedings, XCSP3 Competition. Artois, France. Balint, A.; and Schöning, U. 2018. prob SAT. In Proceedings of SAT Competition 2018: Solver and Benchmark Descriptions. Department of Computer Science, University of Helsinki. Barrett, T.; Clements, W.; Foerster, J.; and Lvovsky, A. 2020. Exploratory combinatorial optimization with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, 3243- 3250. Barrett, T. D.; Parsonson, C. W.; and Laterre, A. 2022. Learning to Solve Combinatorial Graph Partitioning Problems via Efficient Exploration. ar Xiv preprint ar Xiv:2205.14105. Bello, I.; Pham, H.; Le, Q. V.; Norouzi, M.; and Bengio, S. 2016. Neural combinatorial optimization with reinforcement learning. ar Xiv preprint ar Xiv:1611.09940. Benlic, U.; and Hao, J.- K. 2013. Breakout Local Search for the Max- Cutproblem. Engineering Applications of Artificial Intelligence, 26(3): 1162- 1173. Biere, A.; Heule, M.; Van Maaren, H.; and Walsh, T. 2021. Handbook of Satisfiability. IOS Press, 2nd edition. ISBN 978- 1- 64368- 160- 3. Brelaz, D. 1979. New methods to color the vertices of a graph. Communications of the ACM, 22(4): 251- 256. Cai, S.; and Lei, Z. 2020. Old techniques in new ways: Clause weighting, unit propagation and hybridization for maximum satisfiability. Artificial Intelligence, 287: 103354. Cappart, Q.; Chetelat, D.; Khalil, E. B.; Lodi, A.; Morris, C.; and Veličković, P. 2021. Combinatorial Optimization and Reasoning with Graph Neural Networks. In Zhou, Z.- H., ed., Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI- 21, 4348- 4355. International Joint Conferences on Artificial Intelligence Organization. Survey Track. Galinier, P.; and Hao, J.- K. 1999. Hybrid evolutionary algorithms for graph coloring. Journal of combinatorial optimization, 3(4): 379- 397. Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and Dahl, G. E. 2017. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, 1263- 1272. JMLR. org. Goemans, M. X.; and Williamson, D. P. 1995. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6): 1115- 1145. Hagberg, A.; Swart, P.; and S Chult, D. 2008. Exploring network structure, dynamics, and function using Network X. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States). Joshi, C. K.; Cappart, Q.; Rousseau, L.; Laurent, T.; and Bresson, X. 2020. Learning TSP Requires Rethinking Generalization. Co RR, abs/2006.07054. Khalil, E.; Dai, H.; Zhang, Y.; Dilkina, B.; and Song, L. 2017. Learning combinatorial optimization algorithms over graphs. Advances in neural information processing systems, 30. Kool, W.; van Hoof, H.; and Welling, M. 2018. Attention, Learn to Solve Routing Problems! In International Conference on Learning Representations. Lecoutre, C. 2022. ACE A Generic Constraint Solver. In XCSP3 Competition 2022 Proceedings, XCSP3 Competition, 58- 59. Lewis, R. 2015. A guide to graph colouring, volume 7. Springer. Lewis, R.; Thompson, J.; Mumford, C.; and Gillard, J. 2012. A wide- ranging computational comparison of high- performance graph colouring algorithms. Computers & Operations Research, 39(9): 1933- 1950. Luo, C.; Cai, S.; Wu, W.; Jie, Z.; and Su, K. 2015. CCLS: An Efficient Local Search Algorithm for Weighted Maximum Satisfiability. IEEE Transactions on Computers, 64(7): 1830- 1843. Mehta, H. 2019. CVX Graph Algorithms. https://github.com/hermish/cvx- graph-algorithms. Russell, S.; Russell, S.; Norvig, P.; and Davis, E. 2020. Artificial Intelligence: A Modern Approach. Prentice Hall, 4th edition. ISBN 9780136042594. Sato, R.; Yamada, M.; and Kashima, H. 2021. Random features strengthen graph neural networks. In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), 333- 341. SIAM. Selman, B.; Kautz, H. A.; Cohen, B.; et al. 1993. Local search strategies for satisfiability testing. Cliques, coloring, and satisfiability, 26: 521- 532. Selsam, D.; Lamm, M.; Bünz, B.; Liang, P.; de Moura, L.; and Dill, D. L. 2018. Learning a SAT solver from single- bit supervision. ar Xiv preprint ar Xiv:1802.03685. Tönshoff, J.; Ritzert, M.; Wolf, H.; and Grohe, M. 2021. Graph Neural Networks for Maximum Constraint Satisfaction. Frontiers in Artificial Intelligence, 3. Veličković, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2017. Graph attention networks. ar Xiv preprint ar Xiv:1710.10903.

Vinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer networks. Advances in neural information processing systems, 28. Wang, R.; Hua, Z.; Liu, G.; Zhang, J.; Yan, J.; Qi, F.; Yang, S.; Zhou, J.; and Yang, X. 2021. A Bi- Level Framework for Learning to Solve Combinatorial Optimization on Graphs. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems, volume 34, 21453- 21466. Curran Associates, Inc. Williams, R. J. 1992. Simple statistical gradient- following algorithms for connectionist reinforcement learning. Machine learning, 8(3): 229- 256. Xu, K.; and Li, W. 2003. Many Hard Examples in Exact Phase Transitions. Science Direct Working Paper No S1574- 034X (04), 70228- 8. Yao, F.; Cai, R.; and Wang, H. 2021. Reversible Action Design for Combinatorial Optimization with Reinforcement Learning. In AAAI- 22 Workshop on Machine Learning for Operations Research (ML4OR). Ye, Y. 2003. Gset. https://web.stanford.edu/yyye/yyye/Gset/. Yolcu, E.; and Póczos, B. 2019. Learning local search heuristics for boolean satisfiability. Advances in Neural Information Processing Systems, 32. Zhou, N.- F. 2022. An XCSP3 Solver in Picat. In XCSP3 Competition 2022 Proceedings, XCSP3 Competition, 79- 81.