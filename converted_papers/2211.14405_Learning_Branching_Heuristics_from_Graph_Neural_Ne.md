# Learning Branching Heuristics from Graph Neural Ne


## Introduction


is worse than the softmax function. To calculate the joint information entropy of random variables in \(C^{\prime}\cap S =\) \(\{X_{1},\dots,X_{m}\}\) , we try two ways. The first one, called the fast version because the calculation of this way is fast but not accurate, is \(\begin{array}{r}{\sum_{i = 1}^{m}\mathbb{H}(X_{i}) - \Big(\prod_{i = 1}^{m}(1 - } \end{array}\) \(p_{i})\Big)\log_{2}\Big(\prod_{i = 1}^{m}(1 - p_{i})\Big)\) . \(X_{i}\) 's are mutually independent, so their joint information entropy is the sum of each one's information entropy. In addition, if there is a dominating clique, at least one variable in \(C^{\prime}\cap S\) is 1; we thus minus the information entropy of the case that all variables in \(C^{\prime}\cap S\) are 0. The second way, called the accurate version but with a slow calculation, is similar to the improvement on Equation (6). Instead of calculating the joint information entropy of random variables in \((C^{\prime}\cap S)\) , we calculate the joint information entropy under a more precise case. Given a clause \(C^{\prime}\cap S = \{X_{1},\dots,X_{m}\}\) , for \(i = 1\) to \(m\) , we iteratively set \(X_{1},\dots,X_{i - 1}\) as 0 and \(X_{i}\) as 1. We know that \(\{X_{i + 1},\dots,X_{m}\} \setminus \{X_{j}\}_{v_{j}\in N(v_{i})}\) must be 0 from the properties of dominating cliques. Thus, we only need to consider the joint information entropy of the random variables in \(N[v_{i}]\setminus \{v_{1},\dots,v_{i - 1}\}\) . Similar to Equation (7), we calculate the joint information entropy as \[\sum_{i = 1}^{m}\Big(p\big(-\log_{2}(p) + \sum_{v_{r}\in N(v_{i})\setminus \{v_{1},\dots,v_{i - 1}\}}\mathbb{H}(X_{r})\big)\Big) \quad (8)\] where \(\begin{array}{r}{p = p_{i}\prod_{j = 1}^{i - 1}(1 - p_{j})\prod_{v_{k}\in \{v_{i + 1},\dots,v_{m}\}\setminus N(v_{i})}(1 - p_{k}).} \end{array}\) ## 4 Experimental Results ### 4.1 Finding Maximum Cliques Table 1 shows the experimental results of approximation ratio (i.e. the ratio of the size of the clique we find versus the size of the maximum clique). We use the greedy approximation algorithm in [8] to find large cliques, and the maximum clique is from Gurobi [13]. The greedy approximation algorithm has two ways, fast and slow, to decode cliques using the learned probability distributions. The fast way returns quickly but with a less use of the input probability distributions compared to the slow way. We run experiments for two GNN models. These two models have the same neural network architecture as the model in [8]. The only difference is that one uses our loss function (4) and another uses the original loss function in [8]. The datasets Twitter, COLLAB, and IMDB used in the experiments are from [8] as well. ### 4.2 Finding Dominating Cliques We run experiments on three dominating- clique solvers: the original solver in [9] and the GNN- enhanced solvers (the output distributions from the GNN model with the loss function (5) as input) using the fast and accurate calculations of joint information entropy respectively.

Table 1: Results on test dataset: approximation ratios on real-world datasets and \(G(n,p)\) instances; the average number of nodes of \(G(n,p)\) instances is similar to the average number of nodes of graphs in Twitter (about 120 nodes per graph); Sparse means that the instances are generated with \(0.2\leq p\leq 0.4\) ; Dense means that the instances are generated with \(0.6\leq p\leq 0.8\) ; italics indicates the standard deviation. <table><tr><td rowspan="2">Dataset</td><td colspan="2">Loss function as (4)</td><td colspan="2">Loss function in [8]</td></tr><tr><td>Slow</td><td>Fast</td><td>Slow</td><td>Fast</td></tr><tr><td>Twitter</td><td>0.931 ± 0.099</td><td>0.927 ± 0.112</td><td>0.956 ± 0.077</td><td>0.914 ± 0.141</td></tr><tr><td>COLLAB</td><td>1.000 ± 0.000</td><td>1.000 ± 0.000</td><td>1.000 ± 0.000</td><td>1.000 ± 0.001</td></tr><tr><td>IMDB</td><td>1.000 ± 0.000</td><td>1.000 ± 0.000</td><td>1.000 ± 0.010</td><td>1.000 ± 0.000</td></tr><tr><td>G(n,p)(Sparse)</td><td>0.910 ± 0.100</td><td>0.908 ± 0.102</td><td>0.922 ± 0.099</td><td>0.901 ± 0.104</td></tr><tr><td>G(n,p)(Dense)</td><td>0.901 ± 0.050</td><td>0.900 ± 0.047</td><td>0.886 ± 0.062</td><td>0.893 ± 0.053</td></tr></table> <center>Figure 1: Experimental results on test data for finding dominating cliques; X axis for \(n\) </center> Figure 1 shows the experimental results on test data. To be specific, Figure 1a shows the winning ratios of the three solvers on the instances that dominating cliques do not exist, and Figure 1c shows the winning ratios of the three solvers on the instances that dominating cliques exist. Figure 1b shows the ratios of the GNN- enhanced solvers to the original algorithm in terms of geometric

average number \(^2\) of their branches on the instances that dominating cliques do not exist. Figure 1d shows such ratios on the instances that dominating cliques exist. It is reasonable that the GNN- enhanced solvers perform better on the instances that dominating cliques do not exist. For such case, solvers go through the whole search tree, so the learned branching heuristic can prune more search space. Instead, for the case of solving instances that dominating cliques exist, the original heuristic might be equally effective since solvers return once a dominating clique is found. ### 4.3 Finding Minimum Dominating Cliques To find the minimum dominating clique, we add the following features to our dominating- clique solvers. We record the currently minimum dominating clique and update it once we find a dominating clique with lower cardinality, and so on until completing the whole backtracking search. In addition, we apply backjumping to prune the search space. Our evaluation order of the random variables in \(C \cap S\) is the decreasing order of the numbers of the unsatisfied clauses that these random variables involve. Thus, we can backjump three levels whenever we get the first dominating clique or find a dominating clique with lower cardinality than the currently minimum solution. Also, we can backjump two levels whenever the current recursive depth is equal to the cardinality of the currently minimum dominating clique. We prepare three GNN models for the experiments. Their loss functions are (5), (6) with \(\mathbb{E}(|S|) = \sum_{i = 1}^{n} p_{i}\) , and (6) with \(\mathbb{E}(|S|)\) as (7) respectively. Combined with the two calculations of joint information entropy, we have six solvers. Figure 2 shows the experimental results on test data for finding the minimum dominating clique. In particular, Figure 2a shows the winning ratios of the four solvers: the original solver and the other three GNN- enhanced solvers using the fast calculation of joint information entropy. We use (5) in the figure to represent the GNN- enhanced solver using the output probability distributions from the GNN model with the loss function (5). The (6) in the figure represents the GNN- enhanced solver using the output probability distributions from the GNN model with the loss function (6) where \(\mathbb{E}(|S|) = \sum_{i = 1}^{n} p_{i}\) . The (7) in the figure represents the GNN- enhanced solver using the output probability distributions from the GNN model with the loss function (6) where \(E(|S|)\) as (7). Figure 2b shows the winning ratios of these four solvers using the accurate calculation of joint information entropy. Figure 2c shows the ratios of these six GNN- enhanced solvers to the original solver in terms of the geometric average of branches. From Figure 2a and Figure 2b, we can see that the loss functions that have the term \(E(|S|)\) performs better than the loss function (5). Another observation from Figure 2c is that either the accurate calculation of joint information entropy or the accurate calculation of \(\mathbb{E}(|S|)\) improves the solver's performance.

<center>Figure 2: Experimental results on test data for finding minimum dominating cliques; X axis for \(n\) </center> ## 5 Concluding Remarks In this work, we have developed an approach that uses the probabilistic method in the design of GNN models to learn branching heuristics to enhance exact solvers for hard combinatorial problems. This approach can be applied to other COPs of finding an optimal constrained vertex subset of a given graph, such as the maximum/minimum dominating-independent-set problem or the maximum-clique problem. In the future, we would like to enhance this approach to solve the problems that ask for an optimal collection of vertex subsets where a vertex is allowed to be in multiple subsets, e.g. the tree-decomposition problem and the edge-clique-cover problem. We hope that our approach can be a starting effort of applying GNNs techniques to improve classical AI algorithms.

## 6 Acknowledgement 6 Acknowledgement This research was supported in part through computational resources and services provided by UBC Advanced Research Computing, "UBC ARC Sockeye." UBC Advanced Research Computing, 2022, doi: 10.14288/SOCKEYE. ## References References[1] Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton- Brown. Satzilla: portfolio- based algorithm selection for sat. Journal of artificial intelligence research, 32:565- 606, 2008. [2] Hong Xu, Sven Koenig, and TK Kumar. Towards effective deep learning for constraint satisfaction problems. In International Conference on Principles and Practice of Constraint Programming, pages 588- 597. Springer, 2018. [3] Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single- bit supervision. In International Conference on Learning Representations, 2019. [4] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural information processing systems, 28, 2015. [5] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. Advances in neural information processing systems, 31, 2018. [6] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in neural information processing systems, 30, 2017. [7] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2019. [8] Nikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs. In Advances in Neural Information Processing Systems, volume 33, pages 6659- 6672, 2020. [9] Joseph Culberson, Yong Gao, and Calin Anton. Phase transitions of dominating clique problem and their implications to heuristics in satisfiability search. In INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, volume 19, page 78, 2005. [10] Dieter Kratsch. Algorithms. In Domination in Graphs Advanced Topics, pages 191- 232. Routledge, 2017.

[11] Maximilian Bother, Otto Kißig, Martin Taraz, Sarel Cohen, Karen Seidel, and Tobias Friedrich. What's wrong with deep learning in tree search for combinatorial optimization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=mk0Hzdq Y7i1. [12] Noga Alon and Joel H. Spencer. The probabilistic method. 2nd edition, 2000. [13] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2022. URL https://www.gurobi.com. [14] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pages 448- 456, 2015. [16] Yoshua Bengio. Practical recommendations for gradient- based training of deep architectures. In Neural networks: Tricks of the trade, pages 437- 478. Springer, 2012. ## A Neural Network Architecture of GNN Model for Dominating Cliques We implement our GNN model by Py Torch 1.9.1 and Py Torch Geometric 2.0.1. We set the node features of an input graph as 1- dimensional vectors. As shown above, we interpret these features as the Bernoulli distributions associated with vertices. Our GNN model consists of 8 layers: 6 graph- isomorphism- network (GIN) layers and 2 linear layers followed. The reason of using GINs is that the properties of sub- structures of the graphs containing dominating cliques are similar. For example, the diameters of the graphs containing dominating cliques are at most 3, and they all have a clique as a kernel. GIN is a good option here because it has the most powerful ability in GNN layers to detect isomorphic subgraphs [14]. We apply the batch normalization on the output of each layer as it is a standard operation in GNNs, see [15]. In addition, we set the batch size as 32 because this number has a good practice in real applications [16]. Furthermore, our dominating- clique solvers are implemented by the programming language C and run on Ubuntu 20.04.4 LTS with Intel CPU i7- 11700F and 48GB RAM.

## B Preliminary Test for Dominating Cliques To check whether Equation (5) works effectively as the loss function of the GNN model for finding dominating cliques, we create other \(G(n,p)\) instances with different \(n\) and \(p\) . In particular, \(n\) ranges from 25 to 400 in increments of 25, and \(p\) ranges from 0.1 to 0.9 in increments of 0.2, so we have 80 pairs of \(n\) and \(p\) . Then we generate 32 instances from \(G(n,p)\) for each pair of \(n\) and \(p\) and check the values of loss function of these instances output from the well- trained probabilistic- method GNN model. From the above phase transition, the instances generated from \(G(n,p)\) where \(p = 0.1\) have low likelihood of containing a dominating clique; on the contrary, the instances generated from \(G(n,p)\) where \(p = 0.9\) are more likely to have dominating cliques. Figure 3 shows the average of the values of loss function of the 32 instances for each pair of \(n\) and \(p\) . From Figure 3, we can see that the average of the values of loss function of the instances generated from \(G(n,p)\) where \(p = 0.1\) are much greater than the corresponding average of the instances generated from \(G(n,p)\) where \(p = 0.9\) , which is consistent with the known phase transition because the greater loss- function values imply the lower probabilities of existing a dominating clique. <center>Figure 3: Averages of the values of loss function of 32 instances for each pair of \(n\) and \(p\) ; the x-axis is the size of graphs; the y-axis is the average of the values of loss function. </center> ## C Training Data for (Minimum) Dominating Cliques Our data is generated from \(G(n,p)\) . The work in [9] shows that the phase transition of dominating cliques happens at the exact threshold \(p = \frac{3 - \sqrt{5}}{2} \approx 0.381\) . It is also mentioned in [9] that \(p\) around 0.371 is a good empirical probability to create the instances that it is hard to predict the existence of dominating cliques. We call such instances as hard instances.

For learning a branching heuristic to find a dominating clique, our training data contains 2650 instances generated from \(G(n,p)\) with the range of \(n\) being from 75 to 800. Among these instances, 1250 instances are generated from \(G(n,p)\) with \(p\) being around 0.371 as hard instances. The remaining 1400 instances are generated from \(G(n,p)\) with \(p \in (0,0.35) \cup (0.4,1)\) . we call them easy instances as they are generated outside the phase transition and relatively easier to be solved. We use 750 hard instances and all easy instances as our training data. The remaining 500 hard instances are used as validation data and test data. For learning a branching heuristic to find the minimum dominating clique, we create another 1250 instances (750 instances are used in training data, and the remaining 500 instances are used as validation data and test data) from \(G(n,p)\) to replace the hard instances. The range of \(n\) of these new instances is the same as the hard instances above, but the range of \(p\) is between 0.4 and 0.41. The reason is that if a graph is dense, there are many dominating cliques with minimum size. If so, commonly used branching heuristics are powerful enough because the first- found solution is usually the minimum solution. Therefore, we choose this range of \(p\) to generate the instances such that they have some dominating cliques but only few of these dominating cliques with minimum size. Additionally, we keep the same easy instances above in the training data for finding the minimum dominating clique. ## D Raw Experimental Results for Finding Dominating Cliques We run three solvers, the original solver in [9] (abbreviated as O) and the other two GNN- enhanced solvers with the fast and accurate calculations for joint information entropy (abbreviated as F and A) respectively on the training, validation, and test data where \(p\) is around 0.371. We compare the number of branches of their search trees. Table 2 shows the results on the training data. Tables 3 and 4 show the results on the validation and test data respectively. The columns O vs F, O vs A, and A vs F record the number of times that the corresponding solver wins. The Avg columns are the arithmetic average of the number of branches. The numbers in brackets are the results of the instances that dominating cliques exist.

Table 2: Results on Training Data (DC): 50 instances for each \(n\) <table><tr><td rowspan="2">G(n,p)</td><td colspan="3">O vs F</td><td colspan="3">O vs A</td><td colspan="3">F vs A</td><td colspan="3">Avg</td></tr><tr><td>n</td><td>p</td><td>O</td><td>F</td><td>O</td><td>A</td><td>F</td><td>A</td><td>O</td><td>F</td><td>A</td><td></td></tr><tr><td>75</td><td>0.3698</td><td>27 (13)</td><td>23 (10)</td><td>26 (13)</td><td>24 (10)</td><td>12 (3)</td><td>38 (20)</td><td>97.82 (35.7)</td><td>101.74 (44.17)</td><td>102.6 (46.04)</td><td></td><td></td></tr><tr><td>150</td><td>0.3663</td><td>18 (10)</td><td>32 (16)</td><td>17 (10)</td><td>33 (16)</td><td>16 (9)</td><td>34 (17)</td><td>77.86 (307.15)</td><td>75.1 (279.65)</td><td>785.42 (343.81)</td><td></td><td></td></tr><tr><td>225</td><td>0.368</td><td>25 (16)</td><td>25 (11)</td><td>18 (16)</td><td>32 (11)</td><td>12 (10)</td><td>38 (17)</td><td>3320.44 (1151.48)</td><td>3333.22 (1190.11)</td><td>3335.72 (1224.78)</td><td></td><td></td></tr><tr><td>250</td><td>0.3669</td><td>16 (10)</td><td>34 (15)</td><td>17 (11)</td><td>33 (14)</td><td>13 (9)</td><td>37 (16)</td><td>5917 (3062.56)</td><td>5762.3 (2815.04)</td><td>5876.2 (3080.96)</td><td></td><td></td></tr><tr><td>300</td><td>0.3674</td><td>15 (9)</td><td>35 (8)</td><td>10 (5)</td><td>10 (5)</td><td>10 (5)</td><td>40 (12)</td><td>14592.46 (5661.06)</td><td>14584.48 (5857.94)</td><td>14366.88 (5342.59)</td><td></td><td></td></tr><tr><td>350</td><td>0.3671</td><td>13 (10)</td><td>37 (12)</td><td>11 (10)</td><td>39 (12)</td><td>11 (8)</td><td>39 (14)</td><td>23925.8 (7330.55)</td><td>24209.86 (8431.23)</td><td>24070.1 (8051.41)</td><td></td><td></td></tr><tr><td>375</td><td>0.3725</td><td>16 (15)</td><td>34 (21)</td><td>15 (14)</td><td>35 (22)</td><td>13 (12)</td><td>37 (24)</td><td>24414.82 (1056.17)</td><td>23439.86 (9397.83)</td><td>23376.08 (9291.94)</td><td></td><td></td></tr><tr><td>425</td><td>0.3685</td><td>11 (9)</td><td>39 (15)</td><td>11 (9)</td><td>39 (15)</td><td>8 (8)</td><td>42 (16)</td><td>53689.16 (13055.96)</td><td>53672.14 (13066.25)</td><td>53347.44 (12800.75)</td><td></td><td></td></tr><tr><td>475</td><td>0.3663</td><td>26 (8)</td><td>24 (10)</td><td>19 (9)</td><td>31 (9)</td><td>4 (3)</td><td>46 (15)</td><td>99739.56 (24710.89)</td><td>99646.66 (24120.44)</td><td>100467.5 (27396)</td><td></td><td></td></tr><tr><td>525</td><td>0.366</td><td>24 (9)</td><td>26 (10)</td><td>21 (10)</td><td>29 (9)</td><td>4 (4)</td><td>46 (15)</td><td>114092.76 (36525.53)</td><td>114092.76 (36525.53)</td><td>114873.1 (30582.47)</td><td></td><td></td></tr><tr><td>575</td><td>0.368</td><td>35 (10)</td><td>15 (13)</td><td>15 (6)</td><td>35 (17)</td><td>1 (1)</td><td>29 (22)</td><td>259032.36 (91339.04)</td><td>246610.04 (10449.78)</td><td>23634.26 (65521.7)</td><td></td><td></td></tr><tr><td>625</td><td>0.3669</td><td>10 (6)</td><td>40 (8)</td><td>6 (6)</td><td>44 (8)</td><td>2 (2)</td><td>48 (12)</td><td>398240.3 (105061.86)</td><td>398714.52 (115036.64)</td><td>396739.72 (114490.5)</td><td></td><td></td></tr><tr><td>675</td><td>0.3685</td><td>10 (10)</td><td>40 (10)</td><td>9 (9)</td><td>41 (11)</td><td>0 (0)</td><td>50 (20)</td><td>551684.76 (172927.9)</td><td>549085.24 (171353.65)</td><td>545489.22 (169937.75)</td><td></td><td></td></tr><tr><td>700</td><td>0.3671</td><td>6 (6)</td><td>44 (11)</td><td>5 (5)</td><td>45 (12)</td><td>1 (1)</td><td>49 (16)</td><td>657604.96 (155241.12)</td><td>654205.9 (152510.88)</td><td>640940.52 (124779.12)</td><td></td><td></td></tr><tr><td>750</td><td>0.368</td><td>9 (8)</td><td>41 (5)</td><td>8 (7)</td><td>42 (6)</td><td>0 (0)</td><td>50 (13)</td><td>1129832.74 (507520.31)</td><td>1127597.54 (508628.38)</td><td>1120142.64 (505230.54)</td><td></td><td></td></tr></table> Table 3: Results on Validation Data (DC): 50 instances for each \(n\) <table><tr><td rowspan="2">G(n,p)</td><td colspan="3">O vs F</td><td colspan="3">O vs A</td><td colspan="3">F vs A</td><td colspan="3">Avg</td></tr><tr><td>n</td><td>p</td><td>O</td><td>F</td><td>O</td><td>A</td><td>F</td><td>A</td><td>O</td><td>F</td><td>A</td><td></td></tr><tr><td>100</td><td>0.3685</td><td>18 (11)</td><td>32 (20)</td><td>22 (14)</td><td>28 (17)</td><td>19 (10)</td><td>31 (21)</td><td>187.3 (74.61)</td><td>185.14 (74.03)</td><td>186.08 (75.19)</td><td></td><td></td></tr><tr><td>275</td><td>0.3669</td><td>12 (6)</td><td>38 (13)</td><td>11 (6)</td><td>39 (13)</td><td>16 (9)</td><td>34 (17)</td><td>774.86 (307.15)</td><td>755.1 (279.65)</td><td>785.42 (343.81)</td><td></td><td></td></tr><tr><td>400</td><td>0.3698</td><td>16 (14)</td><td>34 (16)</td><td>15 (13)</td><td>35 (17)</td><td>11 (10)</td><td>39 (20)</td><td>36675.62 (13529.03)</td><td>36165.5 (12864.77)</td><td>36733.5 (13968.6)</td><td></td><td></td></tr><tr><td>550</td><td>0.3689</td><td>29 (17)</td><td>21 (11)</td><td>19 (17)</td><td>31 (11)</td><td>4 (4)</td><td>46 (24)</td><td>162022.64 (56828.25)</td><td>162534.38 (62492.5)</td><td>164349.24 (62158.89)</td><td></td><td></td></tr><tr><td>725</td><td>0.3675</td><td>7 (7)</td><td>43 (9)</td><td>7 (7)</td><td>43 (9)</td><td>1 (1)</td><td>49 (15)</td><td>819762.54 (213827.06)</td><td>804655.12 (173782.5)</td><td>799592.72 (172590.38)</td><td></td><td></td></tr></table> Table 4: Results on Test Data (DC): 50 instances for each \(n\) <table><tr><td rowspan="2">G(n,p)</td><td colspan="3">O vs F</td><td colspan="3">O vs A</td><td colspan="3">F vs A</td><td colspan="3">Avg</td></tr><tr><td>n</td><td>p</td><td>O</td><td>F</td><td>O</td><td>A</td><td>F</td><td>A</td><td>O</td><td>F</td><td>A</td><td></td></tr><tr><td>200</td><td>0.3689</td><td>16 (13)</td><td>34 (18)</td><td>14 (11)</td><td>36 (20)</td><td>20 (11)</td><td>30 (20)</td><td>1854.12 (611.52)</td><td>1805.5 (555.9)</td><td>1931.72 (763.52)</td><td></td><td></td></tr><tr><td>325</td><td>0.3685</td><td>13 (9)</td><td>37 (11)</td><td>10 (8)</td><td>40 (12)</td><td>3 (1)</td><td>47 (19)</td><td>18753.9 (4945)</td><td>18788.72 (5244.9)</td><td>18704.34 (5212.85)</td><td></td><td></td></tr><tr><td>450</td><td>0.37</td><td>15 (14)</td><td>35 (16)</td><td>14 (14)</td><td>36 (16)</td><td>8 (7)</td><td>42 (23)</td><td>59911.7 (18365.47)</td><td>61718.2 (21992)</td><td>61375.54 (21728.17)</td><td></td><td></td></tr><tr><td>600</td><td>0.3669</td><td>37 (9)</td><td>13 (9)</td><td>33 (8)</td><td>17 (10)</td><td>1 (1)</td><td>49 (17)</td><td>316711.3 (109317.78)</td><td>316688.28 (104285.89)</td><td>315321.52 (103800.06)</td><td></td><td></td></tr><tr><td>800</td><td>0.366</td><td>5 (4)</td><td>45 (4)</td><td>3 (3)</td><td>47 (5)</td><td>0 (0)</td><td>50 (8)</td><td>1497034.74 (305983)</td><td>1520004.3 (477415.5)</td><td>1485407.12 (326572.38)</td><td></td><td></td></tr></table> ## E Raw Experimental Results for Finding Minimum Dominating Cliques We train three GNN models for finding the minimum dominating clique. The loss function of the first GNN model is Equation (5). For the other two GNN models, both of their loss functions are Equation (6). The difference is that one calculates \(\mathbb{E}(|S|)\) as \(\textstyle \sum_{i = 1}^{n}p_{i}\) and another one calculates \(\mathbb{E}(|S|)\) as Equation (7). With the fast and accurate versions of the joint information entropy, we have six GNN- enhanced solvers. We run these six solvers and the original solver with the new features on the training, validation and test data from the instances generated from \(G(n,p)\) where \(p\in (0.4,0.41)\) . The results are shown in Tables 5, 6, and 7. The O in the tables represents the original solver with the new features. The (5) in the tables represents the solver using the probability distributions from the GNN model with Equation (5) as the loss function. The (6) in the tables represents the solver using the probability distributions from the GNN model with Equation (6) where \(\mathbb{E}(|S|) = \sum_{i = 1}^{n}p_{i}\) as the loss function, and the (7) represents the solver using the probability distributions from the GNN model whose loss function is Equation (6) where \(\mathbb{E}(|S|)\) is calculated as Equation (7).

Table 5: Results on Training Data (minimum DC): 50 instances for each \(n\) <table><tr><td>G(n,p)</td><td>O vs (3)</td><td>O vs (4)</td><td>O vs (5)</td><td>(3) vs (4)</td><td>(3) vs (5)</td><td>(4) vs (5)</td><td>(4) vs (5)</td><td>Avg</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>

different algorithms depending on the characteristics of SAT instances. Convolutional neural networks have been applied to predict the behavior of SAT instances, see [2] and [3]. Recently, graph neural networks (GNNs) have been used to learn a probability distribution to sample the solution space directly or learn greedy heuristics for approximation algorithms of COPs under the framework of reinforcement learning, see [4], [5], [6], and [7]. Our work differs from these approaches in that GNNs are used to learn branching heuristics to enhance exact- backtracking- search methods. While human- designed branching heuristics, such as the minimum- remaining- values (MRV) heuristic, have long been used in backtracking- based solvers, past research indicated that their effectiveness is problem- instance- dependent. Inspired by [8], which (to our best knowledge) is the first work to introduce the probabilistic method into GNNs, we propose a new approach to apply the probabilistic method on GNNs to find subgraphs that satisfy certain combinatorial constraints in a given graph. The node features output by GNNs are interpreted as the probability distributions on vertices under which the information entropy is used in the branching heuristic. In this paper, we focus on applying this GNN- based approach to solve the dominating- clique problem. We choose the dominating- clique problem as our current research interest for the following two reasons. - In [9], Culberson et al. study the phase transition of the dominating clique in \(G(n, p)^{1}\) and propose a solver for finding a dominating clique. An interesting observation in their empirical studies on the random instances at the phase transition is that the variance of the number of branches of the solvers is extremely small as compared with the average number of branches, suggesting that such hard-problems instances seem to resist commonly-used and human-designed branching heuristics;- Previous approaches, such as the framework in [8], attempt to focus on problems that the properties of solutions are defined locally, e.g. the maximum-clique problem, while the dominating clique problem is an interesting example of problems where a solution as a subset of vertices/variables has to satisfy some non-trivial additional global conditions defined by all the vertices in a graph. This paper is organized as follows. The next section introduces the definition of dominating cliques and our probabilistic- method GNN. Section 3 discusses the details of our work. In Section 4, we provide and discuss experimental results. In the last section, we conclude with a brief discussion of potential future work.

## 2 Dominating Clique and Probabilistic-Method GNN Given a graph \(G = (V,E)\) , a dominating clique (DC) is a subset \(S\) of \(V\) such that \(S\) is a clique and dominates \(V\backslash S\) , i.e. \(\forall u\in V\backslash S\) , \(\exists v\in S\) such that \(\{v,u\} \in E\) . Checking the existence of a dominating clique in a graph is an NP- complete problem [10]. Our GNN model is similar to what has been proposed in [8]. Its architecture is standard and is trained in an unsupervised way to learn a probability distribution for each vertex. Unlike [8] where the distribution is directly used to sample a solution, we use the distribution to design branching heuristics. Due to the fact that a solution to the dominating- clique problem needs to satisfy both local and global properties, designing a good loss function is much more challenging than the those used in [8] for solutions that largely require certain local properties. We build a probability space by defining a Bernoulli distribution for each vertex of a given graph. By the correlation inequality in the probabilistic method, we can get an approximation of the probability measure of dominating cliques in the graph. Using the approximation as the loss function of a GNN model, we hope that the approximation grows larger as much as possible. We note that in [8], the first- moment method is used in the loss function of their GNN model, which requires that the value of loss function after training is less than a constant depending on specific graphs. If the requirement is not met, the learned probability distributions do not guarantee the usefulness for the solving problem. Our loss function avoid such restrictions. In our approach, under the learned probability space, the information entropy of vertices is used in the new branching heuristic to replace the original branching heuristic of the dominating- clique solver in [9]. Our experimental results show that the learned branching heuristic can prune more search space than the original branching heuristic. A recent attempt towards learning branching choices in a combinatorial search was given by Li et al. (2018) [5] where they approached the maximum- independent- set problem by choosing a node that is expected to be in an optimal solution. This choice is made by assigning probabilities to the unselected nodes and choosing a node of high probability, and these probabilities are learned. In a sense, it performs a greedy selection of vertices to be added to a maximum independent set. This can be parallelized into a tree search by considering multiple probability maps of the vertices at each stage (according to a pre- chosen branching factor parameter), and continuing the selection process in each probability map. The largest independent set is found over all branches. Later, Bother et al. (2022) [11] showed that the performance of such a tree search of selections can be matched by randomly generating values in place of the learned probability map for this combinatorial problem. This is comparable to a Monte Carlo tree search. We contrast our work here to that of Li et al.'s tree search in that their search

tree is inherently a heuristic search algorithm while ours is an exact backtracking solver. Li et al.'s search tree is parameterized with a pre- set branching factor \(M\) (which they evaluate experimentally and suggest an ideal value of \(M = 32\) ), while our search tree branches on choices made in local structures. As we are interested in exact search, an important metric we use is in counting the branches expanded in our search tree, rather than counting the fraction of solved problems like in [5]. In the following, we denote by \(N(v)\) the open neighborhood of a vertex \(v\) (i.e. the subset of vertices that are adjacent to \(v\) ) and \(N[v] = N(v) \cup \{v\}\) the closed neighborhood. We also denote by \([n]\) the set of integers \(\{1,2,\dots,n\}\) . ## 3 Branching Heuristic from New Probabilistic-Method GNN In this section, we show the restrictions in the design of the loss function of the GNN model in [8]. To avoid such restrictions, we propose a new approach to design loss functions using techniques and tools in the probabilistic method. We then demonstrate the utility of our approach in the maximum- clique problem. In the hope of enhancing exact algorithms for combinatorial optimization problems, we show a method to extract the branching heuristic from the learned probability space output by our GNN model. Then, we apply this method to the dominating- clique problem, and the experimental results show that the learned probability space yields to a better branching heuristic for the dominating- clique problem. Our probabilistic- method GNN gives rise to a probability space similar to the way in [8]. In our probabilistic- method GNN, the node feature associated with each vertex is a 1- dimensional vector and is interpreted as the parameter of a Bernoulli distribution. Intuitively, this Bernoulli distribution characterizes the likelihood for the vertex to be in a solution. We also assume that the collection of Bernoulli distributions associated with the vertices are mutually independent. For a given graph \(G(V,E)\) , the collection of Bernoulli distributions give rise to a probability space \((\Omega ,\mathcal{F},\mathbb{P})\) . The sample space \(\Omega\) is the power set of \(V\) ; the event space \(\mathcal{F}\) is the power set of \(\Omega\) ; for a subset \(S\) of \(V\) , \(\mathbb{P}(S) = \left(\prod_{i}p_{i}\right)\left(\prod_{j}(1 - p_{j})\right)\) with \(v_{i}\in S\) and \(v_{j}\notin S\) where \(p_{i}\) and \(p_{j}\) are the the parameters of the Bernoulli distributions associated with vertices \(v_{i}\) and \(v_{j}\) . ### 3.1 New Probabilistic-Method GNN Model The idea of the design of loss functions in [8] comes from the first- moment method in the probabilistic method \[\mathbb{P}(X< a) > 1 - \mathbb{E}(X) / a\] where \(X\) is a random variable under a certain probability distribution and \(a\) is a positive number. Applying \(a = \mathbb{E}(X) / (1 - t)\) with a strictly positive \(t< 1\) ,

we get \[\mathbb{P}\big(X< \mathbb{E}(X) / (1 - t)\big) > t,\] which tells us that with a strictly positive probability \(t\) , \(X\) is smaller than \(\mathbb{E}(X) / (1 - t)\) . Using the first- moment method, given a combinatorial optimization problem with the quality of a solution \(S\) measured by a function \(f\) (such as the size of \(S\) ), Karalias et al. in [8] set \(f(S) + 1_{S\notin S}\beta\) as a random variable where \(S\) is a randomly selected subset according to the distribution from the GNN output, \(S\) is the set of solutions, \(1_{S\notin S}\) is an indicator function, and \(\beta\) is a positive constant strictly greater than \(\max_{S\in S}\big(f(S)\big)\) . This random variable leads the loss function of GNN models to be \[\mathcal{L} = \mathbb{E}\big(f(S) + 1_{S\notin S}\beta \big) = \mathbb{E}\big(f(S)\big) + \mathbb{P}(S\notin S)\beta .\] After training, if \(\mathcal{L}< (1 - t)\beta\) , then with a strictly positive probability \(t\) there is an \(S\) with \[f(S) + 1_{S\notin S}\beta < \beta\] which implies that \(1_{S\notin S}\) is false, i.e. \(S\in S\) and \(f(S)< \beta\) This approach is quite interesting, but it suffers from the following two drawbacks. - We want \(t\) to be close to 0 so that \(\mathcal{L}< (1 - t)\beta\) has more likelihood after training, but it also implies the less likelihood (i.e. the probability \(t\) ) of the event that there is an \(S\) with \(f(S) + 1_{S\notin S}\beta < \beta\) ;- Choosing the value of \(\beta\) needs special care. Since it is usually hard to get the closed form of \(\mathbb{P}(S\notin S)\) for the given combinatorial optimization problem, we use an upper bound of \(\mathbb{P}(S\notin S)\) to replace it in the loss function, which makes less likely that the loss function will converge to a small number after training if \(\beta\) is too great. To avoid these issues, we propose a new way to design loss functions. Our idea is straightforward: minimizing \(\mathbb{E}\big(f(S)\big)\mathbb{P}(S\in S)^{- 1}\) to let the probability measure of the solutions with low values of \(f\) be as great as possible. Our loss function is \[\ln \left(\mathbb{E}\big(f(S)\big)\right) - \ln \left(\mathbb{P}(S\in S)\right).\] In practice, the difference between \(\mathbb{E}\big(f(S)\big)\) and \(\mathbb{P}(S\in S)\) might be large, which may cause the loss function to weigh \(\mathbb{E}\big(f(S)\big)\) too strongly compared to \(\mathbb{P}(S\in S)\) . We use \(\ln (\cdot)\) in hopes that the loss function would weigh the terms as proportionally as possible. ### 3.2 New Loss Function for Maximum-Clique Problem In this section, we show an application of our GNN model for the maximum- clique problem. For the probability \(\mathbb{P}(S\) is a clique), we use the correlation inequality in the probabilistic method to get its lower bound.

Definition 1. [12] Let \(M\) be a finite universal set \(\{x_{1},x_{2},\ldots ,x_{n}\}\) and \(R\) be a random subset of \(M\) sampled by \(\mathbb{P}(x_{i}\in R) = p_{i}\) . Suppose these samples are mutually independent. An event \(\mathcal{A}\) is a collection of subsets of \(M\) . \(\mathcal{A}\) is an increasing event if a set \(S\) is in \(\mathcal{A}\) implies that every superset of \(S\) is in \(\mathcal{A}\) . Similarly, \(\mathcal{A}\) is a decreasing event if a set \(S\) is in \(\mathcal{A}\) implies that every subset of \(S\) is in \(\mathcal{A}\) . Given two increasing events \(\mathcal{A}\) and \(B\) and two decreasing events \(C\) and \(\mathcal{D}\) , the correlation inequality [12] shows that \[\begin{array}{r l} & {\mathbb{P}(\mathcal{A}\cap \mathcal{B})\geq \mathbb{P}(\mathcal{A})\cdot \mathbb{P}(\mathcal{B}),}\\ & {\mathbb{P}(\mathcal{C}\cap \mathcal{D})\geq \mathbb{P}(\mathcal{C})\cdot \mathbb{P}(\mathcal{D}),}\\ & {\mathbb{P}(\mathcal{A}\cap \mathcal{C})\leq \mathbb{P}(\mathcal{A})\cdot \mathbb{P}(\mathcal{C}).} \end{array} \quad (1)\] By induction, if \(\{\mathcal{A}_{i}\}_{i\in [n]}\) are all increasing or all decreasing events, \[\mathbb{P}(\bigwedge_{i\in [n]}\mathcal{A}_{i})\geq \prod_{i\in [n]}\mathbb{P}(\mathcal{A}_{i}). \quad (2)\] Given a graph \(G(V,E)\) , denote by \(C_{S}\) the event that \(S\) is a clique. For a non- adjacent pair of vertices \(v_{i}\) and \(v_{j}\) , denote by \(B_{i j}\) the event that \(v_{i}\) and \(v_{j}\) are both in \(S\) , so \(\mathbb{P}(B_{i j}) = p_{i}p_{j}\) . Clearly, \(C_{S}\) is equivalent to \(\bigwedge_{i,j}\overline{{B_{i j}}}\) , and \(\overline{{B_{i j}}}\) 's are decreasing events. Thus, by (2), \[\mathbb{P}(C_{S}) = \mathbb{P}(\bigwedge_{\{v_{i},v_{j}\} \notin E}\overline{{B_{i j}}})\geq \prod_{\{v_{i},v_{j}\} \notin E}\mathbb{P}(\overline{{B_{i j}}}). \quad (3)\] Clearly, maximizing \(\prod_{\{v_{i},v_{j}\} \notin E}\mathbb{P}(\overline{{B_{i j}}})\) helps in maximizing \(\mathbb{P}(C_{S})\) . Hence, our loss function for the maximum- clique problem is \[\mathcal{L} = -\ln \left(\mathbb{E}(|S|)\right) - \sum_{\{v_{i},v_{j}\} \notin E}\ln \left(\mathbb{P}(\overline{{B_{i j}}})\right). \quad (4)\] We apply the probability distributions output from the GNN model using the loss function (4) to the greedy approximation algorithm for the maximum- clique problem in [8]. See Table 1 for the experimental results. ### 3.3 Loss Function for (Minimum) Dominating-Clique Problem The maximum- clique problem belongs to the kind of combinatorial problems that the properties of solutions are defined locally. We would like to turn our focus towards solving combinatorial problems whose solutions have both local and global conditions. Also, previous studies of applying GNNs for combinatorial optimization problems usually use the probability distributions from GNN models for greedy approximation algorithms. Thus, we also look for a method to apply the probability distributions from GNN models to exact algorithms.

The dominating- clique problem is a good problem to be studied since 1) it is a combinatorial optimization problem with both local and global conditions; 2) checking the existence of a dominating clique is NP- complete; 3) there is a powerful exact solver for the dominating- clique problem in [9] to which we can apply our improvement. We first design a loss function for finding a dominating clique and then extend it seamlessly for finding the minimum dominating clique. Denote by \(D_{S}\) the event that \(S\) is a dominating set, i.e. \(S\) dominates \(V\backslash S\) . The closed form of \(\mathbb{P}(D_{S}\cap C_{S})\) is a good candidate as a loss function for finding a dominating clique, but it is harder to evaluate the closed form compared to evaluating the closed form of \(\mathbb{P}(C_{S})\) . Thus, we alternatively analyze the upper and lower bounds of \(\mathbb{P}(D_{S}\cap C_{S})\) . Our idea is to increase the upper and lower bounds of \(\mathbb{P}(D_{S}\cap C_{S})\) simultaneously to optimize its value. For a node \(v_{i}\) , denote by \(A_{i}\) the event that the vertices in \(N[v_{i}]\) are all not in \(S\) , so \(\mathbb{P}(A_{i}) = \prod_{v_{j}\in N[v_{i}]}(1 - p_{j})\) . It is easy to see that \(D_{S}\) is equivalent to \(\bigwedge_{i}\overline{{A_{i}}}\) and \(\overline{{A_{i}}}\) 's are increasing events. By (1) and (2), we get \[\mathbb{P}(D_{S}\cap C_{S})\leq \mathbb{P}(D_{S})\mathbb{P}(C_{S}),\] \[\mathbb{P}(D_{S}) = \mathbb{P}(\bigwedge_{i\in [n]}\overline{{A_{i}}})\geq \prod_{i\in [n]}\mathbb{P}(\overline{{A_{i}}}).\] With (3), we have \[\exp \Big(\sum_{i\in [n]}\ln \big(\mathbb{P}(\overline{{A_{i}}})\big) + \sum_{\{v_{i},v_{j}\} \notin E}\ln \big(\mathbb{P}(\overline{{B_{ij}}})\big)\Big) = \prod_{i\in [n]}\mathbb{P}(\overline{{A_{i}}})\prod_{\{v_{i},v_{j}\} \notin E}\mathbb{P}(\overline{{B_{ij}}})\] \[\leq \mathbb{P}(D_{S})\mathbb{P}(C_{S}).\] This inequality tells us that increasing \(\sum_{i\in [n]}\ln \big(\mathbb{P}(\overline{{A_{i}}})\big) + \sum_{\{v_{i},v_{j}\} \notin E}\ln \big(\mathbb{P}(\overline{{B_{ij}}})\big)\) is meanwhile increasing the upper bound of \(\mathbb{P}(D_{S}\cap C_{S})\) . For the lower bounds of \(\mathbb{P}(D_{S}\cap C_{S})\) , we have \[\mathbb{P}(D_{S}\cap C_{S})\geq \mathbb{P}(D_{S}) + \mathbb{P}(C_{S}) - 1\] \[\qquad \geq 2\exp \Big(\frac{1}{2}\big(\ln \mathbb{P}(D_{S}) + \ln \mathbb{P}(C_{S})\big)\Big) - 1\qquad \mathrm{(as~}e^{x}\mathrm{~is~a~convex~function)}\] \[\qquad \geq 2\exp \Big(\frac{1}{2}\Big(\sum_{i\in [n]}\ln \big(\mathbb{P}(\overline{{A_{i}}})\big) + \sum_{\{v_{i},v_{j}\} \notin E}\ln \big(\mathbb{P}(\overline{{B_{ij}}})\big)\Big)\Big) - 1.\] Therefore, we set the loss function for the dominating- clique problem as \[\mathcal{L} = -\Big(\sum_{i\in [n]}\ln \big(\mathbb{P}(\overline{{A_{i}}})\big) + \sum_{\{v_{i},v_{j}\} \notin E}\ln \big(\mathbb{P}(\overline{{B_{ij}}})\big)\Big) \quad (5)\] trying to optimize the upper and lower bounds of \(\mathbb{P}(D_{S}\cap C_{S})\) simultaneously. To find the minimum dominating clique, we can simply add \(\ln \Big(\mathbb{E}(|S|)\Big)\) into the loss function as \[\mathcal{L} = -\Big(\sum_{i\in [n]}\ln \big(\mathbb{P}(\overline{{A_{i}}})\big) + \sum_{\{v_{i},v_{j}\} \notin E}\ln \big(\mathbb{P}(\overline{{B_{ij}}})\big)\Big) + \ln \Big(\mathbb{E}(|S|)\Big). \quad (6)\]

By Equation (6), we make an effort to increase the probability measure of small- sized dominating cliques so that it is easier to identify them. However, calculating \(\mathbb{E}(|S|)\) on \(\Omega\) (i.e. \(\mathbb{E}(|S|) = \sum_{i = 1}^{n}p_{i}\) ) indeed affects all small- sized subsets of \(V\) rather than small- sized dominating cliques. An improvement is to find an event in the event space \(\mathcal{F}\) which is close to the exact event consisting of dominating cliques only. One way to accomplish this is as follows. In each iteration during the GNN training, we generate a random permutation \(\{v_{1},v_{2},\dots,v_{n}\}\) of \(V\) and generate an event (i.e. a set of subset of \(V\) ) iteratively. We initialize the event as an empty set. For \(i = 1\) to \(n\) , we first exclude the subsets of \(V\) that contain any vertex of \(v_{1},\dots,v_{i - 1}\) . Then, we continue to exclude the subsets of \(V\) that do not contain \(v_{i}\) . After that, we exclude the subsets of \(V\) that contain any non- adjacent vertex to \(v_{i}\) . At the end of the current iteration, we add the remaining subsets of \(V\) into the event. This event is much more closer to the event of dominating cliques only than the event \(\Omega\) . Thus, instead of minimizing \(\mathbb{E}(|S|)\) on \(\Omega\) , we try to minimize \(\mathbb{E}(|S|)\) on this event as \[\sum_{i = 1}^{n}\left(p_{i}\prod_{j = 1}^{i - 1}(1 - p_{j})\prod_{v_{k}\in \{v_{i + 1},\dots,v_{n}\}\backslash N(v_{i})}(1 - p_{k})(1 + \sum_{v_{r}\in N(v_{i})\backslash \{v_{1},\dots,v_{i - 1}\}}p_{r})\right) \quad (7)\] in hopes that it gives the loss function a more accurate expected size of dominating cliques. The term \(p_{i}\) is the probability of the event that \(v_{i}\) is in \(S\) . The term \(\prod_{j = 1}^{i - 1}(1 - p_{j})\) is the probability of the event that \(v_{1},\dots,v_{i - 1}\) are not in \(S\) . The term \(\prod_{v_{k}\in \{v_{i + 1},\dots,v_{n}\}\backslash N(v_{i})}(1 - p_{k})\) is the probability of the event that \(S\) do not contain any vertex that is behind \(v_{i}\) in the permutation and non- adjacent to \(v_{i}\) . The term \(\left(1 + \sum_{v_{r}\in N(v_{i})\backslash \{v_{1},\dots,v_{i - 1}\}}p_{r}\right)\) is the conditional expectation of \(|S|\) given the above three events happen. ### 3.4 New Branching Heuristic for Dominating-Clique Solvers Exact solvers for combinatorial optimization problems are generally based on backtracking search. To improve this framework, branching heuristics are applied to give the most promising direction during the search, and branching heuristics are designed by the properties of specific problems. To improve branching heuristics by our GNN model, our idea is to define a function \(f(\mathrm{Var}_{b},\Theta)\) to measure the quality of branches in the search tree where \(\mathrm{Var}_{b}\) is the set of unsigned variables of a branch \(b\) and \(\Theta\) is the probability space from the output of our GNN model. We next utilize this idea for the dominating- clique problem. Culberson et al. [9] propose an efficient solver for the dominating- clique problem. From their experiments, the solver performs better than other general SAT solvers, including Berk Min, March Eq, and SATzilla. The solver is a backtracking- based algorithm. They encode a given graph \(G = (V,E)\) with \(V = \{v_{1},v_{2},\dots,v_{n}\}\) to a CNF formula as follows. - There are \(n\) variables \(\{X_{i}\}_{i \in [n]}\) where \(X_{i} = 1\) means that the corresponding \(v_{i}\) is in the solution;

- Define a clause \(C_{i} = \{X_{j}\}_{v_{j}\in N[v_{i}]}\) for each vertex \(v_{i}\) to indicate that the vertex is in the solution or at least one of its neighbors is in the solution; With the encoded CNF formula, the solver works as Algorithm 1. Algorithm 1 [9] An algorithm for the dominating- clique problem 1: procedure Do MCLq \((D,S,U,G(V,E))\) 2: if \(U\) is \(\emptyset\) then 3: DOMCLQ ← D 4: return TRUE 5: else 6: Find \(C\in U\) such that \(|C\cap S| = \min_{C^{\prime}\in U}|C^{\prime}\cap S|\) 7: if \(C\cap S\) is not \(\emptyset\) then 8: \(S^{\prime \prime}\gets S\) 9: for \(X_{i}\in C\cap S\) do 10: \(X_{i}\gets 1\) ; \(D\gets D\cup \{v_{i}\}\) 11: \(S^{\prime}\leftarrow \{X_{j}\mid X_{j}\in S^{\prime \prime},\{v_{i},v_{j}\} \in E\} ;U^{\prime}\leftarrow U\setminus \{C^{\prime}\mid C^{\prime}\in\) \(U,X_{i}\in C^{\prime}\}\) 12: if \(\mathrm{Dom Clq}(D,S^{\prime},U^{\prime},G(V,E))\) then 13: return TRUE 14: else 15: \(X_{i}\gets 0\) ; \(D\gets D\setminus \{v_{i}\} ;S^{\prime \prime}\gets S^{\prime \prime}\setminus \{X_{i}\}\) 16: return FALSE The solver has three parameters: a potential dominating clique \(D\) , a set \(S\) of the unassigned variables such that \(S = \{X_{u}\mid \forall v\in D,\{u,v\} \in E\}\) , and a set \(U\) of unsatisfied clauses. The three parameters are initialized as \(\emptyset\) , \(\{X_{i}\}_{i\in [n]}\) , and \(\{C_{i}\}_{i\in [n]}\) respectively. From the perspective of CSP solvers, this algorithm uses the MRV heuristic. This heuristic is optimal if every sub- problem by adding one vertex into \(D\) has the same amount of search space. However, it is false for most cases. To improve it, we can see \(\{X_{i}\}_{i\in [n]}\) as random variables under the probability distributions output from our probabilistic- method GNN. We use the information entropy of these random variables to predict the amount of the search space of unsatisfied clauses. In particular, instead of the cardinality of \(C^{\prime}\cap S\) , we measure the joint information entropy of the random variables in \(C^{\prime}\cap S\) . We therefore replace Line 6 in Algorithm 1 by finding \(C\in U\) such that \((C\cap S)\) has the minimum joint information entropy. In other words, we define the function \(f\) as the joint information entropy of unassigned variables of a branch to measure the quality of branches. Note that the probability distributions of unassigned variables should not be fixed during the backtracking search. The reason is that \(S\) , \(D\) , and \(U\) are changing during the search, so the correspondingly unexplored subgraph which consists of \(S\) and \(U\) is also changing. We apply the softmax function on the distributions of the unassigned variables to re- weigh them during the backtracking search. We also try the \(Z\) - score normalization, but its performance