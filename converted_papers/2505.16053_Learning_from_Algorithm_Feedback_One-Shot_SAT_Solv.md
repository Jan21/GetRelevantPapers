# Learning from Algorithm Feedback One Shot SAT Solv


## Introduction


## References Audemard, G. and Simon, L. (2009). Predicting learnt clauses quality in modern sat solvers. In IJCAI, volume 9, pages 399- 404. Citeseer. Audemard, G. and Simon, L. (2017). Glucose and syrup in the sat'17. Proceedings of SAT Competition, pages 16- 17. Biere, A., Faller, T., Fazekas, K., Fleury, M., Froleyks, N., and Pollitt, F. (2024). Ca Di Ca L 2.0. In Gurfinkel, A. and Ganesh, V., editors, Computer Aided Verification - 36th International Conference, CAV 2024, Montreal, QC, Canada, July 24- 27, 2024, Proceedings, Part I, volume 14681 of Lecture Notes in Computer Science, pages 133- 152. Springer. Biere, A., Heule, M., and van Maaren, H., editors (2021). Handbook of satisfiability, volume 185. IOS press, 2nd edition. Cameron, C., Hartford, J., Lundy, T., Truong, T., Milligan, A., Chen, R., and Leyton- Brown, K. (2024). Unsat solver synthesis via monte carlo forest search. In International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research, pages 170- 189. Springer. Cappart, Q., Chetelat, D., Khalil, E. B., Lodi, A., Morris, C., and Veličković, P. (2021). Combinatorial optimization and reasoning with graph neural networks. In Zhou, Z.- H., editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI- 21, pages 4348- 4355. International Joint Conferences on Artificial Intelligence Organization. Survey Track. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Courtois, N., O'Neil, S., and Quisquater, J.- J. (2009). Practical algebraic attacks on the hitag2 stream cipher. volume 5735, pages 167- 176. Crawford, J. M. and Auton, L. D. (1996). Experimental results on the crossover point in random 3- sat. Artificial intelligence, 81(1- 2):31- 57. Eén, N. and Sörensson, N. (2003). An extensible sat- solver. In International conference on theory and applications of satisfiability testing, pages 502- 518. Springer. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. (2025). Deepseek- r1: Incentivizing reasoning capability in llms via reinforcement learning. ar Xiv preprint ar Xiv:2501.12948. Han, J. M. (2020). Learning cubing heuristics for sat from drat proofs. Heule, M., Dufour, M., Van Zwieten, J., and Van Maaren, H. (2005). March_eq: Implementing additional reasoning into an efficient look- ahead sat solver. In Theory and Applications of Satisfiability Testing: 7th International Conference, SAT 2004, Vancouver, BC, Canada, May 10- 13, 2004, Revised Selected Papers 7, pages 345- 359. Springer. Heule, M. J. and Van Maaren, H. (2006). March_dl: Adding adaptive heuristics and a new branching strategy. Journal on Satisfiability, Boolean Modelling and Computation, 2(1- 4):47- 59. Khalil, E., Dai, H., Zhang, Y., Dilkina, B., and Song, L. (2017). Learning combinatorial optimization algorithms over graphs. Advances in neural information processing systems, 30. Khalil, E. B., Morris, C., and Lodi, A. (2022). Mip- gnn: A data- driven framework for guiding combinatorial solvers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10219- 10227. Kullmann, O. (2021). Fundaments of branching heuristics. In Handbook of Satisfiability, pages 351- 390. IOS Press.

Kurin, V., Godil, S., Whiteson, S., and Catanzaro, B. (2020). Can q- learning with graph networks learn a generalizable branching heuristic for a sat solver? In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 9608- 9621. Curran Associates, Inc. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. (2024). Tulu 3: Pushing frontiers in open language model post- training. ar Xiv preprint ar Xiv:2411.15124. Liu, H., Xu, P., Pu, Y., Yin, L., Zhen, H.- L., Yuan, M., Ho, T.- Y., and Yu, B. (2024). Neuroselect: Learning to select clauses in sat solvers. In Proceedings of the 61st ACM/IEEE Design Automation Conference, DAC '24, New York, NY, USA. Association for Computing Machinery. Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., and Grohe, M. (2019). Weisfeiler and Leman go neural: Higher- order graph neural networks. In Proceedings of the Thirty- Third AAAI Conference on Artificial Intelligence (AAAI), pages 4602- 4609. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730- 27744. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. ar Xiv preprint ar Xiv:1707.06347. Selsam, D. and Bjørner, N. (2019). Guiding high- performance sat solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, Lisbon, Portugal, July 9- 12, 2019, Proceedings 22, pages 336- 353. Springer. Selsam, D., Lamm, M., Bünz, B., Liang, P., de Moura, L., and Dill, D. L. (2019). Learning a SAT solver from single- bit supervision. In International Conference on Learning Representations. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ar Xiv preprint ar Xiv:2402.03300. Soos, M. (2010). Grain of salt — an automated way to test stream ciphers through sat solvers. Soos, M., Nohl, K., and Castelluccia, C. (2009). Extending sat solvers to cryptographic problems. In Kullmann, O., editor, Theory and Applications of Satisfiability Testing - SAT 2009, pages 244- 257, Berlin, Heidelberg. Springer Berlin Heidelberg. Tönshoff, J., Kisin, B., Lindner, J., and Grohe, M. (2023). One model, any csp: Graph neural networks as fast global search heuristics for constraint satisfaction. In Proceedings of the Thirty- Second International Joint Conference on Artificial Intelligence, IJCAI- 23, pages 4280- 4288. Wang, W., Hu, Y., Tiwari, M., Khurshid, S., Mc Millan, K., and Miikkulainen, R. (2024). Neuroback: Improving CDCL SAT solving using graph neural networks. In The Twelfth International Conference on Learning Representations. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2019). How powerful are graph neural networks? In Proceedings of the Seventh International Conference on Learning Representations (ICLR). Zdeborová, L. and Krząkała, F. (2007). Phase transitions in the coloring of random graphs. Phys. Rev. E, 76:031131. Zhai, S. and Ge, N. (2025). Learning splitting heuristics in divide- and- conquer SAT solvers with reinforcement learning. In The Thirteenth International Conference on Learning Representations.

## A Method Details ## A.1 Graph Representation and Architecture We represent a formula \(\phi\) as a graph \(G(\phi) = (V(\phi),E(\phi))\) . This is a standard "Literal- Clause Graph" used in prior work, such as Neuro SAT Selsam et al. (2019). Formally, the vertices of this graph \(V(\phi) = \mathrm{Lit}(\phi) \cup \mathrm{Cls}(\phi)\) are the literals and clauses of \(\phi\) . The edges \(E(\phi) = E_{LC}(\phi) \cup E_{LL}(\phi)\) connect literals with the clauses they occur in and with the opposing literal of the same variable: \[\begin{array}{r l} & {E_{L L}(\phi) = \{(x,\neg x)\mid x\in \mathcal{X}(\phi)\}}\\ & {E_{L C}(\phi) = \bigcup_{C\in \mathrm{Cls}(\phi)}\{(C,\ell)\mid \ell \in C\}} \end{array} \quad (12)\] ## A.2 Message-Passing Neural Network For each vertex \(v\in \operatorname {Lit}(\phi)\cup \operatorname {Cls}(\phi)\) we obtain an initial embedding \(h^{0}(v)\in \mathbb{R}^{d}\) \[h^{0}(v) = \mathbf{Enc}(\log (\deg (v) + 1)). \quad (13)\] Here \(d\) is the latent embedding dimension of the model, and Enc is a trainable 2- layer MLP that is applied to the log- normalized degree of \(v\) The GNN model then stacks \(L\in \mathbb{N}\) message passing layers. For \(t\in \{1,\ldots ,L\}\) , the \(t\) - th layer takes as input the previous embedding \(h^{t - 1}\) and outputs a refined embedding \(h^{t}\) by performing a message pass. This message pass is split into two phases. First, each clause \(c\in \mathrm{Cls}\) aggregates information from its associated literals: \[h^{t + 1}(c) = \mathbf{U}_{\mathrm{Cls}}\left(h^{t}(c),\bigoplus_{\ell \in c}h^{\ell}(\ell)\right). \quad (14)\] Here, \(\mathbf{U}_{\mathrm{Cls}}\) is a trainable MLP, and \(\bigoplus\) is an order- invariant aggregation. Throughout all experiments, we use element- wise mean for aggregation. In the second phase, each literal \(\ell \in \operatorname {Lit}\) aggregates the updated embeddings from the clauses it occurs in: \[h^{t + 1}(\ell) = \mathbf{U}_{\mathrm{Lit}}\left(h^{\ell}(\ell),h^{\ell}(-\ell),\bigoplus_{c,\ell \in c}h^{t + 1}(c)\right). \quad (15)\] Here, \(\mathbf{U}_{\mathrm{Lit}}\) is another trainable MLP that additionally also takes the embedding of the opposing literal \(- \ell\) as input. This model architecture is conceptually similar to that of Neuro SAT. One major difference is that we use a more standard fixed- depth feed- forward GNN instead of a recurrent model. Note that all MLPs used in our model have two layers, and the hidden layer is always Si LU- activated and has hidden dimension \(2d\) . The final output is a variable embedding \(y:\mathrm{Var}(\phi)\to \mathbb{R}^{2}\) , which is obtained by concatenating the two literal embeddings associated with each variable \(x\) and then applying a final 2- layer MLP Dec: \[y(x) = \mathbf{Dec}([h^{L}(x),h^{L}(\neg x)]). \quad (16)\] Note that we choose Dec as a 2- layer MLP with input dimension \(2d\) , hidden dimension \(2d\) , and output dimension 2. No activation is applied to the output, and the weights and biases of the final layer of Dec are initialized as zeros. This ensures that at the beginning of training, the initial GNN \(N_{\theta_{0}}\) assigns \(\mu (x) = 0\) and \(\rho (x) = 0\) to all variables. We found this to be a stable configuration for initializing training. In particular, \(\mu (x) = 0\) ensures that the log- normally distributed weight policy \(\pi_{\theta_{0}}^{w}(x)\) has a mode of approximately 1 for all variables while \(\rho (x) = 0\) ensures that the polarity of each variable is initially distributed uniformly.

## A.3 SAT Solver Details ## Glucose Glucose (Audemard and Simon, 2009) is a popular CDCL solver based on Minisat (Eén and Sörensson, 2003). Our modification is based on Glucose 4.2.1 (Audemard and Simon, 2017) \(^3\) . Like many other CDCL solvers, Glucose uses the Variable State Independent Decaying Sum (VSIDS) heuristic for branching. Each variable \(x\) is assigned an activity score activity \((x)\) that reflects its involvement in conflicts. When a conflict occurs, the activity scores of variables involved are increased by a constant \(\Delta\) , i.e., \[\mathrm{activity}(x)\leftarrow \mathrm{activity}(x) + \Delta . \quad (17)\] Periodically, all activity scores are multiplied by a decay factor \(\beta\) (where \(0< \beta < 1\) ): \[\mathrm{activity}(x)\leftarrow \beta \cdot \mathrm{activity}(x). \quad (18)\] The activity then effectively serves as the SCORE function from Algorithm 2. Note that in practice, CDCL solvers commonly use exponential VSIDS (EVSIDS), which is a variation that yields identical decisions but avoids a costly loop over all variables to compute Equation (18). Rather than decaying the activity, the increment \(\Delta\) is instead scaled up: \[\Delta \leftarrow \frac{1}{\beta}\Delta . \quad (19)\] The cumulative values of the activity scores then yield the same decisions. To incorporate our variable weights \(w\) into this process, we simply modify Equation (17) by scaling the increment with the variable weight: \[\mathrm{activity}(x)\leftarrow \mathrm{activity}(x) + w(x)\cdot \Delta . \quad (20)\] This ensures that the total activity score of each variable is scaled by a factor of \(w(x)\) at each step of the search while still preventing loops over all variables. We found that the runtime overhead of the additional multiplication in Equation (20) is negligible. We use the provided polarities \(p(x)\) to initialize the polarity (or phase) of each variable. Note that we leave phase saving on, so this initial polarity may be overwritten by the solver in later search steps. We run all experiments without randomized decisions (rnd- freq \(= 0\) ). We further set the parameter \(K = 0.1\) to minimize solver restarts, which we found to improve performance on the three instance distributions considered in our experiments. Apart from this, we use the default parameters of Glucose. ## March March (Heule et al., 2005; Heule and Van Maaren, 2006) is a DPLL- based solver that uses a branching heuristic based on look- ahead (Biere et al., 2021). \(^4\) It is among the best- known solvers for purely random SAT instances. Look- ahead branching heuristics estimate how each variable's selection as a branching variable would affect the instance. In March, the scoring function SCORE(X) essentially quantifies how many new binary clauses would occur if \(x\) is picked for branching in the current search step. Computing this score is relatively expensive when compared to activity- based approaches, and look- ahead solvers usually make fewer decisions per time. To decrease the cost of each branching step, March first applies a pre- selection step before each branching decision, where a reduced set of candidate variables is selected according to a second scoring function SCORE- PRESELECT \((x)\) . This score aims to approximate the expected look- ahead score but is cheaper to compute. In the modified solver, we also apply the variable weight \(w\) in pre- selection, i.e. the weighted scores \(w(x) \cdot \text{SCORE- PRESELECT} (x)\) are used to select the candidate variables. The ratio of pre- selected candidates is fixed at \(10\%\) . The same weights \(w\) are then applied again to the actual look- ahead scores to obtain the branching variable. Afterwards, we use the given polarities \(p\) in each branching to determine the sign of the branching literal. Aside from these changes, we run March in its default configuration.

1: Input: 2: Training formulas \(\mathcal{F} = \{\phi_{1},\ldots ,\phi_{N}\}\) 3: Number of GRPO iterations \(K\in \mathbb{N}\) 4: Number of samples per instance \(M\in \mathbb{N}\) 5: Number of optimizer steps per GRPO iteration \(S\in \mathbb{N}\) 6: Clip ratio \(\epsilon \in (0,1)\) , KL penalty weight \(\beta \geq 0\) , learning rate \(\eta >0\) 7: Initialize: Random weights \(\theta_{0}\) 8: for \(k = 1,2,\ldots ,K\) do 9: for \(i = 1,2,\ldots ,N\) do 10: for \(j = 1,2,\ldots ,M\) do 11: \(\mathcal{W}_{i,j}\sim \pi_{\theta_{k - 1}}(\phi_{i})\) 12: \(C_{i,j}\gets \mathrm{Cost}(\phi_{i},\mathcal{W}_{i,j})\) 13: \(R(\phi_{i},\mathcal{W}_{i,j})\gets - C_{i,j}\) 14: end for 15: \(\mathbf{R}_{i}\gets \{R(\phi_{i},\mathcal{W}_{i,j})\mid j\in \{1,\ldots ,M\} \}\) 16: for \(j = 1,2,\ldots ,M\) do 17: \(\hat{A}_{i,j}\gets \frac{R(\phi_{i},\mathcal{W}_{i,j}) - \mathrm{mean}(\mathbf{R}_{i})}{\mathrm{std}(\mathbf{R}_{i})}\) 18: end for 19: end for 20: \(\theta \gets \theta_{k - 1}\) 21: for \(s = 1,2,\ldots ,S\) do 22: for \(i = 1,2,\ldots ,N\) do 23: for \(j = 1,2,\ldots ,M\) do 24: \(r_{i,j}(\theta)\gets \frac{\pi_{\theta}(\mathcal{W}_{i,j}|\phi_{i})}{\pi_{\theta_{k - 1}}(\mathcal{W}_{i,j}|\phi_{i})}\) 25: end for 26: \(\mathcal{L}_{\mathrm{PPO}}(\theta \mid \phi_{i})\gets \frac{1}{M}\sum_{j}\left[\min \left(r_{i,j}(\theta)\hat{A}_{i,j},\mathrm{clip}(r_{i,j}(\theta),1 - \epsilon ,1 + \epsilon)\hat{A}_{i,j}\right)\right]\) 27: \(\mathcal{L}(\theta \mid \phi_{i})\gets \mathcal{L}_{\mathrm{PPO}}(\theta \mid \phi_{i}) - \beta \cdot \mathrm{KL}\left(\pi_{\theta}(\phi_{i}),\pi_{\theta_{k - 1}}(\phi_{i})\right)\) 28: end for 29: \(\theta \gets \theta +\eta \nabla_{\theta}\sum_{i}\mathcal{L}(\theta \mid \phi_{i})\) 30: end for 31: \(\theta_{k}\gets \theta\) 32: end for 33: Output: Final model weights \(\theta_{K}\)

## B Experiment Details ## B.1 Data Table 2 provides full dataset statistics for all data distributions and splits. In the following, we provide further details on how each instance distribution is generated. Random 3SAT Uniformly random 3SAT instances are commonly used to benchmark SAT solvers. Here, each clause is sampled by choosing three distinct variables uniformly at random and negating each with a probability of \(50\%\) . Hard instances are known to occur when the number of clauses is around \(m = 4.258n + 58.26n^{-\frac{2}{3}}\) where \(n\) is the number of variables (Crawford and Auton, 1996). This is approximately the critical density where the instances transition from SAT to UNSAT. We define \(3\mathrm{SAT}(n)\) as the distribution of uniformly random 3SAT instances with \(n\) variables and \(\left[4.258n + 58.26n^{-\frac{2}{3}}\right]\) clauses. For training, we use 20K instances sampled from 3SAT(200), which are filtered such that exactly 10K instances are SAT and UNSAT, respectively. Our test sets contain larger instances with \(n\in \{300,350,400\}\) , where we sample 200 instances for each size \(n\) . Graph Coloring Combinatorial problems on graphs are commonly solved by reducing them to Boolean SAT instances. Here, we consider the problem of finding a 3- coloring for Erdős- Rényi graphs. We define \(3\mathrm{COL}(n)\) as the distribution of SAT problems that are obtained by sampling an Erdős- Rényi graph with \(n\) vertices and then encoding the problem of deciding 3- colorability as a SAT instance. We set the edge probability such that the expected vertex degree is 4.67, which is approximately the critical density for 3- colorability where hard instances commonly occur (Zdeborová and Kržakala, 2007). We train on 20K instances sampled from \(3\mathrm{COL}(300)\) . Again, these are filtered such that exactly 10K instances are SAT and UNSAT, respectively. Our test sets consist of larger problems with \(n\in \{400,500,600\}\) . Cryptographic Hard, structured SAT problems commonly arise in the context of cryptoanalysis, for example, for SAT- based decryption attacks (Soos et al., 2009). To generate data in this domain, we use Grain- of- Salt (Soos, 2010) to generate SAT instances for decrypting stream ciphers. We define CRYPTO \((n)\) as the distribution of SAT instances generated for decrypting the Hi Tag2 cipher (Courtois et al., 2009) with \(n\) given help bits. We use the recommended generation parameters (- outputs 56 - base- shift 8 - karnaugh 8). Note that these instances are harder for smaller values of \(n\) and are mostly UNSAT. We train on 20K instances from CRYPTO(22) and test on harder problems with \(n\in \{20,15,10\}\) . For each of these three instance classes we formally define the corresponding training distribution \(\Omega\) from Equation (6) as the uniform distribution over the set of training instances. Table 2: Dataset Statistics <table><tr><td rowspan="2">Distribution</td><td rowspan="2">Split</td><td rowspan="2">Number</td><td rowspan="2">#SAT</td><td rowspan="2">#UNSAT</td><td colspan="3">|Var(φ)|</td><td colspan="3">|Cls(φ)|</td></tr><tr><td>mean</td><td>min</td><td>max</td><td>mean</td><td>min</td><td>max</td></tr><tr><td>0</td><td>3SAT(200)</td><td>Train</td><td>20,000</td><td>10,000</td><td>10,000</td><td>200.00</td><td>200</td><td>200</td><td>853.00</td><td>853</td></tr><tr><td>1</td><td>3SAT(200)</td><td>Val</td><td>200</td><td>100</td><td>100</td><td>200.00</td><td>200</td><td>200</td><td>853.00</td><td>853</td></tr><tr><td>2</td><td>3SAT(300)</td><td>Test</td><td>200</td><td>103</td><td>97</td><td>300.00</td><td>300</td><td>300</td><td>1,278.00</td><td>1,278</td></tr><tr><td>3</td><td>3SAT(350)</td><td>Test</td><td>200</td><td>108</td><td>92</td><td>350.00</td><td>350</td><td>350</td><td>1,491.00</td><td>1,491</td></tr><tr><td>4</td><td>3SAT(400)</td><td>Test</td><td>200</td><td>89</td><td>111</td><td>400.00</td><td>400</td><td>400</td><td>1,704.00</td><td>1,704</td></tr><tr><td>5</td><td>3Col(300)</td><td>Train</td><td>20,000</td><td>10,000</td><td>10,000</td><td>900.00</td><td>900</td><td>900</td><td>3,284.75</td><td>3,009</td></tr><tr><td>6</td><td>3Col(300)</td><td>Val</td><td>200</td><td>100</td><td>100</td><td>900.00</td><td>900</td><td>900</td><td>3,288.63</td><td>3,042</td></tr><tr><td>7</td><td>3Col(400)</td><td>Test</td><td>200</td><td>77</td><td>123</td><td>1,200.00</td><td>1,200</td><td>1,200</td><td>4,392.31</td><td>4,144</td></tr><tr><td>8</td><td>3Col(500)</td><td>Test</td><td>200</td><td>91</td><td>108</td><td>1,500.00</td><td>1,500</td><td>1,500</td><td>5,488.56</td><td>5,216</td></tr><tr><td>9</td><td>3Col(600)</td><td>Test</td><td>200</td><td>87</td><td>113</td><td>1,800.00</td><td>1,800</td><td>1,800</td><td>6,597.24</td><td>6,306</td></tr><tr><td>10</td><td>Crypto(22)</td><td>Train</td><td>20,000</td><td>0</td><td>20,000</td><td>529.41</td><td>518</td><td>544</td><td>8,420.71</td><td>7,669</td></tr><tr><td>11</td><td>Crypto(22)</td><td>Val</td><td>200</td><td>0</td><td>200</td><td>529.24</td><td>523</td><td>537</td><td>8,413.41</td><td>7,937</td></tr><tr><td>12</td><td>Crypto(20)</td><td>Test</td><td>100</td><td>0</td><td>100</td><td>533.43</td><td>526</td><td>544</td><td>8,767.57</td><td>8,182</td></tr><tr><td>13</td><td>Crypto(15)</td><td>Test</td><td>100</td><td>0</td><td>100</td><td>542.89</td><td>537</td><td>552</td><td>9,622.04</td><td>9,129</td></tr><tr><td>14</td><td>Crypto(10)</td><td>Test</td><td>100</td><td>0</td><td>100</td><td>550.99</td><td>544</td><td>568</td><td>10,497.63</td><td>9,947</td></tr></table>

## B.2 Hyperparameters Table 3 provides an overview of all RLAF training runs from our main experiments. We tuned the learning rate in \(\eta \in \{0.0001, 0.00005, 0.00001\}\) and schedule it to warm up over the first 5 GRPO iterations. After warm up the the learning rate stays constant throughout training. The clip ratio was tuned in \(\epsilon \in \{0.1, 0.2\}\) and the KL- penalty \(\beta \in \{0.1, 1.0\}\) . All other hyperparameters were given constant default values, which we found to be stable based on preliminary experiments. Table 3: Hyperparameters <table><tr><td></td><td>3SAT</td><td>Glucose<br>3COL</td><td>CRYPTO</td><td>3SAT</td><td>March<br>3COL</td><td>CRYPTO</td></tr><tr><td>K</td><td>2000</td><td>2000</td><td>2000</td><td>2000</td><td>2000</td><td>2000</td></tr><tr><td>M</td><td>40</td><td>40</td><td>40</td><td>40</td><td>40</td><td>40</td></tr><tr><td>N</td><td>100</td><td>100</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>S</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td><td>50</td></tr><tr><td>σw</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><td>clip ratio ε</td><td>0.2</td><td>0.2</td><td>0.2</td><td>0.1</td><td>0.2</td><td>0.2</td></tr><tr><td>KL-penalty β</td><td>0.1</td><td>1.0</td><td>0.1</td><td>1.0</td><td>0.1</td><td>0.1</td></tr><tr><td>batch size</td><td>20</td><td>20</td><td>20</td><td>20</td><td>20</td><td>20</td></tr><tr><td>learning rate η</td><td>0.0001</td><td>0.00005</td><td>0.00005</td><td>0.00005</td><td>0.00001</td><td>0.0001</td></tr><tr><td>weight decay</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>hidden dim d</td><td>256</td><td>256</td><td>256</td><td>256</td><td>256</td><td>256</td></tr><tr><td>model depth L</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td></tr></table> ## B.3 Training Figure 6 provides the learning curves for the 6 RLAF- trained models in our main experiments. For all models, the cost decreases throughout training. We found that training with the March base solver tends to yield noisier training, particularly on 3SAT instances, where the policy does not improve further after 700 GRPO iterations. Exploring effective strategies for reducing this noise remains future work. Nonetheless, we are able to learn guidance policies that decrease the solver cost of both base solvers on all three problem instances. <center>Figure 6: GRPO training curves of the RLAF models from our main experiment. We plot the mean number of decisions on the validation set against the GRPO iteration. </center>

## B.4 Supervised UNSAT-Core and Backbone Prediction For UNSAT- core and backbone prediction in Section 3.2, we train supervised GNN models that are identical in architecture and size to the models used for our RLAF- based policies. The used hyperparameters are specified in Table 4. In the following, we provide a detailed description of how these models are trained and evaluated. ## UNSAT-Core Selsam and Bjørner (2019) propose to train supervised models that predict the UNSAT- core membership of variables and then use the model prediction to guide branching heuristics. Following their methodology, we phrase the task of predicting whether or not a variable occurs in an UNSAT- core as a variable- level binary classification task and train a GNN for this problem in a supervised manner using a standard cross- entropy loss. The ground- truth on training and validation instances is computed by extracting the cores from DRAT UNSAT proofs generated by the Ca Di Ca L Biere et al. (2024) solver. Note that these cores are not minimal, as computing such would not be feasible. As discussed in Section 3.2, the extracted cores on our unsatisfiable 3SAT training instances contain all variables for almost all instances and are therefore not a meaningful training target. We therefore only train UNSAT- core prediction models for 3COL and CRYPTO. We train a separate model for each distribution and restrict training to the unsatisfiable instances. Note that Selsam and Bjørner (2019) integrate their prediction by periodically resetting the VSIDS scores of the guided CDCL- solver to prediction logits of the GNN. This requires careful tuning of the reset frequency. It is also specific to solvers based on the VSIDS heuristic and would, for example, not be applicable to the March solver. Furthermore, in later ablation experiments, Selsam and Bjørner (2019) report that the performance improvement obtained with a trained GNN is barely distinguishable from when an untrained, randomly initialized model is used, further questioning the effectiveness of guiding solvers with this strategy. To facilitate a direct and fair comparison with RLAF- trained policies, we instead combine the UNSAT- core predictions with our own solver guidance based on multiplicative weights. For a variable \(x\) , let \(p_{\mathrm{core}}(x)\) be the predicted probability of \(x\) being in an UNSAT- core according to the trained GNN model. Then we transform these probabilities to variable weights through the following transformation: \[w(x) = 1 + \alpha \cdot p_{\mathrm{core}}(x). \quad (21)\] Here, \(\alpha \geq 0\) is a parameter that determines how the variable weight scales with the raw model predictions. For this experiment, we found weights of \(w(x) \geq 1\) to perform better, hence the offset of 1 in Equation (21). The value of \(\alpha\) is tuned on the corresponding validation dataset in the range \(\{10^{- 4}, 10^{- 3}, 10^{- 2}, 10^{- 1}, 10^{0}, 10^{1}, 10^{2}, 10^{3}, 10^{4}\}\) . We tune \(\alpha\) separately for both Glucose and March. The polarities are simply set to \(p(x) = 1\) as the prediction of UNSAT- core membership has no clear implication for the sign of the branching literal. Using this methodology, we found that the UNSAT- core predictions can significantly accelerate both base solvers, although by a smaller margin than RLAF- trained policies. ## Backbone Wang et al. (2024) suggests using the backbone membership of literals as a supervised training target and then setting variable polarities using the model predictions. We follow their methodology and train a GNN on the literal- level binary classification task using cross- entropy loss. As discussed in Section 3.2, we only train a model for the 3SAT instances and only use the satisfiable problems for training. The backbone of coloring problems is always empty due to the permutation symmetry of the colors, and some distributions, such as CRYPTO, predominantly consist of UNSAT instances. When evaluating, we set the polarity of a variable \(x\) as \(p(x) = 0\) if \(p_{\mathrm{backbone}}(-\alpha) > p_{\mathrm{backbone}}(x)\) and \(p(x) = 1\) otherwise. Here, \(p_{\mathrm{backbone}}(\ell)\) is the predicted probability of literal \(\ell\) belonging to the backbone. We further assign variable weights \(w(x)\) under the assumption that correctly assigning backbone literals in early search steps positively affects the runtime. To this end, we apply the transformation from Equation (21) to the mean backbone probability \(\overline{p}_{\mathrm{backbone}}(x) = 0.5(p_{\mathrm{backbone}}(-\alpha) + p_{\mathrm{backbone}}(x))\) to obtain a weight for each variable. Again, we tune the transformation parameter \(\alpha\) for both base solvers on the validation set.

Table 4: Hyperparameters of the supervised models. <table><tr><td></td><td>3SAT</td><td>3COL</td><td>CRYPTO</td></tr><tr><td>batch size</td><td>50</td><td>50</td><td>50</td></tr><tr><td>learning rate η</td><td>0.0001</td><td>0.0001</td><td>0.0001</td></tr><tr><td>weight decay</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><td>epochs</td><td>200</td><td>200</td><td>200</td></tr><tr><td>hidden dim d</td><td>256</td><td>256</td><td>256</td></tr><tr><td>model depth L</td><td>10</td><td>10</td><td>10</td></tr><tr><td>α Glucose</td><td>101</td><td>103</td><td>10-2</td></tr><tr><td>α March</td><td>10-2</td><td>10-2</td><td>101</td></tr></table> ## Supervised Comparison with March In Figure 7, we further provide the comparison with supervised baselines from Section 3.2 for the March base solver. On satisfiable 3SAT problems, our RLAF- trained policy and the guidance based on backbone prediction are roughly on par. However, on unsatisfiable 3SAT problems we found that backbone- based guidance increases the solver's runtime be approximately \(10\%\) . Backbone predictions are therefore not a useful guidance signal on this instance type when working with a strong base solver, such as March. Our RLAF- based policy does not share this problem. On the 3COL and CRYPTO distributions, the RLAF- trained policy consistently outperforms the guidance based on UNSAT core prediction, as for the Glucose base solver. <center>Figure 7: Runtimes relative to the base solver March for RLAF and supervised approaches based on Backbones and UNSAT cores. Less is better. We include the time required for the GNN forward pass in the runtime. </center> ## B.5 GNN Overhead In Table 5 we provide the results from our main experiments and additionally report the mean wall- clock runtime of the GNN forward pass. For all instance distributions, this GNN overhead is between 0.02 and 0.1 seconds, which is negligible when compared to SAT solver runtimes on non- trivial instances. However, we note that classical SAT solvers commonly perform over \(10^{4}\) branching decisions per second. In a setting where every guided branching decision requires a separate forward pass, as in prior RL- based work (Kurin et al., 2020; Cameron et al., 2024), it is therefore not possible to guide every branching decision without incurring a massive runtime overhead. Our one- shot setup avoids this problem as it incorporates multiplicative weights obtained in a single GNN pass in every branching decision with minimal runtime overhead.

Table 5: Full results on test instances, including the main time spent for the GNN forward pass. All metrics are averaged across the respective test sets. The mean number of decisions is rounded to the nearest whole number. For results with RLAF, we include the time required for the GNN forward pass in the total runtime. <table><tr><td>Data Distribution</td><td>Result</td><td>Glucose Decisions</td><td>Time (s)</td><td>Glucose + RLAF Decisions</td><td>Time (s)</td><td>Glucose + RLAF Decisions</td><td>Time (s)</td><td>March Decisions</td><td>Time (s)</td><td>March + RLAF Decisions</td><td>Time (s)</td><td>GPI time (s)</td></tr><tr><td>3SAT(300)</td><td>SAT</td><td>341,418</td><td>6.67</td><td>121,184</td><td>1.85</td><td>0.0210</td><td>2,893</td><td>0.25</td><td>2,389</td><td>0.23</td><td>0.0205</td><td></td></tr><tr><td>3SAT(300)</td><td>UNSAT</td><td>725,812</td><td>15.49</td><td>508,676</td><td>8.21</td><td>0.0209</td><td>11,783</td><td>1.01</td><td>11,757</td><td>1.04</td><td>0.0205</td><td></td></tr><tr><td>3SAT(350)</td><td>SAT</td><td>1,568,289</td><td>48.76</td><td>805,035</td><td>18.88</td><td>0.0238</td><td>16,546</td><td>1.64</td><td>11,702</td><td>1.19</td><td>0.0233</td><td></td></tr><tr><td>3SAT(350)</td><td>UNSAT</td><td>3,628,268</td><td>132.28</td><td>3,136,552</td><td>82.84</td><td>0.0237</td><td>52,287</td><td>5.14</td><td>51,846</td><td>5.16</td><td>0.0233</td><td></td></tr><tr><td>3SAT(400)</td><td>SAT</td><td>9,638,668</td><td>598.35</td><td>4,447,304</td><td>186.70</td><td>0.0265</td><td>64,296</td><td>7.27</td><td>47,992</td><td>5.51</td><td>0.0272</td><td></td></tr><tr><td>3SAT(400)</td><td>UNSAT</td><td>22,130,692</td><td>1,895.62</td><td>20,808,043</td><td>1,112.71</td><td>0.0265</td><td>245,064</td><td>27.49</td><td>242,499</td><td>27.51</td><td>0.0270</td><td></td></tr><tr><td>3Col(400)</td><td>SAT</td><td>15,519</td><td>0.36</td><td>6,988</td><td>0.22</td><td>0.0662</td><td>926</td><td>0.22</td><td>598</td><td>0.21</td><td>0.0661</td><td></td></tr><tr><td>3Col(400)</td><td>UNSAT</td><td>70,692</td><td>1.99</td><td>34,920</td><td>0.81</td><td>0.0662</td><td>10,563</td><td>2.61</td><td>5,954</td><td>1.57</td><td>0.0661</td><td></td></tr><tr><td>3Col(500)</td><td>SAT</td><td>82,758</td><td>2.61</td><td>35,901</td><td>1.05</td><td>0.0853</td><td>7,689</td><td>2.55</td><td>4,754</td><td>1.68</td><td>0.0848</td><td></td></tr><tr><td>3Col(500)</td><td>UNSAT</td><td>460,881</td><td>17.47</td><td>363,278</td><td>12.24</td><td>0.0855</td><td>100,811</td><td>33.34</td><td>60,321</td><td>20.52</td><td>0.0849</td><td></td></tr><tr><td>3Col(600)</td><td>SAT</td><td>606,598</td><td>25.03</td><td>339,378</td><td>11.59</td><td>0.0984</td><td>63,512</td><td>27.16</td><td>42,862</td><td>18.71</td><td>0.0988</td><td></td></tr><tr><td>3Col(600)</td><td>UNSAT</td><td>3,092,344</td><td>193.96</td><td>2,811,133</td><td>155.23</td><td>0.0984</td><td>754,720</td><td>313.57</td><td>461,639</td><td>197.12</td><td>0.0990</td><td></td></tr><tr><td>Crypt(20)</td><td>UNSAT</td><td>51,294</td><td>1.16</td><td>3,541</td><td>0.15</td><td>0.0974</td><td>1,203</td><td>0.82</td><td>390</td><td>0.41</td><td>0.0973</td><td></td></tr><tr><td>Crypt(15)</td><td>UNSAT</td><td>225,447</td><td>5.74</td><td>64,150</td><td>1.40</td><td>0.1075</td><td>52,282</td><td>34.56</td><td>8,257</td><td>6.40</td><td>0.1073</td><td></td></tr><tr><td>Crypt(10)</td><td>UNSAT</td><td>3,753,850</td><td>162.45</td><td>1,520,075</td><td>64.95</td><td>0.1148</td><td>679,864</td><td>467.38</td><td>230,905</td><td>169.22</td><td>0.1174</td><td></td></tr></table>

to this training paradigm as Reinforcement Learning from Algorithm Feedback (RLAF). Finally, we demonstrate empirically that modern RL techniques, such as GRPO (Shao et al., 2024), are capable of training effective RLAF policies for different base solvers. The learned policies substantially reduce solver runtimes, generalize to harder problems after training, outperform supervised baselines, and appear to capture solver- agnostic structural properties of SAT problems. ### 1.1 Background SAT Solving A Boolean formula in Conjunctive Normal Form (CNF) is a conjunction of clauses \(\phi = C_{1} \wedge \dots \wedge C_{m}\) , each clause being a disjunction of one or more literals \(C_{j} = (\ell_{j,1} \vee \dots \vee \ell_{j,k})\) . We denote by \(\mathrm{Var}(\phi) = \{x_{1}, \ldots , x_{n}\}\) the set of Boolean variables of \(\phi\) . The Boolean SAT problem is to decide whether or not there exists a satisfying assignment \(\alpha : \mathrm{Var}(\phi) \to \{0, 1\}\) that satisfies all clauses of a given formula \(\phi\) . This problem is well- known to be NP- complete and naturally arises in a wide range of applications (Biere et al., 2021). Modern SAT solvers predominantly stem from the Davis- Putnam- Logemann- Loveland (DPLL) algorithm, a backtracking search approach enhanced by unit propagation and pure literal elimination. Algorithm 1 provides a pseudocode description of a DPLL SAT solver. Many extensions of this general idea have been proposed to scale SAT solvers to larger, industrial instances. In particular, Conflict- Driven Clause Learning (CDCL) solvers significantly extend the DPLL framework by introducing clause learning and non- chronological backtracking. A common property of DPLL- derived solvers is the importance of the branching heuristic that picks the next branching literal in each search step (line 11 in Algorithm 1). Various branching heuristics have been proposed, and which heuristic performs best often depends on the structure of the given SAT formula \(\phi\) (Kullmann, 2021). Customizing branching heuristics towards a specific distribution of inputs generally requires expert knowledge and significant trial and error. Reinforcement Learning Reinforcement learning (RL) aims to learn policies for sequential decision- making problems where an agent interacts with an environment to learn through trial and error. An RL problem is usually formalized as a Markov Decision Process (MDP), which is defined as a tuple \((\mathcal{S}, \mathcal{A}, P, R)\) , where \(\mathcal{S}\) is the set of states, \(\mathcal{A}\) is the set of possible actions, \(P\) denotes the transition probabilities between states and \(R\) is the reward function. In probabilistic settings, policies \(\pi\) are stochastic mappings from states to distributions over actions, i.e., \(\pi (a|s)\) indicates the probability of selecting action \(a\) in state \(s\) . More generally, for continuous action spaces, i.e. \(\mathcal{A} = \mathbb{R}\) , \(\pi (a|s)\) is the probability density that a policy assigns to an action \(a\) . The primary objective in RL is to determine an optimal policy \(\pi^{*}\) that maximizes the expected cumulative discounted reward \(\mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\infty} \gamma^{t} R(s_{t}, a_{t}) \right]\) , where \(\gamma \in [0, 1)\) represents a discount factor that emphasizes earlier rewards. This formulation is the basis for various RL algorithms. Recently, RL algorithms based on policy gradients, such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024) have been used extensively to fine- tune LLMs from human feedback (RLHF, Christiano et al. (2017); Ouyang et al. (2022)) and from verifiable rewards (RLVR, Lambert et al. (2024); Guo et al. (2025)). ### 1.2 Related Work Leveraging deep learning in the context of combinatorial optimization (CO) problems has emerged as a major area of research (Cappart et al., 2021) and has been applied to a wide range of problems such as combinatorial graph problems (Khalil et al., 2017), SAT solving (Selsam et al., 2019), Mixed- Integer Programming (Khalil et al., 2022), and Constraint Satisfaction Problems (Tönshoff et al., 2023). Here, we primarily focus on work that aims to enhance SAT solvers with (graph) neural networks. One line of work suggests using predictions of predefined variable properties to guide SAT solver branching heuristics. Selsam and Björner (2019) train a GNN to predict whether variables belong to an UNSAT core. The branching heuristic is then guided by periodically resetting the solver's VSIDS scores to the GNN's predictions, thus making the guidance specific to VSIDS- based CDCL solvers and dependent on careful tuning of the reset frequency. Wang et al. (2024) predict whether literals occur in the backbone of satisfiable formulas and use these predictions to set the polarity of variables. Another line of work explores purely RL- based training for enhancing branching heuristics, eliminating the need for expert supervision. Kurin et al. (2020) uses Q- learning to train GNNs end- to- end as branching policies to minimize solver runtime, and Cameron et al. (2024) propose Monte Carlo Forest Search for guiding early branching decisions in SAT Solvers on UNSAT problems. Both methods require one GNN forward pass per guided branching decision, which creates a significant bottleneck as the GNN usually requires orders of magnitude more runtime than classical branching

# Algorithm 1 DPLL Solver 1: Input: Formula \(\phi\) 2: function SOLVE \((\phi)\) 3: # Simplify formula 4: \(\phi \leftarrow \mathrm{UNIT - PROPAGATION}(\phi)\) 5: \(\phi \leftarrow \mathrm{PURE - LITERAL - ELIMINATION}(\phi)\) 6: 7: if \(\phi = \emptyset\) : return SAT 8: if \(\emptyset \in \phi\) : return UNSAT 9: 10: # Decide next branching variable 11: \(\ell \leftarrow \mathrm{PICK - LITERAL}(\phi)\) 12: return \(\mathrm{SOLVE}(\phi \land \{\ell \}) \vee \mathrm{SOLVE}(\phi \land \{\neg \ell \})\) 13: end function # Algorithm 2 Decision Heuristic 1: Input: Formula \(\phi\) 2: function PICK- LITERAL \((\phi)\) 3: \(\hat{x}\leftarrow \mathrm{argmax}_{x}\mathrm{SCORE}(x)\) 4: return \(\hat{x}\) if PICK- SIGN \((\hat{x})\) else \(\neg \hat{x}\) 5: end function # Algorithm 3 Guided Decision Heuristic 1: Input: Formula \(\phi\) , Parameters \(\mathcal{W} = (w, p)\) 2: function PICK- LITERAL- GUIDED \((\phi , \mathcal{W})\) 3: \(\hat{x} \leftarrow \mathrm{argmax}_{x} w(x) \cdot \mathrm{SCORE}(x)\) 4: return \(\hat{x}\) if \(p(\hat{x}) = 1\) else \(\neg \hat{x}\) 5: end function Figure 1: DPLL SAT solver and branching heuristics. Algorithm 1: A DPLL SAT solver performs backtracking search to solve a given CNF formula \(\phi\) . At each search step, the formula is simplified through unit propagation and pure literal elimination before selecting the next branching literal. Algorithm 2: Branching heuristics are often implemented by choosing the variable that maximizes some hand- crafted scoring function. Algorithm 3: We propose to extend existing branching heuristics by incorporating given variable weights into the branching decisions that scale the associated score of each variable. We additionally choose the sign of each literal according to a provided polarity. heuristics. Further related work is proposed by Han (2020), who accelerate cube- and- conquer solvers with supervised learning, Liu et al. (2024), who suggest improving clause deletion heuristics in CDCL SAT solvers with GNNs, and Zhai and Ge (2025), who use RL to speed up parallelized divide- and- conquer solvers. ## 2 RLAF-guided SAT Solvers ### 2.1 Guided Branching Heuristics We modify existing SAT solvers to incorporate external variable weights into their branching heuristic. Let some base SAT solver be given. We assume that this solver is a DPLL- derived backtracking search algorithm. We further assume that the branching heuristic is implemented by first selecting a variable \(\hat{x} = \mathrm{argmax}_{x} \mathrm{Score}(x)\) that maximizes some variable scoring function Score before picking a literal sign according to some secondary heuristic, as illustrated in Algorithm 2. Many existing branching heuristics, such as VSIDS and look- ahead heuristics, fit the generic algorithm pattern while relying on different definitions of variable scores. Note that these scores usually depend on the current partial assignment of the search as well as information extracted in previous search steps, such as encountered conflicts. We can modify this decision heuristic to incorporate additional variable weights \(w: \mathrm{Var}(\phi) \to \mathbb{R}_{>0}\) for the given input formula \(\phi\) : \[\hat{x} = \mathrm{argmax}_{x} w(x) \cdot \mathrm{Score}(x) \quad (1)\] These weights are passed to the modified solver as additional input and modulate its branching heuristic by scaling the variable- wise scores. In this manner, we can inject prior knowledge of variable importance into the solver's branching decisions without sacrificing its original heuristic. Naturally, choosing a useful variable weighting \(w\) by hand is difficult. Instead, our focus is on learning to infer effective variable weights from the input formula's structure using a deep neural network. In addition to these weights, we may also specify a mapping \(p: \mathrm{Var}(\phi) \to \{0, 1\}\) that assigns a polarity \(p(x)\) to each variable \(x\) . When \(x\) is chosen as a decision variable, the polarity determines which value is assigned to \(x\) first. Specifying polarities for variables is a common function for modern SAT solvers, and well- chosen values can have a significant impact on run time, especially on satisfiable instances. In this work, we will infer variable- wise polarities alongside the variable weights \(w\) with a learned GNN model. Overall, the modified solver \(\mathrm{Solve}(\phi , \mathcal{W})\) takes as input a CNF formula \(\phi\) as well as a variable parameterization \(\mathcal{W} = (w, p)\) that assigns a weight \(w(x) \in \mathbb{R}_{>0}\) and polarity \(p(x) \in \{0, 1\}\) to each variable \(x \in \mathrm{Var}(\phi)\) .

<center>Figure 2: a) The input formula \(\phi\) is modeled as a graph \(G(\phi)\) . b) The graph is processed by a trainable GNN and outputs a parameterization policy \(\pi_{\theta}(\phi)\) . c) The policy \(\pi_{\theta}(\phi)\) consists of independent variable-wise weight (Log Normal) and polarity (Bernoulli) distributions. d) A variable parameterization \(\mathcal{W} = (w,p)\) is sampled from \(\pi_{\theta}(\phi)\) , mapping each variable \(x\) in \(\phi\) to a weight \(w(x) \in \mathbb{R}_{>0}\) and polarity \(p(x) \in \{0,1\}\) . e) A guided SAT solver incorporates the parameterization \(\mathcal{W}\) to guide its branching heuristic. </center> ### 2.2 Graph Representation and Architecture Our goal is to map an instance \(\phi\) to advantageous variable weights and polarities with a neural network. A natural approach is to map \(\phi\) to a suitable graph representation \(G(\phi) = (V(\phi), E(\phi))\) that captures the instance's structure. This graph can then be processed by a GNN that extracts structural information in a trainable manner. We represent \(\phi\) as a standard "Literal- Clause Graph" proposed in prior work Selsam et al. (2019). Note that this choice is modular; other graph representations have also been suggested in the literature and could also be used. We process this graph with a trainable GNN model \(\mathcal{N}_{\theta}\) that performs message passing to extract latent structural embeddings for every vertex. Here, \(\theta\) represents the vector that contains all trainable model parameters. The output of \(\mathcal{N}_{\theta}\) is a mapping \(y: \operatorname {Var}(\phi) \to \mathbb{R}^{2}\) that assigns two real numbers to each variable in the input formula \(\phi\) . The full model details are provided in Appendix A. ### 2.3 Guidance Policy For a given input formula \(\phi\) , we map the output of the GNN \(\mathcal{N}_{\theta}\) to a policy \(\pi_{\theta}(\phi)\) from which a variable parameterization \(\mathcal{W} \sim \pi_{\theta}(\phi)\) can be sampled. Recall that for a given SAT instance the GNN \(\mathcal{N}_{\theta}\) outputs a mapping \(y: \operatorname {Var}(\phi) \to \mathbb{R}^{2}\) that associates every variable \(x \in \operatorname {Var}(\phi)\) with two real numbers \(\mu (x), \rho (x) \in \mathbb{R}\) , \([\mu (x), \rho (x)] = y(x)\) . These outputs are used to parameterize variable- wise weight and polarity distributions, respectively. Concretely, for each variable \(x\) in \(\phi\) we define its weight policy \(\pi_{\theta}^{w}(x)\) as a Log- Normal distribution over positive real weights: \[\pi_{\theta}^{w}(x) = \mathrm{Log Normal}(\mu (x),\sigma^{w}) \quad (2)\] Here, the inferred parameter \(\mu (x) \in \mathbb{R}\) is used as the log- mean of the distribution, and \(\sigma^{w} \in \mathbb{R}_{>0}\) is a hyperparameter. Log- Normal distributions offer a simple way to model unimodal distributions over positive real numbers and performed best in preliminary experiments. We note that we also observed reasonable training convergence when using both Poisson and truncated normal distributions for variable weights, and more options may be explored in future work. Analogously, we define a variable's polarity policy \(\pi_{\theta}^{p}(x)\) as a Bernoulli distribution where the probability is obtained by applying a sigmoid function to \(\rho (x)\) : \[\pi_{\theta}^{p}(x) = \mathrm{Bernoulli}(\mathrm{Sigmoid}(\rho (x))). \quad (3)\] The complete variable parameterization policy \(\pi_{\theta}\) is then defined as the joint distribution of \(\pi_{\theta}^{w}(x)\) and \(\pi_{\theta}^{p}(x)\) over all variables: \[\pi_{\theta}(\phi) = \pi_{\theta}^{w}(x_{1}) \times \pi_{\theta}^{p}(x_{1}) \times \dots \times \pi_{\theta}^{w}(x_{n}) \times \pi_{\theta}^{p}(x_{n}). \quad (4)\] We sample a variable parameterization \(\mathcal{W} = (w,p) \sim \pi_{\theta}\) from this distribution in one shot by independently sampling a weight \(w(x) \sim \pi_{\theta}^{w}(x)\) and polarity \(p(x) \sim \pi_{\theta}^{p}(x)\) for each variable \(x\) in parallel. The probability density \(\pi_{\theta}(\mathcal{W}|\phi)\) of \(\mathcal{W}\) can then be factorized as \[\pi_{\theta}(\mathcal{W}|\phi) = \prod_{x}\pi_{\theta}^{w}(w(x)|\phi)\cdot \pi_{\theta}^{p}(p(x)|\phi). \quad (5)\]

<center>Figure 3: Learning to accelerate a SAT solver with GRPO: a) For a given training formula \(\phi\) sample multiple variable parameterizations i.i.d. from the current policy \(\pi_{\theta}(\phi)\) . b) Run the SAT solver on \(\phi\) with each parameterization. c) Map the cost of each solver run (i.e. the number of decisions) to the normalized group-relative advantage \(\hat{A} (\phi ,\mathcal{W})\) . d) Optimize the model weights \(\theta\) to maximize \(\mathcal{L}_{\mathrm{PPO}}\) to shift the policy towards faster parameterizations. </center> Note that all trainable weights of the GNN model have a partial derivative with respect to \(\pi_{\theta}(\mathcal{W}|\phi)\) for a given \(\mathcal{W}\) , which enables us to train with policy- gradient methods such as GRPO. During training, we sample multiple \(\mathcal{W}\) i.i.d. from \(\pi_{\theta}(\phi)\) and use the variance of the observed solver runtimes to compute our training signal, as explained in Section 2.4. At test time, we do not sample randomly from \(\pi_{\theta}(\phi)\) but simply use the mode \(\hat{\mathcal{W}}\) , which deterministically chooses the most probable weight and polarity for each variable \(x\) . This eliminates a source of variance when testing and, on average, yields better results than sampling at random from the learned policy. ### 2.4 Policy Optimization Our aim is to learn a policy GNN that guides the SAT solver towards lower computational costs on a given distribution of SAT instances. Formally, let \(\Omega\) be some training distribution of SAT problems. The objective is to learn model weights \(\theta\) that minimize the expected solver cost when applying the learned policy to instances sampled from \(\Omega\) : \[\theta^{*} = \underset {\theta}{\arg \min}\underset {\phi \sim \Omega ,\mathcal{W}\sim \pi_{\theta}(\phi)}{\mathbb{E}}[\mathrm{Cost}(\phi ,\mathcal{W})]. \quad (6)\] Here, \(\mathrm{Cost}(\phi ,\mathcal{W})\) is defined as the number of decisions required when running \(\mathrm{Solve}(\phi ,\mathcal{W})\) , which is the primary target metric we aim to minimize. We can view this objective as an RL problem by modeling the process of choosing \(\mathcal{W}\) as a single- step Markov Decision Process (MDP) where the input formula \(\phi\) is viewed as the state, and a single- step episode unfolds by choosing a variable parameterization \(\mathcal{W}\) as the action. Once the action is taken, the environment transitions immediately to a terminal state, yielding a reward \(R(\phi ,\mathcal{W}) = - \mathrm{Cost}(\phi ,\mathcal{W})\) that is the negative of the solver's cost (e.g., number of decisions). Note that we also experimented with directly using CPU time as a cost measure, but found this to yield less stable training due to the performance variance caused by noisy CPU utilization. We leverage Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to learn a policy for this RL problem. GRPO is a simplification of Proximal Policy Optimization (PPO) (Schulman et al., 2017) that eliminates the need for learning an additional value network. The initial model weights \(\theta_{0}\) are sampled at random. GRPO updates these model weights in iterations \(k\in \{1,\ldots ,K\}\) In iteration \(k\) , we first sample a batch of training instances from \(\mathcal{F} = \{\phi_{1},\ldots ,\phi_{N}\} \sim \Omega^{N}\) from the given training distribution. For each such formula \(\phi_{i}\) we sample \(M\) variable parameterizations \(\mathcal{W}_{i,1},\ldots ,\mathcal{W}_{i,M}\sim \pi_{\theta_{k - 1}}(\phi_{i})\) i.i.d. from the current policy. We then run \(\mathrm{Solve}(\phi_{i},\mathcal{W}_{i,j})\) for all \(i,j\in [N]\times [M]\) and measure the corresponding cost and reward. The group- relative advantage is then defined as \[\hat{A}_{i,j} = \frac{R(\phi_{i},\mathcal{W}_{i,j}) - \mathrm{mean}(\mathbf{R}_{i})}{\mathrm{std}(\mathbf{R}_{i})} \quad (7)\] where \(\mathbf{R}_{i} = \{R(\phi_{i},\mathcal{W}_{i,j}) \mid j\in \{1,\ldots ,M\} \}\) is the set of all rewards collected for the same instance \(\phi_{i}\) . The main objective is to maximize the clipped policy update function for each training instance \(\phi_{i}\) : \[\mathcal{L}_{\mathrm{PPO}}(\theta \mid \phi_{i}) = \frac{1}{M}\sum_{j}\left[\min \left(r_{i,j}(\theta)\hat{A}_{i,j},\mathrm{clip}(r_{i,j}(\theta),1 - \epsilon ,1 + \epsilon)\hat{A}_{i,j}\right)\right]. \quad (8)\]

Here, \(\epsilon \in (0,1)\) is a hyperparameter, and \(r_{i,j}(\theta)\) is defined as the probability ratio of the new policy and the policy learned in the previous GRPO iteration: \[r_{i,j}(\theta) = \frac{\pi_{\theta}(\mathcal{W}_{i,j}|\phi_{i})}{\pi_{\theta_{k - 1}}(\mathcal{W}_{i,j}|\phi_{i})} \quad (9)\] This objective aims to adjust the policy such that actions (e.g., variable parameterizations) with high advantage become more likely while avoiding excessively large distribution shifts by clipping the objective at a probability ratio determined by \(\epsilon\) . The full training objective combines \(\mathcal{L}_{\mathrm{PPO}}\) with an additional term that penalizes the KL divergence relative to the previous model weights \(\theta_{k - 1}\) to stabilize training further: \[\mathcal{L}(\theta \mid \phi_{i}) = \mathcal{L}_{\mathrm{PPO}}(\theta \mid \phi_{i}) - \beta \cdot \mathrm{KL}\left(\pi_{\theta}(\phi_{i}),\pi_{\theta_{k - 1}}(\phi_{i})\right). \quad (10)\] Here, the weight \(\beta \geq 0\) is an additional hyperparameter. Starting from the previous model weights \(\theta_{k - 1}\) , we learn updated model weights \(\theta_{k}\) by performing stochastic gradient ascent for a fixed number of steps to maximize this objective function for all training instances. This overall process repeats in the next round of GRPO. In the appendix, Algorithm 4 provides a complete formal specification of our training. ### 2.5 Training Setup We are utilizing GRPO as an online RL algorithm to learn the parameters of our policy GNN directly from observed solver costs. As a consequence, we train with the SAT solver in- the- loop and make \(M\cdot N\) calls to the solver per GRPO iteration. With our default parameters ( \(N = 100\) , \(M = 40\) ) we make 4000 SAT solver calls in each iteration. This imposes the practical constraint to train on a distribution \(\Omega\) of SAT problems where this number of solver calls is possible in an acceptable time on the underlying hardware. The work presented here intends to be a small- scale demonstration of RLAF as a training paradigm, and all training is performed on machines with one (multi- core) CPU and one GPU. Therefore, the training data in our experiments is chosen so that each instance is solvable by the given base solvers within a fraction of a second. In future work, the hardness and size of the training problems can be scaled up substantially by leveraging a distributed compute cluster for collecting the SAT solver feedback. Crucially, we demonstrate in Section 3 that after training, the learned policies do generalize to significantly harder and larger problems. The reliance on comparatively easy training problems is therefore not a significant limitation for learning effective GNN- guidance with RLAF. ## 3 Experiments In our experiments<sup>2</sup>, we aim to answer two primary research questions: (i) Can RLAF train GNN- based guidance policies that shorten solver runtimes and generalize to harder formulas? (ii) How do RLAF- trained policies fare against guidance based on learning predefined notions of variable importance in a supervised manner? Furthermore, we want to understand whether the learned policies capture known variable properties after training and whether the policies learned with different solvers are related or solver- specific. Solvers We conduct experiments with two distinct base solvers: The well- known CDCL solver Glucose (Audemard and Simon, 2017) and the DPLL solver March (Heule et al., 2005). Glucose uses the VSIDS branching heuristic and is comparatively strong on structured problems, while March uses a look- ahead branching heuristic and is among the best- known solvers for random instances. We provide more technical details about how RLAF is integrated into both solvers in Appendix A.3. Data We consider three well- known classes of SAT problems with significantly different structures to study how well RLAF can adapt the base solvers to each of them. In Appendix B.1 we provide full details on the data generation and dataset statistics. Random 3SAT: We define 3SAT \((n)\) as the distribution of uniformly random 3SAT instances with \(n\) variables and clause- to- variable ratio of 4.26, which is approximately the critical density where the instances transition from SAT to UNSAT. The training data consists of 20K instances sampled from 3SAT(200). We test on larger instances with \(n \in \{300, 350, 400\}\) , where we sample 200 instances for each size \(n\) . Graph Coloring: We consider

Table 1: Results on test instances. All metrics are averaged across the respective test sets. The mean number of decisions is rounded to the nearest whole number. For results with RLAF, we include the time required for the GNN forward pass in the total runtime. We highlight numbers in bold when they are the best value achieved for the respective base solver. <table><tr><td colspan="2">Data</td><td rowspan="2">Result</td><td rowspan="2">Count</td><td colspan="2">Glucose</td><td colspan="2">Glucose + RLAF</td><td rowspan="2">March Decisions</td><td rowspan="2">Time (s)</td><td rowspan="2">March Decisions</td><td rowspan="2">Time (s)</td></tr><tr><td>Distribution</td><td></td><td>Decisions</td><td>Time (s)</td><td>Decisions</td><td>Time (s)</td></tr><tr><td rowspan="2">3SAT(300)</td><td>SAT</td><td>103</td><td>341,418</td><td>6.67</td><td>121,184</td><td>1.85</td><td>2,893</td><td>0.25</td><td>2,389</td><td>0.23</td></tr><tr><td>UNSAT</td><td>97</td><td>725,812</td><td>15.49</td><td>508,676</td><td>8.21</td><td>11,783</td><td>1.01</td><td>11,757</td><td>1.04</td></tr><tr><td rowspan="2">3SAT(350)</td><td>SAT</td><td>108</td><td>1,568,289</td><td>48.76</td><td>805,035</td><td>18.88</td><td>16,546</td><td>1.64</td><td>11,702</td><td>1.19</td></tr><tr><td>UNSAT</td><td>92</td><td>3,628,268</td><td>132.28</td><td>3,136,552</td><td>82.84</td><td>52,287</td><td>5.14</td><td>51,846</td><td>5.16</td></tr><tr><td rowspan="2">3SAT(400)</td><td>SAT</td><td>89</td><td>9,638,668</td><td>598.35</td><td>4,447,304</td><td>186.70</td><td>64,296</td><td>7.27</td><td>47,992</td><td>5.51</td></tr><tr><td>UNSAT</td><td>111</td><td>22,130,692</td><td>1,895.62</td><td>20,808,043</td><td>1,112.71</td><td>245,064</td><td>27.49</td><td>242,499</td><td>27.51</td></tr><tr><td rowspan="2">3COL(400)</td><td>SAT</td><td>77</td><td>15,519</td><td>0.36</td><td>6,988</td><td>0.22</td><td>926</td><td>0.22</td><td>598</td><td>0.21</td></tr><tr><td>UNSAT</td><td>123</td><td>70,692</td><td>1.99</td><td>34,920</td><td>0.81</td><td>10,563</td><td>2.61</td><td>5,954</td><td>1.57</td></tr><tr><td rowspan="2">3COL(500)</td><td>SAT</td><td>91</td><td>82,758</td><td>2.61</td><td>35,901</td><td>1.05</td><td>7,689</td><td>2.55</td><td>4,754</td><td>1.68</td></tr><tr><td>UNSAT</td><td>108</td><td>460,881</td><td>17.47</td><td>363,278</td><td>12.24</td><td>100,811</td><td>33.34</td><td>60,321</td><td>20.52</td></tr><tr><td rowspan="2">3COL(600)</td><td>SAT</td><td>87</td><td>606,598</td><td>25.03</td><td>339,378</td><td>11.59</td><td>63,512</td><td>27.16</td><td>42,862</td><td>18.71</td></tr><tr><td>UNSAT</td><td>113</td><td>3,092,344</td><td>193.96</td><td>2,811,133</td><td>155.23</td><td>754,720</td><td>313.57</td><td>461,639</td><td>197.12</td></tr><tr><td rowspan="3">CRYPTO(20)</td><td>UNSAT</td><td>100</td><td>51,294</td><td>1.16</td><td>3,541</td><td>0.15</td><td>1,203</td><td>0.82</td><td>390</td><td>0.41</td></tr><tr><td>CRYPTO(15)</td><td>UNSAT</td><td>100</td><td>225,447</td><td>5.74</td><td>64,150</td><td>1.40</td><td>52,282</td><td>34.56</td><td>8,257</td><td>6.40</td></tr><tr><td>CRYPTO(10)</td><td>UNSAT</td><td>100</td><td>3,753,850</td><td>162.45</td><td>1,520,075</td><td>64.95</td><td>679,864</td><td>467.38</td><td>230,905</td><td>169.22</td></tr></table> the distribution \(3\mathrm{COL}(n)\) of SAT problems that decide 3- colorability for random Erdős- Rényi graphs with \(n\) vertices. We set the edge probability such that the expected vertex degree is 4.67, which is approximately the critical density for 3- colorability where hard instances are common (Zdeborová and Krząkała, 2007). We train on 20K instances sampled from \(3\mathrm{COL}(300)\) on 200 larger problems each for \(n \in \{400,500,600\}\) . Cryptographic: We further include highly structured instances arising from SAT- based decryption attacks (Soos et al., 2009). We define \(\mathrm{CRYPTO}(n)\) as the distribution of SAT instances generated for decrypting the Hi Tag2 cipher (Courtois et al., 2009) with \(n\) given help bits. Note that these instances are harder for smaller values of \(n\) and are mostly UNSAT. We train on 20K instances from \(\mathrm{CRYPTO}(22)\) and test on harder problems with \(n \in \{20,15,10\}\) . Hyperparameters We train a different model for each solver and each SAT problem class. We configure the GNN with 10 layers with embedding dimension \(d = 256\) . We train for \(K = 2000\) GRPO iterations. In every iteration, we use \(N = 100\) training formulas and collect feedback for \(M = 40\) variable parameterizations for each formula. The SAT solver runs are parallelized across all CPU cores. The model is trained for 50 steps of SGD in each GRPO iteration. Each training run uses a machine equipped with a single H100 GPU, an Intel Xeon 8468 CPU with 48 cores, and 128GB of RAM. The total runtime of all training runs is between 24h and 48h. ### 3.1 Main Results Table 1 provides the main results for both Glucose and March on our test sets. We observe that GNN- guided training with RLAF consistently accelerates the given base solver. The margin of improvement depends on the base solver and the class of problem instances. For 3SAT(400) problems, RLAF- guidance reduces the mean runtime of Glucose by \(69\%\) and \(41\%\) for satisfiable and unsatisfiable instances, respectively. Similar improvements are observed for satisfiable 3- coloring problems as well as cryptographic instances. For unsatisfiable coloring instances with 600 vertices, the runtime of Glucose is reduced by around \(24\%\) . The smallest margin of improvement is observed for the March solver on unsatisfiable 3SAT instances. While March+RLAF does need fewer decisions on average to solve this class of problems, the runtime is worse, as the small improvement does not compensate for the additional runtime overhead of the GNN forward pass. It is known that lookahead DPLL solvers like March are very strong baselines for unsatisfiable random instances, so this result is not surprising. For more structured problem classes, RLAF is able to accelerate the March solver substantially on both satisfiable and unsatisfiable instances. We emphasize that the wall- clock runtime of the GNN forward pass is included in the runtime measurements with RLAF- guidance. For the instances used here, this runtime is generally around 0.1 seconds or less, which is negligible compared to the solver runtimes on harder problems. We refer to Table 5 in the appendix for extended results that report the GNN overhead in detail. Overall, these results demonstrate that RLAF is able to train GNN- based solver guidance and that relying on comparatively easy problems for efficient training does not prevent the learned policy from generalizing to more complex problems at test time.

<center>Figure 4: Runtimes relative to the base solver Glucose for RLAF and supervised approaches based on Backbones and UNSAT cores. Less is better. </center> ### 3.2 Comparison to Supervised Approaches Prior work suggests predicting predefined variable properties, such as UNSAT core Selsam and Bjørner (2019) or backbone Wang et al. (2024) membership, in a supervised manner, and then transforming model predictions into variable weights and polarities for solver guidance. Here, we aim to compare how guidance learned with RLAF compares to this approach. Note that the notions of UNSAT cores and backbones are only sensible training targets for some instance distributions. Backbones can only be non- empty on satisfiable instances, and even for satisfiable graph coloring problems, all backbones are empty due to the permutation symmetry of the vertex colors. Furthermore, on our UNSAT 3SAT training instances, we observed that the UNSAT core extracted by SAT solvers contained all variables on almost all instances, yielding a training target that is effectively constant. Due to these limitations, we use the 3SAT instances to evaluate the effectiveness of predicting the backbone, while we use the graph coloring and cryptographic instances to compare RLAF to core- based solver guidance. For a fair comparison, we use the same GNN architecture used to train with RLAF and train a separate model for each instance distribution. The transformation that maps the backbone/core predictions to variable weights is tuned separately for each instance distribution on the corresponding validation set. Full details about the setup of this comparison are provided in Appendix B.4. Figure 4 compares the results for Glucose in terms of the relative wall- clock runtime compared to the base solver. Overall, the policy learned with RLAF significantly outperforms solver guidance based on both UNSAT core and backbone predictions by achieving a smaller relative runtime. The backbone- based heuristic outperforms RLAF only on satisfiable 3SAT instances with 300 variables, but not on larger problems. On unsatisfiable 3SAT problems, the backbone- guided heuristic performs substantially worse. RLAF also outperforms core- based guidance for both graph coloring and cryptographic SAT problems. Overall, these results demonstrate that pure RL- based learning with RLAF can yield more effective solver guidance than predicting handcrafted notions of variable importance in a supervised manner. ### 3.3 Exploring Learned Variable Weights We further aim to gain insights into the weight distributions learned through RLAF. In particular, we investigate whether the policies learned with different base solvers are related and whether they capture predefined variable properties, such as backbone and UNSAT core membership. To this end, Figure 5 compares the weights for 5000 randomly selected variables from the corresponding validation sets. Specifically, we plot the expected variable weight \(\mathbb{E}[w(x)]\) for the Glucose- trained policy on the x- axis and plot the corresponding value for the March- trained policy on the y- axis. We also report the Pearson correlation coefficient \((r)\) for these weights to quantify their correlation. For 3SAT, we only plot variables from satisfiable instances and additionally indicate whether each variable belongs to its instance's backbone. Likewise, we focus on unsatisfiable instances for 3COL and CRYPTO and indicate if a variable occurs in the UNSAT core extracted for the experiment in Section 3.2. We observe that the variable weights of the two policies are generally correlated, with a Pearson correlation coefficient \(r\) between 0.73 and 0.85. This indicates that the learned weightings capture structural properties that are inherent to the variables and accelerate the search across different

<center>Figure 5: Weight correlation between policies learned with different solvers. For each instance distribution, we randomly sample 5,000 variables \(x\) from the corresponding validation set and plot the expected variable weight \(E[w(x)]\) for the policies learned with either base solver. The color further indicates the backbone or UNSAT core membership of each variable. </center> solvers. We further observe that for the 3Co L and CRYPTO instances, the variables with high weights are predominantly members of the UNSAT core. For these problem instances, the RLAF- based training therefore self- discovered weight policies that correlate to existing handcrafted heuristics while performing better, as demonstrated in Section 3.2. For the 3SAT instances, we do not observe a clear correlation between the learned weight policies and backbone membership, showing that in this case, the trained models express functions that, while effective, do not resemble this particular handcrafted heuristic. ## 4 Discussion We introduced RLAF as a paradigm for training GNN- based policies that guide the branching heuristics of SAT solvers. Our work contributes (i) a generic mechanism for injecting variable weights into branching heuristics, (ii) a formulation of weight selection as a one- shot RL problem, (iii) a way to leverage GNNs as trainable policies in this setting, and (iv) experimental evidence that GRPO can learn policies that reduce the computational cost of different base solvers. In our empirical studies, the learned policies generalize to larger and harder instances, and consistently surpass supervised baselines that rely on handcrafted variable properties. Moreover, policies trained with different base solvers converge toward similar structural signals, suggesting that RLAF is capturing inherent information about SAT instance structure that is not specific to the underlying base solver. The current implementation is designed as a small- scale prototype optimized for training on relatively simple formulas using moderate hardware. This constraint allows for only comparatively simple problems to be solved quickly enough to collect sufficient solver feedback on local CPU cores in each GRPO iteration. Expanding the system to leverage distributed computing resources, such as cloud- based infrastructure, would enable the incorporation of larger and harder SAT problems during training, potentially leading to better guidance policies. GNN scalability also remains a bottleneck, particularly during training, as processing large industrial instances imposes significant memory and computational demands. Research into more compact, domain- specific graph encodings of CNFs remains important future work. At the same time, the expressive power of our network, a standard message- passing GNN, is bounded by color refinement (Morris et al., 2019; Xu et al., 2019), leaving highly symmetric patterns indistinguishable. Combining expressivity and scalability remains a critical challenge for applying GNNs to large, structured problem instances. Finally, the proposed methodology is not strictly limited to SAT solvers. Branching heuristics are critical components not only in SAT solving but also for Mixed- Integer Programming (MIPs) and Constraint Satisfaction Problems (CSPs). More broadly, implementing any kind of selection heuristic as the argmax of some scoring function is a generic pattern of algorithm design found across many domains. For any such algorithm, one can introduce external multiplicative weights that guide the heuristic and then phrase the task of inferring effective weights as an RL problem. In this work, we have demonstrated that this general methodology can be leveraged in the context of SAT solving. Translating it to other domains and algorithms remains as future work.