{
  "sections": {
    "neuroback_improving_cdcl_sat_solving_using_graph_n": "## Introduction\n\n\n## 9 ACKNOWLEDGMENT 9 ACKNOWLEDGMENTWe would like to thank the anonymous reviewers for their valuable feedback. This work was supported by a grant from the Army Research Office accomplished under Cooperative Agreement Number W911NF- 19- 2- 0333. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This work was also supported in part by ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA and by the Intel RARE center. ## 10 APPENDIX ## 10.1 ADDITIONAL EXPERIMENTAL RESULTS 10.1 ADDITIONAL EXPERIMENTAL RESULTSThe scatter plots is another commonly used plot in the SAT community for comparing the solving effectiveness of two solvers on each problem. Fig. 5 shows the scatter plots of Neuro Back- Kissat and its two baseline solvers, Default- Kissat and Random- Kissat. It is evident that more dots are present in the lower triangular area, indicating that there are more problems on which Neuro Back- Kissat outperforms both Default- Kissat and Random- Kissat. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, leading to a reduction in solving time of 98 and 246 seconds per problem. ## 10.2 PERFORMANCE ON SOLVED SAT AND UNSAT PROBLEMS Upon detailed analysis, for 661 problems from both SATCOMP- 2022 and SATCOMP- 2023 testing sets, there are 194 unsat problems and 216 sat problems that are solved by either Default- Kissat or Neuro Back- Kissat. For the 194 solved unsat problems, Neuro Back- Kissat outperformed Default- Kissat in 121 cases (62.4%) while Default- Kissat outperformed Neuro Back- Kissat in only 61 problems (31.4%). For the 216 solved sat problems, Neuro Back- Kissat outperformed Default- Kissat in 110 problems (50.9%), while Default- Kissat outperformed Neuro Back- Kissat in 87 problems (40.3%). While Neuro Back- Kissat showed a higher improvement rate in unsat problems compared to sat ones (62.4% vs 50.9%), the extent of improvement was more significant in sat problems. On average, Neuro Back- Kissat enhanced the performance of sat problems by 53.2%, compared to an average improvement of only 14.6% in unsat problems. These trends were similarly observed when comparing Neuro Back- Kissat with Random- Kissat. The experimental results highlight two key aspects. First, they demonstrate that Neuro Back's predicted variable phases can enhance the efficiency in solving unsat problems. Our explanation is that Neuro Back's phase predictions can aid in directing the search towards the unsatisfiable part of the search space. While Neuro Back cannot satisfy all components of a given SAT problem, it may predict phases that satisfy certain components, thereby allowing the solver to concentrate on the unsat part. Furthermore, in modern SAT solvers such as Default- Kissat Biere & Fleury (2020), an assignment that falsifies the fewer clauses is often preferred in the searching loop, allowing the solver to specifically target the unsat portions of the clause set. Consequently, the phases predicted by Neuro Back can facilitate identifying an assignment that reduces clause falsification, thereby enhancing solving unsat problems. Second, the experimental results also show that Neuro Back achieves a more pronounced improvement in solving sat problems than in solving unsat problems. This distinction stems from the inherent nature of these problems. In sat problems, a complete satisfying assignment exists, where each variable is assigned a phase that leads to a solution. Conversely, in unsat problems, only partial satisfying assignments exist, with phases assigned to just a subset of variables. Consequently, the phases predicted by Neuro Back have a generally greater impact in resolving sat problems. This is because, for these problems, the predicted phases can contribute directly to finding a satisfying\n\n<center>Figure 5: Time taken by Default-Kissat vs. Neuro Back-Kissat on SATCOMP-2022 (a), Random-Kissat vs. Neuro Back-Kissat on SATCOMP-2022 (b), Default-Kissat vs. Neuro Back-Kissat on SATCOMP-2023 (c), and Random-Kissat vs. Neuro Back-Kissat on SATCOMP-2023 (d) to solve each test problem in seconds (for problems that are solved by at least one solver). Each problem is represented by a dot whose location indicates the solving time of each method. The dots on the dashed lines at 5,000 seconds indicate failures. It is evident that more dots are present in the lower triangular areas, indicating that there are more problems on which Neuro Back-Kissat outperforms both Default-Kissat and Random-Kissat. </center> assignment. In contrast, for unsat problems, the utility of predicted phases is somewhat restricted to identifying partial solutions or refining the search scope. This fundamental difference in the nature of sat versus unsat problems underpins the varying degrees of effectiveness observed in Neuro Back's performance. ## 11 SETTING UP THE MEMORY LIMIT FOR NEUROBACK-KISSAT In our experimental setup, which includes a machine equipped with 256GB of memory running 64 solver instances in parallel, we have conservatively set the SAT formula size threshold at 135 MB. This ensures that the memory usage of each solver instance does not exceed our specified memory threshold of 10GB. This threshold setting is based on our practical experience. Increasing this threshold could potentially lead to memory contention issues. Users might choose to adjust the\n\nformula size threshold based on their machine's memory capacity. Alternatively, they might simply establish a memory threshold for each solver instance based on their machine's memory capacity and allow model inference to proceed until this threshold is reached, which typically incurs an overhead of no more than a few seconds.\n\n## REFERENCES Model counting competition 2020 url, 2020. https://mccompetition.org/2021/mc_description.html. Model counting competition 2021 url, 2021. https://mccompetition.org/2021/mc_description.html. Model counting competition 2022 url, 2022. https://mccompetition.org/2022/mc_description.html. Sat competition 2022. https://satcompetition.github.io/2022/, 2022. Accessed: 2023- 08- 10. Sat competition 2023. https://satcompetition.github.io/2023/, 2023. Accessed: 2023- 11- 23. Tasniem Al- Yahya, Mohamed El Bachir Abdelkrim Menai, and Hassan Mathkour. Boosting the performance of cdc1- based sat solvers by exploiting backbones and backdoors. Algorithms, 15(9): 302, 2022. Gilles Audemard and Laurent Simon. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27(01):1840001, 2018. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez- Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. ar Xiv preprint ar Xiv:1806.01261, 2018. Armin Biere and Mathias Fleury. Chasing target phases. In Workshop on the Pragmatics of SAT, 2020. Armin Biere and Mathias Fleury. Gimsatul, Isa SAT and Kissat entering the SAT Competition 2022. In Tomas Balyo, Marijn Heule, Markus Iser, Matti J\u00e4rvisalo, and Martin Suda (eds.), Proc. of SAT Competition 2022 - Solver and Benchmark Descriptions, volume B- 2022- 1 of Department of Computer Science Series of Publications B, pp. 10- 11. University of Helsinki, 2022. Armin Biere, Mathias Fleury, and Maximilian Heisinger. Cadical, kissat, paracooba entering the sat competition 2021. 2021. URL https://api.semanticscholar.org/Corpus ID: 238996423. Armin Biere, Nils Froleyks, and Wenxi Wang. Cadiback: Extracting backbones with cadical. In 26th International Conference on Theory and Applications of Satisfiability Testing (SAT 2023). Schloss Dagstuhl- Leibniz- Zentrum f\u00fcr Informatik, 2023. Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the oversmoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 3438- 3445, 2020. Martin Davis, George Logemann, and Donald Loveland. A machine program for theorem- proving. Communications of the ACM, 5(7):394- 397, 1962. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ar Xiv preprint ar Xiv:2010.11929, 2020. Niklas E\u00e9n and Niklas S\u00f6rensson. An extensible sat- solver. In International conference on theory and applications of satisfiability testing, pp. 502- 518. Springer, 2003. Matthias Fey and Jan E. Lenssen. Fast graph representation learning with Py Torch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nMatthias Fey and Jan E. Lenssen. conv.gatconv. https://pytorch- geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html, 2023a. Matthias Fey and Jan E. Lenssen. conv.ginconv. https://pytorch- geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html, 2023b. Johannes K Fichte, Markus Hecher, and Florim Hamiti. The model counting competition 2020. Journal of Experimental Algorithmics (JEA), 26:1- 26, 2021. ABKFM Fleury and Maximilian Heisinger. Cadical, kissat, paracooba, plingeling and treengeling entering the sat competition 2020. SAT COMPETITION, 2020:50, 2020. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263- 1272. PMLR, 2017. Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729- 734. IEEE, 2005. Youssef Hamadi, Said Jabbour, and Lakhdar Sais. Manysat: a parallel sat solver. Journal on Satisfiability, Boolean Modeling and Computation, 6(4):245- 262, 2010. William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025- 1035, 2017. Jesse Michael Han. Enhancing sat solvers with glue variable predictions. ar Xiv preprint ar Xiv:2007.02559, 2020. Marijn JH Heule, Matti Juhani J\u00e4rvisalo, Martin Suda, et al. Proceedings of SAT competition 2018: Solver and benchmark descriptions. 2018. Holger H Hoos and Thomas St\u00fctzle. Satlib: An online resource for research on sat. Sat, 2000: 283- 292, 2000. Mikol\u00e1\u0161 Janota. SAT solving in interactive configuration. Ph D thesis, University College Dublin, 2010. Sebastian Jaszczur, Micha\u0142 \u0141uszczyk, and Henryk Michalewski. Neural heuristics for sat solving. ar Xiv preprint ar Xiv:2005.13406, 2020. Philip Kilby, John Slaney, Sylvie Thiebaux, Toby Walsh, et al. Backbones and backdoors in satisfiability. In AAAI, volume 5, pp. 1368- 1373, 2005. Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. ar Xiv preprint ar Xiv:1609.02907, 2016. Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristics with graph networks and reinforcement learning. 2019. Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristic with graph networks and reinforcement learning. In Advances in Neural Information Processing Systems, 2020. Massimo Lauria, Jan Elffers, Jakob Nordstr\u00f6m, and Marc Vinyals. Cnfgcn: A generator of crafted benchmarks. In International Conference on Theory and Applications of Satisfiability Testing, pp. 464- 473. Springer, 2017. Jia Hui Liang, Vijay Ganesh, Pascal Poupart, and Krzysztof Czarnecki. Learning rate based branching heuristic for sat solvers. In Theory and Applications of Satisfiability Testing- SAT 2016: 19th International Conference, Bordeaux, France, July 5- 8, 2016, Proceedings 19, pp. 123- 140. Springer, 2016.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ar Xiv preprint ar Xiv:1711.05101, 2017. Joao Marques- Silva, In\u00eas Lynce, and Sharad Malik. Conflict- driven clause learning sat solvers. In Handbook of Satisfiability: Second Edition. Part 1/Part 2, pp. 133- 182. IOS Press BV, 2021. Joao P Marques- Silva and Karem A Sakallah. Grasp\u2014a new search algorithm for satisfiability. In The Best of ICCAD, pp. 73- 89. Springer, 2003. Ruben Martins, Vasco Manquinho, and In\u00eas Lynce. An overview of parallel sat solving. Constraints, 17:304- 347, 2012. Gr\u00e9goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph structure in transformers. ar Xiv preprint ar Xiv:2106.05667, 2021. Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong. Transformer for graphs: An overview from architecture perspective. ar Xiv preprint ar Xiv:2202.08455, 2022. Matthew W. Moskewicz, Conor F. Madigan, Ying Zhao, Lintao Zhang, and Sharad Malik. Chaff: Engineering an efficient sat solver. In Proceedings of the 38th Annual Design Automation Conference, DAC '01, pp. 530- 535, New York, NY, USA, 2001. Association for Computing Machinery. Piotr Padlewski and Josip Djolonga. Scaling vision transformers to 22 billion parameters. https://ai.googleblog.com/2023/03/scaling- vision- transformers- to- 22. html, 2023. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. ar Xiv preprint ar Xiv:1912.01703, 2019. Knot Pipatsrisawat and Adnan Darwiche. A lightweight component caching scheme for satisfiability solvers. In Theory and Applications of Satisfiability Testing- SAT 2007: 10th International Conference, Lisbon, Portugal, May 28- 31, 2007. Proceedings 10, pp. 294- 299. Springer, 2007. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self- supervised graph transformer on large- scale molecular data. Advances in Neural Information Processing Systems, 33:12559- 12571, 2020. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61- 80, 2008. Dominik Schreiber and Peter Sanders. Scalable sat solving in the cloud. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, Barcelona, Spain, July 5- 9, 2021, Proceedings 24, pp. 518- 534. Springer, 2021. Daniel Selsam and Nikolaj Bjorner. Guiding high- performance SAT solvers with unsat- core predictions. In International Conference on Theory and Applications of Satisfiability Testing, pp. 336- 353. Springer, 2019. Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a SAT solver from single- bit supervision. ar Xiv preprint ar Xiv:1802.03685, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ar Xiv preprint ar Xiv:1710.10903, 2017. Haoze Wu. Improving sat- solving with machine learning. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education, pp. 787- 788, 2017.\n\nZhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion Stoica. Representing long- range context for graph neural networks with global attention. Advances in Neural Information Processing Systems, 34:13266- 13279, 2021. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4- 24, 2020. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? ar Xiv preprint ar Xiv:1810.00826, 2018. Emre Yolcu and Barnabas P\u00f3czos. Learning local search heuristics for boolean satisfiability. In Advances in Neural Information Processing Systems, pp. 7992- 8003, 2019. Ziwei Zhang and Yang Zhang. Elimination mechanism of glue variables for solving sat problems in linguistics. In The Asian Conference on Language, pp. 147- 167, 2021. Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57- 81, 2020. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43- 76, 2020.\n\nWe propose Neuro Back, a novel approach to make CDCL SAT solving more effective and avoid frequent online model inferences, thus making the GNN approach more practical. The main idea of Neuro Back is to make offline model inference, i.e., prior to the solving process, to obtain instructive static information for improving CDCL SAT solving. Once trained, the offline model inference allows Neuro Back to execute solely on the CPU, thereby making it completely independent of GPU resources. In particular, Neuro Back seeks to refine the phase selection heuristics in CDCL solvers by leveraging offline neural predictions on variable phases appearing in the majority (or even all) of the satisfying assignments. The offline predictions on such phase information are based on the generalization of backbone variables, which are variables whose phases remain consistent across all satisfying assignments. Recent work Biere et al. (2021); Al- Yahya et al. (2022) has shown that backbone variables are crucial for enhancing CDCL SAT solving. Choosing the correct phase for a backbone variable prevents conflicts, while an incorrect choice inevitably leads to backtracking in the search. Moreover, predicting the correct phases of non- backbone variables appearing in the majority of satisfying assignments is also important, because such phases prevent backtracking with high probabilities. Our conjecture is that the knowledge learned from predicting the phases of backbone variables can be transferred to predicting the phases of non- backbone variables exhibited in the majority of satisfying assignments. Therefore, Neuro Back applies a GNN model, trained solely on predicting the phases of backbone variables, to predict the phases of all variables. Neuro Back converts the SAT formula with diverse scales into a compact and more learnable graph representation, turning the problem of predicting variable phases into a binary node classification problem. To make the GNN model both compact and robust, Neuro Back employs a novel Graph Transformer architecture with light- weight self- attention mechanisms. To train the model with supervised learning, a balanced dataset called Data Back containing 120,286 labeled formulas with diversity was created from five different sources: CNFgen Lauria et al. (2017), SATLIB Hoos & St\u00fctzle (2000), model counting competitions from 2020 to 2022 MCC (2020; 2021; 2022), and main and random tracks in SAT competitions from 2004 to 2021. To evaluate the effectiveness of our approach, Neuro Back is incorporated into a state- of- the- art CDCL SAT solver called Kissat Biere & Fleury (2022), resulting in a new solver called Neuro Back- Kissat. The experimental results on all SAT problems from SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) show that Neuro Back allows Kissat to solve up to \\(5.2\\%\\) and \\(7.4\\%\\) more problems, respectively. The experiments thus demonstrate that Neuro Back is a practical neural approach to improving CDCL SAT solvers. The contributions of our paper are: 1. Approach. To our knowledge, Neuro Back presents the first practical neural approach to make the CDCL SAT solving more effective, without requiring any GPU resource during its application. 2. Dataset. A new dataset Data Back containing 120,286 data samples is created for backbone phase classification. Data Back is publicly available at https://huggingface.co/datasets/neuroback/Data Back 3. Implementation. Neuro Back is incorporated into a state-of-the-art SAT solver, Kissat. The source code of Neuro Back model and Neuro Back-Kissat is publicly available at https://github.com/wenxiwang/neuroback. ## 2 BACKGROUND This section introduces the SAT problem, CDCL algorithm, phase selection heuristics in CDCL solvers, and basics of GNN and Graph Transformer. Preliminaries of SAT In SAT, a propositional logic formula \\(\\phi\\) is usually encoded in Conjunctive Normal Form (CNF), which is a conjunction \\((\\wedge)\\) of clauses. Each clause is a disjunction \\((\\vee)\\) of literals. A literal is either a variable \\(v\\) , or its complement \\(\\neg v\\) . Each variable can be assigned a logical phase, 1 (true) or 0 (false). A CNF formula has a satisfying assignment if and only if every clause has at least one true literal. For example, a CNF formula \\(\\phi = (v_{1} \\vee \\neg v_{2}) \\wedge (v_{2} \\vee v_{3}) \\wedge v_{2}\\) consists of three clauses \\(v_{1} \\vee \\neg v_{2}\\) , \\(v_{2} \\vee v_{3}\\) and \\(v_{2}\\) ; four literals \\(v_{1}\\) , \\(\\neg v_{2}\\) , \\(v_{2}\\) and \\(v_{3}\\) ; and three variables \\(v_{1}\\) , \\(v_{2}\\) , and \\(v_{3}\\) . One satisfying assignment of \\(\\phi\\) is \\(v_{1} = 1\\) , \\(v_{2} = 1\\) , \\(v_{3} = 0\\) . The goal of a SAT solver is to check if a formula \\(\\phi\\) is sat or unsat. A complete solver either outputs a satisfying assignment for \\(\\phi\\) , or proves that no such assignment exists.\n\nThe backbone of a sat formula is the set of literals that are true in all its satisfying assignments. Thus backbone variables are the variables whose phases remain consistent across all satisfying assignments. In the given CNF formula \\(\\phi\\) , there are two backbone variables, \\(v_{1}\\) and \\(v_{2}\\) , both maintaining a phase of 1 in all satisfying assignments of \\(\\phi\\) . CDCL Algorithm CDCL makes SAT solvers efficient in practice and is one of the main reasons for the widespread use of SAT applications. The general idea of CDCL algorithm is as follows (see Marques- Silva et al. (2021) for details). First, it picks a variable on which to branch and decides a phase to assign to it with heuristics. It then conducts a Boolean propagation based on the decision. In the propagation, if a conflict occurs (i.e., at least one clause is mapped to 0), it performs a conflict analysis; otherwise, it makes a new decision on another selected variable. In the conflict analysis, CDCL first analyzes the decisions and propagations to investigate the reason for the conflict, then extracts the most relevant wrong decisions, undoes them, and adds the reason to its memory as a learned lesson (encoded in a clause called learned clause) in order to avoid making the same mistake in the future. After conflict analysis, the solver backtracks to the earliest decision level where the conflict could be resolved and continues the search from there. The process continues until all variables are assigned a phase (sat), or until it learns the empty clause (unsat). Phase Selection Heuristics As indicated above, CDCL SAT solving mainly relies on two kinds of variable related heuristics: variable branching heuristics and phase selection heuristics, which are orthogonal to each other. Phase Saving Pipatsrisawat & Darwiche (2007) is a prevalent phase selection heuristic in modern CDCL solvers. It returns a variable's last assigned polarity, either through decision or propagation. This heuristic addresses the issue of solvers forgetting prior valid assignments due to non- chronological backtracking. Rephasing Biere & Fleury (2020); Fleury & Heisinger (2020) is more recent phase selection heuristic, proposed to reset or modify saved phases to diversify the exploration of the search space. The state- of- the- art CDCL solver, Kissat Biere & Fleury (2022), incorporates the latest advancements in phase saving and rephrasing heuristics. GNN GNNs Wu et al. (2020); Zhou et al. (2020) are a family of neural network architectures that operate on graphs Gori et al. (2005); Scarselli et al. (2008); Gilmer et al. (2017); Battaglia et al. (2018). Typical GNNs follow a recursive neighborhood aggregation scheme called message passing Gilmer et al. (2017). Formally, the input of a GNN is a graph defined as a tuple \\(G = (V, E, W, H)\\) , where \\(V\\) denotes the set of nodes; \\(E \\subseteq V \\times V\\) denotes the set of edges; \\(W = \\{W_{u,v} | (u, v) \\in E\\}\\) contains the feature vector \\(W_{u,v}\\) of each edge \\((u, v)\\) ; and \\(H = \\{H_v | v \\in V\\}\\) contains the feature vector \\(H_v\\) of each node \\(v\\) . A GNN maps each node to a vector- space embedding by updating the feature vector of the node iteratively based on its neighbors. For each iteration, a message- passing layer \\(\\mathcal{L}\\) takes a graph \\(G = (V, E, W, H)\\) as an input and outputs a graph \\(G' = (V, E, W, H')\\) with updated node feature vectors, i.e., \\(G' = \\mathcal{L}(G)\\) . Classic GNN models Gilmer et al. (2017); Kipf & Welling (2016); Hamilton et al. (2017) usually stack several message- passing layers to realize iterative updating. Prior work on utilizing GNNs to improve CDCL SAT solving will be reviewed in the next section. Graph Transformer Architecture Transformer Vaswani et al. (2017) is a family of neural network architectures for processing sequential data (e.g., text or image), which has recently won great success in nature language processing and computer vision. Central to the transformer is the self- attention mechanism, which calculates attention scores among elements in a sequence, thereby allowing each element to focus on other relevant elements in the sequence for capturing long- range dependencies effectively. Recent research Min et al. (2022); Wu et al. (2021); Mialon et al. (2021); Rong et al. (2020) has shown that combining the transformer with GNN results in competitive performance for graph and node classification tasks, forming the Graph Transformer architecture. Notably, Graph Trans Wu et al. (2021) is a representative model which utilizes a GNN subnet consisting of multiple GNN layers for local structure encoding, followed by a Transformer subnet with global self- attention to capture global dependencies, and finally incorporates a Feed Forward Network (FFN) for classification. The GNN model design in Neuro Back is inspired by Graph Trans. ## 3 RELATED WORK This section presents related work on identifying the backbone and machine learning techniques for improving CDCL SAT solving.\n\n<center>Figure 1: Overview of Neuro Back. First, the input CNF formula is converted into a compact and more learnable graph representation. A trained GNN model is then applied once on the graph before SAT solving begins for phase selection. The SAT solver utilizes phase information in the resulting labeled graph as an initialization to guide its solving process. Thus, with the offline process of making instructive phase predictions, Neuro Back makes the solving more effective and practical. </center> Backbone for CDCL Solvers. Janota proved that identifying the backbone is co- NP complete Janota (2010). Furthermore, Kilby et al. demonstrated that even approximating the backbone is generally intractable Kilby et al. (2005). Wu Wu (2017) applies a logistic regression model to predict the phase of backbone variables to improve a classic CDCL SAT solver called Mini Sat E\u00e9n & S\u00f6rensson (2003). Although the approach correctly predicts the phases of \\(78\\%\\) backbone variables, it fails to make improvements over Mini Sat in solving time. In addition, recent works Biere et al. (2021); Al- Yahya et al. (2022) have been focusing on enhancing CDCL SAT solving by employing heuristic search to partially compute the backbone during the solving process. In contrast, Neuro Back applies GNN to predict the backbone in an offline manner to obtain the phase information of all variables to improve CDCL solving. Machine Learning for CDCL Solvers. Recently, several approaches have been developed to utilize GNNs to facilitate CDCL SAT solving. Neuro SAT Selsam et al. (2018) was the first such framework adapting a neural model into an end- to- end SAT solver, which was not intended as a complete SAT solver. Others Jaszczur et al. (2020); Davis et al. (1962); Kurin et al. (2019); Han (2020); Audemard & Simon (2018); Zhang & Zhang (2021) aim to provide SAT solvers with better branching or phase selection heuristics. These approaches either reduce the number of solving iterations or enhance the solving effectiveness on selected small- scale problems with up to a few thousand variables. However, they do not provide obvious improvements in solving effectiveness for large- scale problems. In contrast, Neuro Core Selsam & Bj\u00f8rner (2019), the most closely related approach to this paper, aims to make the solving more effective especially for large- scale problems as in SAT competitions. It enhances the branching heuristic for CDCL using supervised learning to map unsat problems to unsat core variables (i.e., the variables involved in the unsat core). Based on the dynamically learned clauses during the solving process, Neuro Core performs frequent online model inferences to tune the predictions. However, this online inference is computationally demanding. Neuro Back is distinct from Neuro Core in two main aspects. One, while Neuro Core is designed to refine the branching heuristic in CDCL SAT solvers, Neuro Back is invented to enhance their phase selection heuristics. Two, while Neuro Core extracts dynamic unsat core information from unsat formulas through online model inferences, Neuro Back captures static backbone information from sat formulas using offline model inference. Details of Neuro Back are introduced in the following section. ## 4 NEUROBACK In order to reduce the computational cost of the online model inference and to make CDCL SAT solving more effective, Neuro Back employs offline model predictions on variable phases to enhance the phase selection heuristics in CDCL solvers. Figure 1 shows the overview of Neuro Back. First, it converts the input CNF formula into a compact and more learnable graph representation. Then, a well- designed GNN model trained to predict the phases of backbone variables, is applied on the converted graph representation to infer the phases of all variables. The model inference is performed only once before the SAT solving process. The resulting offline prediction is applied as an initialization for the SAT solving process. Finally, the enhanced SAT solver outputs the satisfiability of the input CNF formula. The key components of Neuro Back including the graph representation of CNF formulas, the GNN- based phase selection, and the phase prediction application in SAT solvers, are illustrated in the subsections below. ### 4.1 GRAPH REPRESENTATION FOR CNF FORMULAS As in recent work Kurin et al. (2020); Yolcu & P\u00f3czos (2019), a SAT formula is represented using a more compact undirected bipartite graph than the one adopted in Neuro Core. Two node types\n\n<center>Figure 3: The architecture of Neuro Back model, consisting of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. </center> represent the variables and clauses, respectively. Each edge connects a variable node to a clause node, representing that the clause contains the variable. Two edge types represent two polarities of a variable appearing in the clause, i.e., the variable itself and its complement. Although the representation is compact, its diameter might be substantial for large- scale SAT formulas, which could result in insufficient message passing during the learning process. To mitigate this issue, we introduce a meta node for each connected component in the graph, with meta edges connecting the meta node to all clause nodes in the component. With the added meta nodes and edges, every variable node not appearing in the same clauses can reach each other through their corresponding clause nodes and the meta node, thereby making the diameter at most four. Figure 2 shows an example of our graph representation for the CNF formula \\((v_{1} \\lor v_{2}) \\land (v_{2} \\lor v_{3}) \\land (v_{3} \\lor v_{4})\\) . It includes one meta node, four variable nodes and three clause nodes \\(c_{1}, c_{2}, c_{3}\\) , representing clauses \\(v_{1} \\lor v_{2}, v_{2} \\lor v_{3}\\) and \\(v_{3} \\lor v_{4}\\) , respectively. Without the added meta node and edges, the longest path in the graph runs from variable node \\(v_{1}\\) to variable node \\(v_{4}\\) , making the diameter six. However, by introducing the meta node and edges, the diameter is reduced to four. Formally, for a graph representation \\(G = (V, E, W, H)\\) of a CNF formula, the edge feature \\(W_{u, v}\\) of each edge \\((u, v)\\) is initialized by its edge type with the value 0 representing the meta edge, the value 1 representing positive polarity, and \\(- 1\\) negative polarity; the node feature \\(H_{v}\\) of each node \\(v\\) is initialized by its node type with 0 representing a meta node, 1 representing a variable node, and \\(- 1\\) representing a clause node. <center>Figure 2: An example graph representation of the CNF formula \\((v_{1} \\lor v_{2}) \\land (v_{2} \\lor v_{3}) \\land (v_{3} \\lor v_{4})\\) . A meta node \\(m\\) is added along with meta edges (represented by dashed lines) connecting to all clause nodes in the connected component, reducing the graph diameter from six to four. </center> ### 4.2 GNN-based PHASE PREDICTION Given that the phases of backbone variables remain consistent across all satisfying assignments, selecting the correct phase for a backbone variable prevents backtracking. Conversely, an incorrect choice inevitably leads to a conflict. Moreover, the proportion of backbone variables is typically significant. For instance, during our data collection from five notable sources (i.e., CNFgen, SATLIB, model counting competitions, SATCOMP random tracks, and SATCOMP main tracks), backbone variables constitute an average of \\(27\\%\\) of the total variables. Therefore, accurately identifying the phases of the backbone variables is crucial for efficiently solving a SAT formula. Furthermore, identifying the phases of the non- backbone variables appearing in the majority of satisfying assignments is also important. Because such phases could prevent backtracking with high probabilities. With the converted graph representation, predicting the phases of all variables, including backbone and non- backbone variables, is a binary node classification problem, which can be addressed by a GNN model. Inspired by transfer learning Zhuang et al. (2020), we first train a GNN model to predict the phases of backbone variables, and then leverage the trained model to predict the phases of all variables. Our key insight is that the knowledge learned from predicting the phases of backbone variables can provide a valuable guidance on predicting the phases of non- backbone variables exhibiting in the majority of satisfying assignments. The following subsections introduce the design, implementation, and training of our GNN model.\n\n#### 4.2.1 GNN MODEL DESIGN The sizes of converted graphs representing practical SAT formulas (usually with millions of variables and clauses) are typically substantial. To enable effective training within the constraints of limited GPU memory, it is essential for our model to be both compact and robust. Our GNN model design is inspired by the robust graph transformer architecture, Graph Trans Wu et al. (2021). However, in our particular SAT application context, Graph Trans exhibits two limitations, both arising from the global self- attention mechanism within its transformer subnet. First, the mechanism does not explicitly integrate the topological graph structure information when determining attention scores. However, such information is essential in characterizing a SAT formula. Second, the global self- attention mechanism computes attention scores for all possible node pairs, leading to quadratic memory complexity with respect to the number of nodes in the graph. This is obviously infeasible for tackling the large- scale SAT formulas in our task. To overcome the limitations, we introduce a novel transformer subnet that both distinctly harnesses topological structure information and significantly enhances memory efficiency. It combines Graph Self- Attention (GSA) and Local Self- Attention (LSA), replacing the original transformer's global self- attention. Instead of computing attention scores for all node pairs as global self- attention, GSA calculates attention scores solely for directly connected node pairs, leveraging information of edges and edge weights. This not only explicitly incorporates the topological structure information of the graph, but also reduces the memory complexity to linear in terms of the number of edges in the graph. Additionally, to further reduce the memory complexity, LSA segments each node embedding into multiple node patches and computes attention scores for each pair of node patches. The results in a linear memory complexity in terms of the number of nodes in the graph. Figure 3 illustrates the design of our GNN model architecture. It consists of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. Within each GNN block, a GNN layer is preceded by a normalization layer, with a skip connection bridging the two. The transformer block is crafted to accelerate training on a significant collection of large- scale graphs. Inspired by the recent vision transformer architecture, Vi T- 22B Padlewski & Djolonga (2023), each transformer block integrates a normalization layer, succeeded by both an FFN layer and a GSA/LSA layer that operate concurrently to optimize training efficiency. #### 4.2.2 IMPLEMENTATION The current implementation of Neuro Back model utilizes GINConv Fey & Lenssen (2023b); Xu et al. (2018) to build the GNN layer, for its proficiency in distinguishing non- isomorphic graph structures. However, GINConv lacks the capability to encode edge weight information. To address this, we employ three GINConv layers, each corresponding to a distinct edge weight in our graph representation. Each GINConv layer exclusively performs message passing for edges with its corresponding weight. The node embeddings from these three GINConv layers are finally aggregated as the output of the GNN layer. GATConv layers Fey & Lenssen (2023a); Velickovic et al. (2017) is utilized to built the GSA transformer block. The patch encoder in the Vi T transformer Dosovitskiy et al. (2020) is applied to construct the LSA transformer block. Layer Norm Ba et al. (2016) is employed as our normalization layer. To avoid potential over- smoothing issues, as instructed in Chen et al. (2020), the number of blocks in the GNN subnet is set to the maximum diameter of the graph representation (i.e., \\(L = 4\\) ). To ensure the accuracy of our model, while taking into account our limited GPU memory, the number of both GSA and LSA blocks are set to three (i.e., \\(M = 3\\) and \\(N = 3\\) ). Additionally, FFNs within the transformer blocks contain no hidden layers, while the final FFN utilized for node classification is structured to include one hidden layer. The model is implemented using Py Torch Paszke et al. (2019) and Py Torch Geometric Fey & Lenssen (2019). #### 4.2.3 MODEL PRE-TRAINING AND FINE-TUNING The Neuro Back model undergoes a two- stage training process. Initially, it is pre- trained on an extensive and diverse dataset gathered from various sources. This pre- training equips it with the fundamental knowledge to classify the backbone variable phases across a broad spectrum of CNF formulas. Subsequently, this pre- trained model is refined or fine- tuned on a smaller, domain- specific dataset. This fine- tuning process enhances the model's proficiency in classifying backbone variable\n\n<table><tr><td>Data Back-PT</td><td>CNFgen</td><td>SATLIB</td><td>MCCOMP</td><td>SATCOMP (random)</td><td>Overall</td><td>Data Back-FT</td><td>SATCOMP (main)</td></tr><tr><td># CNF</td><td>21,718</td><td>80,306</td><td>11,560</td><td>4,876</td><td>118,460</td><td># CNF</td><td>1,826</td></tr><tr><td># Var</td><td>779</td><td>114</td><td>16,299</td><td>25,310</td><td>2,852</td><td># Var</td><td>206,470</td></tr><tr><td># Cla</td><td>281,888</td><td>529</td><td>63,501</td><td>88,978</td><td>61,898</td><td># Cla</td><td>1,218,519</td></tr><tr><td># Backbone Var</td><td>280 (36%)</td><td>58 (51%)</td><td>8,587 (53%)</td><td>1,960 (8%)</td><td>1,009 (35%)</td><td># Backbone Var</td><td>48,266 (23%)</td></tr></table> Table 1: Details of Data Back. For each source, we summarize the number of CNF formulas; the average number of variables and clauses characterizing the size of each CNF formula; and the average number of backbone variables, with their proportion in total variables (indicated in brackets). Overall, for each dataset, we summarize the total number of CNF formulas and the corresponding average counts regarding all formulas in the set. phases within a specific category of CNF formulas. Details about the pre- training and fine- tuning datasets are introduced in Section 5. Binary Cross Entropy (BCE) loss is adopted as our loss function. Besides, Adam W optimizer Loshchilov & Hutter (2017) with a learning rate of \\(10^{- 4}\\) is applied for model pre- training and fine- tuning. The number of epoch is set to 40 for pre- training, and 60 for fine- tuning. It took 48 hours in total to accomplish both the pre- training and fine- tuning on our commodity computer. ### 4.3 APPLYING PHASE PREDICTIONS IN SAT The goal is to leverage phase predictions derived from the GNN model to enhance the phase selection heuristics within CDCL SAT solvers. While numerous ways exist to integrate these neural predictions into CDCL SAT solvers, the most straightforward and generic approach is to initialize the phases of the corresponding variables in CDCL SAT solvers based on the predicted phases. In this paper, we adopted the state- of- the- art solver Kissat Biere & Fleury (2022) to support the phase initialization with Neuro Back predictions. The resulting implementation is called Neuro Back- Kissat. ## 5 DATABACK We created a new dataset, Data Back, comprising sat CNF formulas labeled with phases of backbone variables, for pre- training (PT) and fine- tuning (FT) the Neuro Back model. Accordingly, there are two subsets in Data Back: the pre- training set, Data Back- PT, and the fine- tuning set, Data Back- FT. To get the backbone label, the very recent state- of- the- art backbone extractor called Cadi Back Biere et al. (2023) is utilized. Given that the fine- tuning formulas are typically more challenging to solve than the pre- training ones, label collection timeouts are set as 1,000 seconds for the pre- training formulas and 5,000 seconds for the fine- tuning formulas. Data Back includes formulas solved within the time limit with at least one backbone variable. We observe that there exists a significant label imbalance in both Data Back- PT and Data Back- FT. To tackle this, for each labeled formula, we create a dual formula by negating all backbone variables in the original formula. The labels of the dual formula are the negated phases of the backbone variables in the original formula. Formally, for a formula \\(f\\) with \\(n\\) backbone variables \\(b_{1},\\ldots ,b_{n}\\) , let \\(\\mathcal{L}_{f}:\\{b_{1},\\ldots ,b_{n}\\} \\to \\{1,0\\}\\) denote the mapping of each backbone variable to its phase. The dual formula \\(f^{\\prime}\\) is obtained from \\(f\\) by negating each backbone variable: \\(f^{\\prime} = f[b_{1}\\to\\) \\(\\neg b_{1},\\ldots ,b_{n}\\mapsto \\neg b_{n}]\\) . The dual \\(f^{\\prime}\\) is still satisfiable and retains the same backbone variables as \\(f\\) , but with the opposite phases \\(\\mathcal{L}_{f^{\\prime}}(b_{i}) = \\neg \\mathcal{L}_{f}(b_{i}),i\\in \\{1,\\ldots ,n\\}\\) . For the given CNF formula example in Section 2 \\(\\phi = (v_{1}\\vee \\neg v_{2})\\wedge (v_{2}\\vee v_{3})\\wedge v_{2}\\) , having \\(v_{1}\\) and \\(v_{2}\\) as its backbone variables with phases \\(\\{v_{1},v_{2}\\} \\to \\{1\\}\\) , the dual formula is \\(\\phi^{\\prime} = (\\neg v_{1}\\vee v_{2})\\wedge (\\neg v_{2}\\vee v_{3})\\wedge \\neg v_{2}\\) , still having \\(v_{1}\\) and \\(v_{2}\\) as the backbone variables but with opposite phases \\(\\{v_{1},v_{2}\\} \\to \\{0\\}\\) . This data augmentation strategy doubles the size of Data Back with a perfect balance in positive and negative backbone labels. In the rest of the paper, Data Back- PT and Data Back- FT refer to the augmented, balanced datasets. Data Back- PT The CNF formulas in Data Back- PT are sourced from four origins: 1) CNFgen Lauria et al. (2017), a recent CNF generator renowned for crafting CNF formulas that feature in proof complexity literature; 2) SATLIB Hoos & St\u00fctzle (2000), an online benchmark library housing well- known benchmark instances, serving as a foundational challenge for the SAT community; 3) model counting competitions from 2020 to 2022 Fichte et al. (2021); MCC (2021; 2022), including wide range of CNF formulas for model counters to count their satisfying assignments, which provide a good source for backbone extraction; 4) Random tracks in SAT competitions from 2004 to 2021, offering numerous challenging random SAT problems.\n\n<table><tr><td>metrics model</td><td>precision</td><td>recall</td><td>F1</td><td>accuracy</td></tr><tr><td>pre-trained model</td><td>0.903</td><td>0.766</td><td>0.829</td><td>0.751</td></tr><tr><td>fine-tuned model</td><td>0.941</td><td>0.914</td><td>0.928</td><td>0.887</td></tr></table> Table 2: The performance on the validation set of both pre- trained and fine- tuned Neuro Back models for classifying the phases of backbone variables, in terms of precision, recall, F1 score and accuracy. Table 1 shows the detailed statistics of the resulting dataset. CNFgen generated 10,859 labeled sat formulas (21,718 augmented formulas) grouped by five categories: random SAT, clique- coloring, graph coloring, counting principle, and parity principle. From SATLIB, 40,153 sat formulas with labels (80,306 augmented formulas) were collected, encoding seven kinds of problems: random SAT, graph coloring, planning, bounded model checking, Latin square, circuit fault analysis, and all- interval series. Model counting competitions and SAT competition random tracks contribute 5,780 (11,578 augmented) and 2,438 (4,876 augmented) labeled formulas, respectively. In total, 59,230 (118,460 augmented) formulas are included Data Back- PT. The comparative size of formulas, as indicated by the average number of variables and clauses, reveals that formulas from model counting competitions and SAT competition random tracks are generally larger than those generated by CNFgen, which are in turn larger than those originating from SATLIB. In addition, formulas from model counting competitions and SATLIB exhibit relatively high average backbone proportions. These are followed by formulas from CNFgen, while formulas from the SAT competition's random tracks demonstrate the lowest backbone proportion. Data Back- PT thus contains a diverse set of formulas. Data Back- FT Given that Neuro Back will be tested on large- scale SAT formulas from the main track of recent SAT competitions, the fine- tuning dataset, Data Back- FT, incorporates CNF formulas from the main track of earlier SAT competitions spanning from 2004 to 2021. As shown in Table 1, it contains 913 (1,826 augmented) labeled formulas. While Data Back- FT is considerably smaller in size compared to Data Back- PT, its individual formulas are distinctly larger than those in Data Back- PT. ## 6 EXPERIMENTS Platform All experiments were run on an ordinary commodity computer with one NVIDIA Ge Force RTX 3080 GPU (10GB memory), one AMD Ryzen Threadripper 3970X processor (64 logical cores), and 256GB RAM. Research Questions The experiments aim to answer two research questions: RQ1: How accurately does the Neuro Back model classify the phases of backbone variables? RQ2: How effective is the Neuro Back approach? RQ1: Neuro Back Model Performance The Neuro Back model was pre- trained on the entire Data Back- PT dataset, then fine- tuned on a random \\(90\\%\\) of Data Back- FT samples, and evaluated on the remaining \\(10\\%\\) as a validation set. Table 2 details the performance of both the pre- trained and fine- tuned models in classifying the phases of backbone variables. Notably, the pre- trained model achieved \\(75.1\\%\\) accuracy in classifying backbone variables, with a precision exceeding \\(90\\%\\) and a recall rate of \\(76.7\\%\\) . Considering the distinct data sources of Data Back- PT and Data Back- FT, the results suggest that the pre- training enables the model to extract generalized knowledge about backbone phase prediction. Fine- tuning further augments model performance, with improvements ranging between \\(4\\%\\) and \\(15\\%\\) across all metrics, making precision, recall and F1 score all exceeding \\(90\\%\\) . In conclusion, Neuro Back model effectively learns to predict the phases of backbone variables through both pre- training and fine- tuning. RQ2: Neuro Back Performance To evaluate the solving effectiveness, we collect all 800 CNF formulas from the main track of SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) as our testing dataset. For our baseline solvers, we selected the default configuration of Kissat, named Default- Kissat, which simply sets the initial phase of each variable to true. We implemented an additional baseline solver, Random- Kissat, which randomly assigns the initial phase of each variable as either true or false. Neuro Back- Kissat and its baseline solvers, were applied to all 800 SAT problems in the testing dataset, with the standard solving time limit of 5,000 seconds. Each solver utilized up to 64 different processes in parallel on the dedicated 64- core machine. The model inference for each Neuro Back solver was conducted solely on the CPU, with a memory limit of 10GB to mitigate memory contention issues. Consequently, 308 problems from SATCOMP- 2022 and\n\n<center>Figure 4: Progress of Default-Kissat, Random-Kissat, and Neuro Back-Kissat over time in solving problems (time in seconds) on SATCOMP-2022 (left) and SATCOMP-2023 (right), respectively. Neuro Back-Kissat outperforms the two baseline solvers on both testing sets. </center> 353 problems from SATCOMP- 2023 were successfully inferred. The CPU inference time for each of these problems ranged from 0.3 to 16.5 seconds, averaging at 1.7 seconds. Given that problems unsuccessfully inferred make the Neuro Back solver revert to its baseline, we exclude such problems from our evaluation of how Neuro Back contributes to SAT solving. As a result, for 308 inferred problems in SATCOMP- 2022, Default- Kissat and Random- Kissat solved 193 and 197 problems, respectively. In comparison, Neuro Back- Kissat solved 203 problems, representing an improvement of \\(5.2\\%\\) over Default- Kissat and \\(3.0\\%\\) over Random- Kissat, respectively. For 353 inferred problems in SATCOMP- 2023, Default- Kissat and Random- Kissat successfully solved 198 and 190 problems, respectively. In contrast, Neuro Back- Kissat solved 204 problems, making improvements of \\(3.0\\%\\) and \\(7.4\\%\\) over Default- Kissat and Random- Kissat, respectively. The cactus plot is a commonly used plot in the SAT community for demonstrating the solving, as shown in Fig. 4. Random- Kissat performs better than Default- Kissat in SATCOMP- 2022, but worse in SATCOMP- 2023. In contrast, Neuro Back- Kissat consistently outperforms both baseline solvers. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, reducing solving time by 98 and 246 seconds per problem. For additional results, please refer to the Appendix section. Overall, results suggest that the phase initialization provided by Neuro Back outperforms both the default and random phase initializations in Kissat, exhibiting enhanced proficiency in solving SATCOMP- 2022 and SATCOMP- 2023 problems. ## 7 DISCUSSION AND FUTURE WORK The current implementation of Neuro Back has two main aspects to improve. First, pre- training the GNN model requires a large amount of data. Once trained, however, the model is able to easily generalize to new problem categories, so the investment in putting together a large dataset is warranted. The dataset will also be publicly available to benefit future research. Second, this paper employs neural phase predictions solely as an initial setting for variable phases in SAT solvers. However, multiple methods exist to utilize these predictions in SAT, either in a static or dynamic manner, which are likely to yield further performance enhancements. For instance, phase predictions could be leveraged dynamically during rephasing to adjust or diversify the exploration of the search space. ## 8 CONCLUSION This paper proposes a machine learning approach, Neuro Back, to make CDCL SAT solvers more effective without requiring any GPU resource during its application. The main idea is to make offline model inference on variable phases appearing in the majority of satisfying assignments, for enhancing the phase selection heuristic in CDCL solvers. Incorporated in the state- of- the- art SAT solver, Kissat, this approach significantly reduces the solving time and makes it possible to solve more instances in SATCOMP- 2022 and SATCOMP- 2023. Neuro Back is thus a promising approach to improving the SAT solvers through machine learning.",
    "introduction": "",
    "9_acknowledgment_9_acknowledgmentwe_would_like_to_thank_the_anonymous_reviewers_for_their_valuable_feedback_this_work_was_supported_by_a_grant_from_the_army_research_office_accomplished_under_cooperative_agreement_number_w911nf-_19-_2-_0333_the_views_and_conclusions_contained_in_this_document_are_those_of_the_authors_and_should_not_be_interpreted_as_representing_the_official_policies_either_expressed_or_implied_of_the_army_research_office_or_the_us_government_the_us_government_is_authorized_to_reproduce_and_distribute_reprints_for_government_purposes_notwithstanding_any_copyright_notation_herein_this_work_was_also_supported_in_part_by_ace_one_of_the_seven_centers_in_jump_20_a_semiconductor_research_corporation_src_program_sponsored_by_darpa_and_by_the_intel_rare_center_10_appendix_101_additional_experimental_results_101_additional_experimental_resultsthe_scatter_plots_is_another_commonly_used_plot_in_the_sat_community_for_comparing_the_solving_effectiveness_of_two_solvers_on_each_problem_fig_5_shows_the_scatter_plots_of_neuro_back-_kissat_and_its_two_baseline_solvers_default-_kissat_and_random-_kissat_it_is_evident_that_more_dots_are_present_in_the_lower_triangular_area_indicating_that_there_are_more_problems_on_which_neuro_back-_kissat_outperforms_both_default-_kissat_and_random-_kissat_specifically_neuro_back-_kissat_outperforms_default-_kissat_on_43_and_40_additional_problems_in_satcomp-_2022_and_satcomp-_2023_respectively_reducing_solving_time_by_117_and_36_seconds_per_problem_similarly_neuro_back-_kissat_outperforms_random-_kissat_in_satcomp-_2022_and_satcomp-_2023_on_22_and_29_more_problems_respectively_leading_to_a_reduction_in_solving_time_of_98_and_246_seconds_per_problem_102_performance_on_solved_sat_and_unsat_problems_upon_detailed_analysis_for_661_problems_from_both_satcomp-_2022_and_satcomp-_2023_testing_sets_there_are_194_unsat_problems_and_216_sat_problems_that_are_solved_by_either_default-_kissat_or_neuro_back-_kissat_for_the_194_solved_unsat_problems_neuro_back-_kissat_outperformed_default-_kissat_in_121_cases_624_while_default-_kissat_outperformed_neuro_back-_kissat_in_only_61_problems_314_for_the_216_solved_sat_problems_neuro_back-_kissat_outperformed_default-_kissat_in_110_problems_509_while_default-_kissat_outperformed_neuro_back-_kissat_in_87_problems_403_while_neuro_back-_kissat_showed_a_higher_improvement_rate_in_unsat_problems_compared_to_sat_ones_624_vs_509_the_extent_of_improvement_was_more_significant_in_sat_problems_on_average_neuro_back-_kissat_enhanced_the_performance_of_sat_problems_by_532_compared_to_an_average_improvement_of_only_146_in_unsat_problems_these_trends_were_similarly_observed_when_comparing_neuro_back-_kissat_with_random-_kissat_the_experimental_results_highlight_two_key_aspects_first_they_demonstrate_that_neuro_backs_predicted_variable_phases_can_enhance_the_efficiency_in_solving_unsat_problems_our_explanation_is_that_neuro_backs_phase_predictions_can_aid_in_directing_the_search_towards_the_unsatisfiable_part_of_the_search_space_while_neuro_back_cannot_satisfy_all_components_of_a_given_sat_problem_it_may_predict_phases_that_satisfy_certain_components_thereby_allowing_the_solver_to_concentrate_on_the_unsat_part_furthermore_in_modern_sat_solvers_such_as_default-_kissat_biere_fleury_2020_an_assignment_that_falsifies_the_fewer_clauses_is_often_preferred_in_the_searching_loop_allowing_the_solver_to_specifically_target_the_unsat_portions_of_the_clause_set_consequently_the_phases_predicted_by_neuro_back_can_facilitate_identifying_an_assignment_that_reduces_clause_falsification_thereby_enhancing_solving_unsat_problems_second_the_experimental_results_also_show_that_neuro_back_achieves_a_more_pronounced_improvement_in_solving_sat_problems_than_in_solving_unsat_problems_this_distinction_stems_from_the_inherent_nature_of_these_problems_in_sat_problems_a_complete_satisfying_assignment_exists_where_each_variable_is_assigned_a_phase_that_leads_to_a_solution_conversely_in_unsat_problems_only_partial_satisfying_assignments_exist_with_phases_assigned_to_just_a_subset_of_variables_consequently_the_phases_predicted_by_neuro_back_have_a_generally_greater_impact_in_resolving_sat_problems_this_is_because_for_these_problems_the_predicted_phases_can_contribute_directly_to_finding_a_satisfying": "<center>Figure 5: Time taken by Default-Kissat vs. Neuro Back-Kissat on SATCOMP-2022 (a), Random-Kissat vs. Neuro Back-Kissat on SATCOMP-2022 (b), Default-Kissat vs. Neuro Back-Kissat on SATCOMP-2023 (c), and Random-Kissat vs. Neuro Back-Kissat on SATCOMP-2023 (d) to solve each test problem in seconds (for problems that are solved by at least one solver). Each problem is represented by a dot whose location indicates the solving time of each method. The dots on the dashed lines at 5,000 seconds indicate failures. It is evident that more dots are present in the lower triangular areas, indicating that there are more problems on which Neuro Back-Kissat outperforms both Default-Kissat and Random-Kissat. </center> assignment. In contrast, for unsat problems, the utility of predicted phases is somewhat restricted to identifying partial solutions or refining the search scope. This fundamental difference in the nature of sat versus unsat problems underpins the varying degrees of effectiveness observed in Neuro Back's performance. ## 11 SETTING UP THE MEMORY LIMIT FOR NEUROBACK-KISSAT In our experimental setup, which includes a machine equipped with 256GB of memory running 64 solver instances in parallel, we have conservatively set the SAT formula size threshold at 135 MB. This ensures that the memory usage of each solver instance does not exceed our specified memory threshold of 10GB. This threshold setting is based on our practical experience. Increasing this threshold could potentially lead to memory contention issues. Users might choose to adjust the\n\nformula size threshold based on their machine's memory capacity. Alternatively, they might simply establish a memory threshold for each solver instance based on their machine's memory capacity and allow model inference to proceed until this threshold is reached, which typically incurs an overhead of no more than a few seconds.",
    "references_model_counting_competition_2020_url_2020_httpsmccompetitionorg2021mc_descriptionhtml_model_counting_competition_2021_url_2021_httpsmccompetitionorg2021mc_descriptionhtml_model_counting_competition_2022_url_2022_httpsmccompetitionorg2022mc_descriptionhtml_sat_competition_2022_httpssatcompetitiongithubio2022_2022_accessed_2023-_08-_10_sat_competition_2023_httpssatcompetitiongithubio2023_2023_accessed_2023-_11-_23_tasniem_al-_yahya_mohamed_el_bachir_abdelkrim_menai_and_hassan_mathkour_boosting_the_performance_of_cdc1-_based_sat_solvers_by_exploiting_backbones_and_backdoors_algorithms_159_302_2022_gilles_audemard_and_laurent_simon_on_the_glucose_sat_solver_international_journal_on_artificial_intelligence_tools_27011840001_2018_jimmy_lei_ba_jamie_ryan_kiros_and_geoffrey_e_hinton_layer_normalization_ar_xiv_preprint_ar_xiv160706450_2016_peter_w_battaglia_jessica_b_hamrick_victor_bapst_alvaro_sanchez-_gonzalez_vinicius_zambaldi_mateusz_malinowski_andrea_tacchetti_david_raposo_adam_santoro_ryan_faulkner_et_al_relational_inductive_biases_deep_learning_and_graph_networks_ar_xiv_preprint_ar_xiv180601261_2018_armin_biere_and_mathias_fleury_chasing_target_phases_in_workshop_on_the_pragmatics_of_sat_2020_armin_biere_and_mathias_fleury_gimsatul_isa_sat_and_kissat_entering_the_sat_competition_2022_in_tomas_balyo_marijn_heule_markus_iser_matti_j\u00e4rvisalo_and_martin_suda_eds_proc_of_sat_competition_2022_-_solver_and_benchmark_descriptions_volume_b-_2022-_1_of_department_of_computer_science_series_of_publications_b_pp_10-_11_university_of_helsinki_2022_armin_biere_mathias_fleury_and_maximilian_heisinger_cadical_kissat_paracooba_entering_the_sat_competition_2021_2021_url_httpsapisemanticscholarorgcorpus_id_238996423_armin_biere_nils_froleyks_and_wenxi_wang_cadiback_extracting_backbones_with_cadical_in_26th_international_conference_on_theory_and_applications_of_satisfiability_testing_sat_2023_schloss_dagstuhl-_leibniz-_zentrum_f\u00fcr_informatik_2023_deli_chen_yankai_lin_wei_li_peng_li_jie_zhou_and_xu_sun_measuring_and_relieving_the_oversmoothing_problem_for_graph_neural_networks_from_the_topological_view_in_proceedings_of_the_aaai_conference_on_artificial_intelligence_volume_34_pp_3438-_3445_2020_martin_davis_george_logemann_and_donald_loveland_a_machine_program_for_theorem-_proving_communications_of_the_acm_57394-_397_1962_alexey_dosovitskiy_lucas_beyer_alexander_kolesnikov_dirk_weissenborn_xiaohua_zhai_thomas_unterthiner_mostafa_dehghani_matthias_minderer_georg_heigold_sylvain_gelly_et_al_an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale_ar_xiv_preprint_ar_xiv201011929_2020_niklas_e\u00e9n_and_niklas_s\u00f6rensson_an_extensible_sat-_solver_in_international_conference_on_theory_and_applications_of_satisfiability_testing_pp_502-_518_springer_2003_matthias_fey_and_jan_e_lenssen_fast_graph_representation_learning_with_py_torch_geometric_in_iclr_workshop_on_representation_learning_on_graphs_and_manifolds_2019": "Matthias Fey and Jan E. Lenssen. conv.gatconv. https://pytorch- geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html, 2023a. Matthias Fey and Jan E. Lenssen. conv.ginconv. https://pytorch- geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html, 2023b. Johannes K Fichte, Markus Hecher, and Florim Hamiti. The model counting competition 2020. Journal of Experimental Algorithmics (JEA), 26:1- 26, 2021. ABKFM Fleury and Maximilian Heisinger. Cadical, kissat, paracooba, plingeling and treengeling entering the sat competition 2020. SAT COMPETITION, 2020:50, 2020. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263- 1272. PMLR, 2017. Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729- 734. IEEE, 2005. Youssef Hamadi, Said Jabbour, and Lakhdar Sais. Manysat: a parallel sat solver. Journal on Satisfiability, Boolean Modeling and Computation, 6(4):245- 262, 2010. William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025- 1035, 2017. Jesse Michael Han. Enhancing sat solvers with glue variable predictions. ar Xiv preprint ar Xiv:2007.02559, 2020. Marijn JH Heule, Matti Juhani J\u00e4rvisalo, Martin Suda, et al. Proceedings of SAT competition 2018: Solver and benchmark descriptions. 2018. Holger H Hoos and Thomas St\u00fctzle. Satlib: An online resource for research on sat. Sat, 2000: 283- 292, 2000. Mikol\u00e1\u0161 Janota. SAT solving in interactive configuration. Ph D thesis, University College Dublin, 2010. Sebastian Jaszczur, Micha\u0142 \u0141uszczyk, and Henryk Michalewski. Neural heuristics for sat solving. ar Xiv preprint ar Xiv:2005.13406, 2020. Philip Kilby, John Slaney, Sylvie Thiebaux, Toby Walsh, et al. Backbones and backdoors in satisfiability. In AAAI, volume 5, pp. 1368- 1373, 2005. Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. ar Xiv preprint ar Xiv:1609.02907, 2016. Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristics with graph networks and reinforcement learning. 2019. Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristic with graph networks and reinforcement learning. In Advances in Neural Information Processing Systems, 2020. Massimo Lauria, Jan Elffers, Jakob Nordstr\u00f6m, and Marc Vinyals. Cnfgcn: A generator of crafted benchmarks. In International Conference on Theory and Applications of Satisfiability Testing, pp. 464- 473. Springer, 2017. Jia Hui Liang, Vijay Ganesh, Pascal Poupart, and Krzysztof Czarnecki. Learning rate based branching heuristic for sat solvers. In Theory and Applications of Satisfiability Testing- SAT 2016: 19th International Conference, Bordeaux, France, July 5- 8, 2016, Proceedings 19, pp. 123- 140. Springer, 2016.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ar Xiv preprint ar Xiv:1711.05101, 2017. Joao Marques- Silva, In\u00eas Lynce, and Sharad Malik. Conflict- driven clause learning sat solvers. In Handbook of Satisfiability: Second Edition. Part 1/Part 2, pp. 133- 182. IOS Press BV, 2021. Joao P Marques- Silva and Karem A Sakallah. Grasp\u2014a new search algorithm for satisfiability. In The Best of ICCAD, pp. 73- 89. Springer, 2003. Ruben Martins, Vasco Manquinho, and In\u00eas Lynce. An overview of parallel sat solving. Constraints, 17:304- 347, 2012. Gr\u00e9goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph structure in transformers. ar Xiv preprint ar Xiv:2106.05667, 2021. Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong. Transformer for graphs: An overview from architecture perspective. ar Xiv preprint ar Xiv:2202.08455, 2022. Matthew W. Moskewicz, Conor F. Madigan, Ying Zhao, Lintao Zhang, and Sharad Malik. Chaff: Engineering an efficient sat solver. In Proceedings of the 38th Annual Design Automation Conference, DAC '01, pp. 530- 535, New York, NY, USA, 2001. Association for Computing Machinery. Piotr Padlewski and Josip Djolonga. Scaling vision transformers to 22 billion parameters. https://ai.googleblog.com/2023/03/scaling- vision- transformers- to- 22. html, 2023. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. ar Xiv preprint ar Xiv:1912.01703, 2019. Knot Pipatsrisawat and Adnan Darwiche. A lightweight component caching scheme for satisfiability solvers. In Theory and Applications of Satisfiability Testing- SAT 2007: 10th International Conference, Lisbon, Portugal, May 28- 31, 2007. Proceedings 10, pp. 294- 299. Springer, 2007. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self- supervised graph transformer on large- scale molecular data. Advances in Neural Information Processing Systems, 33:12559- 12571, 2020. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61- 80, 2008. Dominik Schreiber and Peter Sanders. Scalable sat solving in the cloud. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, Barcelona, Spain, July 5- 9, 2021, Proceedings 24, pp. 518- 534. Springer, 2021. Daniel Selsam and Nikolaj Bjorner. Guiding high- performance SAT solvers with unsat- core predictions. In International Conference on Theory and Applications of Satisfiability Testing, pp. 336- 353. Springer, 2019. Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a SAT solver from single- bit supervision. ar Xiv preprint ar Xiv:1802.03685, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ar Xiv preprint ar Xiv:1710.10903, 2017. Haoze Wu. Improving sat- solving with machine learning. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education, pp. 787- 788, 2017.\n\nZhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion Stoica. Representing long- range context for graph neural networks with global attention. Advances in Neural Information Processing Systems, 34:13266- 13279, 2021. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4- 24, 2020. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? ar Xiv preprint ar Xiv:1810.00826, 2018. Emre Yolcu and Barnabas P\u00f3czos. Learning local search heuristics for boolean satisfiability. In Advances in Neural Information Processing Systems, pp. 7992- 8003, 2019. Ziwei Zhang and Yang Zhang. Elimination mechanism of glue variables for solving sat problems in linguistics. In The Asian Conference on Language, pp. 147- 167, 2021. Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57- 81, 2020. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43- 76, 2020.\n\nWe propose Neuro Back, a novel approach to make CDCL SAT solving more effective and avoid frequent online model inferences, thus making the GNN approach more practical. The main idea of Neuro Back is to make offline model inference, i.e., prior to the solving process, to obtain instructive static information for improving CDCL SAT solving. Once trained, the offline model inference allows Neuro Back to execute solely on the CPU, thereby making it completely independent of GPU resources. In particular, Neuro Back seeks to refine the phase selection heuristics in CDCL solvers by leveraging offline neural predictions on variable phases appearing in the majority (or even all) of the satisfying assignments. The offline predictions on such phase information are based on the generalization of backbone variables, which are variables whose phases remain consistent across all satisfying assignments. Recent work Biere et al. (2021); Al- Yahya et al. (2022) has shown that backbone variables are crucial for enhancing CDCL SAT solving. Choosing the correct phase for a backbone variable prevents conflicts, while an incorrect choice inevitably leads to backtracking in the search. Moreover, predicting the correct phases of non- backbone variables appearing in the majority of satisfying assignments is also important, because such phases prevent backtracking with high probabilities. Our conjecture is that the knowledge learned from predicting the phases of backbone variables can be transferred to predicting the phases of non- backbone variables exhibited in the majority of satisfying assignments. Therefore, Neuro Back applies a GNN model, trained solely on predicting the phases of backbone variables, to predict the phases of all variables. Neuro Back converts the SAT formula with diverse scales into a compact and more learnable graph representation, turning the problem of predicting variable phases into a binary node classification problem. To make the GNN model both compact and robust, Neuro Back employs a novel Graph Transformer architecture with light- weight self- attention mechanisms. To train the model with supervised learning, a balanced dataset called Data Back containing 120,286 labeled formulas with diversity was created from five different sources: CNFgen Lauria et al. (2017), SATLIB Hoos & St\u00fctzle (2000), model counting competitions from 2020 to 2022 MCC (2020; 2021; 2022), and main and random tracks in SAT competitions from 2004 to 2021. To evaluate the effectiveness of our approach, Neuro Back is incorporated into a state- of- the- art CDCL SAT solver called Kissat Biere & Fleury (2022), resulting in a new solver called Neuro Back- Kissat. The experimental results on all SAT problems from SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) show that Neuro Back allows Kissat to solve up to \\(5.2\\%\\) and \\(7.4\\%\\) more problems, respectively. The experiments thus demonstrate that Neuro Back is a practical neural approach to improving CDCL SAT solvers. The contributions of our paper are: 1. Approach. To our knowledge, Neuro Back presents the first practical neural approach to make the CDCL SAT solving more effective, without requiring any GPU resource during its application. 2. Dataset. A new dataset Data Back containing 120,286 data samples is created for backbone phase classification. Data Back is publicly available at https://huggingface.co/datasets/neuroback/Data Back 3. Implementation. Neuro Back is incorporated into a state-of-the-art SAT solver, Kissat. The source code of Neuro Back model and Neuro Back-Kissat is publicly available at https://github.com/wenxiwang/neuroback. ## 2 BACKGROUND This section introduces the SAT problem, CDCL algorithm, phase selection heuristics in CDCL solvers, and basics of GNN and Graph Transformer. Preliminaries of SAT In SAT, a propositional logic formula \\(\\phi\\) is usually encoded in Conjunctive Normal Form (CNF), which is a conjunction \\((\\wedge)\\) of clauses. Each clause is a disjunction \\((\\vee)\\) of literals. A literal is either a variable \\(v\\) , or its complement \\(\\neg v\\) . Each variable can be assigned a logical phase, 1 (true) or 0 (false). A CNF formula has a satisfying assignment if and only if every clause has at least one true literal. For example, a CNF formula \\(\\phi = (v_{1} \\vee \\neg v_{2}) \\wedge (v_{2} \\vee v_{3}) \\wedge v_{2}\\) consists of three clauses \\(v_{1} \\vee \\neg v_{2}\\) , \\(v_{2} \\vee v_{3}\\) and \\(v_{2}\\) ; four literals \\(v_{1}\\) , \\(\\neg v_{2}\\) , \\(v_{2}\\) and \\(v_{3}\\) ; and three variables \\(v_{1}\\) , \\(v_{2}\\) , and \\(v_{3}\\) . One satisfying assignment of \\(\\phi\\) is \\(v_{1} = 1\\) , \\(v_{2} = 1\\) , \\(v_{3} = 0\\) . The goal of a SAT solver is to check if a formula \\(\\phi\\) is sat or unsat. A complete solver either outputs a satisfying assignment for \\(\\phi\\) , or proves that no such assignment exists.\n\nThe backbone of a sat formula is the set of literals that are true in all its satisfying assignments. Thus backbone variables are the variables whose phases remain consistent across all satisfying assignments. In the given CNF formula \\(\\phi\\) , there are two backbone variables, \\(v_{1}\\) and \\(v_{2}\\) , both maintaining a phase of 1 in all satisfying assignments of \\(\\phi\\) . CDCL Algorithm CDCL makes SAT solvers efficient in practice and is one of the main reasons for the widespread use of SAT applications. The general idea of CDCL algorithm is as follows (see Marques- Silva et al. (2021) for details). First, it picks a variable on which to branch and decides a phase to assign to it with heuristics. It then conducts a Boolean propagation based on the decision. In the propagation, if a conflict occurs (i.e., at least one clause is mapped to 0), it performs a conflict analysis; otherwise, it makes a new decision on another selected variable. In the conflict analysis, CDCL first analyzes the decisions and propagations to investigate the reason for the conflict, then extracts the most relevant wrong decisions, undoes them, and adds the reason to its memory as a learned lesson (encoded in a clause called learned clause) in order to avoid making the same mistake in the future. After conflict analysis, the solver backtracks to the earliest decision level where the conflict could be resolved and continues the search from there. The process continues until all variables are assigned a phase (sat), or until it learns the empty clause (unsat). Phase Selection Heuristics As indicated above, CDCL SAT solving mainly relies on two kinds of variable related heuristics: variable branching heuristics and phase selection heuristics, which are orthogonal to each other. Phase Saving Pipatsrisawat & Darwiche (2007) is a prevalent phase selection heuristic in modern CDCL solvers. It returns a variable's last assigned polarity, either through decision or propagation. This heuristic addresses the issue of solvers forgetting prior valid assignments due to non- chronological backtracking. Rephasing Biere & Fleury (2020); Fleury & Heisinger (2020) is more recent phase selection heuristic, proposed to reset or modify saved phases to diversify the exploration of the search space. The state- of- the- art CDCL solver, Kissat Biere & Fleury (2022), incorporates the latest advancements in phase saving and rephrasing heuristics. GNN GNNs Wu et al. (2020); Zhou et al. (2020) are a family of neural network architectures that operate on graphs Gori et al. (2005); Scarselli et al. (2008); Gilmer et al. (2017); Battaglia et al. (2018). Typical GNNs follow a recursive neighborhood aggregation scheme called message passing Gilmer et al. (2017). Formally, the input of a GNN is a graph defined as a tuple \\(G = (V, E, W, H)\\) , where \\(V\\) denotes the set of nodes; \\(E \\subseteq V \\times V\\) denotes the set of edges; \\(W = \\{W_{u,v} | (u, v) \\in E\\}\\) contains the feature vector \\(W_{u,v}\\) of each edge \\((u, v)\\) ; and \\(H = \\{H_v | v \\in V\\}\\) contains the feature vector \\(H_v\\) of each node \\(v\\) . A GNN maps each node to a vector- space embedding by updating the feature vector of the node iteratively based on its neighbors. For each iteration, a message- passing layer \\(\\mathcal{L}\\) takes a graph \\(G = (V, E, W, H)\\) as an input and outputs a graph \\(G' = (V, E, W, H')\\) with updated node feature vectors, i.e., \\(G' = \\mathcal{L}(G)\\) . Classic GNN models Gilmer et al. (2017); Kipf & Welling (2016); Hamilton et al. (2017) usually stack several message- passing layers to realize iterative updating. Prior work on utilizing GNNs to improve CDCL SAT solving will be reviewed in the next section. Graph Transformer Architecture Transformer Vaswani et al. (2017) is a family of neural network architectures for processing sequential data (e.g., text or image), which has recently won great success in nature language processing and computer vision. Central to the transformer is the self- attention mechanism, which calculates attention scores among elements in a sequence, thereby allowing each element to focus on other relevant elements in the sequence for capturing long- range dependencies effectively. Recent research Min et al. (2022); Wu et al. (2021); Mialon et al. (2021); Rong et al. (2020) has shown that combining the transformer with GNN results in competitive performance for graph and node classification tasks, forming the Graph Transformer architecture. Notably, Graph Trans Wu et al. (2021) is a representative model which utilizes a GNN subnet consisting of multiple GNN layers for local structure encoding, followed by a Transformer subnet with global self- attention to capture global dependencies, and finally incorporates a Feed Forward Network (FFN) for classification. The GNN model design in Neuro Back is inspired by Graph Trans. ## 3 RELATED WORK This section presents related work on identifying the backbone and machine learning techniques for improving CDCL SAT solving.\n\n<center>Figure 1: Overview of Neuro Back. First, the input CNF formula is converted into a compact and more learnable graph representation. A trained GNN model is then applied once on the graph before SAT solving begins for phase selection. The SAT solver utilizes phase information in the resulting labeled graph as an initialization to guide its solving process. Thus, with the offline process of making instructive phase predictions, Neuro Back makes the solving more effective and practical. </center> Backbone for CDCL Solvers. Janota proved that identifying the backbone is co- NP complete Janota (2010). Furthermore, Kilby et al. demonstrated that even approximating the backbone is generally intractable Kilby et al. (2005). Wu Wu (2017) applies a logistic regression model to predict the phase of backbone variables to improve a classic CDCL SAT solver called Mini Sat E\u00e9n & S\u00f6rensson (2003). Although the approach correctly predicts the phases of \\(78\\%\\) backbone variables, it fails to make improvements over Mini Sat in solving time. In addition, recent works Biere et al. (2021); Al- Yahya et al. (2022) have been focusing on enhancing CDCL SAT solving by employing heuristic search to partially compute the backbone during the solving process. In contrast, Neuro Back applies GNN to predict the backbone in an offline manner to obtain the phase information of all variables to improve CDCL solving. Machine Learning for CDCL Solvers. Recently, several approaches have been developed to utilize GNNs to facilitate CDCL SAT solving. Neuro SAT Selsam et al. (2018) was the first such framework adapting a neural model into an end- to- end SAT solver, which was not intended as a complete SAT solver. Others Jaszczur et al. (2020); Davis et al. (1962); Kurin et al. (2019); Han (2020); Audemard & Simon (2018); Zhang & Zhang (2021) aim to provide SAT solvers with better branching or phase selection heuristics. These approaches either reduce the number of solving iterations or enhance the solving effectiveness on selected small- scale problems with up to a few thousand variables. However, they do not provide obvious improvements in solving effectiveness for large- scale problems. In contrast, Neuro Core Selsam & Bj\u00f8rner (2019), the most closely related approach to this paper, aims to make the solving more effective especially for large- scale problems as in SAT competitions. It enhances the branching heuristic for CDCL using supervised learning to map unsat problems to unsat core variables (i.e., the variables involved in the unsat core). Based on the dynamically learned clauses during the solving process, Neuro Core performs frequent online model inferences to tune the predictions. However, this online inference is computationally demanding. Neuro Back is distinct from Neuro Core in two main aspects. One, while Neuro Core is designed to refine the branching heuristic in CDCL SAT solvers, Neuro Back is invented to enhance their phase selection heuristics. Two, while Neuro Core extracts dynamic unsat core information from unsat formulas through online model inferences, Neuro Back captures static backbone information from sat formulas using offline model inference. Details of Neuro Back are introduced in the following section. ## 4 NEUROBACK In order to reduce the computational cost of the online model inference and to make CDCL SAT solving more effective, Neuro Back employs offline model predictions on variable phases to enhance the phase selection heuristics in CDCL solvers. Figure 1 shows the overview of Neuro Back. First, it converts the input CNF formula into a compact and more learnable graph representation. Then, a well- designed GNN model trained to predict the phases of backbone variables, is applied on the converted graph representation to infer the phases of all variables. The model inference is performed only once before the SAT solving process. The resulting offline prediction is applied as an initialization for the SAT solving process. Finally, the enhanced SAT solver outputs the satisfiability of the input CNF formula. The key components of Neuro Back including the graph representation of CNF formulas, the GNN- based phase selection, and the phase prediction application in SAT solvers, are illustrated in the subsections below. ### 4.1 GRAPH REPRESENTATION FOR CNF FORMULAS As in recent work Kurin et al. (2020); Yolcu & P\u00f3czos (2019), a SAT formula is represented using a more compact undirected bipartite graph than the one adopted in Neuro Core. Two node types\n\n<center>Figure 3: The architecture of Neuro Back model, consisting of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. </center> represent the variables and clauses, respectively. Each edge connects a variable node to a clause node, representing that the clause contains the variable. Two edge types represent two polarities of a variable appearing in the clause, i.e., the variable itself and its complement. Although the representation is compact, its diameter might be substantial for large- scale SAT formulas, which could result in insufficient message passing during the learning process. To mitigate this issue, we introduce a meta node for each connected component in the graph, with meta edges connecting the meta node to all clause nodes in the component. With the added meta nodes and edges, every variable node not appearing in the same clauses can reach each other through their corresponding clause nodes and the meta node, thereby making the diameter at most four. Figure 2 shows an example of our graph representation for the CNF formula \\((v_{1} \\lor v_{2}) \\land (v_{2} \\lor v_{3}) \\land (v_{3} \\lor v_{4})\\) . It includes one meta node, four variable nodes and three clause nodes \\(c_{1}, c_{2}, c_{3}\\) , representing clauses \\(v_{1} \\lor v_{2}, v_{2} \\lor v_{3}\\) and \\(v_{3} \\lor v_{4}\\) , respectively. Without the added meta node and edges, the longest path in the graph runs from variable node \\(v_{1}\\) to variable node \\(v_{4}\\) , making the diameter six. However, by introducing the meta node and edges, the diameter is reduced to four. Formally, for a graph representation \\(G = (V, E, W, H)\\) of a CNF formula, the edge feature \\(W_{u, v}\\) of each edge \\((u, v)\\) is initialized by its edge type with the value 0 representing the meta edge, the value 1 representing positive polarity, and \\(- 1\\) negative polarity; the node feature \\(H_{v}\\) of each node \\(v\\) is initialized by its node type with 0 representing a meta node, 1 representing a variable node, and \\(- 1\\) representing a clause node. <center>Figure 2: An example graph representation of the CNF formula \\((v_{1} \\lor v_{2}) \\land (v_{2} \\lor v_{3}) \\land (v_{3} \\lor v_{4})\\) . A meta node \\(m\\) is added along with meta edges (represented by dashed lines) connecting to all clause nodes in the connected component, reducing the graph diameter from six to four. </center> ### 4.2 GNN-based PHASE PREDICTION Given that the phases of backbone variables remain consistent across all satisfying assignments, selecting the correct phase for a backbone variable prevents backtracking. Conversely, an incorrect choice inevitably leads to a conflict. Moreover, the proportion of backbone variables is typically significant. For instance, during our data collection from five notable sources (i.e., CNFgen, SATLIB, model counting competitions, SATCOMP random tracks, and SATCOMP main tracks), backbone variables constitute an average of \\(27\\%\\) of the total variables. Therefore, accurately identifying the phases of the backbone variables is crucial for efficiently solving a SAT formula. Furthermore, identifying the phases of the non- backbone variables appearing in the majority of satisfying assignments is also important. Because such phases could prevent backtracking with high probabilities. With the converted graph representation, predicting the phases of all variables, including backbone and non- backbone variables, is a binary node classification problem, which can be addressed by a GNN model. Inspired by transfer learning Zhuang et al. (2020), we first train a GNN model to predict the phases of backbone variables, and then leverage the trained model to predict the phases of all variables. Our key insight is that the knowledge learned from predicting the phases of backbone variables can provide a valuable guidance on predicting the phases of non- backbone variables exhibiting in the majority of satisfying assignments. The following subsections introduce the design, implementation, and training of our GNN model.\n\n#### 4.2.1 GNN MODEL DESIGN The sizes of converted graphs representing practical SAT formulas (usually with millions of variables and clauses) are typically substantial. To enable effective training within the constraints of limited GPU memory, it is essential for our model to be both compact and robust. Our GNN model design is inspired by the robust graph transformer architecture, Graph Trans Wu et al. (2021). However, in our particular SAT application context, Graph Trans exhibits two limitations, both arising from the global self- attention mechanism within its transformer subnet. First, the mechanism does not explicitly integrate the topological graph structure information when determining attention scores. However, such information is essential in characterizing a SAT formula. Second, the global self- attention mechanism computes attention scores for all possible node pairs, leading to quadratic memory complexity with respect to the number of nodes in the graph. This is obviously infeasible for tackling the large- scale SAT formulas in our task. To overcome the limitations, we introduce a novel transformer subnet that both distinctly harnesses topological structure information and significantly enhances memory efficiency. It combines Graph Self- Attention (GSA) and Local Self- Attention (LSA), replacing the original transformer's global self- attention. Instead of computing attention scores for all node pairs as global self- attention, GSA calculates attention scores solely for directly connected node pairs, leveraging information of edges and edge weights. This not only explicitly incorporates the topological structure information of the graph, but also reduces the memory complexity to linear in terms of the number of edges in the graph. Additionally, to further reduce the memory complexity, LSA segments each node embedding into multiple node patches and computes attention scores for each pair of node patches. The results in a linear memory complexity in terms of the number of nodes in the graph. Figure 3 illustrates the design of our GNN model architecture. It consists of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. Within each GNN block, a GNN layer is preceded by a normalization layer, with a skip connection bridging the two. The transformer block is crafted to accelerate training on a significant collection of large- scale graphs. Inspired by the recent vision transformer architecture, Vi T- 22B Padlewski & Djolonga (2023), each transformer block integrates a normalization layer, succeeded by both an FFN layer and a GSA/LSA layer that operate concurrently to optimize training efficiency. #### 4.2.2 IMPLEMENTATION The current implementation of Neuro Back model utilizes GINConv Fey & Lenssen (2023b); Xu et al. (2018) to build the GNN layer, for its proficiency in distinguishing non- isomorphic graph structures. However, GINConv lacks the capability to encode edge weight information. To address this, we employ three GINConv layers, each corresponding to a distinct edge weight in our graph representation. Each GINConv layer exclusively performs message passing for edges with its corresponding weight. The node embeddings from these three GINConv layers are finally aggregated as the output of the GNN layer. GATConv layers Fey & Lenssen (2023a); Velickovic et al. (2017) is utilized to built the GSA transformer block. The patch encoder in the Vi T transformer Dosovitskiy et al. (2020) is applied to construct the LSA transformer block. Layer Norm Ba et al. (2016) is employed as our normalization layer. To avoid potential over- smoothing issues, as instructed in Chen et al. (2020), the number of blocks in the GNN subnet is set to the maximum diameter of the graph representation (i.e., \\(L = 4\\) ). To ensure the accuracy of our model, while taking into account our limited GPU memory, the number of both GSA and LSA blocks are set to three (i.e., \\(M = 3\\) and \\(N = 3\\) ). Additionally, FFNs within the transformer blocks contain no hidden layers, while the final FFN utilized for node classification is structured to include one hidden layer. The model is implemented using Py Torch Paszke et al. (2019) and Py Torch Geometric Fey & Lenssen (2019). #### 4.2.3 MODEL PRE-TRAINING AND FINE-TUNING The Neuro Back model undergoes a two- stage training process. Initially, it is pre- trained on an extensive and diverse dataset gathered from various sources. This pre- training equips it with the fundamental knowledge to classify the backbone variable phases across a broad spectrum of CNF formulas. Subsequently, this pre- trained model is refined or fine- tuned on a smaller, domain- specific dataset. This fine- tuning process enhances the model's proficiency in classifying backbone variable\n\n<table><tr><td>Data Back-PT</td><td>CNFgen</td><td>SATLIB</td><td>MCCOMP</td><td>SATCOMP (random)</td><td>Overall</td><td>Data Back-FT</td><td>SATCOMP (main)</td></tr><tr><td># CNF</td><td>21,718</td><td>80,306</td><td>11,560</td><td>4,876</td><td>118,460</td><td># CNF</td><td>1,826</td></tr><tr><td># Var</td><td>779</td><td>114</td><td>16,299</td><td>25,310</td><td>2,852</td><td># Var</td><td>206,470</td></tr><tr><td># Cla</td><td>281,888</td><td>529</td><td>63,501</td><td>88,978</td><td>61,898</td><td># Cla</td><td>1,218,519</td></tr><tr><td># Backbone Var</td><td>280 (36%)</td><td>58 (51%)</td><td>8,587 (53%)</td><td>1,960 (8%)</td><td>1,009 (35%)</td><td># Backbone Var</td><td>48,266 (23%)</td></tr></table> Table 1: Details of Data Back. For each source, we summarize the number of CNF formulas; the average number of variables and clauses characterizing the size of each CNF formula; and the average number of backbone variables, with their proportion in total variables (indicated in brackets). Overall, for each dataset, we summarize the total number of CNF formulas and the corresponding average counts regarding all formulas in the set. phases within a specific category of CNF formulas. Details about the pre- training and fine- tuning datasets are introduced in Section 5. Binary Cross Entropy (BCE) loss is adopted as our loss function. Besides, Adam W optimizer Loshchilov & Hutter (2017) with a learning rate of \\(10^{- 4}\\) is applied for model pre- training and fine- tuning. The number of epoch is set to 40 for pre- training, and 60 for fine- tuning. It took 48 hours in total to accomplish both the pre- training and fine- tuning on our commodity computer. ### 4.3 APPLYING PHASE PREDICTIONS IN SAT The goal is to leverage phase predictions derived from the GNN model to enhance the phase selection heuristics within CDCL SAT solvers. While numerous ways exist to integrate these neural predictions into CDCL SAT solvers, the most straightforward and generic approach is to initialize the phases of the corresponding variables in CDCL SAT solvers based on the predicted phases. In this paper, we adopted the state- of- the- art solver Kissat Biere & Fleury (2022) to support the phase initialization with Neuro Back predictions. The resulting implementation is called Neuro Back- Kissat. ## 5 DATABACK We created a new dataset, Data Back, comprising sat CNF formulas labeled with phases of backbone variables, for pre- training (PT) and fine- tuning (FT) the Neuro Back model. Accordingly, there are two subsets in Data Back: the pre- training set, Data Back- PT, and the fine- tuning set, Data Back- FT. To get the backbone label, the very recent state- of- the- art backbone extractor called Cadi Back Biere et al. (2023) is utilized. Given that the fine- tuning formulas are typically more challenging to solve than the pre- training ones, label collection timeouts are set as 1,000 seconds for the pre- training formulas and 5,000 seconds for the fine- tuning formulas. Data Back includes formulas solved within the time limit with at least one backbone variable. We observe that there exists a significant label imbalance in both Data Back- PT and Data Back- FT. To tackle this, for each labeled formula, we create a dual formula by negating all backbone variables in the original formula. The labels of the dual formula are the negated phases of the backbone variables in the original formula. Formally, for a formula \\(f\\) with \\(n\\) backbone variables \\(b_{1},\\ldots ,b_{n}\\) , let \\(\\mathcal{L}_{f}:\\{b_{1},\\ldots ,b_{n}\\} \\to \\{1,0\\}\\) denote the mapping of each backbone variable to its phase. The dual formula \\(f^{\\prime}\\) is obtained from \\(f\\) by negating each backbone variable: \\(f^{\\prime} = f[b_{1}\\to\\) \\(\\neg b_{1},\\ldots ,b_{n}\\mapsto \\neg b_{n}]\\) . The dual \\(f^{\\prime}\\) is still satisfiable and retains the same backbone variables as \\(f\\) , but with the opposite phases \\(\\mathcal{L}_{f^{\\prime}}(b_{i}) = \\neg \\mathcal{L}_{f}(b_{i}),i\\in \\{1,\\ldots ,n\\}\\) . For the given CNF formula example in Section 2 \\(\\phi = (v_{1}\\vee \\neg v_{2})\\wedge (v_{2}\\vee v_{3})\\wedge v_{2}\\) , having \\(v_{1}\\) and \\(v_{2}\\) as its backbone variables with phases \\(\\{v_{1},v_{2}\\} \\to \\{1\\}\\) , the dual formula is \\(\\phi^{\\prime} = (\\neg v_{1}\\vee v_{2})\\wedge (\\neg v_{2}\\vee v_{3})\\wedge \\neg v_{2}\\) , still having \\(v_{1}\\) and \\(v_{2}\\) as the backbone variables but with opposite phases \\(\\{v_{1},v_{2}\\} \\to \\{0\\}\\) . This data augmentation strategy doubles the size of Data Back with a perfect balance in positive and negative backbone labels. In the rest of the paper, Data Back- PT and Data Back- FT refer to the augmented, balanced datasets. Data Back- PT The CNF formulas in Data Back- PT are sourced from four origins: 1) CNFgen Lauria et al. (2017), a recent CNF generator renowned for crafting CNF formulas that feature in proof complexity literature; 2) SATLIB Hoos & St\u00fctzle (2000), an online benchmark library housing well- known benchmark instances, serving as a foundational challenge for the SAT community; 3) model counting competitions from 2020 to 2022 Fichte et al. (2021); MCC (2021; 2022), including wide range of CNF formulas for model counters to count their satisfying assignments, which provide a good source for backbone extraction; 4) Random tracks in SAT competitions from 2004 to 2021, offering numerous challenging random SAT problems.\n\n<table><tr><td>metrics model</td><td>precision</td><td>recall</td><td>F1</td><td>accuracy</td></tr><tr><td>pre-trained model</td><td>0.903</td><td>0.766</td><td>0.829</td><td>0.751</td></tr><tr><td>fine-tuned model</td><td>0.941</td><td>0.914</td><td>0.928</td><td>0.887</td></tr></table> Table 2: The performance on the validation set of both pre- trained and fine- tuned Neuro Back models for classifying the phases of backbone variables, in terms of precision, recall, F1 score and accuracy. Table 1 shows the detailed statistics of the resulting dataset. CNFgen generated 10,859 labeled sat formulas (21,718 augmented formulas) grouped by five categories: random SAT, clique- coloring, graph coloring, counting principle, and parity principle. From SATLIB, 40,153 sat formulas with labels (80,306 augmented formulas) were collected, encoding seven kinds of problems: random SAT, graph coloring, planning, bounded model checking, Latin square, circuit fault analysis, and all- interval series. Model counting competitions and SAT competition random tracks contribute 5,780 (11,578 augmented) and 2,438 (4,876 augmented) labeled formulas, respectively. In total, 59,230 (118,460 augmented) formulas are included Data Back- PT. The comparative size of formulas, as indicated by the average number of variables and clauses, reveals that formulas from model counting competitions and SAT competition random tracks are generally larger than those generated by CNFgen, which are in turn larger than those originating from SATLIB. In addition, formulas from model counting competitions and SATLIB exhibit relatively high average backbone proportions. These are followed by formulas from CNFgen, while formulas from the SAT competition's random tracks demonstrate the lowest backbone proportion. Data Back- PT thus contains a diverse set of formulas. Data Back- FT Given that Neuro Back will be tested on large- scale SAT formulas from the main track of recent SAT competitions, the fine- tuning dataset, Data Back- FT, incorporates CNF formulas from the main track of earlier SAT competitions spanning from 2004 to 2021. As shown in Table 1, it contains 913 (1,826 augmented) labeled formulas. While Data Back- FT is considerably smaller in size compared to Data Back- PT, its individual formulas are distinctly larger than those in Data Back- PT. ## 6 EXPERIMENTS Platform All experiments were run on an ordinary commodity computer with one NVIDIA Ge Force RTX 3080 GPU (10GB memory), one AMD Ryzen Threadripper 3970X processor (64 logical cores), and 256GB RAM. Research Questions The experiments aim to answer two research questions: RQ1: How accurately does the Neuro Back model classify the phases of backbone variables? RQ2: How effective is the Neuro Back approach? RQ1: Neuro Back Model Performance The Neuro Back model was pre- trained on the entire Data Back- PT dataset, then fine- tuned on a random \\(90\\%\\) of Data Back- FT samples, and evaluated on the remaining \\(10\\%\\) as a validation set. Table 2 details the performance of both the pre- trained and fine- tuned models in classifying the phases of backbone variables. Notably, the pre- trained model achieved \\(75.1\\%\\) accuracy in classifying backbone variables, with a precision exceeding \\(90\\%\\) and a recall rate of \\(76.7\\%\\) . Considering the distinct data sources of Data Back- PT and Data Back- FT, the results suggest that the pre- training enables the model to extract generalized knowledge about backbone phase prediction. Fine- tuning further augments model performance, with improvements ranging between \\(4\\%\\) and \\(15\\%\\) across all metrics, making precision, recall and F1 score all exceeding \\(90\\%\\) . In conclusion, Neuro Back model effectively learns to predict the phases of backbone variables through both pre- training and fine- tuning. RQ2: Neuro Back Performance To evaluate the solving effectiveness, we collect all 800 CNF formulas from the main track of SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) as our testing dataset. For our baseline solvers, we selected the default configuration of Kissat, named Default- Kissat, which simply sets the initial phase of each variable to true. We implemented an additional baseline solver, Random- Kissat, which randomly assigns the initial phase of each variable as either true or false. Neuro Back- Kissat and its baseline solvers, were applied to all 800 SAT problems in the testing dataset, with the standard solving time limit of 5,000 seconds. Each solver utilized up to 64 different processes in parallel on the dedicated 64- core machine. The model inference for each Neuro Back solver was conducted solely on the CPU, with a memory limit of 10GB to mitigate memory contention issues. Consequently, 308 problems from SATCOMP- 2022 and\n\n<center>Figure 4: Progress of Default-Kissat, Random-Kissat, and Neuro Back-Kissat over time in solving problems (time in seconds) on SATCOMP-2022 (left) and SATCOMP-2023 (right), respectively. Neuro Back-Kissat outperforms the two baseline solvers on both testing sets. </center> 353 problems from SATCOMP- 2023 were successfully inferred. The CPU inference time for each of these problems ranged from 0.3 to 16.5 seconds, averaging at 1.7 seconds. Given that problems unsuccessfully inferred make the Neuro Back solver revert to its baseline, we exclude such problems from our evaluation of how Neuro Back contributes to SAT solving. As a result, for 308 inferred problems in SATCOMP- 2022, Default- Kissat and Random- Kissat solved 193 and 197 problems, respectively. In comparison, Neuro Back- Kissat solved 203 problems, representing an improvement of \\(5.2\\%\\) over Default- Kissat and \\(3.0\\%\\) over Random- Kissat, respectively. For 353 inferred problems in SATCOMP- 2023, Default- Kissat and Random- Kissat successfully solved 198 and 190 problems, respectively. In contrast, Neuro Back- Kissat solved 204 problems, making improvements of \\(3.0\\%\\) and \\(7.4\\%\\) over Default- Kissat and Random- Kissat, respectively. The cactus plot is a commonly used plot in the SAT community for demonstrating the solving, as shown in Fig. 4. Random- Kissat performs better than Default- Kissat in SATCOMP- 2022, but worse in SATCOMP- 2023. In contrast, Neuro Back- Kissat consistently outperforms both baseline solvers. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, reducing solving time by 98 and 246 seconds per problem. For additional results, please refer to the Appendix section. Overall, results suggest that the phase initialization provided by Neuro Back outperforms both the default and random phase initializations in Kissat, exhibiting enhanced proficiency in solving SATCOMP- 2022 and SATCOMP- 2023 problems. ## 7 DISCUSSION AND FUTURE WORK The current implementation of Neuro Back has two main aspects to improve. First, pre- training the GNN model requires a large amount of data. Once trained, however, the model is able to easily generalize to new problem categories, so the investment in putting together a large dataset is warranted. The dataset will also be publicly available to benefit future research. Second, this paper employs neural phase predictions solely as an initial setting for variable phases in SAT solvers. However, multiple methods exist to utilize these predictions in SAT, either in a static or dynamic manner, which are likely to yield further performance enhancements. For instance, phase predictions could be leveraged dynamically during rephasing to adjust or diversify the exploration of the search space. ## 8 CONCLUSION This paper proposes a machine learning approach, Neuro Back, to make CDCL SAT solvers more effective without requiring any GPU resource during its application. The main idea is to make offline model inference on variable phases appearing in the majority of satisfying assignments, for enhancing the phase selection heuristic in CDCL solvers. Incorporated in the state- of- the- art SAT solver, Kissat, this approach significantly reduces the solving time and makes it possible to solve more instances in SATCOMP- 2022 and SATCOMP- 2023. Neuro Back is thus a promising approach to improving the SAT solvers through machine learning.",
    "421_gnn_model_design_the_sizes_of_converted_graphs_representing_practical_sat_formulas_usually_with_millions_of_variables_and_clauses_are_typically_substantial_to_enable_effective_training_within_the_constraints_of_limited_gpu_memory_it_is_essential_for_our_model_to_be_both_compact_and_robust_our_gnn_model_design_is_inspired_by_the_robust_graph_transformer_architecture_graph_trans_wu_et_al_2021_however_in_our_particular_sat_application_context_graph_trans_exhibits_two_limitations_both_arising_from_the_global_self-_attention_mechanism_within_its_transformer_subnet_first_the_mechanism_does_not_explicitly_integrate_the_topological_graph_structure_information_when_determining_attention_scores_however_such_information_is_essential_in_characterizing_a_sat_formula_second_the_global_self-_attention_mechanism_computes_attention_scores_for_all_possible_node_pairs_leading_to_quadratic_memory_complexity_with_respect_to_the_number_of_nodes_in_the_graph_this_is_obviously_infeasible_for_tackling_the_large-_scale_sat_formulas_in_our_task_to_overcome_the_limitations_we_introduce_a_novel_transformer_subnet_that_both_distinctly_harnesses_topological_structure_information_and_significantly_enhances_memory_efficiency_it_combines_graph_self-_attention_gsa_and_local_self-_attention_lsa_replacing_the_original_transformers_global_self-_attention_instead_of_computing_attention_scores_for_all_node_pairs_as_global_self-_attention_gsa_calculates_attention_scores_solely_for_directly_connected_node_pairs_leveraging_information_of_edges_and_edge_weights_this_not_only_explicitly_incorporates_the_topological_structure_information_of_the_graph_but_also_reduces_the_memory_complexity_to_linear_in_terms_of_the_number_of_edges_in_the_graph_additionally_to_further_reduce_the_memory_complexity_lsa_segments_each_node_embedding_into_multiple_node_patches_and_computes_attention_scores_for_each_pair_of_node_patches_the_results_in_a_linear_memory_complexity_in_terms_of_the_number_of_nodes_in_the_graph_figure_3_illustrates_the_design_of_our_gnn_model_architecture_it_consists_of_three_main_components_a_gnn_subnet_with_l_stacked_gnn_blocks_a_transformer_subnet_with_m_gsa_transformer_blocks_and_n_lsa_transformer_blocks_and_a_ffn_layer_for_node_classification_within_each_gnn_block_a_gnn_layer_is_preceded_by_a_normalization_layer_with_a_skip_connection_bridging_the_two_the_transformer_block_is_crafted_to_accelerate_training_on_a_significant_collection_of_large-_scale_graphs_inspired_by_the_recent_vision_transformer_architecture_vi_t-_22b_padlewski_djolonga_2023_each_transformer_block_integrates_a_normalization_layer_succeeded_by_both_an_ffn_layer_and_a_gsalsa_layer_that_operate_concurrently_to_optimize_training_efficiency_422_implementation_the_current_implementation_of_neuro_back_model_utilizes_ginconv_fey_lenssen_2023b_xu_et_al_2018_to_build_the_gnn_layer_for_its_proficiency_in_distinguishing_non-_isomorphic_graph_structures_however_ginconv_lacks_the_capability_to_encode_edge_weight_information_to_address_this_we_employ_three_ginconv_layers_each_corresponding_to_a_distinct_edge_weight_in_our_graph_representation_each_ginconv_layer_exclusively_performs_message_passing_for_edges_with_its_corresponding_weight_the_node_embeddings_from_these_three_ginconv_layers_are_finally_aggregated_as_the_output_of_the_gnn_layer_gatconv_layers_fey_lenssen_2023a_velickovic_et_al_2017_is_utilized_to_built_the_gsa_transformer_block_the_patch_encoder_in_the_vi_t_transformer_dosovitskiy_et_al_2020_is_applied_to_construct_the_lsa_transformer_block_layer_norm_ba_et_al_2016_is_employed_as_our_normalization_layer_to_avoid_potential_over-_smoothing_issues_as_instructed_in_chen_et_al_2020_the_number_of_blocks_in_the_gnn_subnet_is_set_to_the_maximum_diameter_of_the_graph_representation_ie_l_4_to_ensure_the_accuracy_of_our_model_while_taking_into_account_our_limited_gpu_memory_the_number_of_both_gsa_and_lsa_blocks_are_set_to_three_ie_m_3_and_n_3_additionally_ffns_within_the_transformer_blocks_contain_no_hidden_layers_while_the_final_ffn_utilized_for_node_classification_is_structured_to_include_one_hidden_layer_the_model_is_implemented_using_py_torch_paszke_et_al_2019_and_py_torch_geometric_fey_lenssen_2019_423_model_pre-training_and_fine-tuning_the_neuro_back_model_undergoes_a_two-_stage_training_process_initially_it_is_pre-_trained_on_an_extensive_and_diverse_dataset_gathered_from_various_sources_this_pre-_training_equips_it_with_the_fundamental_knowledge_to_classify_the_backbone_variable_phases_across_a_broad_spectrum_of_cnf_formulas_subsequently_this_pre-_trained_model_is_refined_or_fine-_tuned_on_a_smaller_domain-_specific_dataset_this_fine-_tuning_process_enhances_the_models_proficiency_in_classifying_backbone_variable": "<table><tr><td>Data Back-PT</td><td>CNFgen</td><td>SATLIB</td><td>MCCOMP</td><td>SATCOMP (random)</td><td>Overall</td><td>Data Back-FT</td><td>SATCOMP (main)</td></tr><tr><td># CNF</td><td>21,718</td><td>80,306</td><td>11,560</td><td>4,876</td><td>118,460</td><td># CNF</td><td>1,826</td></tr><tr><td># Var</td><td>779</td><td>114</td><td>16,299</td><td>25,310</td><td>2,852</td><td># Var</td><td>206,470</td></tr><tr><td># Cla</td><td>281,888</td><td>529</td><td>63,501</td><td>88,978</td><td>61,898</td><td># Cla</td><td>1,218,519</td></tr><tr><td># Backbone Var</td><td>280 (36%)</td><td>58 (51%)</td><td>8,587 (53%)</td><td>1,960 (8%)</td><td>1,009 (35%)</td><td># Backbone Var</td><td>48,266 (23%)</td></tr></table> Table 1: Details of Data Back. For each source, we summarize the number of CNF formulas; the average number of variables and clauses characterizing the size of each CNF formula; and the average number of backbone variables, with their proportion in total variables (indicated in brackets). Overall, for each dataset, we summarize the total number of CNF formulas and the corresponding average counts regarding all formulas in the set. phases within a specific category of CNF formulas. Details about the pre- training and fine- tuning datasets are introduced in Section 5. Binary Cross Entropy (BCE) loss is adopted as our loss function. Besides, Adam W optimizer Loshchilov & Hutter (2017) with a learning rate of \\(10^{- 4}\\) is applied for model pre- training and fine- tuning. The number of epoch is set to 40 for pre- training, and 60 for fine- tuning. It took 48 hours in total to accomplish both the pre- training and fine- tuning on our commodity computer. ### 4.3 APPLYING PHASE PREDICTIONS IN SAT The goal is to leverage phase predictions derived from the GNN model to enhance the phase selection heuristics within CDCL SAT solvers. While numerous ways exist to integrate these neural predictions into CDCL SAT solvers, the most straightforward and generic approach is to initialize the phases of the corresponding variables in CDCL SAT solvers based on the predicted phases. In this paper, we adopted the state- of- the- art solver Kissat Biere & Fleury (2022) to support the phase initialization with Neuro Back predictions. The resulting implementation is called Neuro Back- Kissat. ## 5 DATABACK We created a new dataset, Data Back, comprising sat CNF formulas labeled with phases of backbone variables, for pre- training (PT) and fine- tuning (FT) the Neuro Back model. Accordingly, there are two subsets in Data Back: the pre- training set, Data Back- PT, and the fine- tuning set, Data Back- FT. To get the backbone label, the very recent state- of- the- art backbone extractor called Cadi Back Biere et al. (2023) is utilized. Given that the fine- tuning formulas are typically more challenging to solve than the pre- training ones, label collection timeouts are set as 1,000 seconds for the pre- training formulas and 5,000 seconds for the fine- tuning formulas. Data Back includes formulas solved within the time limit with at least one backbone variable. We observe that there exists a significant label imbalance in both Data Back- PT and Data Back- FT. To tackle this, for each labeled formula, we create a dual formula by negating all backbone variables in the original formula. The labels of the dual formula are the negated phases of the backbone variables in the original formula. Formally, for a formula \\(f\\) with \\(n\\) backbone variables \\(b_{1},\\ldots ,b_{n}\\) , let \\(\\mathcal{L}_{f}:\\{b_{1},\\ldots ,b_{n}\\} \\to \\{1,0\\}\\) denote the mapping of each backbone variable to its phase. The dual formula \\(f^{\\prime}\\) is obtained from \\(f\\) by negating each backbone variable: \\(f^{\\prime} = f[b_{1}\\to\\) \\(\\neg b_{1},\\ldots ,b_{n}\\mapsto \\neg b_{n}]\\) . The dual \\(f^{\\prime}\\) is still satisfiable and retains the same backbone variables as \\(f\\) , but with the opposite phases \\(\\mathcal{L}_{f^{\\prime}}(b_{i}) = \\neg \\mathcal{L}_{f}(b_{i}),i\\in \\{1,\\ldots ,n\\}\\) . For the given CNF formula example in Section 2 \\(\\phi = (v_{1}\\vee \\neg v_{2})\\wedge (v_{2}\\vee v_{3})\\wedge v_{2}\\) , having \\(v_{1}\\) and \\(v_{2}\\) as its backbone variables with phases \\(\\{v_{1},v_{2}\\} \\to \\{1\\}\\) , the dual formula is \\(\\phi^{\\prime} = (\\neg v_{1}\\vee v_{2})\\wedge (\\neg v_{2}\\vee v_{3})\\wedge \\neg v_{2}\\) , still having \\(v_{1}\\) and \\(v_{2}\\) as the backbone variables but with opposite phases \\(\\{v_{1},v_{2}\\} \\to \\{0\\}\\) . This data augmentation strategy doubles the size of Data Back with a perfect balance in positive and negative backbone labels. In the rest of the paper, Data Back- PT and Data Back- FT refer to the augmented, balanced datasets. Data Back- PT The CNF formulas in Data Back- PT are sourced from four origins: 1) CNFgen Lauria et al. (2017), a recent CNF generator renowned for crafting CNF formulas that feature in proof complexity literature; 2) SATLIB Hoos & St\u00fctzle (2000), an online benchmark library housing well- known benchmark instances, serving as a foundational challenge for the SAT community; 3) model counting competitions from 2020 to 2022 Fichte et al. (2021); MCC (2021; 2022), including wide range of CNF formulas for model counters to count their satisfying assignments, which provide a good source for backbone extraction; 4) Random tracks in SAT competitions from 2004 to 2021, offering numerous challenging random SAT problems.\n\n<table><tr><td>metrics model</td><td>precision</td><td>recall</td><td>F1</td><td>accuracy</td></tr><tr><td>pre-trained model</td><td>0.903</td><td>0.766</td><td>0.829</td><td>0.751</td></tr><tr><td>fine-tuned model</td><td>0.941</td><td>0.914</td><td>0.928</td><td>0.887</td></tr></table> Table 2: The performance on the validation set of both pre- trained and fine- tuned Neuro Back models for classifying the phases of backbone variables, in terms of precision, recall, F1 score and accuracy. Table 1 shows the detailed statistics of the resulting dataset. CNFgen generated 10,859 labeled sat formulas (21,718 augmented formulas) grouped by five categories: random SAT, clique- coloring, graph coloring, counting principle, and parity principle. From SATLIB, 40,153 sat formulas with labels (80,306 augmented formulas) were collected, encoding seven kinds of problems: random SAT, graph coloring, planning, bounded model checking, Latin square, circuit fault analysis, and all- interval series. Model counting competitions and SAT competition random tracks contribute 5,780 (11,578 augmented) and 2,438 (4,876 augmented) labeled formulas, respectively. In total, 59,230 (118,460 augmented) formulas are included Data Back- PT. The comparative size of formulas, as indicated by the average number of variables and clauses, reveals that formulas from model counting competitions and SAT competition random tracks are generally larger than those generated by CNFgen, which are in turn larger than those originating from SATLIB. In addition, formulas from model counting competitions and SATLIB exhibit relatively high average backbone proportions. These are followed by formulas from CNFgen, while formulas from the SAT competition's random tracks demonstrate the lowest backbone proportion. Data Back- PT thus contains a diverse set of formulas. Data Back- FT Given that Neuro Back will be tested on large- scale SAT formulas from the main track of recent SAT competitions, the fine- tuning dataset, Data Back- FT, incorporates CNF formulas from the main track of earlier SAT competitions spanning from 2004 to 2021. As shown in Table 1, it contains 913 (1,826 augmented) labeled formulas. While Data Back- FT is considerably smaller in size compared to Data Back- PT, its individual formulas are distinctly larger than those in Data Back- PT. ## 6 EXPERIMENTS Platform All experiments were run on an ordinary commodity computer with one NVIDIA Ge Force RTX 3080 GPU (10GB memory), one AMD Ryzen Threadripper 3970X processor (64 logical cores), and 256GB RAM. Research Questions The experiments aim to answer two research questions: RQ1: How accurately does the Neuro Back model classify the phases of backbone variables? RQ2: How effective is the Neuro Back approach? RQ1: Neuro Back Model Performance The Neuro Back model was pre- trained on the entire Data Back- PT dataset, then fine- tuned on a random \\(90\\%\\) of Data Back- FT samples, and evaluated on the remaining \\(10\\%\\) as a validation set. Table 2 details the performance of both the pre- trained and fine- tuned models in classifying the phases of backbone variables. Notably, the pre- trained model achieved \\(75.1\\%\\) accuracy in classifying backbone variables, with a precision exceeding \\(90\\%\\) and a recall rate of \\(76.7\\%\\) . Considering the distinct data sources of Data Back- PT and Data Back- FT, the results suggest that the pre- training enables the model to extract generalized knowledge about backbone phase prediction. Fine- tuning further augments model performance, with improvements ranging between \\(4\\%\\) and \\(15\\%\\) across all metrics, making precision, recall and F1 score all exceeding \\(90\\%\\) . In conclusion, Neuro Back model effectively learns to predict the phases of backbone variables through both pre- training and fine- tuning. RQ2: Neuro Back Performance To evaluate the solving effectiveness, we collect all 800 CNF formulas from the main track of SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) as our testing dataset. For our baseline solvers, we selected the default configuration of Kissat, named Default- Kissat, which simply sets the initial phase of each variable to true. We implemented an additional baseline solver, Random- Kissat, which randomly assigns the initial phase of each variable as either true or false. Neuro Back- Kissat and its baseline solvers, were applied to all 800 SAT problems in the testing dataset, with the standard solving time limit of 5,000 seconds. Each solver utilized up to 64 different processes in parallel on the dedicated 64- core machine. The model inference for each Neuro Back solver was conducted solely on the CPU, with a memory limit of 10GB to mitigate memory contention issues. Consequently, 308 problems from SATCOMP- 2022 and\n\n<center>Figure 4: Progress of Default-Kissat, Random-Kissat, and Neuro Back-Kissat over time in solving problems (time in seconds) on SATCOMP-2022 (left) and SATCOMP-2023 (right), respectively. Neuro Back-Kissat outperforms the two baseline solvers on both testing sets. </center> 353 problems from SATCOMP- 2023 were successfully inferred. The CPU inference time for each of these problems ranged from 0.3 to 16.5 seconds, averaging at 1.7 seconds. Given that problems unsuccessfully inferred make the Neuro Back solver revert to its baseline, we exclude such problems from our evaluation of how Neuro Back contributes to SAT solving. As a result, for 308 inferred problems in SATCOMP- 2022, Default- Kissat and Random- Kissat solved 193 and 197 problems, respectively. In comparison, Neuro Back- Kissat solved 203 problems, representing an improvement of \\(5.2\\%\\) over Default- Kissat and \\(3.0\\%\\) over Random- Kissat, respectively. For 353 inferred problems in SATCOMP- 2023, Default- Kissat and Random- Kissat successfully solved 198 and 190 problems, respectively. In contrast, Neuro Back- Kissat solved 204 problems, making improvements of \\(3.0\\%\\) and \\(7.4\\%\\) over Default- Kissat and Random- Kissat, respectively. The cactus plot is a commonly used plot in the SAT community for demonstrating the solving, as shown in Fig. 4. Random- Kissat performs better than Default- Kissat in SATCOMP- 2022, but worse in SATCOMP- 2023. In contrast, Neuro Back- Kissat consistently outperforms both baseline solvers. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, reducing solving time by 98 and 246 seconds per problem. For additional results, please refer to the Appendix section. Overall, results suggest that the phase initialization provided by Neuro Back outperforms both the default and random phase initializations in Kissat, exhibiting enhanced proficiency in solving SATCOMP- 2022 and SATCOMP- 2023 problems. ## 7 DISCUSSION AND FUTURE WORK The current implementation of Neuro Back has two main aspects to improve. First, pre- training the GNN model requires a large amount of data. Once trained, however, the model is able to easily generalize to new problem categories, so the investment in putting together a large dataset is warranted. The dataset will also be publicly available to benefit future research. Second, this paper employs neural phase predictions solely as an initial setting for variable phases in SAT solvers. However, multiple methods exist to utilize these predictions in SAT, either in a static or dynamic manner, which are likely to yield further performance enhancements. For instance, phase predictions could be leveraged dynamically during rephasing to adjust or diversify the exploration of the search space. ## 8 CONCLUSION This paper proposes a machine learning approach, Neuro Back, to make CDCL SAT solvers more effective without requiring any GPU resource during its application. The main idea is to make offline model inference on variable phases appearing in the majority of satisfying assignments, for enhancing the phase selection heuristic in CDCL solvers. Incorporated in the state- of- the- art SAT solver, Kissat, this approach significantly reduces the solving time and makes it possible to solve more instances in SATCOMP- 2022 and SATCOMP- 2023. Neuro Back is thus a promising approach to improving the SAT solvers through machine learning."
  },
  "section_objects": [
    {
      "heading": "NeuroBack Improving CDCL SAT Solving using Graph N",
      "content": "## Introduction\n\n\n## 9 ACKNOWLEDGMENT 9 ACKNOWLEDGMENTWe would like to thank the anonymous reviewers for their valuable feedback. This work was supported by a grant from the Army Research Office accomplished under Cooperative Agreement Number W911NF- 19- 2- 0333. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This work was also supported in part by ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA and by the Intel RARE center. ## 10 APPENDIX ## 10.1 ADDITIONAL EXPERIMENTAL RESULTS 10.1 ADDITIONAL EXPERIMENTAL RESULTSThe scatter plots is another commonly used plot in the SAT community for comparing the solving effectiveness of two solvers on each problem. Fig. 5 shows the scatter plots of Neuro Back- Kissat and its two baseline solvers, Default- Kissat and Random- Kissat. It is evident that more dots are present in the lower triangular area, indicating that there are more problems on which Neuro Back- Kissat outperforms both Default- Kissat and Random- Kissat. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, leading to a reduction in solving time of 98 and 246 seconds per problem. ## 10.2 PERFORMANCE ON SOLVED SAT AND UNSAT PROBLEMS Upon detailed analysis, for 661 problems from both SATCOMP- 2022 and SATCOMP- 2023 testing sets, there are 194 unsat problems and 216 sat problems that are solved by either Default- Kissat or Neuro Back- Kissat. For the 194 solved unsat problems, Neuro Back- Kissat outperformed Default- Kissat in 121 cases (62.4%) while Default- Kissat outperformed Neuro Back- Kissat in only 61 problems (31.4%). For the 216 solved sat problems, Neuro Back- Kissat outperformed Default- Kissat in 110 problems (50.9%), while Default- Kissat outperformed Neuro Back- Kissat in 87 problems (40.3%). While Neuro Back- Kissat showed a higher improvement rate in unsat problems compared to sat ones (62.4% vs 50.9%), the extent of improvement was more significant in sat problems. On average, Neuro Back- Kissat enhanced the performance of sat problems by 53.2%, compared to an average improvement of only 14.6% in unsat problems. These trends were similarly observed when comparing Neuro Back- Kissat with Random- Kissat. The experimental results highlight two key aspects. First, they demonstrate that Neuro Back's predicted variable phases can enhance the efficiency in solving unsat problems. Our explanation is that Neuro Back's phase predictions can aid in directing the search towards the unsatisfiable part of the search space. While Neuro Back cannot satisfy all components of a given SAT problem, it may predict phases that satisfy certain components, thereby allowing the solver to concentrate on the unsat part. Furthermore, in modern SAT solvers such as Default- Kissat Biere & Fleury (2020), an assignment that falsifies the fewer clauses is often preferred in the searching loop, allowing the solver to specifically target the unsat portions of the clause set. Consequently, the phases predicted by Neuro Back can facilitate identifying an assignment that reduces clause falsification, thereby enhancing solving unsat problems. Second, the experimental results also show that Neuro Back achieves a more pronounced improvement in solving sat problems than in solving unsat problems. This distinction stems from the inherent nature of these problems. In sat problems, a complete satisfying assignment exists, where each variable is assigned a phase that leads to a solution. Conversely, in unsat problems, only partial satisfying assignments exist, with phases assigned to just a subset of variables. Consequently, the phases predicted by Neuro Back have a generally greater impact in resolving sat problems. This is because, for these problems, the predicted phases can contribute directly to finding a satisfying\n\n<center>Figure 5: Time taken by Default-Kissat vs. Neuro Back-Kissat on SATCOMP-2022 (a), Random-Kissat vs. Neuro Back-Kissat on SATCOMP-2022 (b), Default-Kissat vs. Neuro Back-Kissat on SATCOMP-2023 (c), and Random-Kissat vs. Neuro Back-Kissat on SATCOMP-2023 (d) to solve each test problem in seconds (for problems that are solved by at least one solver). Each problem is represented by a dot whose location indicates the solving time of each method. The dots on the dashed lines at 5,000 seconds indicate failures. It is evident that more dots are present in the lower triangular areas, indicating that there are more problems on which Neuro Back-Kissat outperforms both Default-Kissat and Random-Kissat. </center> assignment. In contrast, for unsat problems, the utility of predicted phases is somewhat restricted to identifying partial solutions or refining the search scope. This fundamental difference in the nature of sat versus unsat problems underpins the varying degrees of effectiveness observed in Neuro Back's performance. ## 11 SETTING UP THE MEMORY LIMIT FOR NEUROBACK-KISSAT In our experimental setup, which includes a machine equipped with 256GB of memory running 64 solver instances in parallel, we have conservatively set the SAT formula size threshold at 135 MB. This ensures that the memory usage of each solver instance does not exceed our specified memory threshold of 10GB. This threshold setting is based on our practical experience. Increasing this threshold could potentially lead to memory contention issues. Users might choose to adjust the\n\nformula size threshold based on their machine's memory capacity. Alternatively, they might simply establish a memory threshold for each solver instance based on their machine's memory capacity and allow model inference to proceed until this threshold is reached, which typically incurs an overhead of no more than a few seconds.\n\n## REFERENCES Model counting competition 2020 url, 2020. https://mccompetition.org/2021/mc_description.html. Model counting competition 2021 url, 2021. https://mccompetition.org/2021/mc_description.html. Model counting competition 2022 url, 2022. https://mccompetition.org/2022/mc_description.html. Sat competition 2022. https://satcompetition.github.io/2022/, 2022. Accessed: 2023- 08- 10. Sat competition 2023. https://satcompetition.github.io/2023/, 2023. Accessed: 2023- 11- 23. Tasniem Al- Yahya, Mohamed El Bachir Abdelkrim Menai, and Hassan Mathkour. Boosting the performance of cdc1- based sat solvers by exploiting backbones and backdoors. Algorithms, 15(9): 302, 2022. Gilles Audemard and Laurent Simon. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27(01):1840001, 2018. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez- Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. ar Xiv preprint ar Xiv:1806.01261, 2018. Armin Biere and Mathias Fleury. Chasing target phases. In Workshop on the Pragmatics of SAT, 2020. Armin Biere and Mathias Fleury. Gimsatul, Isa SAT and Kissat entering the SAT Competition 2022. In Tomas Balyo, Marijn Heule, Markus Iser, Matti J\u00e4rvisalo, and Martin Suda (eds.), Proc. of SAT Competition 2022 - Solver and Benchmark Descriptions, volume B- 2022- 1 of Department of Computer Science Series of Publications B, pp. 10- 11. University of Helsinki, 2022. Armin Biere, Mathias Fleury, and Maximilian Heisinger. Cadical, kissat, paracooba entering the sat competition 2021. 2021. URL https://api.semanticscholar.org/Corpus ID: 238996423. Armin Biere, Nils Froleyks, and Wenxi Wang. Cadiback: Extracting backbones with cadical. In 26th International Conference on Theory and Applications of Satisfiability Testing (SAT 2023). Schloss Dagstuhl- Leibniz- Zentrum f\u00fcr Informatik, 2023. Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the oversmoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 3438- 3445, 2020. Martin Davis, George Logemann, and Donald Loveland. A machine program for theorem- proving. Communications of the ACM, 5(7):394- 397, 1962. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ar Xiv preprint ar Xiv:2010.11929, 2020. Niklas E\u00e9n and Niklas S\u00f6rensson. An extensible sat- solver. In International conference on theory and applications of satisfiability testing, pp. 502- 518. Springer, 2003. Matthias Fey and Jan E. Lenssen. Fast graph representation learning with Py Torch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n\nMatthias Fey and Jan E. Lenssen. conv.gatconv. https://pytorch- geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html, 2023a. Matthias Fey and Jan E. Lenssen. conv.ginconv. https://pytorch- geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html, 2023b. Johannes K Fichte, Markus Hecher, and Florim Hamiti. The model counting competition 2020. Journal of Experimental Algorithmics (JEA), 26:1- 26, 2021. ABKFM Fleury and Maximilian Heisinger. Cadical, kissat, paracooba, plingeling and treengeling entering the sat competition 2020. SAT COMPETITION, 2020:50, 2020. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263- 1272. PMLR, 2017. Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729- 734. IEEE, 2005. Youssef Hamadi, Said Jabbour, and Lakhdar Sais. Manysat: a parallel sat solver. Journal on Satisfiability, Boolean Modeling and Computation, 6(4):245- 262, 2010. William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025- 1035, 2017. Jesse Michael Han. Enhancing sat solvers with glue variable predictions. ar Xiv preprint ar Xiv:2007.02559, 2020. Marijn JH Heule, Matti Juhani J\u00e4rvisalo, Martin Suda, et al. Proceedings of SAT competition 2018: Solver and benchmark descriptions. 2018. Holger H Hoos and Thomas St\u00fctzle. Satlib: An online resource for research on sat. Sat, 2000: 283- 292, 2000. Mikol\u00e1\u0161 Janota. SAT solving in interactive configuration. Ph D thesis, University College Dublin, 2010. Sebastian Jaszczur, Micha\u0142 \u0141uszczyk, and Henryk Michalewski. Neural heuristics for sat solving. ar Xiv preprint ar Xiv:2005.13406, 2020. Philip Kilby, John Slaney, Sylvie Thiebaux, Toby Walsh, et al. Backbones and backdoors in satisfiability. In AAAI, volume 5, pp. 1368- 1373, 2005. Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. ar Xiv preprint ar Xiv:1609.02907, 2016. Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristics with graph networks and reinforcement learning. 2019. Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristic with graph networks and reinforcement learning. In Advances in Neural Information Processing Systems, 2020. Massimo Lauria, Jan Elffers, Jakob Nordstr\u00f6m, and Marc Vinyals. Cnfgcn: A generator of crafted benchmarks. In International Conference on Theory and Applications of Satisfiability Testing, pp. 464- 473. Springer, 2017. Jia Hui Liang, Vijay Ganesh, Pascal Poupart, and Krzysztof Czarnecki. Learning rate based branching heuristic for sat solvers. In Theory and Applications of Satisfiability Testing- SAT 2016: 19th International Conference, Bordeaux, France, July 5- 8, 2016, Proceedings 19, pp. 123- 140. Springer, 2016.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ar Xiv preprint ar Xiv:1711.05101, 2017. Joao Marques- Silva, In\u00eas Lynce, and Sharad Malik. Conflict- driven clause learning sat solvers. In Handbook of Satisfiability: Second Edition. Part 1/Part 2, pp. 133- 182. IOS Press BV, 2021. Joao P Marques- Silva and Karem A Sakallah. Grasp\u2014a new search algorithm for satisfiability. In The Best of ICCAD, pp. 73- 89. Springer, 2003. Ruben Martins, Vasco Manquinho, and In\u00eas Lynce. An overview of parallel sat solving. Constraints, 17:304- 347, 2012. Gr\u00e9goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph structure in transformers. ar Xiv preprint ar Xiv:2106.05667, 2021. Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong. Transformer for graphs: An overview from architecture perspective. ar Xiv preprint ar Xiv:2202.08455, 2022. Matthew W. Moskewicz, Conor F. Madigan, Ying Zhao, Lintao Zhang, and Sharad Malik. Chaff: Engineering an efficient sat solver. In Proceedings of the 38th Annual Design Automation Conference, DAC '01, pp. 530- 535, New York, NY, USA, 2001. Association for Computing Machinery. Piotr Padlewski and Josip Djolonga. Scaling vision transformers to 22 billion parameters. https://ai.googleblog.com/2023/03/scaling- vision- transformers- to- 22. html, 2023. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. ar Xiv preprint ar Xiv:1912.01703, 2019. Knot Pipatsrisawat and Adnan Darwiche. A lightweight component caching scheme for satisfiability solvers. In Theory and Applications of Satisfiability Testing- SAT 2007: 10th International Conference, Lisbon, Portugal, May 28- 31, 2007. Proceedings 10, pp. 294- 299. Springer, 2007. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self- supervised graph transformer on large- scale molecular data. Advances in Neural Information Processing Systems, 33:12559- 12571, 2020. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61- 80, 2008. Dominik Schreiber and Peter Sanders. Scalable sat solving in the cloud. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, Barcelona, Spain, July 5- 9, 2021, Proceedings 24, pp. 518- 534. Springer, 2021. Daniel Selsam and Nikolaj Bjorner. Guiding high- performance SAT solvers with unsat- core predictions. In International Conference on Theory and Applications of Satisfiability Testing, pp. 336- 353. Springer, 2019. Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a SAT solver from single- bit supervision. ar Xiv preprint ar Xiv:1802.03685, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ar Xiv preprint ar Xiv:1710.10903, 2017. Haoze Wu. Improving sat- solving with machine learning. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education, pp. 787- 788, 2017.\n\nZhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion Stoica. Representing long- range context for graph neural networks with global attention. Advances in Neural Information Processing Systems, 34:13266- 13279, 2021. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4- 24, 2020. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? ar Xiv preprint ar Xiv:1810.00826, 2018. Emre Yolcu and Barnabas P\u00f3czos. Learning local search heuristics for boolean satisfiability. In Advances in Neural Information Processing Systems, pp. 7992- 8003, 2019. Ziwei Zhang and Yang Zhang. Elimination mechanism of glue variables for solving sat problems in linguistics. In The Asian Conference on Language, pp. 147- 167, 2021. Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57- 81, 2020. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43- 76, 2020.\n\nWe propose Neuro Back, a novel approach to make CDCL SAT solving more effective and avoid frequent online model inferences, thus making the GNN approach more practical. The main idea of Neuro Back is to make offline model inference, i.e., prior to the solving process, to obtain instructive static information for improving CDCL SAT solving. Once trained, the offline model inference allows Neuro Back to execute solely on the CPU, thereby making it completely independent of GPU resources. In particular, Neuro Back seeks to refine the phase selection heuristics in CDCL solvers by leveraging offline neural predictions on variable phases appearing in the majority (or even all) of the satisfying assignments. The offline predictions on such phase information are based on the generalization of backbone variables, which are variables whose phases remain consistent across all satisfying assignments. Recent work Biere et al. (2021); Al- Yahya et al. (2022) has shown that backbone variables are crucial for enhancing CDCL SAT solving. Choosing the correct phase for a backbone variable prevents conflicts, while an incorrect choice inevitably leads to backtracking in the search. Moreover, predicting the correct phases of non- backbone variables appearing in the majority of satisfying assignments is also important, because such phases prevent backtracking with high probabilities. Our conjecture is that the knowledge learned from predicting the phases of backbone variables can be transferred to predicting the phases of non- backbone variables exhibited in the majority of satisfying assignments. Therefore, Neuro Back applies a GNN model, trained solely on predicting the phases of backbone variables, to predict the phases of all variables. Neuro Back converts the SAT formula with diverse scales into a compact and more learnable graph representation, turning the problem of predicting variable phases into a binary node classification problem. To make the GNN model both compact and robust, Neuro Back employs a novel Graph Transformer architecture with light- weight self- attention mechanisms. To train the model with supervised learning, a balanced dataset called Data Back containing 120,286 labeled formulas with diversity was created from five different sources: CNFgen Lauria et al. (2017), SATLIB Hoos & St\u00fctzle (2000), model counting competitions from 2020 to 2022 MCC (2020; 2021; 2022), and main and random tracks in SAT competitions from 2004 to 2021. To evaluate the effectiveness of our approach, Neuro Back is incorporated into a state- of- the- art CDCL SAT solver called Kissat Biere & Fleury (2022), resulting in a new solver called Neuro Back- Kissat. The experimental results on all SAT problems from SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) show that Neuro Back allows Kissat to solve up to \\(5.2\\%\\) and \\(7.4\\%\\) more problems, respectively. The experiments thus demonstrate that Neuro Back is a practical neural approach to improving CDCL SAT solvers. The contributions of our paper are: 1. Approach. To our knowledge, Neuro Back presents the first practical neural approach to make the CDCL SAT solving more effective, without requiring any GPU resource during its application. 2. Dataset. A new dataset Data Back containing 120,286 data samples is created for backbone phase classification. Data Back is publicly available at https://huggingface.co/datasets/neuroback/Data Back 3. Implementation. Neuro Back is incorporated into a state-of-the-art SAT solver, Kissat. The source code of Neuro Back model and Neuro Back-Kissat is publicly available at https://github.com/wenxiwang/neuroback. ## 2 BACKGROUND This section introduces the SAT problem, CDCL algorithm, phase selection heuristics in CDCL solvers, and basics of GNN and Graph Transformer. Preliminaries of SAT In SAT, a propositional logic formula \\(\\phi\\) is usually encoded in Conjunctive Normal Form (CNF), which is a conjunction \\((\\wedge)\\) of clauses. Each clause is a disjunction \\((\\vee)\\) of literals. A literal is either a variable \\(v\\) , or its complement \\(\\neg v\\) . Each variable can be assigned a logical phase, 1 (true) or 0 (false). A CNF formula has a satisfying assignment if and only if every clause has at least one true literal. For example, a CNF formula \\(\\phi = (v_{1} \\vee \\neg v_{2}) \\wedge (v_{2} \\vee v_{3}) \\wedge v_{2}\\) consists of three clauses \\(v_{1} \\vee \\neg v_{2}\\) , \\(v_{2} \\vee v_{3}\\) and \\(v_{2}\\) ; four literals \\(v_{1}\\) , \\(\\neg v_{2}\\) , \\(v_{2}\\) and \\(v_{3}\\) ; and three variables \\(v_{1}\\) , \\(v_{2}\\) , and \\(v_{3}\\) . One satisfying assignment of \\(\\phi\\) is \\(v_{1} = 1\\) , \\(v_{2} = 1\\) , \\(v_{3} = 0\\) . The goal of a SAT solver is to check if a formula \\(\\phi\\) is sat or unsat. A complete solver either outputs a satisfying assignment for \\(\\phi\\) , or proves that no such assignment exists.\n\nThe backbone of a sat formula is the set of literals that are true in all its satisfying assignments. Thus backbone variables are the variables whose phases remain consistent across all satisfying assignments. In the given CNF formula \\(\\phi\\) , there are two backbone variables, \\(v_{1}\\) and \\(v_{2}\\) , both maintaining a phase of 1 in all satisfying assignments of \\(\\phi\\) . CDCL Algorithm CDCL makes SAT solvers efficient in practice and is one of the main reasons for the widespread use of SAT applications. The general idea of CDCL algorithm is as follows (see Marques- Silva et al. (2021) for details). First, it picks a variable on which to branch and decides a phase to assign to it with heuristics. It then conducts a Boolean propagation based on the decision. In the propagation, if a conflict occurs (i.e., at least one clause is mapped to 0), it performs a conflict analysis; otherwise, it makes a new decision on another selected variable. In the conflict analysis, CDCL first analyzes the decisions and propagations to investigate the reason for the conflict, then extracts the most relevant wrong decisions, undoes them, and adds the reason to its memory as a learned lesson (encoded in a clause called learned clause) in order to avoid making the same mistake in the future. After conflict analysis, the solver backtracks to the earliest decision level where the conflict could be resolved and continues the search from there. The process continues until all variables are assigned a phase (sat), or until it learns the empty clause (unsat). Phase Selection Heuristics As indicated above, CDCL SAT solving mainly relies on two kinds of variable related heuristics: variable branching heuristics and phase selection heuristics, which are orthogonal to each other. Phase Saving Pipatsrisawat & Darwiche (2007) is a prevalent phase selection heuristic in modern CDCL solvers. It returns a variable's last assigned polarity, either through decision or propagation. This heuristic addresses the issue of solvers forgetting prior valid assignments due to non- chronological backtracking. Rephasing Biere & Fleury (2020); Fleury & Heisinger (2020) is more recent phase selection heuristic, proposed to reset or modify saved phases to diversify the exploration of the search space. The state- of- the- art CDCL solver, Kissat Biere & Fleury (2022), incorporates the latest advancements in phase saving and rephrasing heuristics. GNN GNNs Wu et al. (2020); Zhou et al. (2020) are a family of neural network architectures that operate on graphs Gori et al. (2005); Scarselli et al. (2008); Gilmer et al. (2017); Battaglia et al. (2018). Typical GNNs follow a recursive neighborhood aggregation scheme called message passing Gilmer et al. (2017). Formally, the input of a GNN is a graph defined as a tuple \\(G = (V, E, W, H)\\) , where \\(V\\) denotes the set of nodes; \\(E \\subseteq V \\times V\\) denotes the set of edges; \\(W = \\{W_{u,v} | (u, v) \\in E\\}\\) contains the feature vector \\(W_{u,v}\\) of each edge \\((u, v)\\) ; and \\(H = \\{H_v | v \\in V\\}\\) contains the feature vector \\(H_v\\) of each node \\(v\\) . A GNN maps each node to a vector- space embedding by updating the feature vector of the node iteratively based on its neighbors. For each iteration, a message- passing layer \\(\\mathcal{L}\\) takes a graph \\(G = (V, E, W, H)\\) as an input and outputs a graph \\(G' = (V, E, W, H')\\) with updated node feature vectors, i.e., \\(G' = \\mathcal{L}(G)\\) . Classic GNN models Gilmer et al. (2017); Kipf & Welling (2016); Hamilton et al. (2017) usually stack several message- passing layers to realize iterative updating. Prior work on utilizing GNNs to improve CDCL SAT solving will be reviewed in the next section. Graph Transformer Architecture Transformer Vaswani et al. (2017) is a family of neural network architectures for processing sequential data (e.g., text or image), which has recently won great success in nature language processing and computer vision. Central to the transformer is the self- attention mechanism, which calculates attention scores among elements in a sequence, thereby allowing each element to focus on other relevant elements in the sequence for capturing long- range dependencies effectively. Recent research Min et al. (2022); Wu et al. (2021); Mialon et al. (2021); Rong et al. (2020) has shown that combining the transformer with GNN results in competitive performance for graph and node classification tasks, forming the Graph Transformer architecture. Notably, Graph Trans Wu et al. (2021) is a representative model which utilizes a GNN subnet consisting of multiple GNN layers for local structure encoding, followed by a Transformer subnet with global self- attention to capture global dependencies, and finally incorporates a Feed Forward Network (FFN) for classification. The GNN model design in Neuro Back is inspired by Graph Trans. ## 3 RELATED WORK This section presents related work on identifying the backbone and machine learning techniques for improving CDCL SAT solving.\n\n<center>Figure 1: Overview of Neuro Back. First, the input CNF formula is converted into a compact and more learnable graph representation. A trained GNN model is then applied once on the graph before SAT solving begins for phase selection. The SAT solver utilizes phase information in the resulting labeled graph as an initialization to guide its solving process. Thus, with the offline process of making instructive phase predictions, Neuro Back makes the solving more effective and practical. </center> Backbone for CDCL Solvers. Janota proved that identifying the backbone is co- NP complete Janota (2010). Furthermore, Kilby et al. demonstrated that even approximating the backbone is generally intractable Kilby et al. (2005). Wu Wu (2017) applies a logistic regression model to predict the phase of backbone variables to improve a classic CDCL SAT solver called Mini Sat E\u00e9n & S\u00f6rensson (2003). Although the approach correctly predicts the phases of \\(78\\%\\) backbone variables, it fails to make improvements over Mini Sat in solving time. In addition, recent works Biere et al. (2021); Al- Yahya et al. (2022) have been focusing on enhancing CDCL SAT solving by employing heuristic search to partially compute the backbone during the solving process. In contrast, Neuro Back applies GNN to predict the backbone in an offline manner to obtain the phase information of all variables to improve CDCL solving. Machine Learning for CDCL Solvers. Recently, several approaches have been developed to utilize GNNs to facilitate CDCL SAT solving. Neuro SAT Selsam et al. (2018) was the first such framework adapting a neural model into an end- to- end SAT solver, which was not intended as a complete SAT solver. Others Jaszczur et al. (2020); Davis et al. (1962); Kurin et al. (2019); Han (2020); Audemard & Simon (2018); Zhang & Zhang (2021) aim to provide SAT solvers with better branching or phase selection heuristics. These approaches either reduce the number of solving iterations or enhance the solving effectiveness on selected small- scale problems with up to a few thousand variables. However, they do not provide obvious improvements in solving effectiveness for large- scale problems. In contrast, Neuro Core Selsam & Bj\u00f8rner (2019), the most closely related approach to this paper, aims to make the solving more effective especially for large- scale problems as in SAT competitions. It enhances the branching heuristic for CDCL using supervised learning to map unsat problems to unsat core variables (i.e., the variables involved in the unsat core). Based on the dynamically learned clauses during the solving process, Neuro Core performs frequent online model inferences to tune the predictions. However, this online inference is computationally demanding. Neuro Back is distinct from Neuro Core in two main aspects. One, while Neuro Core is designed to refine the branching heuristic in CDCL SAT solvers, Neuro Back is invented to enhance their phase selection heuristics. Two, while Neuro Core extracts dynamic unsat core information from unsat formulas through online model inferences, Neuro Back captures static backbone information from sat formulas using offline model inference. Details of Neuro Back are introduced in the following section. ## 4 NEUROBACK In order to reduce the computational cost of the online model inference and to make CDCL SAT solving more effective, Neuro Back employs offline model predictions on variable phases to enhance the phase selection heuristics in CDCL solvers. Figure 1 shows the overview of Neuro Back. First, it converts the input CNF formula into a compact and more learnable graph representation. Then, a well- designed GNN model trained to predict the phases of backbone variables, is applied on the converted graph representation to infer the phases of all variables. The model inference is performed only once before the SAT solving process. The resulting offline prediction is applied as an initialization for the SAT solving process. Finally, the enhanced SAT solver outputs the satisfiability of the input CNF formula. The key components of Neuro Back including the graph representation of CNF formulas, the GNN- based phase selection, and the phase prediction application in SAT solvers, are illustrated in the subsections below. ### 4.1 GRAPH REPRESENTATION FOR CNF FORMULAS As in recent work Kurin et al. (2020); Yolcu & P\u00f3czos (2019), a SAT formula is represented using a more compact undirected bipartite graph than the one adopted in Neuro Core. Two node types\n\n<center>Figure 3: The architecture of Neuro Back model, consisting of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. </center> represent the variables and clauses, respectively. Each edge connects a variable node to a clause node, representing that the clause contains the variable. Two edge types represent two polarities of a variable appearing in the clause, i.e., the variable itself and its complement. Although the representation is compact, its diameter might be substantial for large- scale SAT formulas, which could result in insufficient message passing during the learning process. To mitigate this issue, we introduce a meta node for each connected component in the graph, with meta edges connecting the meta node to all clause nodes in the component. With the added meta nodes and edges, every variable node not appearing in the same clauses can reach each other through their corresponding clause nodes and the meta node, thereby making the diameter at most four. Figure 2 shows an example of our graph representation for the CNF formula \\((v_{1} \\lor v_{2}) \\land (v_{2} \\lor v_{3}) \\land (v_{3} \\lor v_{4})\\) . It includes one meta node, four variable nodes and three clause nodes \\(c_{1}, c_{2}, c_{3}\\) , representing clauses \\(v_{1} \\lor v_{2}, v_{2} \\lor v_{3}\\) and \\(v_{3} \\lor v_{4}\\) , respectively. Without the added meta node and edges, the longest path in the graph runs from variable node \\(v_{1}\\) to variable node \\(v_{4}\\) , making the diameter six. However, by introducing the meta node and edges, the diameter is reduced to four. Formally, for a graph representation \\(G = (V, E, W, H)\\) of a CNF formula, the edge feature \\(W_{u, v}\\) of each edge \\((u, v)\\) is initialized by its edge type with the value 0 representing the meta edge, the value 1 representing positive polarity, and \\(- 1\\) negative polarity; the node feature \\(H_{v}\\) of each node \\(v\\) is initialized by its node type with 0 representing a meta node, 1 representing a variable node, and \\(- 1\\) representing a clause node. <center>Figure 2: An example graph representation of the CNF formula \\((v_{1} \\lor v_{2}) \\land (v_{2} \\lor v_{3}) \\land (v_{3} \\lor v_{4})\\) . A meta node \\(m\\) is added along with meta edges (represented by dashed lines) connecting to all clause nodes in the connected component, reducing the graph diameter from six to four. </center> ### 4.2 GNN-based PHASE PREDICTION Given that the phases of backbone variables remain consistent across all satisfying assignments, selecting the correct phase for a backbone variable prevents backtracking. Conversely, an incorrect choice inevitably leads to a conflict. Moreover, the proportion of backbone variables is typically significant. For instance, during our data collection from five notable sources (i.e., CNFgen, SATLIB, model counting competitions, SATCOMP random tracks, and SATCOMP main tracks), backbone variables constitute an average of \\(27\\%\\) of the total variables. Therefore, accurately identifying the phases of the backbone variables is crucial for efficiently solving a SAT formula. Furthermore, identifying the phases of the non- backbone variables appearing in the majority of satisfying assignments is also important. Because such phases could prevent backtracking with high probabilities. With the converted graph representation, predicting the phases of all variables, including backbone and non- backbone variables, is a binary node classification problem, which can be addressed by a GNN model. Inspired by transfer learning Zhuang et al. (2020), we first train a GNN model to predict the phases of backbone variables, and then leverage the trained model to predict the phases of all variables. Our key insight is that the knowledge learned from predicting the phases of backbone variables can provide a valuable guidance on predicting the phases of non- backbone variables exhibiting in the majority of satisfying assignments. The following subsections introduce the design, implementation, and training of our GNN model.\n\n#### 4.2.1 GNN MODEL DESIGN The sizes of converted graphs representing practical SAT formulas (usually with millions of variables and clauses) are typically substantial. To enable effective training within the constraints of limited GPU memory, it is essential for our model to be both compact and robust. Our GNN model design is inspired by the robust graph transformer architecture, Graph Trans Wu et al. (2021). However, in our particular SAT application context, Graph Trans exhibits two limitations, both arising from the global self- attention mechanism within its transformer subnet. First, the mechanism does not explicitly integrate the topological graph structure information when determining attention scores. However, such information is essential in characterizing a SAT formula. Second, the global self- attention mechanism computes attention scores for all possible node pairs, leading to quadratic memory complexity with respect to the number of nodes in the graph. This is obviously infeasible for tackling the large- scale SAT formulas in our task. To overcome the limitations, we introduce a novel transformer subnet that both distinctly harnesses topological structure information and significantly enhances memory efficiency. It combines Graph Self- Attention (GSA) and Local Self- Attention (LSA), replacing the original transformer's global self- attention. Instead of computing attention scores for all node pairs as global self- attention, GSA calculates attention scores solely for directly connected node pairs, leveraging information of edges and edge weights. This not only explicitly incorporates the topological structure information of the graph, but also reduces the memory complexity to linear in terms of the number of edges in the graph. Additionally, to further reduce the memory complexity, LSA segments each node embedding into multiple node patches and computes attention scores for each pair of node patches. The results in a linear memory complexity in terms of the number of nodes in the graph. Figure 3 illustrates the design of our GNN model architecture. It consists of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. Within each GNN block, a GNN layer is preceded by a normalization layer, with a skip connection bridging the two. The transformer block is crafted to accelerate training on a significant collection of large- scale graphs. Inspired by the recent vision transformer architecture, Vi T- 22B Padlewski & Djolonga (2023), each transformer block integrates a normalization layer, succeeded by both an FFN layer and a GSA/LSA layer that operate concurrently to optimize training efficiency. #### 4.2.2 IMPLEMENTATION The current implementation of Neuro Back model utilizes GINConv Fey & Lenssen (2023b); Xu et al. (2018) to build the GNN layer, for its proficiency in distinguishing non- isomorphic graph structures. However, GINConv lacks the capability to encode edge weight information. To address this, we employ three GINConv layers, each corresponding to a distinct edge weight in our graph representation. Each GINConv layer exclusively performs message passing for edges with its corresponding weight. The node embeddings from these three GINConv layers are finally aggregated as the output of the GNN layer. GATConv layers Fey & Lenssen (2023a); Velickovic et al. (2017) is utilized to built the GSA transformer block. The patch encoder in the Vi T transformer Dosovitskiy et al. (2020) is applied to construct the LSA transformer block. Layer Norm Ba et al. (2016) is employed as our normalization layer. To avoid potential over- smoothing issues, as instructed in Chen et al. (2020), the number of blocks in the GNN subnet is set to the maximum diameter of the graph representation (i.e., \\(L = 4\\) ). To ensure the accuracy of our model, while taking into account our limited GPU memory, the number of both GSA and LSA blocks are set to three (i.e., \\(M = 3\\) and \\(N = 3\\) ). Additionally, FFNs within the transformer blocks contain no hidden layers, while the final FFN utilized for node classification is structured to include one hidden layer. The model is implemented using Py Torch Paszke et al. (2019) and Py Torch Geometric Fey & Lenssen (2019). #### 4.2.3 MODEL PRE-TRAINING AND FINE-TUNING The Neuro Back model undergoes a two- stage training process. Initially, it is pre- trained on an extensive and diverse dataset gathered from various sources. This pre- training equips it with the fundamental knowledge to classify the backbone variable phases across a broad spectrum of CNF formulas. Subsequently, this pre- trained model is refined or fine- tuned on a smaller, domain- specific dataset. This fine- tuning process enhances the model's proficiency in classifying backbone variable\n\n<table><tr><td>Data Back-PT</td><td>CNFgen</td><td>SATLIB</td><td>MCCOMP</td><td>SATCOMP (random)</td><td>Overall</td><td>Data Back-FT</td><td>SATCOMP (main)</td></tr><tr><td># CNF</td><td>21,718</td><td>80,306</td><td>11,560</td><td>4,876</td><td>118,460</td><td># CNF</td><td>1,826</td></tr><tr><td># Var</td><td>779</td><td>114</td><td>16,299</td><td>25,310</td><td>2,852</td><td># Var</td><td>206,470</td></tr><tr><td># Cla</td><td>281,888</td><td>529</td><td>63,501</td><td>88,978</td><td>61,898</td><td># Cla</td><td>1,218,519</td></tr><tr><td># Backbone Var</td><td>280 (36%)</td><td>58 (51%)</td><td>8,587 (53%)</td><td>1,960 (8%)</td><td>1,009 (35%)</td><td># Backbone Var</td><td>48,266 (23%)</td></tr></table> Table 1: Details of Data Back. For each source, we summarize the number of CNF formulas; the average number of variables and clauses characterizing the size of each CNF formula; and the average number of backbone variables, with their proportion in total variables (indicated in brackets). Overall, for each dataset, we summarize the total number of CNF formulas and the corresponding average counts regarding all formulas in the set. phases within a specific category of CNF formulas. Details about the pre- training and fine- tuning datasets are introduced in Section 5. Binary Cross Entropy (BCE) loss is adopted as our loss function. Besides, Adam W optimizer Loshchilov & Hutter (2017) with a learning rate of \\(10^{- 4}\\) is applied for model pre- training and fine- tuning. The number of epoch is set to 40 for pre- training, and 60 for fine- tuning. It took 48 hours in total to accomplish both the pre- training and fine- tuning on our commodity computer. ### 4.3 APPLYING PHASE PREDICTIONS IN SAT The goal is to leverage phase predictions derived from the GNN model to enhance the phase selection heuristics within CDCL SAT solvers. While numerous ways exist to integrate these neural predictions into CDCL SAT solvers, the most straightforward and generic approach is to initialize the phases of the corresponding variables in CDCL SAT solvers based on the predicted phases. In this paper, we adopted the state- of- the- art solver Kissat Biere & Fleury (2022) to support the phase initialization with Neuro Back predictions. The resulting implementation is called Neuro Back- Kissat. ## 5 DATABACK We created a new dataset, Data Back, comprising sat CNF formulas labeled with phases of backbone variables, for pre- training (PT) and fine- tuning (FT) the Neuro Back model. Accordingly, there are two subsets in Data Back: the pre- training set, Data Back- PT, and the fine- tuning set, Data Back- FT. To get the backbone label, the very recent state- of- the- art backbone extractor called Cadi Back Biere et al. (2023) is utilized. Given that the fine- tuning formulas are typically more challenging to solve than the pre- training ones, label collection timeouts are set as 1,000 seconds for the pre- training formulas and 5,000 seconds for the fine- tuning formulas. Data Back includes formulas solved within the time limit with at least one backbone variable. We observe that there exists a significant label imbalance in both Data Back- PT and Data Back- FT. To tackle this, for each labeled formula, we create a dual formula by negating all backbone variables in the original formula. The labels of the dual formula are the negated phases of the backbone variables in the original formula. Formally, for a formula \\(f\\) with \\(n\\) backbone variables \\(b_{1},\\ldots ,b_{n}\\) , let \\(\\mathcal{L}_{f}:\\{b_{1},\\ldots ,b_{n}\\} \\to \\{1,0\\}\\) denote the mapping of each backbone variable to its phase. The dual formula \\(f^{\\prime}\\) is obtained from \\(f\\) by negating each backbone variable: \\(f^{\\prime} = f[b_{1}\\to\\) \\(\\neg b_{1},\\ldots ,b_{n}\\mapsto \\neg b_{n}]\\) . The dual \\(f^{\\prime}\\) is still satisfiable and retains the same backbone variables as \\(f\\) , but with the opposite phases \\(\\mathcal{L}_{f^{\\prime}}(b_{i}) = \\neg \\mathcal{L}_{f}(b_{i}),i\\in \\{1,\\ldots ,n\\}\\) . For the given CNF formula example in Section 2 \\(\\phi = (v_{1}\\vee \\neg v_{2})\\wedge (v_{2}\\vee v_{3})\\wedge v_{2}\\) , having \\(v_{1}\\) and \\(v_{2}\\) as its backbone variables with phases \\(\\{v_{1},v_{2}\\} \\to \\{1\\}\\) , the dual formula is \\(\\phi^{\\prime} = (\\neg v_{1}\\vee v_{2})\\wedge (\\neg v_{2}\\vee v_{3})\\wedge \\neg v_{2}\\) , still having \\(v_{1}\\) and \\(v_{2}\\) as the backbone variables but with opposite phases \\(\\{v_{1},v_{2}\\} \\to \\{0\\}\\) . This data augmentation strategy doubles the size of Data Back with a perfect balance in positive and negative backbone labels. In the rest of the paper, Data Back- PT and Data Back- FT refer to the augmented, balanced datasets. Data Back- PT The CNF formulas in Data Back- PT are sourced from four origins: 1) CNFgen Lauria et al. (2017), a recent CNF generator renowned for crafting CNF formulas that feature in proof complexity literature; 2) SATLIB Hoos & St\u00fctzle (2000), an online benchmark library housing well- known benchmark instances, serving as a foundational challenge for the SAT community; 3) model counting competitions from 2020 to 2022 Fichte et al. (2021); MCC (2021; 2022), including wide range of CNF formulas for model counters to count their satisfying assignments, which provide a good source for backbone extraction; 4) Random tracks in SAT competitions from 2004 to 2021, offering numerous challenging random SAT problems.\n\n<table><tr><td>metrics model</td><td>precision</td><td>recall</td><td>F1</td><td>accuracy</td></tr><tr><td>pre-trained model</td><td>0.903</td><td>0.766</td><td>0.829</td><td>0.751</td></tr><tr><td>fine-tuned model</td><td>0.941</td><td>0.914</td><td>0.928</td><td>0.887</td></tr></table> Table 2: The performance on the validation set of both pre- trained and fine- tuned Neuro Back models for classifying the phases of backbone variables, in terms of precision, recall, F1 score and accuracy. Table 1 shows the detailed statistics of the resulting dataset. CNFgen generated 10,859 labeled sat formulas (21,718 augmented formulas) grouped by five categories: random SAT, clique- coloring, graph coloring, counting principle, and parity principle. From SATLIB, 40,153 sat formulas with labels (80,306 augmented formulas) were collected, encoding seven kinds of problems: random SAT, graph coloring, planning, bounded model checking, Latin square, circuit fault analysis, and all- interval series. Model counting competitions and SAT competition random tracks contribute 5,780 (11,578 augmented) and 2,438 (4,876 augmented) labeled formulas, respectively. In total, 59,230 (118,460 augmented) formulas are included Data Back- PT. The comparative size of formulas, as indicated by the average number of variables and clauses, reveals that formulas from model counting competitions and SAT competition random tracks are generally larger than those generated by CNFgen, which are in turn larger than those originating from SATLIB. In addition, formulas from model counting competitions and SATLIB exhibit relatively high average backbone proportions. These are followed by formulas from CNFgen, while formulas from the SAT competition's random tracks demonstrate the lowest backbone proportion. Data Back- PT thus contains a diverse set of formulas. Data Back- FT Given that Neuro Back will be tested on large- scale SAT formulas from the main track of recent SAT competitions, the fine- tuning dataset, Data Back- FT, incorporates CNF formulas from the main track of earlier SAT competitions spanning from 2004 to 2021. As shown in Table 1, it contains 913 (1,826 augmented) labeled formulas. While Data Back- FT is considerably smaller in size compared to Data Back- PT, its individual formulas are distinctly larger than those in Data Back- PT. ## 6 EXPERIMENTS Platform All experiments were run on an ordinary commodity computer with one NVIDIA Ge Force RTX 3080 GPU (10GB memory), one AMD Ryzen Threadripper 3970X processor (64 logical cores), and 256GB RAM. Research Questions The experiments aim to answer two research questions: RQ1: How accurately does the Neuro Back model classify the phases of backbone variables? RQ2: How effective is the Neuro Back approach? RQ1: Neuro Back Model Performance The Neuro Back model was pre- trained on the entire Data Back- PT dataset, then fine- tuned on a random \\(90\\%\\) of Data Back- FT samples, and evaluated on the remaining \\(10\\%\\) as a validation set. Table 2 details the performance of both the pre- trained and fine- tuned models in classifying the phases of backbone variables. Notably, the pre- trained model achieved \\(75.1\\%\\) accuracy in classifying backbone variables, with a precision exceeding \\(90\\%\\) and a recall rate of \\(76.7\\%\\) . Considering the distinct data sources of Data Back- PT and Data Back- FT, the results suggest that the pre- training enables the model to extract generalized knowledge about backbone phase prediction. Fine- tuning further augments model performance, with improvements ranging between \\(4\\%\\) and \\(15\\%\\) across all metrics, making precision, recall and F1 score all exceeding \\(90\\%\\) . In conclusion, Neuro Back model effectively learns to predict the phases of backbone variables through both pre- training and fine- tuning. RQ2: Neuro Back Performance To evaluate the solving effectiveness, we collect all 800 CNF formulas from the main track of SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) as our testing dataset. For our baseline solvers, we selected the default configuration of Kissat, named Default- Kissat, which simply sets the initial phase of each variable to true. We implemented an additional baseline solver, Random- Kissat, which randomly assigns the initial phase of each variable as either true or false. Neuro Back- Kissat and its baseline solvers, were applied to all 800 SAT problems in the testing dataset, with the standard solving time limit of 5,000 seconds. Each solver utilized up to 64 different processes in parallel on the dedicated 64- core machine. The model inference for each Neuro Back solver was conducted solely on the CPU, with a memory limit of 10GB to mitigate memory contention issues. Consequently, 308 problems from SATCOMP- 2022 and\n\n<center>Figure 4: Progress of Default-Kissat, Random-Kissat, and Neuro Back-Kissat over time in solving problems (time in seconds) on SATCOMP-2022 (left) and SATCOMP-2023 (right), respectively. Neuro Back-Kissat outperforms the two baseline solvers on both testing sets. </center> 353 problems from SATCOMP- 2023 were successfully inferred. The CPU inference time for each of these problems ranged from 0.3 to 16.5 seconds, averaging at 1.7 seconds. Given that problems unsuccessfully inferred make the Neuro Back solver revert to its baseline, we exclude such problems from our evaluation of how Neuro Back contributes to SAT solving. As a result, for 308 inferred problems in SATCOMP- 2022, Default- Kissat and Random- Kissat solved 193 and 197 problems, respectively. In comparison, Neuro Back- Kissat solved 203 problems, representing an improvement of \\(5.2\\%\\) over Default- Kissat and \\(3.0\\%\\) over Random- Kissat, respectively. For 353 inferred problems in SATCOMP- 2023, Default- Kissat and Random- Kissat successfully solved 198 and 190 problems, respectively. In contrast, Neuro Back- Kissat solved 204 problems, making improvements of \\(3.0\\%\\) and \\(7.4\\%\\) over Default- Kissat and Random- Kissat, respectively. The cactus plot is a commonly used plot in the SAT community for demonstrating the solving, as shown in Fig. 4. Random- Kissat performs better than Default- Kissat in SATCOMP- 2022, but worse in SATCOMP- 2023. In contrast, Neuro Back- Kissat consistently outperforms both baseline solvers. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, reducing solving time by 98 and 246 seconds per problem. For additional results, please refer to the Appendix section. Overall, results suggest that the phase initialization provided by Neuro Back outperforms both the default and random phase initializations in Kissat, exhibiting enhanced proficiency in solving SATCOMP- 2022 and SATCOMP- 2023 problems. ## 7 DISCUSSION AND FUTURE WORK The current implementation of Neuro Back has two main aspects to improve. First, pre- training the GNN model requires a large amount of data. Once trained, however, the model is able to easily generalize to new problem categories, so the investment in putting together a large dataset is warranted. The dataset will also be publicly available to benefit future research. Second, this paper employs neural phase predictions solely as an initial setting for variable phases in SAT solvers. However, multiple methods exist to utilize these predictions in SAT, either in a static or dynamic manner, which are likely to yield further performance enhancements. For instance, phase predictions could be leveraged dynamically during rephasing to adjust or diversify the exploration of the search space. ## 8 CONCLUSION This paper proposes a machine learning approach, Neuro Back, to make CDCL SAT solvers more effective without requiring any GPU resource during its application. The main idea is to make offline model inference on variable phases appearing in the majority of satisfying assignments, for enhancing the phase selection heuristic in CDCL solvers. Incorporated in the state- of- the- art SAT solver, Kissat, this approach significantly reduces the solving time and makes it possible to solve more instances in SATCOMP- 2022 and SATCOMP- 2023. Neuro Back is thus a promising approach to improving the SAT solvers through machine learning.",
      "level": 1,
      "line_start": 1,
      "line_end": 35
    },
    {
      "heading": "Introduction",
      "content": "",
      "level": 2,
      "line_start": 4,
      "line_end": 6
    },
    {
      "heading": "9 ACKNOWLEDGMENT 9 ACKNOWLEDGMENTWe would like to thank the anonymous reviewers for their valuable feedback. This work was supported by a grant from the Army Research Office accomplished under Cooperative Agreement Number W911NF- 19- 2- 0333. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This work was also supported in part by ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA and by the Intel RARE center. ## 10 APPENDIX ## 10.1 ADDITIONAL EXPERIMENTAL RESULTS 10.1 ADDITIONAL EXPERIMENTAL RESULTSThe scatter plots is another commonly used plot in the SAT community for comparing the solving effectiveness of two solvers on each problem. Fig. 5 shows the scatter plots of Neuro Back- Kissat and its two baseline solvers, Default- Kissat and Random- Kissat. It is evident that more dots are present in the lower triangular area, indicating that there are more problems on which Neuro Back- Kissat outperforms both Default- Kissat and Random- Kissat. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, leading to a reduction in solving time of 98 and 246 seconds per problem. ## 10.2 PERFORMANCE ON SOLVED SAT AND UNSAT PROBLEMS Upon detailed analysis, for 661 problems from both SATCOMP- 2022 and SATCOMP- 2023 testing sets, there are 194 unsat problems and 216 sat problems that are solved by either Default- Kissat or Neuro Back- Kissat. For the 194 solved unsat problems, Neuro Back- Kissat outperformed Default- Kissat in 121 cases (62.4%) while Default- Kissat outperformed Neuro Back- Kissat in only 61 problems (31.4%). For the 216 solved sat problems, Neuro Back- Kissat outperformed Default- Kissat in 110 problems (50.9%), while Default- Kissat outperformed Neuro Back- Kissat in 87 problems (40.3%). While Neuro Back- Kissat showed a higher improvement rate in unsat problems compared to sat ones (62.4% vs 50.9%), the extent of improvement was more significant in sat problems. On average, Neuro Back- Kissat enhanced the performance of sat problems by 53.2%, compared to an average improvement of only 14.6% in unsat problems. These trends were similarly observed when comparing Neuro Back- Kissat with Random- Kissat. The experimental results highlight two key aspects. First, they demonstrate that Neuro Back's predicted variable phases can enhance the efficiency in solving unsat problems. Our explanation is that Neuro Back's phase predictions can aid in directing the search towards the unsatisfiable part of the search space. While Neuro Back cannot satisfy all components of a given SAT problem, it may predict phases that satisfy certain components, thereby allowing the solver to concentrate on the unsat part. Furthermore, in modern SAT solvers such as Default- Kissat Biere & Fleury (2020), an assignment that falsifies the fewer clauses is often preferred in the searching loop, allowing the solver to specifically target the unsat portions of the clause set. Consequently, the phases predicted by Neuro Back can facilitate identifying an assignment that reduces clause falsification, thereby enhancing solving unsat problems. Second, the experimental results also show that Neuro Back achieves a more pronounced improvement in solving sat problems than in solving unsat problems. This distinction stems from the inherent nature of these problems. In sat problems, a complete satisfying assignment exists, where each variable is assigned a phase that leads to a solution. Conversely, in unsat problems, only partial satisfying assignments exist, with phases assigned to just a subset of variables. Consequently, the phases predicted by Neuro Back have a generally greater impact in resolving sat problems. This is because, for these problems, the predicted phases can contribute directly to finding a satisfying",
      "content": "<center>Figure 5: Time taken by Default-Kissat vs. Neuro Back-Kissat on SATCOMP-2022 (a), Random-Kissat vs. Neuro Back-Kissat on SATCOMP-2022 (b), Default-Kissat vs. Neuro Back-Kissat on SATCOMP-2023 (c), and Random-Kissat vs. Neuro Back-Kissat on SATCOMP-2023 (d) to solve each test problem in seconds (for problems that are solved by at least one solver). Each problem is represented by a dot whose location indicates the solving time of each method. The dots on the dashed lines at 5,000 seconds indicate failures. It is evident that more dots are present in the lower triangular areas, indicating that there are more problems on which Neuro Back-Kissat outperforms both Default-Kissat and Random-Kissat. </center> assignment. In contrast, for unsat problems, the utility of predicted phases is somewhat restricted to identifying partial solutions or refining the search scope. This fundamental difference in the nature of sat versus unsat problems underpins the varying degrees of effectiveness observed in Neuro Back's performance. ## 11 SETTING UP THE MEMORY LIMIT FOR NEUROBACK-KISSAT In our experimental setup, which includes a machine equipped with 256GB of memory running 64 solver instances in parallel, we have conservatively set the SAT formula size threshold at 135 MB. This ensures that the memory usage of each solver instance does not exceed our specified memory threshold of 10GB. This threshold setting is based on our practical experience. Increasing this threshold could potentially lead to memory contention issues. Users might choose to adjust the\n\nformula size threshold based on their machine's memory capacity. Alternatively, they might simply establish a memory threshold for each solver instance based on their machine's memory capacity and allow model inference to proceed until this threshold is reached, which typically incurs an overhead of no more than a few seconds.",
      "level": 2,
      "line_start": 7,
      "line_end": 12
    },
    {
      "heading": "REFERENCES Model counting competition 2020 url, 2020. https://mccompetition.org/2021/mc_description.html. Model counting competition 2021 url, 2021. https://mccompetition.org/2021/mc_description.html. Model counting competition 2022 url, 2022. https://mccompetition.org/2022/mc_description.html. Sat competition 2022. https://satcompetition.github.io/2022/, 2022. Accessed: 2023- 08- 10. Sat competition 2023. https://satcompetition.github.io/2023/, 2023. Accessed: 2023- 11- 23. Tasniem Al- Yahya, Mohamed El Bachir Abdelkrim Menai, and Hassan Mathkour. Boosting the performance of cdc1- based sat solvers by exploiting backbones and backdoors. Algorithms, 15(9): 302, 2022. Gilles Audemard and Laurent Simon. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27(01):1840001, 2018. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez- Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. ar Xiv preprint ar Xiv:1806.01261, 2018. Armin Biere and Mathias Fleury. Chasing target phases. In Workshop on the Pragmatics of SAT, 2020. Armin Biere and Mathias Fleury. Gimsatul, Isa SAT and Kissat entering the SAT Competition 2022. In Tomas Balyo, Marijn Heule, Markus Iser, Matti J\u00e4rvisalo, and Martin Suda (eds.), Proc. of SAT Competition 2022 - Solver and Benchmark Descriptions, volume B- 2022- 1 of Department of Computer Science Series of Publications B, pp. 10- 11. University of Helsinki, 2022. Armin Biere, Mathias Fleury, and Maximilian Heisinger. Cadical, kissat, paracooba entering the sat competition 2021. 2021. URL https://api.semanticscholar.org/Corpus ID: 238996423. Armin Biere, Nils Froleyks, and Wenxi Wang. Cadiback: Extracting backbones with cadical. In 26th International Conference on Theory and Applications of Satisfiability Testing (SAT 2023). Schloss Dagstuhl- Leibniz- Zentrum f\u00fcr Informatik, 2023. Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the oversmoothing problem for graph neural networks from the topological view. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 3438- 3445, 2020. Martin Davis, George Logemann, and Donald Loveland. A machine program for theorem- proving. Communications of the ACM, 5(7):394- 397, 1962. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ar Xiv preprint ar Xiv:2010.11929, 2020. Niklas E\u00e9n and Niklas S\u00f6rensson. An extensible sat- solver. In International conference on theory and applications of satisfiability testing, pp. 502- 518. Springer, 2003. Matthias Fey and Jan E. Lenssen. Fast graph representation learning with Py Torch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.",
      "content": "Matthias Fey and Jan E. Lenssen. conv.gatconv. https://pytorch- geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html, 2023a. Matthias Fey and Jan E. Lenssen. conv.ginconv. https://pytorch- geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html, 2023b. Johannes K Fichte, Markus Hecher, and Florim Hamiti. The model counting competition 2020. Journal of Experimental Algorithmics (JEA), 26:1- 26, 2021. ABKFM Fleury and Maximilian Heisinger. Cadical, kissat, paracooba, plingeling and treengeling entering the sat competition 2020. SAT COMPETITION, 2020:50, 2020. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263- 1272. PMLR, 2017. Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pp. 729- 734. IEEE, 2005. Youssef Hamadi, Said Jabbour, and Lakhdar Sais. Manysat: a parallel sat solver. Journal on Satisfiability, Boolean Modeling and Computation, 6(4):245- 262, 2010. William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025- 1035, 2017. Jesse Michael Han. Enhancing sat solvers with glue variable predictions. ar Xiv preprint ar Xiv:2007.02559, 2020. Marijn JH Heule, Matti Juhani J\u00e4rvisalo, Martin Suda, et al. Proceedings of SAT competition 2018: Solver and benchmark descriptions. 2018. Holger H Hoos and Thomas St\u00fctzle. Satlib: An online resource for research on sat. Sat, 2000: 283- 292, 2000. Mikol\u00e1\u0161 Janota. SAT solving in interactive configuration. Ph D thesis, University College Dublin, 2010. Sebastian Jaszczur, Micha\u0142 \u0141uszczyk, and Henryk Michalewski. Neural heuristics for sat solving. ar Xiv preprint ar Xiv:2005.13406, 2020. Philip Kilby, John Slaney, Sylvie Thiebaux, Toby Walsh, et al. Backbones and backdoors in satisfiability. In AAAI, volume 5, pp. 1368- 1373, 2005. Thomas N Kipf and Max Welling. Semi- supervised classification with graph convolutional networks. ar Xiv preprint ar Xiv:1609.02907, 2016. Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristics with graph networks and reinforcement learning. 2019. Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristic with graph networks and reinforcement learning. In Advances in Neural Information Processing Systems, 2020. Massimo Lauria, Jan Elffers, Jakob Nordstr\u00f6m, and Marc Vinyals. Cnfgcn: A generator of crafted benchmarks. In International Conference on Theory and Applications of Satisfiability Testing, pp. 464- 473. Springer, 2017. Jia Hui Liang, Vijay Ganesh, Pascal Poupart, and Krzysztof Czarnecki. Learning rate based branching heuristic for sat solvers. In Theory and Applications of Satisfiability Testing- SAT 2016: 19th International Conference, Bordeaux, France, July 5- 8, 2016, Proceedings 19, pp. 123- 140. Springer, 2016.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ar Xiv preprint ar Xiv:1711.05101, 2017. Joao Marques- Silva, In\u00eas Lynce, and Sharad Malik. Conflict- driven clause learning sat solvers. In Handbook of Satisfiability: Second Edition. Part 1/Part 2, pp. 133- 182. IOS Press BV, 2021. Joao P Marques- Silva and Karem A Sakallah. Grasp\u2014a new search algorithm for satisfiability. In The Best of ICCAD, pp. 73- 89. Springer, 2003. Ruben Martins, Vasco Manquinho, and In\u00eas Lynce. An overview of parallel sat solving. Constraints, 17:304- 347, 2012. Gr\u00e9goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph structure in transformers. ar Xiv preprint ar Xiv:2106.05667, 2021. Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong. Transformer for graphs: An overview from architecture perspective. ar Xiv preprint ar Xiv:2202.08455, 2022. Matthew W. Moskewicz, Conor F. Madigan, Ying Zhao, Lintao Zhang, and Sharad Malik. Chaff: Engineering an efficient sat solver. In Proceedings of the 38th Annual Design Automation Conference, DAC '01, pp. 530- 535, New York, NY, USA, 2001. Association for Computing Machinery. Piotr Padlewski and Josip Djolonga. Scaling vision transformers to 22 billion parameters. https://ai.googleblog.com/2023/03/scaling- vision- transformers- to- 22. html, 2023. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. ar Xiv preprint ar Xiv:1912.01703, 2019. Knot Pipatsrisawat and Adnan Darwiche. A lightweight component caching scheme for satisfiability solvers. In Theory and Applications of Satisfiability Testing- SAT 2007: 10th International Conference, Lisbon, Portugal, May 28- 31, 2007. Proceedings 10, pp. 294- 299. Springer, 2007. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self- supervised graph transformer on large- scale molecular data. Advances in Neural Information Processing Systems, 33:12559- 12571, 2020. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61- 80, 2008. Dominik Schreiber and Peter Sanders. Scalable sat solving in the cloud. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, Barcelona, Spain, July 5- 9, 2021, Proceedings 24, pp. 518- 534. Springer, 2021. Daniel Selsam and Nikolaj Bjorner. Guiding high- performance SAT solvers with unsat- core predictions. In International Conference on Theory and Applications of Satisfiability Testing, pp. 336- 353. Springer, 2019. Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a SAT solver from single- bit supervision. ar Xiv preprint ar Xiv:1802.03685, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. ar Xiv preprint ar Xiv:1710.10903, 2017. Haoze Wu. Improving sat- solving with machine learning. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education, pp. 787- 788, 2017.\n\nZhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion Stoica. Representing long- range context for graph neural networks with global attention. Advances in Neural Information Processing Systems, 34:13266- 13279, 2021. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4- 24, 2020. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? ar Xiv preprint ar Xiv:1810.00826, 2018. Emre Yolcu and Barnabas P\u00f3czos. Learning local search heuristics for boolean satisfiability. In Advances in Neural Information Processing Systems, pp. 7992- 8003, 2019. Ziwei Zhang and Yang Zhang. Elimination mechanism of glue variables for solving sat problems in linguistics. In The Asian Conference on Language, pp. 147- 167, 2021. Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57- 81, 2020. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43- 76, 2020.\n\nWe propose Neuro Back, a novel approach to make CDCL SAT solving more effective and avoid frequent online model inferences, thus making the GNN approach more practical. The main idea of Neuro Back is to make offline model inference, i.e., prior to the solving process, to obtain instructive static information for improving CDCL SAT solving. Once trained, the offline model inference allows Neuro Back to execute solely on the CPU, thereby making it completely independent of GPU resources. In particular, Neuro Back seeks to refine the phase selection heuristics in CDCL solvers by leveraging offline neural predictions on variable phases appearing in the majority (or even all) of the satisfying assignments. The offline predictions on such phase information are based on the generalization of backbone variables, which are variables whose phases remain consistent across all satisfying assignments. Recent work Biere et al. (2021); Al- Yahya et al. (2022) has shown that backbone variables are crucial for enhancing CDCL SAT solving. Choosing the correct phase for a backbone variable prevents conflicts, while an incorrect choice inevitably leads to backtracking in the search. Moreover, predicting the correct phases of non- backbone variables appearing in the majority of satisfying assignments is also important, because such phases prevent backtracking with high probabilities. Our conjecture is that the knowledge learned from predicting the phases of backbone variables can be transferred to predicting the phases of non- backbone variables exhibited in the majority of satisfying assignments. Therefore, Neuro Back applies a GNN model, trained solely on predicting the phases of backbone variables, to predict the phases of all variables. Neuro Back converts the SAT formula with diverse scales into a compact and more learnable graph representation, turning the problem of predicting variable phases into a binary node classification problem. To make the GNN model both compact and robust, Neuro Back employs a novel Graph Transformer architecture with light- weight self- attention mechanisms. To train the model with supervised learning, a balanced dataset called Data Back containing 120,286 labeled formulas with diversity was created from five different sources: CNFgen Lauria et al. (2017), SATLIB Hoos & St\u00fctzle (2000), model counting competitions from 2020 to 2022 MCC (2020; 2021; 2022), and main and random tracks in SAT competitions from 2004 to 2021. To evaluate the effectiveness of our approach, Neuro Back is incorporated into a state- of- the- art CDCL SAT solver called Kissat Biere & Fleury (2022), resulting in a new solver called Neuro Back- Kissat. The experimental results on all SAT problems from SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) show that Neuro Back allows Kissat to solve up to \\(5.2\\%\\) and \\(7.4\\%\\) more problems, respectively. The experiments thus demonstrate that Neuro Back is a practical neural approach to improving CDCL SAT solvers. The contributions of our paper are: 1. Approach. To our knowledge, Neuro Back presents the first practical neural approach to make the CDCL SAT solving more effective, without requiring any GPU resource during its application. 2. Dataset. A new dataset Data Back containing 120,286 data samples is created for backbone phase classification. Data Back is publicly available at https://huggingface.co/datasets/neuroback/Data Back 3. Implementation. Neuro Back is incorporated into a state-of-the-art SAT solver, Kissat. The source code of Neuro Back model and Neuro Back-Kissat is publicly available at https://github.com/wenxiwang/neuroback. ## 2 BACKGROUND This section introduces the SAT problem, CDCL algorithm, phase selection heuristics in CDCL solvers, and basics of GNN and Graph Transformer. Preliminaries of SAT In SAT, a propositional logic formula \\(\\phi\\) is usually encoded in Conjunctive Normal Form (CNF), which is a conjunction \\((\\wedge)\\) of clauses. Each clause is a disjunction \\((\\vee)\\) of literals. A literal is either a variable \\(v\\) , or its complement \\(\\neg v\\) . Each variable can be assigned a logical phase, 1 (true) or 0 (false). A CNF formula has a satisfying assignment if and only if every clause has at least one true literal. For example, a CNF formula \\(\\phi = (v_{1} \\vee \\neg v_{2}) \\wedge (v_{2} \\vee v_{3}) \\wedge v_{2}\\) consists of three clauses \\(v_{1} \\vee \\neg v_{2}\\) , \\(v_{2} \\vee v_{3}\\) and \\(v_{2}\\) ; four literals \\(v_{1}\\) , \\(\\neg v_{2}\\) , \\(v_{2}\\) and \\(v_{3}\\) ; and three variables \\(v_{1}\\) , \\(v_{2}\\) , and \\(v_{3}\\) . One satisfying assignment of \\(\\phi\\) is \\(v_{1} = 1\\) , \\(v_{2} = 1\\) , \\(v_{3} = 0\\) . The goal of a SAT solver is to check if a formula \\(\\phi\\) is sat or unsat. A complete solver either outputs a satisfying assignment for \\(\\phi\\) , or proves that no such assignment exists.\n\nThe backbone of a sat formula is the set of literals that are true in all its satisfying assignments. Thus backbone variables are the variables whose phases remain consistent across all satisfying assignments. In the given CNF formula \\(\\phi\\) , there are two backbone variables, \\(v_{1}\\) and \\(v_{2}\\) , both maintaining a phase of 1 in all satisfying assignments of \\(\\phi\\) . CDCL Algorithm CDCL makes SAT solvers efficient in practice and is one of the main reasons for the widespread use of SAT applications. The general idea of CDCL algorithm is as follows (see Marques- Silva et al. (2021) for details). First, it picks a variable on which to branch and decides a phase to assign to it with heuristics. It then conducts a Boolean propagation based on the decision. In the propagation, if a conflict occurs (i.e., at least one clause is mapped to 0), it performs a conflict analysis; otherwise, it makes a new decision on another selected variable. In the conflict analysis, CDCL first analyzes the decisions and propagations to investigate the reason for the conflict, then extracts the most relevant wrong decisions, undoes them, and adds the reason to its memory as a learned lesson (encoded in a clause called learned clause) in order to avoid making the same mistake in the future. After conflict analysis, the solver backtracks to the earliest decision level where the conflict could be resolved and continues the search from there. The process continues until all variables are assigned a phase (sat), or until it learns the empty clause (unsat). Phase Selection Heuristics As indicated above, CDCL SAT solving mainly relies on two kinds of variable related heuristics: variable branching heuristics and phase selection heuristics, which are orthogonal to each other. Phase Saving Pipatsrisawat & Darwiche (2007) is a prevalent phase selection heuristic in modern CDCL solvers. It returns a variable's last assigned polarity, either through decision or propagation. This heuristic addresses the issue of solvers forgetting prior valid assignments due to non- chronological backtracking. Rephasing Biere & Fleury (2020); Fleury & Heisinger (2020) is more recent phase selection heuristic, proposed to reset or modify saved phases to diversify the exploration of the search space. The state- of- the- art CDCL solver, Kissat Biere & Fleury (2022), incorporates the latest advancements in phase saving and rephrasing heuristics. GNN GNNs Wu et al. (2020); Zhou et al. (2020) are a family of neural network architectures that operate on graphs Gori et al. (2005); Scarselli et al. (2008); Gilmer et al. (2017); Battaglia et al. (2018). Typical GNNs follow a recursive neighborhood aggregation scheme called message passing Gilmer et al. (2017). Formally, the input of a GNN is a graph defined as a tuple \\(G = (V, E, W, H)\\) , where \\(V\\) denotes the set of nodes; \\(E \\subseteq V \\times V\\) denotes the set of edges; \\(W = \\{W_{u,v} | (u, v) \\in E\\}\\) contains the feature vector \\(W_{u,v}\\) of each edge \\((u, v)\\) ; and \\(H = \\{H_v | v \\in V\\}\\) contains the feature vector \\(H_v\\) of each node \\(v\\) . A GNN maps each node to a vector- space embedding by updating the feature vector of the node iteratively based on its neighbors. For each iteration, a message- passing layer \\(\\mathcal{L}\\) takes a graph \\(G = (V, E, W, H)\\) as an input and outputs a graph \\(G' = (V, E, W, H')\\) with updated node feature vectors, i.e., \\(G' = \\mathcal{L}(G)\\) . Classic GNN models Gilmer et al. (2017); Kipf & Welling (2016); Hamilton et al. (2017) usually stack several message- passing layers to realize iterative updating. Prior work on utilizing GNNs to improve CDCL SAT solving will be reviewed in the next section. Graph Transformer Architecture Transformer Vaswani et al. (2017) is a family of neural network architectures for processing sequential data (e.g., text or image), which has recently won great success in nature language processing and computer vision. Central to the transformer is the self- attention mechanism, which calculates attention scores among elements in a sequence, thereby allowing each element to focus on other relevant elements in the sequence for capturing long- range dependencies effectively. Recent research Min et al. (2022); Wu et al. (2021); Mialon et al. (2021); Rong et al. (2020) has shown that combining the transformer with GNN results in competitive performance for graph and node classification tasks, forming the Graph Transformer architecture. Notably, Graph Trans Wu et al. (2021) is a representative model which utilizes a GNN subnet consisting of multiple GNN layers for local structure encoding, followed by a Transformer subnet with global self- attention to capture global dependencies, and finally incorporates a Feed Forward Network (FFN) for classification. The GNN model design in Neuro Back is inspired by Graph Trans. ## 3 RELATED WORK This section presents related work on identifying the backbone and machine learning techniques for improving CDCL SAT solving.\n\n<center>Figure 1: Overview of Neuro Back. First, the input CNF formula is converted into a compact and more learnable graph representation. A trained GNN model is then applied once on the graph before SAT solving begins for phase selection. The SAT solver utilizes phase information in the resulting labeled graph as an initialization to guide its solving process. Thus, with the offline process of making instructive phase predictions, Neuro Back makes the solving more effective and practical. </center> Backbone for CDCL Solvers. Janota proved that identifying the backbone is co- NP complete Janota (2010). Furthermore, Kilby et al. demonstrated that even approximating the backbone is generally intractable Kilby et al. (2005). Wu Wu (2017) applies a logistic regression model to predict the phase of backbone variables to improve a classic CDCL SAT solver called Mini Sat E\u00e9n & S\u00f6rensson (2003). Although the approach correctly predicts the phases of \\(78\\%\\) backbone variables, it fails to make improvements over Mini Sat in solving time. In addition, recent works Biere et al. (2021); Al- Yahya et al. (2022) have been focusing on enhancing CDCL SAT solving by employing heuristic search to partially compute the backbone during the solving process. In contrast, Neuro Back applies GNN to predict the backbone in an offline manner to obtain the phase information of all variables to improve CDCL solving. Machine Learning for CDCL Solvers. Recently, several approaches have been developed to utilize GNNs to facilitate CDCL SAT solving. Neuro SAT Selsam et al. (2018) was the first such framework adapting a neural model into an end- to- end SAT solver, which was not intended as a complete SAT solver. Others Jaszczur et al. (2020); Davis et al. (1962); Kurin et al. (2019); Han (2020); Audemard & Simon (2018); Zhang & Zhang (2021) aim to provide SAT solvers with better branching or phase selection heuristics. These approaches either reduce the number of solving iterations or enhance the solving effectiveness on selected small- scale problems with up to a few thousand variables. However, they do not provide obvious improvements in solving effectiveness for large- scale problems. In contrast, Neuro Core Selsam & Bj\u00f8rner (2019), the most closely related approach to this paper, aims to make the solving more effective especially for large- scale problems as in SAT competitions. It enhances the branching heuristic for CDCL using supervised learning to map unsat problems to unsat core variables (i.e., the variables involved in the unsat core). Based on the dynamically learned clauses during the solving process, Neuro Core performs frequent online model inferences to tune the predictions. However, this online inference is computationally demanding. Neuro Back is distinct from Neuro Core in two main aspects. One, while Neuro Core is designed to refine the branching heuristic in CDCL SAT solvers, Neuro Back is invented to enhance their phase selection heuristics. Two, while Neuro Core extracts dynamic unsat core information from unsat formulas through online model inferences, Neuro Back captures static backbone information from sat formulas using offline model inference. Details of Neuro Back are introduced in the following section. ## 4 NEUROBACK In order to reduce the computational cost of the online model inference and to make CDCL SAT solving more effective, Neuro Back employs offline model predictions on variable phases to enhance the phase selection heuristics in CDCL solvers. Figure 1 shows the overview of Neuro Back. First, it converts the input CNF formula into a compact and more learnable graph representation. Then, a well- designed GNN model trained to predict the phases of backbone variables, is applied on the converted graph representation to infer the phases of all variables. The model inference is performed only once before the SAT solving process. The resulting offline prediction is applied as an initialization for the SAT solving process. Finally, the enhanced SAT solver outputs the satisfiability of the input CNF formula. The key components of Neuro Back including the graph representation of CNF formulas, the GNN- based phase selection, and the phase prediction application in SAT solvers, are illustrated in the subsections below. ### 4.1 GRAPH REPRESENTATION FOR CNF FORMULAS As in recent work Kurin et al. (2020); Yolcu & P\u00f3czos (2019), a SAT formula is represented using a more compact undirected bipartite graph than the one adopted in Neuro Core. Two node types\n\n<center>Figure 3: The architecture of Neuro Back model, consisting of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. </center> represent the variables and clauses, respectively. Each edge connects a variable node to a clause node, representing that the clause contains the variable. Two edge types represent two polarities of a variable appearing in the clause, i.e., the variable itself and its complement. Although the representation is compact, its diameter might be substantial for large- scale SAT formulas, which could result in insufficient message passing during the learning process. To mitigate this issue, we introduce a meta node for each connected component in the graph, with meta edges connecting the meta node to all clause nodes in the component. With the added meta nodes and edges, every variable node not appearing in the same clauses can reach each other through their corresponding clause nodes and the meta node, thereby making the diameter at most four. Figure 2 shows an example of our graph representation for the CNF formula \\((v_{1} \\lor v_{2}) \\land (v_{2} \\lor v_{3}) \\land (v_{3} \\lor v_{4})\\) . It includes one meta node, four variable nodes and three clause nodes \\(c_{1}, c_{2}, c_{3}\\) , representing clauses \\(v_{1} \\lor v_{2}, v_{2} \\lor v_{3}\\) and \\(v_{3} \\lor v_{4}\\) , respectively. Without the added meta node and edges, the longest path in the graph runs from variable node \\(v_{1}\\) to variable node \\(v_{4}\\) , making the diameter six. However, by introducing the meta node and edges, the diameter is reduced to four. Formally, for a graph representation \\(G = (V, E, W, H)\\) of a CNF formula, the edge feature \\(W_{u, v}\\) of each edge \\((u, v)\\) is initialized by its edge type with the value 0 representing the meta edge, the value 1 representing positive polarity, and \\(- 1\\) negative polarity; the node feature \\(H_{v}\\) of each node \\(v\\) is initialized by its node type with 0 representing a meta node, 1 representing a variable node, and \\(- 1\\) representing a clause node. <center>Figure 2: An example graph representation of the CNF formula \\((v_{1} \\lor v_{2}) \\land (v_{2} \\lor v_{3}) \\land (v_{3} \\lor v_{4})\\) . A meta node \\(m\\) is added along with meta edges (represented by dashed lines) connecting to all clause nodes in the connected component, reducing the graph diameter from six to four. </center> ### 4.2 GNN-based PHASE PREDICTION Given that the phases of backbone variables remain consistent across all satisfying assignments, selecting the correct phase for a backbone variable prevents backtracking. Conversely, an incorrect choice inevitably leads to a conflict. Moreover, the proportion of backbone variables is typically significant. For instance, during our data collection from five notable sources (i.e., CNFgen, SATLIB, model counting competitions, SATCOMP random tracks, and SATCOMP main tracks), backbone variables constitute an average of \\(27\\%\\) of the total variables. Therefore, accurately identifying the phases of the backbone variables is crucial for efficiently solving a SAT formula. Furthermore, identifying the phases of the non- backbone variables appearing in the majority of satisfying assignments is also important. Because such phases could prevent backtracking with high probabilities. With the converted graph representation, predicting the phases of all variables, including backbone and non- backbone variables, is a binary node classification problem, which can be addressed by a GNN model. Inspired by transfer learning Zhuang et al. (2020), we first train a GNN model to predict the phases of backbone variables, and then leverage the trained model to predict the phases of all variables. Our key insight is that the knowledge learned from predicting the phases of backbone variables can provide a valuable guidance on predicting the phases of non- backbone variables exhibiting in the majority of satisfying assignments. The following subsections introduce the design, implementation, and training of our GNN model.\n\n#### 4.2.1 GNN MODEL DESIGN The sizes of converted graphs representing practical SAT formulas (usually with millions of variables and clauses) are typically substantial. To enable effective training within the constraints of limited GPU memory, it is essential for our model to be both compact and robust. Our GNN model design is inspired by the robust graph transformer architecture, Graph Trans Wu et al. (2021). However, in our particular SAT application context, Graph Trans exhibits two limitations, both arising from the global self- attention mechanism within its transformer subnet. First, the mechanism does not explicitly integrate the topological graph structure information when determining attention scores. However, such information is essential in characterizing a SAT formula. Second, the global self- attention mechanism computes attention scores for all possible node pairs, leading to quadratic memory complexity with respect to the number of nodes in the graph. This is obviously infeasible for tackling the large- scale SAT formulas in our task. To overcome the limitations, we introduce a novel transformer subnet that both distinctly harnesses topological structure information and significantly enhances memory efficiency. It combines Graph Self- Attention (GSA) and Local Self- Attention (LSA), replacing the original transformer's global self- attention. Instead of computing attention scores for all node pairs as global self- attention, GSA calculates attention scores solely for directly connected node pairs, leveraging information of edges and edge weights. This not only explicitly incorporates the topological structure information of the graph, but also reduces the memory complexity to linear in terms of the number of edges in the graph. Additionally, to further reduce the memory complexity, LSA segments each node embedding into multiple node patches and computes attention scores for each pair of node patches. The results in a linear memory complexity in terms of the number of nodes in the graph. Figure 3 illustrates the design of our GNN model architecture. It consists of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. Within each GNN block, a GNN layer is preceded by a normalization layer, with a skip connection bridging the two. The transformer block is crafted to accelerate training on a significant collection of large- scale graphs. Inspired by the recent vision transformer architecture, Vi T- 22B Padlewski & Djolonga (2023), each transformer block integrates a normalization layer, succeeded by both an FFN layer and a GSA/LSA layer that operate concurrently to optimize training efficiency. #### 4.2.2 IMPLEMENTATION The current implementation of Neuro Back model utilizes GINConv Fey & Lenssen (2023b); Xu et al. (2018) to build the GNN layer, for its proficiency in distinguishing non- isomorphic graph structures. However, GINConv lacks the capability to encode edge weight information. To address this, we employ three GINConv layers, each corresponding to a distinct edge weight in our graph representation. Each GINConv layer exclusively performs message passing for edges with its corresponding weight. The node embeddings from these three GINConv layers are finally aggregated as the output of the GNN layer. GATConv layers Fey & Lenssen (2023a); Velickovic et al. (2017) is utilized to built the GSA transformer block. The patch encoder in the Vi T transformer Dosovitskiy et al. (2020) is applied to construct the LSA transformer block. Layer Norm Ba et al. (2016) is employed as our normalization layer. To avoid potential over- smoothing issues, as instructed in Chen et al. (2020), the number of blocks in the GNN subnet is set to the maximum diameter of the graph representation (i.e., \\(L = 4\\) ). To ensure the accuracy of our model, while taking into account our limited GPU memory, the number of both GSA and LSA blocks are set to three (i.e., \\(M = 3\\) and \\(N = 3\\) ). Additionally, FFNs within the transformer blocks contain no hidden layers, while the final FFN utilized for node classification is structured to include one hidden layer. The model is implemented using Py Torch Paszke et al. (2019) and Py Torch Geometric Fey & Lenssen (2019). #### 4.2.3 MODEL PRE-TRAINING AND FINE-TUNING The Neuro Back model undergoes a two- stage training process. Initially, it is pre- trained on an extensive and diverse dataset gathered from various sources. This pre- training equips it with the fundamental knowledge to classify the backbone variable phases across a broad spectrum of CNF formulas. Subsequently, this pre- trained model is refined or fine- tuned on a smaller, domain- specific dataset. This fine- tuning process enhances the model's proficiency in classifying backbone variable\n\n<table><tr><td>Data Back-PT</td><td>CNFgen</td><td>SATLIB</td><td>MCCOMP</td><td>SATCOMP (random)</td><td>Overall</td><td>Data Back-FT</td><td>SATCOMP (main)</td></tr><tr><td># CNF</td><td>21,718</td><td>80,306</td><td>11,560</td><td>4,876</td><td>118,460</td><td># CNF</td><td>1,826</td></tr><tr><td># Var</td><td>779</td><td>114</td><td>16,299</td><td>25,310</td><td>2,852</td><td># Var</td><td>206,470</td></tr><tr><td># Cla</td><td>281,888</td><td>529</td><td>63,501</td><td>88,978</td><td>61,898</td><td># Cla</td><td>1,218,519</td></tr><tr><td># Backbone Var</td><td>280 (36%)</td><td>58 (51%)</td><td>8,587 (53%)</td><td>1,960 (8%)</td><td>1,009 (35%)</td><td># Backbone Var</td><td>48,266 (23%)</td></tr></table> Table 1: Details of Data Back. For each source, we summarize the number of CNF formulas; the average number of variables and clauses characterizing the size of each CNF formula; and the average number of backbone variables, with their proportion in total variables (indicated in brackets). Overall, for each dataset, we summarize the total number of CNF formulas and the corresponding average counts regarding all formulas in the set. phases within a specific category of CNF formulas. Details about the pre- training and fine- tuning datasets are introduced in Section 5. Binary Cross Entropy (BCE) loss is adopted as our loss function. Besides, Adam W optimizer Loshchilov & Hutter (2017) with a learning rate of \\(10^{- 4}\\) is applied for model pre- training and fine- tuning. The number of epoch is set to 40 for pre- training, and 60 for fine- tuning. It took 48 hours in total to accomplish both the pre- training and fine- tuning on our commodity computer. ### 4.3 APPLYING PHASE PREDICTIONS IN SAT The goal is to leverage phase predictions derived from the GNN model to enhance the phase selection heuristics within CDCL SAT solvers. While numerous ways exist to integrate these neural predictions into CDCL SAT solvers, the most straightforward and generic approach is to initialize the phases of the corresponding variables in CDCL SAT solvers based on the predicted phases. In this paper, we adopted the state- of- the- art solver Kissat Biere & Fleury (2022) to support the phase initialization with Neuro Back predictions. The resulting implementation is called Neuro Back- Kissat. ## 5 DATABACK We created a new dataset, Data Back, comprising sat CNF formulas labeled with phases of backbone variables, for pre- training (PT) and fine- tuning (FT) the Neuro Back model. Accordingly, there are two subsets in Data Back: the pre- training set, Data Back- PT, and the fine- tuning set, Data Back- FT. To get the backbone label, the very recent state- of- the- art backbone extractor called Cadi Back Biere et al. (2023) is utilized. Given that the fine- tuning formulas are typically more challenging to solve than the pre- training ones, label collection timeouts are set as 1,000 seconds for the pre- training formulas and 5,000 seconds for the fine- tuning formulas. Data Back includes formulas solved within the time limit with at least one backbone variable. We observe that there exists a significant label imbalance in both Data Back- PT and Data Back- FT. To tackle this, for each labeled formula, we create a dual formula by negating all backbone variables in the original formula. The labels of the dual formula are the negated phases of the backbone variables in the original formula. Formally, for a formula \\(f\\) with \\(n\\) backbone variables \\(b_{1},\\ldots ,b_{n}\\) , let \\(\\mathcal{L}_{f}:\\{b_{1},\\ldots ,b_{n}\\} \\to \\{1,0\\}\\) denote the mapping of each backbone variable to its phase. The dual formula \\(f^{\\prime}\\) is obtained from \\(f\\) by negating each backbone variable: \\(f^{\\prime} = f[b_{1}\\to\\) \\(\\neg b_{1},\\ldots ,b_{n}\\mapsto \\neg b_{n}]\\) . The dual \\(f^{\\prime}\\) is still satisfiable and retains the same backbone variables as \\(f\\) , but with the opposite phases \\(\\mathcal{L}_{f^{\\prime}}(b_{i}) = \\neg \\mathcal{L}_{f}(b_{i}),i\\in \\{1,\\ldots ,n\\}\\) . For the given CNF formula example in Section 2 \\(\\phi = (v_{1}\\vee \\neg v_{2})\\wedge (v_{2}\\vee v_{3})\\wedge v_{2}\\) , having \\(v_{1}\\) and \\(v_{2}\\) as its backbone variables with phases \\(\\{v_{1},v_{2}\\} \\to \\{1\\}\\) , the dual formula is \\(\\phi^{\\prime} = (\\neg v_{1}\\vee v_{2})\\wedge (\\neg v_{2}\\vee v_{3})\\wedge \\neg v_{2}\\) , still having \\(v_{1}\\) and \\(v_{2}\\) as the backbone variables but with opposite phases \\(\\{v_{1},v_{2}\\} \\to \\{0\\}\\) . This data augmentation strategy doubles the size of Data Back with a perfect balance in positive and negative backbone labels. In the rest of the paper, Data Back- PT and Data Back- FT refer to the augmented, balanced datasets. Data Back- PT The CNF formulas in Data Back- PT are sourced from four origins: 1) CNFgen Lauria et al. (2017), a recent CNF generator renowned for crafting CNF formulas that feature in proof complexity literature; 2) SATLIB Hoos & St\u00fctzle (2000), an online benchmark library housing well- known benchmark instances, serving as a foundational challenge for the SAT community; 3) model counting competitions from 2020 to 2022 Fichte et al. (2021); MCC (2021; 2022), including wide range of CNF formulas for model counters to count their satisfying assignments, which provide a good source for backbone extraction; 4) Random tracks in SAT competitions from 2004 to 2021, offering numerous challenging random SAT problems.\n\n<table><tr><td>metrics model</td><td>precision</td><td>recall</td><td>F1</td><td>accuracy</td></tr><tr><td>pre-trained model</td><td>0.903</td><td>0.766</td><td>0.829</td><td>0.751</td></tr><tr><td>fine-tuned model</td><td>0.941</td><td>0.914</td><td>0.928</td><td>0.887</td></tr></table> Table 2: The performance on the validation set of both pre- trained and fine- tuned Neuro Back models for classifying the phases of backbone variables, in terms of precision, recall, F1 score and accuracy. Table 1 shows the detailed statistics of the resulting dataset. CNFgen generated 10,859 labeled sat formulas (21,718 augmented formulas) grouped by five categories: random SAT, clique- coloring, graph coloring, counting principle, and parity principle. From SATLIB, 40,153 sat formulas with labels (80,306 augmented formulas) were collected, encoding seven kinds of problems: random SAT, graph coloring, planning, bounded model checking, Latin square, circuit fault analysis, and all- interval series. Model counting competitions and SAT competition random tracks contribute 5,780 (11,578 augmented) and 2,438 (4,876 augmented) labeled formulas, respectively. In total, 59,230 (118,460 augmented) formulas are included Data Back- PT. The comparative size of formulas, as indicated by the average number of variables and clauses, reveals that formulas from model counting competitions and SAT competition random tracks are generally larger than those generated by CNFgen, which are in turn larger than those originating from SATLIB. In addition, formulas from model counting competitions and SATLIB exhibit relatively high average backbone proportions. These are followed by formulas from CNFgen, while formulas from the SAT competition's random tracks demonstrate the lowest backbone proportion. Data Back- PT thus contains a diverse set of formulas. Data Back- FT Given that Neuro Back will be tested on large- scale SAT formulas from the main track of recent SAT competitions, the fine- tuning dataset, Data Back- FT, incorporates CNF formulas from the main track of earlier SAT competitions spanning from 2004 to 2021. As shown in Table 1, it contains 913 (1,826 augmented) labeled formulas. While Data Back- FT is considerably smaller in size compared to Data Back- PT, its individual formulas are distinctly larger than those in Data Back- PT. ## 6 EXPERIMENTS Platform All experiments were run on an ordinary commodity computer with one NVIDIA Ge Force RTX 3080 GPU (10GB memory), one AMD Ryzen Threadripper 3970X processor (64 logical cores), and 256GB RAM. Research Questions The experiments aim to answer two research questions: RQ1: How accurately does the Neuro Back model classify the phases of backbone variables? RQ2: How effective is the Neuro Back approach? RQ1: Neuro Back Model Performance The Neuro Back model was pre- trained on the entire Data Back- PT dataset, then fine- tuned on a random \\(90\\%\\) of Data Back- FT samples, and evaluated on the remaining \\(10\\%\\) as a validation set. Table 2 details the performance of both the pre- trained and fine- tuned models in classifying the phases of backbone variables. Notably, the pre- trained model achieved \\(75.1\\%\\) accuracy in classifying backbone variables, with a precision exceeding \\(90\\%\\) and a recall rate of \\(76.7\\%\\) . Considering the distinct data sources of Data Back- PT and Data Back- FT, the results suggest that the pre- training enables the model to extract generalized knowledge about backbone phase prediction. Fine- tuning further augments model performance, with improvements ranging between \\(4\\%\\) and \\(15\\%\\) across all metrics, making precision, recall and F1 score all exceeding \\(90\\%\\) . In conclusion, Neuro Back model effectively learns to predict the phases of backbone variables through both pre- training and fine- tuning. RQ2: Neuro Back Performance To evaluate the solving effectiveness, we collect all 800 CNF formulas from the main track of SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) as our testing dataset. For our baseline solvers, we selected the default configuration of Kissat, named Default- Kissat, which simply sets the initial phase of each variable to true. We implemented an additional baseline solver, Random- Kissat, which randomly assigns the initial phase of each variable as either true or false. Neuro Back- Kissat and its baseline solvers, were applied to all 800 SAT problems in the testing dataset, with the standard solving time limit of 5,000 seconds. Each solver utilized up to 64 different processes in parallel on the dedicated 64- core machine. The model inference for each Neuro Back solver was conducted solely on the CPU, with a memory limit of 10GB to mitigate memory contention issues. Consequently, 308 problems from SATCOMP- 2022 and\n\n<center>Figure 4: Progress of Default-Kissat, Random-Kissat, and Neuro Back-Kissat over time in solving problems (time in seconds) on SATCOMP-2022 (left) and SATCOMP-2023 (right), respectively. Neuro Back-Kissat outperforms the two baseline solvers on both testing sets. </center> 353 problems from SATCOMP- 2023 were successfully inferred. The CPU inference time for each of these problems ranged from 0.3 to 16.5 seconds, averaging at 1.7 seconds. Given that problems unsuccessfully inferred make the Neuro Back solver revert to its baseline, we exclude such problems from our evaluation of how Neuro Back contributes to SAT solving. As a result, for 308 inferred problems in SATCOMP- 2022, Default- Kissat and Random- Kissat solved 193 and 197 problems, respectively. In comparison, Neuro Back- Kissat solved 203 problems, representing an improvement of \\(5.2\\%\\) over Default- Kissat and \\(3.0\\%\\) over Random- Kissat, respectively. For 353 inferred problems in SATCOMP- 2023, Default- Kissat and Random- Kissat successfully solved 198 and 190 problems, respectively. In contrast, Neuro Back- Kissat solved 204 problems, making improvements of \\(3.0\\%\\) and \\(7.4\\%\\) over Default- Kissat and Random- Kissat, respectively. The cactus plot is a commonly used plot in the SAT community for demonstrating the solving, as shown in Fig. 4. Random- Kissat performs better than Default- Kissat in SATCOMP- 2022, but worse in SATCOMP- 2023. In contrast, Neuro Back- Kissat consistently outperforms both baseline solvers. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, reducing solving time by 98 and 246 seconds per problem. For additional results, please refer to the Appendix section. Overall, results suggest that the phase initialization provided by Neuro Back outperforms both the default and random phase initializations in Kissat, exhibiting enhanced proficiency in solving SATCOMP- 2022 and SATCOMP- 2023 problems. ## 7 DISCUSSION AND FUTURE WORK The current implementation of Neuro Back has two main aspects to improve. First, pre- training the GNN model requires a large amount of data. Once trained, however, the model is able to easily generalize to new problem categories, so the investment in putting together a large dataset is warranted. The dataset will also be publicly available to benefit future research. Second, this paper employs neural phase predictions solely as an initial setting for variable phases in SAT solvers. However, multiple methods exist to utilize these predictions in SAT, either in a static or dynamic manner, which are likely to yield further performance enhancements. For instance, phase predictions could be leveraged dynamically during rephasing to adjust or diversify the exploration of the search space. ## 8 CONCLUSION This paper proposes a machine learning approach, Neuro Back, to make CDCL SAT solvers more effective without requiring any GPU resource during its application. The main idea is to make offline model inference on variable phases appearing in the majority of satisfying assignments, for enhancing the phase selection heuristic in CDCL solvers. Incorporated in the state- of- the- art SAT solver, Kissat, this approach significantly reduces the solving time and makes it possible to solve more instances in SATCOMP- 2022 and SATCOMP- 2023. Neuro Back is thus a promising approach to improving the SAT solvers through machine learning.",
      "level": 2,
      "line_start": 13,
      "line_end": 35
    },
    {
      "heading": "4.2.1 GNN MODEL DESIGN The sizes of converted graphs representing practical SAT formulas (usually with millions of variables and clauses) are typically substantial. To enable effective training within the constraints of limited GPU memory, it is essential for our model to be both compact and robust. Our GNN model design is inspired by the robust graph transformer architecture, Graph Trans Wu et al. (2021). However, in our particular SAT application context, Graph Trans exhibits two limitations, both arising from the global self- attention mechanism within its transformer subnet. First, the mechanism does not explicitly integrate the topological graph structure information when determining attention scores. However, such information is essential in characterizing a SAT formula. Second, the global self- attention mechanism computes attention scores for all possible node pairs, leading to quadratic memory complexity with respect to the number of nodes in the graph. This is obviously infeasible for tackling the large- scale SAT formulas in our task. To overcome the limitations, we introduce a novel transformer subnet that both distinctly harnesses topological structure information and significantly enhances memory efficiency. It combines Graph Self- Attention (GSA) and Local Self- Attention (LSA), replacing the original transformer's global self- attention. Instead of computing attention scores for all node pairs as global self- attention, GSA calculates attention scores solely for directly connected node pairs, leveraging information of edges and edge weights. This not only explicitly incorporates the topological structure information of the graph, but also reduces the memory complexity to linear in terms of the number of edges in the graph. Additionally, to further reduce the memory complexity, LSA segments each node embedding into multiple node patches and computes attention scores for each pair of node patches. The results in a linear memory complexity in terms of the number of nodes in the graph. Figure 3 illustrates the design of our GNN model architecture. It consists of three main components: a GNN subnet with \\(L\\) stacked GNN blocks, a transformer subnet with \\(M\\) GSA transformer blocks and \\(N\\) LSA transformer blocks, and a FFN layer for node classification. Within each GNN block, a GNN layer is preceded by a normalization layer, with a skip connection bridging the two. The transformer block is crafted to accelerate training on a significant collection of large- scale graphs. Inspired by the recent vision transformer architecture, Vi T- 22B Padlewski & Djolonga (2023), each transformer block integrates a normalization layer, succeeded by both an FFN layer and a GSA/LSA layer that operate concurrently to optimize training efficiency. #### 4.2.2 IMPLEMENTATION The current implementation of Neuro Back model utilizes GINConv Fey & Lenssen (2023b); Xu et al. (2018) to build the GNN layer, for its proficiency in distinguishing non- isomorphic graph structures. However, GINConv lacks the capability to encode edge weight information. To address this, we employ three GINConv layers, each corresponding to a distinct edge weight in our graph representation. Each GINConv layer exclusively performs message passing for edges with its corresponding weight. The node embeddings from these three GINConv layers are finally aggregated as the output of the GNN layer. GATConv layers Fey & Lenssen (2023a); Velickovic et al. (2017) is utilized to built the GSA transformer block. The patch encoder in the Vi T transformer Dosovitskiy et al. (2020) is applied to construct the LSA transformer block. Layer Norm Ba et al. (2016) is employed as our normalization layer. To avoid potential over- smoothing issues, as instructed in Chen et al. (2020), the number of blocks in the GNN subnet is set to the maximum diameter of the graph representation (i.e., \\(L = 4\\) ). To ensure the accuracy of our model, while taking into account our limited GPU memory, the number of both GSA and LSA blocks are set to three (i.e., \\(M = 3\\) and \\(N = 3\\) ). Additionally, FFNs within the transformer blocks contain no hidden layers, while the final FFN utilized for node classification is structured to include one hidden layer. The model is implemented using Py Torch Paszke et al. (2019) and Py Torch Geometric Fey & Lenssen (2019). #### 4.2.3 MODEL PRE-TRAINING AND FINE-TUNING The Neuro Back model undergoes a two- stage training process. Initially, it is pre- trained on an extensive and diverse dataset gathered from various sources. This pre- training equips it with the fundamental knowledge to classify the backbone variable phases across a broad spectrum of CNF formulas. Subsequently, this pre- trained model is refined or fine- tuned on a smaller, domain- specific dataset. This fine- tuning process enhances the model's proficiency in classifying backbone variable",
      "content": "<table><tr><td>Data Back-PT</td><td>CNFgen</td><td>SATLIB</td><td>MCCOMP</td><td>SATCOMP (random)</td><td>Overall</td><td>Data Back-FT</td><td>SATCOMP (main)</td></tr><tr><td># CNF</td><td>21,718</td><td>80,306</td><td>11,560</td><td>4,876</td><td>118,460</td><td># CNF</td><td>1,826</td></tr><tr><td># Var</td><td>779</td><td>114</td><td>16,299</td><td>25,310</td><td>2,852</td><td># Var</td><td>206,470</td></tr><tr><td># Cla</td><td>281,888</td><td>529</td><td>63,501</td><td>88,978</td><td>61,898</td><td># Cla</td><td>1,218,519</td></tr><tr><td># Backbone Var</td><td>280 (36%)</td><td>58 (51%)</td><td>8,587 (53%)</td><td>1,960 (8%)</td><td>1,009 (35%)</td><td># Backbone Var</td><td>48,266 (23%)</td></tr></table> Table 1: Details of Data Back. For each source, we summarize the number of CNF formulas; the average number of variables and clauses characterizing the size of each CNF formula; and the average number of backbone variables, with their proportion in total variables (indicated in brackets). Overall, for each dataset, we summarize the total number of CNF formulas and the corresponding average counts regarding all formulas in the set. phases within a specific category of CNF formulas. Details about the pre- training and fine- tuning datasets are introduced in Section 5. Binary Cross Entropy (BCE) loss is adopted as our loss function. Besides, Adam W optimizer Loshchilov & Hutter (2017) with a learning rate of \\(10^{- 4}\\) is applied for model pre- training and fine- tuning. The number of epoch is set to 40 for pre- training, and 60 for fine- tuning. It took 48 hours in total to accomplish both the pre- training and fine- tuning on our commodity computer. ### 4.3 APPLYING PHASE PREDICTIONS IN SAT The goal is to leverage phase predictions derived from the GNN model to enhance the phase selection heuristics within CDCL SAT solvers. While numerous ways exist to integrate these neural predictions into CDCL SAT solvers, the most straightforward and generic approach is to initialize the phases of the corresponding variables in CDCL SAT solvers based on the predicted phases. In this paper, we adopted the state- of- the- art solver Kissat Biere & Fleury (2022) to support the phase initialization with Neuro Back predictions. The resulting implementation is called Neuro Back- Kissat. ## 5 DATABACK We created a new dataset, Data Back, comprising sat CNF formulas labeled with phases of backbone variables, for pre- training (PT) and fine- tuning (FT) the Neuro Back model. Accordingly, there are two subsets in Data Back: the pre- training set, Data Back- PT, and the fine- tuning set, Data Back- FT. To get the backbone label, the very recent state- of- the- art backbone extractor called Cadi Back Biere et al. (2023) is utilized. Given that the fine- tuning formulas are typically more challenging to solve than the pre- training ones, label collection timeouts are set as 1,000 seconds for the pre- training formulas and 5,000 seconds for the fine- tuning formulas. Data Back includes formulas solved within the time limit with at least one backbone variable. We observe that there exists a significant label imbalance in both Data Back- PT and Data Back- FT. To tackle this, for each labeled formula, we create a dual formula by negating all backbone variables in the original formula. The labels of the dual formula are the negated phases of the backbone variables in the original formula. Formally, for a formula \\(f\\) with \\(n\\) backbone variables \\(b_{1},\\ldots ,b_{n}\\) , let \\(\\mathcal{L}_{f}:\\{b_{1},\\ldots ,b_{n}\\} \\to \\{1,0\\}\\) denote the mapping of each backbone variable to its phase. The dual formula \\(f^{\\prime}\\) is obtained from \\(f\\) by negating each backbone variable: \\(f^{\\prime} = f[b_{1}\\to\\) \\(\\neg b_{1},\\ldots ,b_{n}\\mapsto \\neg b_{n}]\\) . The dual \\(f^{\\prime}\\) is still satisfiable and retains the same backbone variables as \\(f\\) , but with the opposite phases \\(\\mathcal{L}_{f^{\\prime}}(b_{i}) = \\neg \\mathcal{L}_{f}(b_{i}),i\\in \\{1,\\ldots ,n\\}\\) . For the given CNF formula example in Section 2 \\(\\phi = (v_{1}\\vee \\neg v_{2})\\wedge (v_{2}\\vee v_{3})\\wedge v_{2}\\) , having \\(v_{1}\\) and \\(v_{2}\\) as its backbone variables with phases \\(\\{v_{1},v_{2}\\} \\to \\{1\\}\\) , the dual formula is \\(\\phi^{\\prime} = (\\neg v_{1}\\vee v_{2})\\wedge (\\neg v_{2}\\vee v_{3})\\wedge \\neg v_{2}\\) , still having \\(v_{1}\\) and \\(v_{2}\\) as the backbone variables but with opposite phases \\(\\{v_{1},v_{2}\\} \\to \\{0\\}\\) . This data augmentation strategy doubles the size of Data Back with a perfect balance in positive and negative backbone labels. In the rest of the paper, Data Back- PT and Data Back- FT refer to the augmented, balanced datasets. Data Back- PT The CNF formulas in Data Back- PT are sourced from four origins: 1) CNFgen Lauria et al. (2017), a recent CNF generator renowned for crafting CNF formulas that feature in proof complexity literature; 2) SATLIB Hoos & St\u00fctzle (2000), an online benchmark library housing well- known benchmark instances, serving as a foundational challenge for the SAT community; 3) model counting competitions from 2020 to 2022 Fichte et al. (2021); MCC (2021; 2022), including wide range of CNF formulas for model counters to count their satisfying assignments, which provide a good source for backbone extraction; 4) Random tracks in SAT competitions from 2004 to 2021, offering numerous challenging random SAT problems.\n\n<table><tr><td>metrics model</td><td>precision</td><td>recall</td><td>F1</td><td>accuracy</td></tr><tr><td>pre-trained model</td><td>0.903</td><td>0.766</td><td>0.829</td><td>0.751</td></tr><tr><td>fine-tuned model</td><td>0.941</td><td>0.914</td><td>0.928</td><td>0.887</td></tr></table> Table 2: The performance on the validation set of both pre- trained and fine- tuned Neuro Back models for classifying the phases of backbone variables, in terms of precision, recall, F1 score and accuracy. Table 1 shows the detailed statistics of the resulting dataset. CNFgen generated 10,859 labeled sat formulas (21,718 augmented formulas) grouped by five categories: random SAT, clique- coloring, graph coloring, counting principle, and parity principle. From SATLIB, 40,153 sat formulas with labels (80,306 augmented formulas) were collected, encoding seven kinds of problems: random SAT, graph coloring, planning, bounded model checking, Latin square, circuit fault analysis, and all- interval series. Model counting competitions and SAT competition random tracks contribute 5,780 (11,578 augmented) and 2,438 (4,876 augmented) labeled formulas, respectively. In total, 59,230 (118,460 augmented) formulas are included Data Back- PT. The comparative size of formulas, as indicated by the average number of variables and clauses, reveals that formulas from model counting competitions and SAT competition random tracks are generally larger than those generated by CNFgen, which are in turn larger than those originating from SATLIB. In addition, formulas from model counting competitions and SATLIB exhibit relatively high average backbone proportions. These are followed by formulas from CNFgen, while formulas from the SAT competition's random tracks demonstrate the lowest backbone proportion. Data Back- PT thus contains a diverse set of formulas. Data Back- FT Given that Neuro Back will be tested on large- scale SAT formulas from the main track of recent SAT competitions, the fine- tuning dataset, Data Back- FT, incorporates CNF formulas from the main track of earlier SAT competitions spanning from 2004 to 2021. As shown in Table 1, it contains 913 (1,826 augmented) labeled formulas. While Data Back- FT is considerably smaller in size compared to Data Back- PT, its individual formulas are distinctly larger than those in Data Back- PT. ## 6 EXPERIMENTS Platform All experiments were run on an ordinary commodity computer with one NVIDIA Ge Force RTX 3080 GPU (10GB memory), one AMD Ryzen Threadripper 3970X processor (64 logical cores), and 256GB RAM. Research Questions The experiments aim to answer two research questions: RQ1: How accurately does the Neuro Back model classify the phases of backbone variables? RQ2: How effective is the Neuro Back approach? RQ1: Neuro Back Model Performance The Neuro Back model was pre- trained on the entire Data Back- PT dataset, then fine- tuned on a random \\(90\\%\\) of Data Back- FT samples, and evaluated on the remaining \\(10\\%\\) as a validation set. Table 2 details the performance of both the pre- trained and fine- tuned models in classifying the phases of backbone variables. Notably, the pre- trained model achieved \\(75.1\\%\\) accuracy in classifying backbone variables, with a precision exceeding \\(90\\%\\) and a recall rate of \\(76.7\\%\\) . Considering the distinct data sources of Data Back- PT and Data Back- FT, the results suggest that the pre- training enables the model to extract generalized knowledge about backbone phase prediction. Fine- tuning further augments model performance, with improvements ranging between \\(4\\%\\) and \\(15\\%\\) across all metrics, making precision, recall and F1 score all exceeding \\(90\\%\\) . In conclusion, Neuro Back model effectively learns to predict the phases of backbone variables through both pre- training and fine- tuning. RQ2: Neuro Back Performance To evaluate the solving effectiveness, we collect all 800 CNF formulas from the main track of SATCOMP- 2022 SAT (2022) and SATCOMP- 2023 SAT (2023) as our testing dataset. For our baseline solvers, we selected the default configuration of Kissat, named Default- Kissat, which simply sets the initial phase of each variable to true. We implemented an additional baseline solver, Random- Kissat, which randomly assigns the initial phase of each variable as either true or false. Neuro Back- Kissat and its baseline solvers, were applied to all 800 SAT problems in the testing dataset, with the standard solving time limit of 5,000 seconds. Each solver utilized up to 64 different processes in parallel on the dedicated 64- core machine. The model inference for each Neuro Back solver was conducted solely on the CPU, with a memory limit of 10GB to mitigate memory contention issues. Consequently, 308 problems from SATCOMP- 2022 and\n\n<center>Figure 4: Progress of Default-Kissat, Random-Kissat, and Neuro Back-Kissat over time in solving problems (time in seconds) on SATCOMP-2022 (left) and SATCOMP-2023 (right), respectively. Neuro Back-Kissat outperforms the two baseline solvers on both testing sets. </center> 353 problems from SATCOMP- 2023 were successfully inferred. The CPU inference time for each of these problems ranged from 0.3 to 16.5 seconds, averaging at 1.7 seconds. Given that problems unsuccessfully inferred make the Neuro Back solver revert to its baseline, we exclude such problems from our evaluation of how Neuro Back contributes to SAT solving. As a result, for 308 inferred problems in SATCOMP- 2022, Default- Kissat and Random- Kissat solved 193 and 197 problems, respectively. In comparison, Neuro Back- Kissat solved 203 problems, representing an improvement of \\(5.2\\%\\) over Default- Kissat and \\(3.0\\%\\) over Random- Kissat, respectively. For 353 inferred problems in SATCOMP- 2023, Default- Kissat and Random- Kissat successfully solved 198 and 190 problems, respectively. In contrast, Neuro Back- Kissat solved 204 problems, making improvements of \\(3.0\\%\\) and \\(7.4\\%\\) over Default- Kissat and Random- Kissat, respectively. The cactus plot is a commonly used plot in the SAT community for demonstrating the solving, as shown in Fig. 4. Random- Kissat performs better than Default- Kissat in SATCOMP- 2022, but worse in SATCOMP- 2023. In contrast, Neuro Back- Kissat consistently outperforms both baseline solvers. Specifically, Neuro Back- Kissat outperforms Default- Kissat on 43 and 40 additional problems in SATCOMP- 2022 and SATCOMP- 2023, respectively, reducing solving time by 117 and 36 seconds per problem. Similarly, Neuro Back- Kissat outperforms Random- Kissat in SATCOMP- 2022 and SATCOMP- 2023 on 22 and 29 more problems, respectively, reducing solving time by 98 and 246 seconds per problem. For additional results, please refer to the Appendix section. Overall, results suggest that the phase initialization provided by Neuro Back outperforms both the default and random phase initializations in Kissat, exhibiting enhanced proficiency in solving SATCOMP- 2022 and SATCOMP- 2023 problems. ## 7 DISCUSSION AND FUTURE WORK The current implementation of Neuro Back has two main aspects to improve. First, pre- training the GNN model requires a large amount of data. Once trained, however, the model is able to easily generalize to new problem categories, so the investment in putting together a large dataset is warranted. The dataset will also be publicly available to benefit future research. Second, this paper employs neural phase predictions solely as an initial setting for variable phases in SAT solvers. However, multiple methods exist to utilize these predictions in SAT, either in a static or dynamic manner, which are likely to yield further performance enhancements. For instance, phase predictions could be leveraged dynamically during rephasing to adjust or diversify the exploration of the search space. ## 8 CONCLUSION This paper proposes a machine learning approach, Neuro Back, to make CDCL SAT solvers more effective without requiring any GPU resource during its application. The main idea is to make offline model inference on variable phases appearing in the majority of satisfying assignments, for enhancing the phase selection heuristic in CDCL solvers. Incorporated in the state- of- the- art SAT solver, Kissat, this approach significantly reduces the solving time and makes it possible to solve more instances in SATCOMP- 2022 and SATCOMP- 2023. Neuro Back is thus a promising approach to improving the SAT solvers through machine learning.",
      "level": 4,
      "line_start": 29,
      "line_end": 35
    }
  ]
}