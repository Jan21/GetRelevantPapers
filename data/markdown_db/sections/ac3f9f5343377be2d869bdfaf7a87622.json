{
  "sections": {
    "on_the_hardness_of_learning_gnn_based_sat_solvers": "## Introduction\n\n\n## Acknowledgements Acknowledgements Geri Skenderi is funded by the European Union through the Next Generation EU - MIUR PRIN PNRR 2022 Grant P20229PBZR. The views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible. ## References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. 1, 3 [2] Maria Chiara Angelini and Federico Ricci- Tersenghi. Monte carlo algorithms are very effective in finding the largest independent set in sparse random graphs. Physical Review E, 100(1): 013302, 2019. 1, 5 [3] Alvaro Arroyo, Alessio Gravina, Benjamin Gutteridge, Federico Barbero, Claudio Gallicchio, Xiaowen Dong, Michael Bronstein, and Pierre Vandergheynst. On vanishing gradients, oversmoothing, and over- squashing in gnns: Bridging recurrent and graph learning. ar Xiv preprint ar Xiv:2502.10818, 2025. 8 [4] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing. SIAM review, 59(1):65- 98, 2017. URL https://doi.org/10.1137/141000671. 7 [5] Bhaswar B Bhattacharya and Sumit Mukherjee. Exact and asymptotic results on coarse ricci curvature of graphs. Discrete Mathematics, 338(1):23- 42, 2015. 4, 13, 15, 19 [6] Armin Biere, Marijn Heule, and Hans van Maaren. Handbook of satisfiability, volume 185. IOS press, 2009. 1, 3 [7] Lucas Bordeaux, Youssef Hamadi, and Lintao Zhang. Propositional satisfiability and constraint programming: A comparative survey. ACM Computing Surveys (CSUR), 38(4):12- es, 2006. 1 [8] A. Braunstein, M. Mezard, and R. Zecchina. Survey propagation: An algorithm for satisfiability. Random Structures & Algorithms, 27(2):201- 226, 2005. doi: https://doi.org/10.1002/rsa.20057. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.20057. 1 [9] Michael M Bronstein, Joan Bruna, Yann Le Cun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18- 42, 2017. 3 [10] Quentin Cappart, Didier Chetelat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Veli\u010dkovi\u0107. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 1 [11] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele Rossi. Grand: Graph neural diffusion. In International conference on machine learning, pages 1407- 1418. PMLR, 2021. 9 [12] Wenjing Chang and Wenlong Liu. Sat- gatv2: A dynamic attention- based graph neural network for solving boolean satisfiability problem. Electronics, 14(3):423, 2025. 1 [13] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, page 151- 158, New York, NY, USA, 1971. Association for Computing Machinery. ISBN 9781450374644. doi: 10.1145/800157.805047. URL https://doi.org/10.1145/800157.805047. 1 [14] Gabriele Corso, Hannes Stark, Stefanie Jegelka, Tommi Jaakkola, and Regina Barzilay. Graph neural networks. Nature Reviews Methods Primers, 4(1):17, 2024. 3 [15] William Falcon and The Py Torch Lightning team. Py Torch Lightning, March 2019. URL https://github.com/Lightning- AI/lightning. 7 [16] Lukas Fesser and Melanie Weber. Effective structural encodings via local curvature profiles. In International Conference on Learning Representations, 2024. 18 [17] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. ar Xiv preprint ar Xiv:1903.02428, 2019. 7\n\n[18] Forman. Bochner's method for cell complexes and combinatorial ricci curvature. Discrete & Computational Geometry, 29:323- 374, 2003. 3, 14 [19] Karlis Freivalds and Sergejs Kozlovics. Denoising Diffusion for Sampling SAT Solutions, November 2022. URL http://arxiv.org/abs/2212.00121. ar Xiv:2212.00121 [cs]. 1 [20] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263- 1272. PMLR, 2017. 1, 3 [21] Andi Han, Dai Shi, Lequan Lin, and Junbin Gao. From continuous dynamics to graph neural networks: Neural diffusion and beyond. Transactions on Machine Learning Research, 2024. 9 [22] Masato Inagaki. Spectral convergence of graph laplacians with ricci curvature bounds and in non- collapsed ricci limit spaces. ar Xiv preprint ar Xiv:2506.07427, 2025. 13 [23] Jurgen Jost and Shiping Liu. Ollivier's ricci curvature, local clustering and curvature- dimension inequalities on graphs. Discrete & Computational Geometry, 51(2):300- 322, 2014. 4 [24] Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer computations, pages 85- 103. Springer, 1972. 1 [25] TN Kipf. Semi- supervised classification with graph convolutional networks. ar Xiv preprint ar Xiv:1609.02907, 2016. 8 [26] Florent Krzakala, Andrea Montanari, Federico Ricci- Tersenghi, Guilhem Semerjian, and Lenka Zdeborov\u00e1. Gibbs states and the set of solutions of random constraint satisfaction problems. Proceedings of the National Academy of Sciences, 104(25):10318- 10323, 2007. 1, 2 [27] Qimai Li, Zhichao Han, and Xiao- Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. 1 [28] Zhaoyu Li, Jinpei Guo, and Xujie Si. G4sabench: Benchmarking and advancing sat solving with graph neural networks. Transactions on Machine Learning Research, 2024. 1, 2, 3, 4, 7, 8, 9, 18 [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 7 [30] Raffaele Marino, Giorgio Parisi, and Federico Ricci- Tersenghi. The backtracking survey propagation algorithm for solving random k- sat problems. Nature communications, 7(1):12996, 2016. 1 [31] Joao Marques- Silva. Practical applications of boolean satisfiability. In 2008 9th International Workshop on Discrete Event Systems, pages 74- 80, 2008. doi: 10.1109/WODES.2008.4605925. 1 [32] Stephan Mertens, Marc Mezard, and Riccardo Zecchina. Threshold values of random k- sat from the cavity method. Random Structures & Algorithms, 28(3):340- 373, 2006. doi: https://doi.org/10.1002/rsa.20090. 7, 20, 21 [33] Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009. 1, 2 [34] Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientific Publishing Company, 1987. 2 [35] David Moj\u017ei\u0161ek, Jan \u0171la, Ziwei Li, Ziyu Zhou, and Mikol\u00e1\u0161 Janota. Neural approaches to sat solving: Design choices and interpretability. ar Xiv preprint ar Xiv:2504.01173, 2025. 8 [36] R\u00e9mi Monasson and Riccardo Zecchina. Statistical mechanics of the random k- satisfiability model. Physical Review E, 56(2):1357, 1997. 1 [37] M. Mezard, G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812- 815, 2002. doi: 10.1126/science.1073287. URL https://www.science.org/doi/abs/10.1126/science.1073287. 2 [38] Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 2, 3, 8\n\n[39] Yann Ollivier. Ricci curvature of markov chains on metric spaces. Journal of Functional Analysis, 256(3):810- 864, 2009. 3, 13 [40] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. Goal- Aware Neural SAT Solver. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1- 8, July 2022. doi: 10.1109/IJCNN55064.2022.9892733. URL http://arxiv.org/abs/2106.07162. ar Xiv:2106.07162 [cs]. 1, 4, 8 [41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. Advances in neural information processing systems, 32, 2019. 7 [42] Daniel Paulin. Mixing and concentration by ricci curvature. Journal of Functional Analysis, 270(5):1623- 1662, 2016. 13 [43] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5- 6):355- 607, 2019. 13 [44] Areejit Samal, RP Sreejith, Jiao Gu, Shiping Liu, Emil Saucan, and J\u00fcrgen Jost. Comparative analysis of two discretizations of ricci curvature for complex networks. Scientific reports, 8(1): 8650, 2018. 3, 5, 14 [45] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61- 80, 2008. 1, 3 [46] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, David L. Dill, and Leonardo De Moura. Learning a SAT solver from single- bit supervision. 7th International Conference on Learning Representations, ICLR 2019, pages 1- 11, 2019. ar Xiv: 1802.03685. 1, 4, 7, 8, 20 [47] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. Understanding over- squashing and bottlenecks on graphs via curvature. In International Conference on Learning Representations, 2022. 2, 3, 4, 5, 6, 8, 13, 14, 15, 17 [48] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth Mc Millan, and Risto Miikkulainen. Neuro Comb: Improving SAT Solving with Graph Neural Networks, June 2022. URL http://arxiv.org/abs/2110.14053. ar Xiv:2110.14053 [cs] version: 2. 9 [49] Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. Curvature graph network. In International Conference on Learning Representations, 2019. 18 [50] Lenka Zdeborov\u00e1. Statistical physics of hard optimization problems. Acta Physica Slovaca, 59 (3):169- 303, 2009. 1, 2\n\n## A Appendix In what follows, we provide supplementary material that complements the main manuscript. The appendix is organized as follows: 1. Appendix A.1 begins with background on graph Ricci curvature, where we review both the Ollivier and Balanced Forman discretizations in order to give the reader an intuitive and formal foundation for subsequent results. 2. In Appendix A.2, we provide detailed proofs of the propositions and theorems discussed in the manuscript. 3. Appendix A.3 describes the test-time graph rewiring procedure used in our experiments, including a full presentation of the stochastic curvature-guided algorithm. 4. In Appendix A.4, we outline preliminary ideas and implementations for curvature-aware solvers, along with empirical results that illustrate their potential. 5. Finally, Appendix A.5 contains additional plots and visualizations that further clarify the relationship between curvature, problem hardness, and solver behavior. ## A.1 Ricci curvature of graphs In this section, we provide, for the sake of completeness, both definitions and intuitive explanations of the two types of graph Ricci Curvature (RC) mentioned in this paper. The definitions are taken from Bhattacharya and Mukherjee [5] and Topping et al. [47] respectively, we simply report them here in a synthesized manner. We kindly refer the reader to the aforementioned works for more details. ## A.1.1 Ollivier Ricci curvature (OC) The formulation of Ollivier [39] aligns with the intuition from differential geometry: edges with negative curvature act as structural bottlenecks, separating dense regions and limiting smooth information propagation. Edges with positive curvature in the other hand facilitate smooth information propagation and are indicators of community structure. Graph Ollivier- Ricci Curvature (OC) is very well studied and has being linked to properties of graph Lapacians and mixing times of Markov Chain Monte Carlo (MCMC) methods [22, 42]. The intuitive idea of this discretization is to directly implement the idea from differential geometry: we use a ratio between the amount of \"mass\" moved around of an edge neighborhood with the shortest path distance, i.e., the graph geodesic. Let us formalize this concept. For two probability measures \\(\\mu_{1},\\mu_{2}\\) on a metric space \\((X,d)\\) , the the Wasserstein distance between them is defined as \\[W_{1}(\\mu_{1},\\mu_{2}) = \\inf_{\\nu \\in M(\\mu_{1},\\mu_{2})}\\int_{X\\times X}d(x,y)\\mathrm{d}\\nu (x,y), \\quad (12)\\] where \\(M(\\mu_{1},\\mu_{2})\\) is the collection of probability measures on \\(X\\times X\\) with marginals \\(\\mu_{1}\\) and \\(\\mu_{2}\\) . The Wasserstein distance is the result of the solution to a famous problem called Optimal Transport [43]. Intuitively, this distance measures the optimal cost to move one pile of sand to another one with the same mass. Let a metric measure space \\((X,d,m)\\) be a metric space \\((X,d)\\) , with a collection of probability measures \\(m = \\{m_{x}:x\\in X\\}\\) indexed by the points of \\(X\\) . The (coarse) Ricci curvature of a metric measure space is defined as follows: Definition A.1 (Ollivier [39]). On any metric measure space \\((X,d,m)\\) , for any two distinct points \\(x,y\\in X\\) , the (coarse) Ricci curvature of \\((X,d,m)\\) of \\((x,y)\\) is defined as: \\[\\kappa (x,y)\\coloneqq 1 - \\frac{W_{1}(m_{x},m_{y})}{d(x,y)} \\quad (13)\\] Extending this definition to graphs requires some additional steps. Consider a locally finite and possibly weighted simple graph \\(G = (V,E)\\) , where each edge \\((i,j)\\in E\\) is assigned a positive weight \\(w_{ij} = w_{ji}\\) . The graph is equipped with the standard shortest path graph distance \\(d_{G}\\) , that is, for \\(i,j\\in V\\) , \\(d_{G}(i,j)\\) is the length of the shortest path in \\(G\\) connecting nodes \\(i\\) and \\(j\\) . For \\(i\\in V\\)\n\ndefine the degree \\(d_{i} \\coloneqq \\sum_{(i,j) \\in E} w_{ij}\\) and the neighborhood \\(\\mathcal{N}(i) \\coloneqq \\{j \\in V: (i,j) \\in E\\}\\) . For each \\(i \\in V\\) define a probability measure \\[m_{i}(j) = \\left\\{ \\begin{array}{ll} \\frac{w_{i,j}}{d_{i}}, & \\mathrm{if} j \\in \\mathcal{N}(i) \\\\ 0, & \\mathrm{otherwise.} \\end{array} \\right.\\] Note that these are just the transition probabilities of a weighted random walk on the vertices of \\(G\\) . If \\(m_{G} = \\{m_{i}: i \\in V\\}\\) , then considering the metric measure space \\(\\mathcal{M}_{G} \\coloneqq (V, d_{G}, m_{G})\\) , we can define the OC curvature for any edge \\((i,j) \\in E\\) as \\(\\kappa (\\mathbf{i},\\mathbf{j}) \\coloneqq \\mathbf{1} - \\mathbf{W}_{1}^{G}(\\mathbf{m}_{i}, \\mathbf{m}_{j})\\) , where \\(W_{1}^{G}(m_{i}, m_{j})\\) is obtained by discretizing Equation (12) on \\(\\mathcal{M}_{G}\\) : \\[W_{1}^{G}(m_{i}, m_{j}) = \\inf_{\\nu \\in \\mathcal{A}} \\sum_{z_{1} \\in \\mathcal{N}(i)} \\sum_{z_{2} \\in \\mathcal{N}(j)} \\nu (z_{1}, z_{2}) d(z_{1}, z_{2}). \\quad (14)\\] \\(\\mathcal{A}\\) denotes the set of all \\(d_{i} \\times d_{j}\\) matrices with entries indexed by \\(\\mathcal{N}(i) \\times \\mathcal{N}(j)\\) such that \\(\\nu (i', j') \\geq 0\\) , \\(\\sum_{z \\in \\mathcal{N}(j)} \\nu (i', z) = \\frac{w_{i,i'}}{d_{i}}\\) , and \\(\\sum_{z \\in \\mathcal{N}(i)} \\nu (z, j') = \\frac{w_{j,j'}}{d_{j}}\\) , for all \\(i' \\in \\mathcal{N}(i)\\) and \\(j' \\in \\mathcal{N}(j)\\) . For a matrix \\(\\nu \\in \\mathcal{A}\\) , \\(\\nu (i', j')\\) represents the mass moving from \\(i' \\in \\mathcal{N}(i)\\) to \\(j' \\in \\mathcal{N}(j)\\) . For this reason, the matrix \\(\\nu\\) is often called the transfer plan. ## A.1.2 Balanced Forman curvature (BFC) The Forman- Ricci Curvature (FC) is a discretization of Ricci curvature that holds for a broad class of topological objects, namely so called (regular) cellular (CW) complexes [44]. The original definition [18] is both extremely technical and out of the scope of this paper. Given that graphs edges are 1- dimensional cells (topologically), the definition simplifies greatly making use of very simple graph properties. Consider a simple, unweighted graph for simplicity, then the FC of edge \\((i,j)\\) is defined as \\(\\kappa_{G}^{f}(i,j) = 4 - d_{i} - d_{j}\\) . The main issue of this definition is that it ignores higher order correlations in terms of triangles and 4- cycles, which prove crucial to discriminate positively, flat, and negatively curved graphs. Inspired by these issues, Topping et al. [47] propose an extension of the FC dubbed Balanced Forman Curvature (BFC), such that it is both fast to compute and encodes accurate curvature information. Let us start by defining the sphere and ball of radius \\(r\\) centered at a node \\(i\\) of the graph by: \\[S_{r}(i) \\coloneqq \\{j \\in V: d_{G}(i,j) = r\\} , \\quad B_{r}(i) \\coloneqq \\{j \\in V: d_{G}(i,j) \\leq r\\} . \\quad (15)\\] The BFC is then defined using three combinatorial components: (i) \\(\\sharp_{\\Delta}(i,j)\\) , the number of triangles based at the edge \\(i \\sim j\\) . (ii) \\(\\sharp_{\\square}^{i}(i,j) \\coloneqq\\) , the number of nodes \\(k \\in S_{1}(i)\\) forming a 4- cycle based at \\(i \\sim j\\) without diagonals inside. (iii) \\(Q_{i}(j) \\coloneqq S_{1}(i) \\setminus (\\{j\\} \\cup \\sharp_{\\Delta}(i,j) \\cup \\sharp_{\\square}^{i}(i,j))\\) , simply the complement of the neighbours of \\(i\\) with respect to the sets introduced in (i) and (ii) once we also exclude \\(j\\) . Definition A.2 (Topping et al. [47]). For any edge \\(i \\sim j\\) in a simple, unweighted graph \\(G = (V, E)\\) with adjacency matrix \\(A\\) let: \\(S_{1}(i) \\coloneqq \\{j \\in V: i \\sim j \\in E\\}\\) be the set of 1- hop neighbors of \\(i\\) . \\(\\sharp_{\\Delta}(i,j) \\coloneqq S_{1}(i) \\cap S_{1}(j)\\) be the set of triangles based at \\(i \\sim j\\) . \\(\\sharp_{\\square}^{i}(i,j) \\coloneqq \\{k \\in S_{1}(i) \\setminus S_{1}(j), k \\neq j: \\exists w \\in (S_{1}(k) \\cap S_{1}(j)) \\setminus S_{1}(i)\\}\\) be the neighbors of \\(i\\) forming a 4- cycle based at \\(i \\sim j\\) without diagonals inside. \\(\\gamma_{\\max}(i,j) \\coloneqq \\max \\left\\{\\max_{k \\in \\sharp_{\\square}^{i}} \\{(A_{k} \\cdot (A_{j} - A_{i} \\odot A_{j})) - 1\\} , \\max_{w \\in \\sharp_{\\square}^{i}} \\{(A_{w} \\cdot (A_{i} - A_{j} \\odot A_{i})) - 1\\} \\right\\}\\) , with \\(\\cdot\\) being the dot product and \\(\\odot\\) the elementwise product, be the maximal number of 4- cycles based at \\(i \\sim j\\) traversing a common node. The BFC \\(Ric(i,j)\\) is zero if \\(\\min \\{d_{i}, d_{j}\\} = 1\\) and alternatively: \\[R i c(i,j) \\coloneqq \\frac{2}{d_{i}} + \\frac{2}{d_{j}} -2 + 2\\frac{|\\sharp_{\\Delta}(i,j)|}{\\max \\{d_{i},d_{j}\\}} +\\frac{|\\sharp_{\\Delta}(i,j)|}{\\min \\{d_{i},d_{j}\\}} +\\frac{(\\gamma_{m a x}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|). \\quad (16)\\]\n\n## A.2 Proofs For the sake of clarity and exposition, we report here both the formal statements alongside their respective proofs. Proposition 3.1. Let \\(i\\sim j\\) be an edge from the Literal- Clause Graph (LCG) representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\to 0\\) and \\(\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\to 0\\) as \\(\\alpha = \\frac{M}{N}\\to 0\\) Proof. As \\(\\alpha \\to 0\\) the expected value of the literal degree becomes: \\[\\lim_{\\alpha \\to 0}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\lim_{\\alpha \\to 0}\\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} = \\lim_{\\alpha \\to 0}e^{\\lambda}\\cdot \\lim_{\\alpha \\to 0}\\frac{\\lambda}{e^{\\lambda} - 1} = 1\\cdot \\lim_{\\alpha \\to 0}\\frac{1}{\\frac{e^{\\lambda} - 1}{\\lambda}} = 1, \\quad (17)\\] where the last equality is obtained due to the fact that \\(\\begin{array}{r}{\\lim_{\\alpha \\to 0}\\lambda = \\lim_{\\alpha \\to 0}\\frac{1}{2}\\alpha k = 0} \\end{array}\\) and the limit formula \\(\\begin{array}{r}{\\lim_{x\\to 0}\\frac{a^{x} - 1}{x} = \\ln \\left(a\\right)} \\end{array}\\) Given the average degree of the literals that act as an endpoint of at least one edge is 1 in this limiting case, by definition of the lower bound in Equation 8, we obtain that \\(\\begin{array}{r}{\\lim_{\\alpha \\to 0}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = 0} \\end{array}\\) . As a lower bound, \\(\\bar{R} (i,j)\\) satisfies the following inequality [5, 47]: \\[-2< \\bar{R} (i,j)\\leq R i c(i,j)\\leq \\kappa (i,j)\\leq 0 \\quad (18)\\] Given that both limits and expectations preserve weak inequalities, by sandwiching we obtain: \\[\\begin{array}{r l} & {\\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = 0\\leq \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\leq \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\leq 0}\\\\ & {\\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)] = \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)] = 0} \\end{array} \\quad (20)\\] Proposition 3.2. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) Proof. As \\(\\alpha \\to \\infty\\) the expected value of the literal degree becomes: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\lim_{\\alpha \\to \\infty}\\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} = \\lim_{\\alpha \\to \\infty}\\frac{1}{\\frac{1}{\\lambda} - \\frac{1}{\\lambda e^{\\lambda}}} = \\infty , \\quad (21)\\] therefore we can consider the lower bound as consisting only of \\(\\begin{array}{r}{\\underline{{R}} (i,j)] = \\frac{2}{d_{i}} +\\frac{2}{d_{j}} - 2} \\end{array}\\) (ignoring the uninteresting case when \\(k = 1\\) ). Given that the expected value is a linear operation, we obtain that: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}]} +\\frac{2}{k} -2 = \\frac{2}{k} -2 \\quad (22)\\] Theorem 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. The BFC at \\(i\\sim j\\) is bounded from above by the quantity: \\[\\bar{R} (i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\max \\{d_{i},d_{j}\\}}. \\quad (23)\\] Furthermore, as \\(\\begin{array}{r}{\\alpha = \\frac{M}{N}\\to \\infty ,\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2} \\end{array}\\) and therefore the average BFC over the edges of \\(G\\) converges to \\(\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\to \\frac{2}{k} - 2\\)\n\nProof. We first prove the construction of the upper bound, which we can then use to prove the convergence of the expectation via sandwiching, similarly to the proof of Proposition 3.1. Please note that the definitions for each component of the graph BFC are given in Appendix A.1.2 and we will not be restating them here to avoid repetition. To start, notice that the BFC on \\(G\\) takes a simpler form compared to the more general, original definition A.2: \\[R i c(i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{(\\gamma_{\\mathrm{max}}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|), \\quad (24)\\] This result follows from the fact that bipartite graphs have no triangles, i.e., \\(\\sharp_{\\Delta}(i,j) = 0 \\forall (i,j) \\in E\\) . Our main focus for now will the rightmost term, which constitutes the difference between \\(\\mathrm{Ric}(i,j)\\) and \\(\\underline{{R}}(i,j)\\) (Equation 8). Firstly note that this term is, by definition, non- negative. This implies that \\(\\underline{{R}}(i,j)\\) of \\(\\mathrm{Ric}(i,j)\\) . We can simplify the definitions of the 4- cycle forming neighbors to match the bipartite topology of \\(G\\) : \\[\\begin{array}{r l} & {\\sharp_{\\square}^{i}(i,j)\\coloneqq \\{k\\in S_{1}(i)\\setminus \\{k\\} :\\exists w\\in (S_{1}(k)\\cap S_{1}(j))\\setminus \\{i\\} \\} ,}\\\\ & {\\sharp_{\\square}^{j}(i,j)\\coloneqq \\{k\\in S_{1}(j)\\setminus \\{k\\} :\\exists w\\in (S_{1}(k)\\cap S_{1}(i))\\setminus \\{j\\} \\} .} \\end{array} \\quad (25)\\] By definition of \\(S_{1}(\\cdot)\\) it follows immediately that the cardinality of both sets is bounded above by the node degrees: \\[\\begin{array}{r l} & {|\\sharp_{\\square}^{i}(i,j)|\\leq d_{i} - 1,}\\\\ & {|\\sharp_{\\square}^{j}(i,j)|\\leq d_{j} - 1 = k - 1.} \\end{array} \\quad (28)\\] We now turn to the term \\(\\gamma_{\\mathrm{max}}(i,j)\\) , whose definition can also be simplified, as a consequence of the topology of \\(G\\) . Consider first the symmetric adjacency matrix of a bipartite graph, given by: \\[A = \\left[ \\begin{array}{ll}0 & B\\\\ B^{T} & 0 \\end{array} \\right]\\in \\{0,1\\}^{N + M\\times N + M}, \\quad (29)\\] where \\(B\\) is an \\(N\\times M\\) incidence matrix with \\(B_{i j} = 1\\) if there if \\(i\\sim j\\in E\\) , and \\(B_{i j} = 0\\) otherwise, \\(B^{T}\\) is the transpose of \\(B\\) (which ensures that \\(A\\) is symmetric), and \\(\\mathbf{0}\\) are zero blocks of size \\(N\\times N\\) and \\(M\\times N\\) corresponding to the absence of edges within the partitions. We can thus express \\(\\gamma_{\\mathrm{max}}(i,j)\\) as: \\[\\gamma_{\\mathrm{max}}(i,j)\\coloneqq \\max \\left\\{\\max_{k\\in \\sharp_{\\square}^{i}}\\{A_{k}\\cdot A_{j} - 1\\} ,\\max_{w\\in \\sharp_{\\square}^{j}}\\{A_{w}\\cdot A_{i} - 1\\} \\right\\} , \\quad (30)\\] since \\(A_{i}\\odot A_{j} = \\mathbf{0}\\) . The operation \\(A_{k}\\cdot A_{j} = \\nu \\in \\mathbb{N}_{0}\\) returns the number of common neighbors \\(\\nu\\) that nodes \\(k\\) and \\(j\\) have, with \\(k\\) and \\(j\\) being in the same partition. Therefore, we have the following inequalities that can be used to derive an upper bound for \\(\\gamma_{\\mathrm{max}}(i,j)\\) : \\[\\begin{array}{r l} & {\\max_{k\\in \\sharp_{\\square}^{i}}\\{A_{k}\\cdot A_{j} - 1\\} \\leq k - 1,}\\\\ & {\\max_{w\\in \\sharp_{\\square}^{j}}\\{A_{w}\\cdot A_{i} - 1\\} \\leq M - 1,}\\\\ & {\\gamma_{\\mathrm{max}}(i,j)\\leq \\max \\{k - 1,M - 1\\} .} \\end{array} \\quad (31)\\] Putting everything together we obtain: \\[0\\leq \\frac{(\\gamma_{\\mathrm{max}}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|)\\leq \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\cdot \\max \\{d_{i},d_{j}\\}}, \\quad (34)\\] so that we can write: \\[-2< \\underline{{R}} (i,j)\\leq R i c(i,j)\\leq \\bar{R} (i,j)\\leq 0. \\quad (35)\\] In Proposition 3.2), we have previously that as \\(\\alpha \\to \\infty\\) \\(\\mathbb{E}_{(i\\sim j)}[\\underline{{R}} (i,j)]\\to \\frac{2}{k} - 2\\) , due to the fact that the literal degree becomes a dominant term. Therefore \\(\\max \\{d_{i},d_{j}\\} = d_{i}\\) , and under the same limiting assumption we have that \\(\\max \\{k - 1,M - 1\\} = M - 1\\) . Given that the expected value is a linear operation, we obtain that: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}]} +\\frac{2}{k} -2 + \\frac{1}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[M - 1]} = \\frac{2}{k} -2. \\quad (36)\\]\n\nGiven that both limits and expectations preserve weak inequalities, by sandwiching we obtain: \\[\\frac{2}{k} -2 = \\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\leq \\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\leq \\lim_{\\alpha \\to 0}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{k} -2, \\quad (37)\\] ## A.3 Details on Test-Time Rewiring Graph rewiring is the process of modifying a graph's connectivity by adding, removing, or reweighting edges to optimize information flow. In our case, we make use of the rewiring procedure presented by Topping et al. [47], which consists of a discrete and stochastic Ricci flow, guided by the BFC. At each iteration, the edge with the most negative curvature (i.e., the structurally most \"strained\" connection) is identified. Candidate rewiring edges are then generated between the neighborhoods of the two endpoints of this edge. A new edge is stochastically selected either at random (with probability \\(p\\) ) or by maximizing the curvature improvement obtained from its addition. The selected edge is added to the graph and the corresponding curvature values are updated. By repeating this procedure for a fixed number of iterations, the algorithm progressively increases the average curvature of the graph, yielding a rewired version that contains information bottlenecks that are weaker compared to the input. Algorithm 1 contains the pseudocode for our rewiring algorithm. We would like to stress here that this modifies the constraints of the Boolean Satisfiability Problem (SAT) problem under consideration, but the goal of this procedure is to show that \"flatter\" problems will in fact be easier to solver for Graph Neural Network (GNN)- based solvers. Algorithm 1: Balanced Forman Curvature Stochastic Rewiring Input: Graph \\(G = (V,E)\\) with edge BFC values \\(R i c(i,j),\\forall (i,j)\\in E\\) , probability value \\(p\\in [0,1]\\) , number of iterations \\(N\\in \\mathbb{N}\\) Output: Rewired graph \\(G^{\\prime}\\) with updated BFC values for \\(t\\gets 1\\) to \\(N\\) do Select edge \\((i,j)\\) with most negative curvature \\(R i c(i,j)\\) From the neighbors \\(S_{1}(i)\\) and \\(S_{1}(j)\\) form candidate edge set \\[C = \\{(k,l):k\\in S_{1}(i),l\\in S_{1}(j),k\\neq l,(k,l)\\notin E\\}\\] if \\(C = \\emptyset\\) then L continue With probability \\(p\\) , choose a random edge \\((k,l)\\in C\\) Otherwise, for each \\((k,l)\\in C\\) 1. Compute updated curvature \\(R i c^{\\prime}(i,j)\\) after adding \\((k,l)\\) 2. Evaluate improvement score \\(\\Delta_{k l} = -(R i c(i,j) - R i c^{\\prime}(i,j))\\) Select \\((k,l)\\) with maximum \\(\\Delta_{k l}\\) Add edge \\((k,l)\\) to \\(G\\) and update neighborhood sets; Update curvatures \\(R i c(i,j)\\) and \\(R i c(k,l)\\) accordingly; Finalize bipartite structure of literals and clauses \\((L,C)\\) ensuring no intra- partition edges and set \\(G^{\\prime} = G\\) for \\(t = N\\) return \\(G^{\\prime}\\) ## A.4 Some initial ideas for curvature-aware solvers As discussed during the conclusion, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. In this section, we provide some starting points and experiments for simple implementations of curvature aware solvers that could lead to future improvements. The goal of our implementations was to maintain efficiency and rely on straightforward ideas that could lead to performance improvements. For these purposes, we introduce two simple curvature- aware variants of message passing. The first is an adaptation of Ye et al.\n\nTable 3: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers with different message-passing schemes: 1) Vanilla uses the typical message-passing operation; 2) Curvature Gate learns a gating function for each edge based on its curvature value [49]; 3) Online LCP extends the work of Fesser and Weber [16] and concatenates the local curvature statistics around each nodes as features during each recurrent step; 4) Both uses Curvature Gate and Online LCP. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface. <table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Variation</td><td colspan=\"4\">Datasets</td></tr><tr><td>3-SAT</td><td>4-SAT</td><td>SR</td><td>CA</td></tr><tr><td rowspan=\"3\">GCN</td><td>Vanilla</td><td>0.510 \u00b1 0.012</td><td>0.180 \u00b1 0.048</td><td>0.470 \u00b1 0.031</td><td>0.650 \u00b1 0.016</td></tr><tr><td>+ Curvature Gate</td><td>0.514 \u00b1 0.018</td><td>0.154 \u00b1 0.017</td><td>0.422 \u00b1 0.013</td><td>0.664 \u00b1 0.042</td></tr><tr><td>+ Online LCP</td><td>0.510 \u00b1 0.014</td><td>0.170 \u00b1 0.010</td><td>0.422 \u00b1 0.016</td><td>0.662 \u00b1 0.016</td></tr><tr><td></td><td>+ Both</td><td>0.500 \u00b1 0.012</td><td>0.176 \u00b1 0.031</td><td>0.416 \u00b1 0.027</td><td>0.654 \u00b1 0.027</td></tr><tr><td rowspan=\"3\">Neuro SAT</td><td>Vanilla</td><td>0.690 \u00b1 0.022</td><td>0.436 \u00b1 0.032</td><td>0.734 \u00b1 0.017</td><td>0.746 \u00b1 0.018</td></tr><tr><td>+ Curvature Gate</td><td>0.682 \u00b1 0.030</td><td>0.436 \u00b1 0.015</td><td>0.742 \u00b1 0.027</td><td>0.758 \u00b1 0.016</td></tr><tr><td>+ Online LCP</td><td>0.692 \u00b1 0.020</td><td>0.416 \u00b1 0.046</td><td>0.724 \u00b1 0.022</td><td>0.726 \u00b1 0.059</td></tr><tr><td></td><td>+ Both</td><td>0.664 \u00b1 0.018</td><td>0.438 \u00b1 0.028</td><td>0.742 \u00b1 0.025</td><td>0.758 \u00b1 0.020</td></tr></table> <center>Figure 3: Low dimensional visualization of the literal embeddings produced by Neuro SAT on random 4-SAT with (a) vanilla message passing and (b) Curvature Gate. Even though there is no major change in performance, the learned representations can be linearly separated into truth value assignments in the curvature-aware case, indicating promise for the inclusion of these principles in future GNN-based solver design. </center> [49], where the curvature of an edge is used to learn a gating mechanism that modulates message contributions. The second is a simple recurrent extension to Fesser and Weber [16], where the statistics of the curvature around each node are used as additional features at each recurrent step. Our empirical findings reveal that naively injecting curvature into GNN- based solvers sometimes leads to improved performance, but it does not always provide clear advantages, as seen in Table 3. Furthermore, we also experimented with a contemporary use of both variations. The training protocol and experimental settings are kept identical to previous experiments. The results highlight a subtle but important point: while curvature exposes structural bottlenecks, effective GNN solvers must also learn how to properly use geometric information. A major weakness of both methods is that random \\(k\\) - SAT problems have a lot of regularity, in the sense that the clause partitions will have very similar curvature statistics and thus the learning signal becomes redundant. Nevertheless, we believe that both these implementations provide interesting starting points for future work and research.\n\n<center>(a) Average graph Ricci curvature lower bound (Equation 8) as a function of \\(k\\) and \\(\\alpha\\) . </center> <center>(b) Average graph Ollivier-Ricci curvature as a function of \\(k\\) and \\(\\alpha\\) . </center> Figure 4: Contour plots of the average graph Ricci curvature lower bound (a) and Ollivier- Ricci curvature (b) displaying the changes in average curvature as a function of \\(k\\) and \\(\\alpha\\) . Both quantities behave similarly, especially as \\(\\alpha \\to 0\\) . We can see that in the case of the ORC, the stronger correction towards positive curvature due to the presence of cycles as \\(\\alpha \\to \\infty\\) is in line with the general theory related to this discretization of the Ricci curvature [5]. Notice the difference between the contours of (b) and Figure 1b for large \\(\\alpha\\) and \\(k\\) . Best viewed in color. <center>Figure 5: Visualization of an easy, in terms of clause density \\(\\alpha\\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very small number of long-range interactions, as can clearly be seen in (b). Furthermore, for such very small \\(\\alpha\\) , the average BFC approaches 0, in line with the developed theory. </center> ## A.5 Additional plots In this section we provide additional and miscellaneous plot that help with visualizing various concepts explained in the main manuscript, such as the relationship between curvature and hardness, as well as visualizations of easy and hard problems. More details can be found in Figures 4, 5, 6 and 7.\n\ncompressed into finite- dimensional embeddings. This bottleneck severely restricts the ability of GNNs to model long- range dependencies, and can be thought of as a vanishing gradient problem. Given that the difficulty of SAT can actually be understood intuitively as being proportional to the number of long- range dependencies between nodes, we question whether learning GNN- based solvers on these difficult problems is impacted by oversquashing. Recent geometric analyses propose that oversquashing is directly tied to negative Ricci Curvature (RC) of the underlying graph [38, 47]. These insights invite a fundamental question: Can graph RC serve as a predictive and constructive notion in unraveling novel hardness concepts for GNN- based SAT solvers? In this work, we address this question by studying the typical- case behavior of particular instantiations of graph RC on random \\(k\\) - SAT problems represented as bipartite graphs. Our starting point is the observation that edges of bipartite graphs are always non- positively curved. We show that, on average, the edges become more negatively curved as problems get harder, and less negatively curved as they become easier. Finally, we derive an exact expression for the average Balanced Forman Curvature (BFC) in the limit of unsolvable problems, from which we derive a connection between curvature and oversquashing in GNNs- based SAT solvers, following the theory of Topping et al. [47]. This is, to the best of our knowledge, the first successful attempt at a theoretical characterization of the limitations of GNN- based SAT solvers. To validate our theory, we perform experiments on random 3- and 4- SAT problems, and also on a vast array of datasets coming from the recent benchmark of Li et al. [28]. Firstly, we observe a phase transition- like phenomenon in random 3- SAT solving probability as a function of the mean and variance of the curvature. We further affirm the aforementioned limitations by rewiring only the testing graphs of the benchmarks at test- time to increase their average BFC, and show that these rewired problems become much easier to solve. Finally, we find that heuristics based on the BFC of a dataset correlate extremely well with generalization error, unlike the average clause density, which is typically used to characterize the hardness of a single instance. Overall, our findings suggest that GNN- based SAT solvers have two distinct types of hardness: the hardness of learning representations in negatively curved structures, followed by the well- established algorithmic hardness of SAT. We conclude by relating our findings with modeling principles and design choices of existing approaches. Outline. The remainder of this article is structured as follows: In Section 2, we provide a recap of the most relevant concepts discussed in the paper: random \\(k\\) - SAT, GNNs, and graph RC. Section 3.1 presents the main theoretical results, including an in- depth discussion on how oversquashing affects GNNs- based solvers. We then demonstrate the experimental evidence in Section 4. Finally, we provide a discussion regarding current design principles and future work in Section 5. ## 2 Background This section is only intended to formally introduce the objects studied in the paper and render the material self- contained. We kindly ask the reader to tolerate the occasional whirlwind and abuse of notation, as it will be formalized later in Section 3. ### 2.1 Random \\(k\\) Boolean Satisfiability Problem The random \\(k\\) - SAT (assignment) problem, which is the central object of study in this work, is made up of \\(N\\) variables \\(\\{x_{i}\\}_{i = 1}^{N}\\) that can take binary values \\(x_{i} \\in \\{0,1\\}\\) . Using these variables, one constructs \\(M\\) clauses containing a disjunction of \\(k\\) variables or their negations (called literals). For example, a 3- SAT problem would have clauses of the form \\((x_{i} \\vee \\neg x_{j} \\vee x_{h})\\) . The goal is to assign a value to all literals such that they satisfy the conjunction of all clauses. This logical formula is called a Conjunctive Normal Form (CNF). In the random formulation, it is possible to identify different phases of problem hardness based on a parameter called the clause density \\(\\alpha = M / N\\) . This phenomenon has been actively researched in statistical physics due to the analogies between random \\(k\\) - SAT and spin- glass models [33, 34]. Notable results[26, 37, 50] include the discovery of two transitions for typical instances at a given \\(k\\) , based on the value of \\(\\alpha\\) in the thermodynamic limit \\((M, N \\to \\infty)\\) : As \\(\\alpha\\) increases, the measure over the space of possible solutions first decomposes into an exponential number of clusters at the dynamical transition \\(\\alpha_{d}(k)\\) and subsequently condensates over the largest such states at the critical transition \\(\\alpha_{c}(k)\\) . These phase transitions naturally affect the performances of many algorithms, e.g., when \\(k \\geq 4\\) , going beyond \\(\\alpha_{c}\\) is almost impossible with existing algorithms.\n\n<center>Figure 6: Visualization of a hard, in terms of clause density \\(\\alpha\\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very large number of long-range interactions, as can clearly be seen in (b). Furthermore, for such large \\(\\alpha\\) , the average BFC is strongly negative, in line with the developed theory. </center> <center>Figure 7: Average BFC as a function of \\(\\alpha\\) for random 3 and 4 -SAT problems with \\(N = 256\\) . The size of the blobs is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model [46]. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c}\\) [32]. The average BFC drops monotonically with \\(\\alpha\\) in both cases. For 3-SAT (a), this drop appears to be almost linear, with a substantial amount of variance, as also depicted in Figure 2. 4-SAT on the other hand contains problems where the curvature is substantially more negative and concentrated, which when coupled with the larger number of constraints leads to oversquashing, as discussed in our theory. This can be clearly seen by the fast performance drop-off in (b), which happens much earlier than \\(\\alpha_{c}\\) , indicating the additional hardness phase related to representation learning discussed in the main paper. </center>\n\n<center>Figure 8: Analogue of Figure 2a for 4-SAT. Average BFC as a function of \\(\\alpha\\) for random 4-SAT problems with \\(N = 256\\) . The color is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model, with a group labeled as solvable if \\(50\\%\\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c} \\approx 9.931\\) [32]. The average BFC is negative and drops with \\(\\alpha\\) , but for a small amount. The plot in the top-right corner shows the model's solution probability curve as a function of \\(\\alpha\\) , where it is possible to notice that the GNN-based solver has severe limitations on these problems. Differently from 3-SAT (Figure 2a), the average curvature is negative and concentrated even for problems that would be considered very simple in terms of \\(\\alpha\\) ( \\(\\alpha \\rightarrow 8\\) ). This is due to the larger value of \\(k\\) , the higher number of long-range interactions, and the connection of both factors with oversquashing, as discussed and predicted by our contributions. </center>\n\n### 2.2 Graph Neural Networks GNNs are a subclass of Neural Networks (NNs) that can learn a representation of graph data by locally aggregating information[20, 45]. The main goal of the architecture is to implement inductive biases natural to graph data [9]. An example of such a property is learning graph- level functions invariant to the nodes' ordering. Consider, for simplicity, an unweighted and undirected graph \\(G\\) with \\(N\\) nodes, represented by a symmetric binary adjacency matrix \\(A\\in \\{0,1\\}^{N\\times N}\\) . This setting can be easily extended to deal with more general connectivity structures [14]. By associating a node feature matrix \\(X\\in \\mathbb{R}^{N\\times d}\\) to the graph, we can describe a GNNs as a convolution of the graph signal with \\(A\\) as the shift operator. A generalization of this concept can be obtained by considering the message- passing framework [20]: \\[x_{i}^{(k)} = \\theta^{(k)}\\left(x_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\phi^{(k)}\\left(x_{i}^{(k - 1)},x_{j}^{(k - 1)},e_{j i}\\right)\\right), \\quad (1)\\] where \\(x_{i}^{(k)}\\) denotes node features of node \\(x_{i}\\) at layer \\(k\\) \\(e_{j i}\\) the (optional) edge features from node \\(j\\) to node \\(i\\) \\(\\mathcal{N}(\\cdot)\\) the set of (1- hop) neighbor nodes, \\(\\bigoplus\\) a differentiable, permutation invariant function, (e.g., sum, mean), and \\(\\phi ,\\theta\\) denote differentiable and (optionally) nonlinear functions such as Multi- Layer Perceptrons (MLPs). A CNF formula can be easily translated into a bipartite graph [6], which can then be fed into a GNN- based solver. The particular bipartition we consider in this work is detailed later in Section 3. The application of the above message- passing scheme for SAT problems can be done by applying Equation1 to the clause and literal partitions [28]. Let \\(i\\) be a literal node and \\(j\\) be a clause node, then: \\[\\begin{array}{r l} & {h_{j}^{(k)} = \\theta_{c}^{(k)}\\left(h_{j}^{(k - 1)},\\bigoplus_{i\\in \\mathcal{N}(j)}\\left(\\left\\{\\phi_{i}^{(k)}\\left(h_{i}^{(k - 1)}\\right)\\right\\}\\right)\\right),}\\\\ & {h_{i}^{(k)} = \\theta_{l}^{(k)}\\left(h_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\left(\\left\\{\\phi_{c}^{(k)}\\left(h_{j}^{(k - 1)}\\right)\\right\\}\\right)\\right),} \\end{array} \\quad (2)\\] where the subscripts \\(c\\) and \\(l\\) refer to NNs specialized on the clause and literal partitions respectively. ### 2.3 Ricci Curvature of Graphs In Riemannian geometry, RC quantifies the local deviation of a manifold \\(\\mathcal{M}\\) from flat Euclidean space, as a result of the metric defined on \\(\\mathcal{M}\\) . Intuitively, RC captures how the neighborhoods of two adjacent points relate when moving from one point to the other. On smooth manifolds, it compares how a small ball of mass around a point is distorted when transported along a geodesic to a neighboring point. Extending this notion to more general structures, such as metric spaces or combinatorial complexes, has been an extremely active area of mathematical research, with the works of Ollivier [39] and Forman [18] standing out. In the case of graphs, we ask ourselves how local connectivity either concentrates or disperses. Ollivier [39] implements this idea by comparing probability mass on local neighborhoods, i.e., a random walk distribution on the endpoints of an edge. Given these two distributions, one can compare the ratio between their Wasserstein and shortest path distance, serving as a direct and discrete analogue of geodesic transport. See Appendix A.1.1 for more details. The definition of Forman [18] relies heavily on topology, and thus it takes a combinatorial form. Essentially, given a cell complex, the curvature of a p- cell depends only on the topological structures between the cell and its neighbors. This makes Forman- Ricci Curvature (FC) simpler to compute numerically, since it can avoid the optimization of the optimal transport problem that arises in Ollivier- Ricci Curvature (OC) curvature. Given that RC is directly related to the structure of local neighborhoods, it has emerged as a powerful way of theoretically analyzing limitations of GNNs. In a seminal paper, Topping et al. [47] provide both a balanced version of the FC curvature and show that the oversquashing problem [1] can be directly connected to edges with high negative curvature. This definition, namely the BFC, is central to this paper, therefore please consult Appendix A.1.2 for the definition and additional details. Nguyen et al. [38] have shown that similar results can be derived using the OC curvature. It is worth noting that these notions of curvature are naturally correlated with one another, as shown empirically in a multitude of complex networks by Samal et al. [44].\n\n## 3 Curvature of Random k-SAT Problems and Its Relationship with GNNs Setting and Notation. We consider random \\(k\\) - SAT problems with \\(N\\) variables and \\(M\\) clauses, with \\(\\alpha = M / N\\) and \\(k,N,M\\in \\mathbb{N}\\) . These problems are represented through a simple bipartite graph \\(G = (V,E)\\) , where the node set is a literal- clause bipartition \\(V = L\\cup C\\) , with \\(L\\cap C = \\emptyset\\) and \\(|L| = 2N,|C| = M\\) . The edge set takes the form \\(E = \\{(i,j)\\in V\\times V:i\\sim j,i\\in L,j\\in C\\}\\) where \\(i\\sim j\\) indicates a connection between nodes. Given \\(v\\in V\\) , we denote its degree by \\(d_{v}\\) . Finally, we denote the expected value of the random variable \\(X\\) with probability distribution \\(\\mathrm{P}\\) by \\(\\mathbb{E}_{P}[X]\\) and \\(\\mathbb{E}_{p\\sim P}[X]\\) the expectation over samples drawn from \\(P\\) . Unless noted otherwise, when we refer to the expected value in simulations, we imply its estimate via the sample mean statistic. Data Model. Our bipartite formulation is a simplification of the input graphs considered in many GNN- based solvers [40, 46]. Recent literature [28] refers to this data structure as a Literal- Clause Graph (LCG). Following an Erd\u0151s\u2013R\u00e9nyi- like procedure, each clause is assigned \\(k\\) literals independently at random with probability \\(p\\) . Assuming that all literals are equally likely to appear in a given clause, we obtain the following degree distributions: \\[\\begin{array}{c}{P(d_{j} = h) = \\delta (h - k)}\\\\ {P(d_{i} = h) = \\binom{M}{h} p^{h}(1 - p)^{M - h},} \\end{array} \\quad (3)\\] where \\(\\delta (\\cdot)\\) represents the Dirac delta function, \\(\\binom{\\cdot}{}\\) the binomial coefficient, and \\(p\\coloneqq \\frac{k}{2N}\\) . In the limit \\(M,N\\to \\infty\\) , we can approximate the Binomial form of the literal degree distribution with a Poisson distribution: \\[P(d_{i} = h) = \\frac{\\lambda^{h}e^{-\\lambda}}{h!}, \\quad (5)\\] with \\(\\lambda = M p = \\textstyle {\\frac{1}{2}}\\alpha k\\) . We will be interested in using this approximation to calculate properties of the graph RC, which is an edge level property, therefore the case \\(d_{i} = 0\\) should be truncated. This fact leads us to propose an alternative probability mass function based on zero- truncated Poisson distribution the for the literal degrees: \\[P^{*}(d_{i} = h) = P(d_{i} = h:h\\geq 1) = \\frac{P(d_{i} = h)}{1 - P(d_{i} = 0)} = \\frac{\\lambda^{h}}{h!(e^{\\lambda} - 1)}. \\quad (6)\\] Characterizing Average Curvature. We will now proceed by showing that the above bipartite representation of SAT problems has significant implications on the average RC. Most importantly, we will precisely characterize the average behavior of a specific definition curvature, namely the BFC, which in turn has strong implications on oversquashing in GNNs. The proofs of all statements are deferred to Appendix A.2. We start from an established lower bound of the OC. Definition 3.1 (OC lower bound in bipartite graphs [5, 23]). For any edge \\(i\\sim j\\) in a simple, unweighted bipartite graph \\(G\\) , we have that that: \\[\\underline{{\\kappa}}(i,j) = -2\\cdot \\max \\{0,1 - \\frac{1}{d_{i}} -\\frac{1}{d_{j}}\\} \\leq \\kappa (i,j)\\leq 0, \\quad (7)\\] where \\(\\kappa\\) is the OC. The above lower bound can be redefined by alternatively stating the condition encoded inside the maximum function in Equation 7: \\[\\underline{{R}}(i,j) = \\left\\{ \\begin{array}{l l}{0} & {\\mathrm{if~min}\\{d_{i},d_{j}\\} = 1}\\\\ {\\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2} & {\\mathrm{otherwise}} \\end{array} \\right. \\quad (8)\\] This alternative definition shows that Equation 8 is also a lower bound for the BFC s.t., \\(\\underline{{R}}(i,j)\\leq\\) \\(R i c(i,j)\\) . This statement is a direct consequence of the definition of the BFC, and a proof is given inside the proof of Theorem 3.1. Furthermore, we also have that \\(- 2< R i c(i,j)\\leq \\kappa (i,j)\\leq 0\\) , which is a direct consequence of the facts that \\(\\kappa (i,j)\\geq R i c(i,j)\\) [47] and Equation 7. Starting from these statements, we can proceed to show that the average behavior of the graph RC is naturally dictated by average degree of the literals, given that \\(d_{j} = k\\) always holds. The literal degree distribution is a zero- truncated Poisson distribution, from which the average degree of a literal \\(i\\) can be calculated as: \\[\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} \\quad (9)\\]\n\nProposition 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\to 0\\) and \\(\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]\\to 0\\) as \\(\\alpha = \\frac{M}{N}\\to 0\\) . Proposition 3.1 tells us that as problems get easier, i.e., they are in the satisfiable phase, their graph representations get (Ricci) flatter. This statement implies a relationship between very simple problems and their bipartite topology, which is that each clause is made of distinct literals. In this scenario, producing a satisfying assignment becomes trivial since assigning truth values to these unique literals does not affect other clauses. This implication aligns well with common knowledge, therefore the next thing to check is whether there is an opposite behavior when dealing with very difficult problems. This later case is important because it is close to the unsatisfiable phase where the faults of existing algorithms emerge [2]. Through a similar argument as before, we can formalize this intuition and characterize the average lower bound in the case when problems are surely unsatisfiable. Proposition 3.2. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) . In this scenario, the average lower bound of the graph RC will tend to be maximally negative, therefore we cannot directly speak on the average curvature by sandwiching as before. Nevertheless, Proposition 3.2 provides an extremely interesting insight into the interplay between \\(\\alpha\\) and \\(k\\) . As the problems get harder, the number of constraints (i.e., \\(k\\) ) becomes a decisive factor on the curvature. A larger value of \\(k\\) implies more constraints and thus many long range interactions between the literals, but it also implies that edges will become more negatively curved. In turn, this implies the existence of bottlenecks in the graph, which makes long range communication becomes difficult [47]. While it is not possible to connect the exact graph OC to this result, it is possible to extend it to the average graph BFC: Theorem 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. The BFC at \\(i\\sim j\\) is bounded from above by the quantity: \\[\\bar{R} (i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\max \\{d_{i},d_{j}\\}}. \\quad (10)\\] Furthermore, as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) , \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) and therefore the average BFC over the edges of \\(G\\) converges to \\(\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]\\to \\frac{2}{k} - 2\\) . Theorem 3.1 contains a crucial result in its characterization of the typical- case BFC, which is that it is connected to both \\(\\alpha\\) and \\(k\\) . While the connection to \\(\\alpha\\) might seem obvious, the one with \\(k\\) is quite insightful. One can observe that as problems become harder to satisfy (large values of \\(\\alpha\\) ), \\(k\\) plays an important role in how negatively curved the graph edges can become. This phenomenon can be seen in Figure 1, where for smaller \\(k\\) , larger values of \\(\\alpha\\) are required to have highly negatively curved edges. This latter point is crucial in developing a more profound understanding of the limitations of GNN- based solvers, and we will expand upon it in the following subsection. Another interesting consequence of our results, is that due to their shared lower bound and the results in Propositions 3.1 and 3.2, we confirm a tight link between graph OC and BFC. In general, the correlation between graph OC and FC has been studied in great detail on an array of complex networks by Samal et al. [44]. In our case, we confirm that both OC and BFC discretizations behave identically for simpler problems and similarly in relation to \\(\\alpha\\) and \\(k\\) , a phenomenon that can also be verified empirically through Figures 1 and 4 (Appendix). ### 3.1 Message-Passing Bottlenecks and Downstream Performance The results presented in the previous section allow us to proceed with a principled way of understanding performance limitations in GNN- based SAT solvers. This is due to the direct connection between the result in Theorem 3.1 to Theorem 4 of Topping et al. [47], which establishes that \"edges with high negative curvature are those causing the graph bottleneck and thus leading to the oversquashing phenomenon\". This seminal result states that if the gradients of the message passing functions ( \\(\\theta\\) and \\(\\phi\\) in Equation 1) are bounded, and there exists a sufficiently negatively curved edge compared to the degrees of its endpoints, then the derivative of the learned node representations around that edge vanishes. Intuitively, this can be understood as a difficulty of propagating the information in nodes at a reachable distance due to the fact that the graph structure limits the pathways where\n\n<center>Figure 1: Average graph Ricci Curvature lower bound (a) and Balanced Forman Curvature (b) as a function of \\(\\alpha\\) and \\(k\\) . Both quantities behave very similarly, both in terms of the smooth transition from flat to negative curvature and their magnitude, especially as \\(\\alpha \\to 0\\) and \\(\\alpha \\to \\infty\\) , in line with the developed theory. An alternative version of this figure displaying the average graph OC in (b) is presented in Figure 4 of the Appendix. Best viewed in color. </center> information can travel. Simply stated, nodes in different neighborhoods need to pass all messages through the same edge(s), leading to a difficulty in learning fixed- length representations that can hold information on long range correlations. Formally, for large values of \\(k\\) , as the clause density \\(\\alpha \\to \\infty\\) , any infinitesimally small value \\(\\delta > 0\\) could be used in Theorem 4 of Topping et al. [47] such that \\(Ric(i,j) \\leq - 2 + \\delta\\) , leading to an exponentially decaying Jacobian of the node representations around \\(i \\sim j\\) . This result leads us to the conclusion that GNN- based solvers are limited by both these parameters and suffer from two distinct hardness types: the algorithmic hardness inherent to SAT and the hardness of learning representations for long range communication. The interplay between \\(k\\) and \\(\\alpha\\) in Theorem 3.1 provides additional insights. Indeed, for problems with large values of \\(k\\) or large values of \\(\\alpha\\) , highly negatively curved edges are guaranteed to exist on average, and this quantity will concentrate. On the other hand, for large values of \\(\\alpha\\) and relatively small values of \\(k\\) , the latter becomes crucial in deciding how well a GNN- based solver will be able to learn, i.e., it should be easier to learn a solver for smaller values of \\(k\\) . We confirm this fact empirically in Section 4. The intuition we provide for a more complete understanding of this crucial result is the following: At increasing connectivity, literals become very distant on the interaction network, i.e., the number of long- range codependencies increases. In this scenario, the GNN will not be able to learn a fixed length representation that can \"remember\" the information of reachable, but not directly adjacent nodes. This means that the ability to learn a solver is compromised by an oversquashing phenomenon. For large values of \\(k\\) , this problem becomes prevalent even before the hardness of exploring the solution space, due to the effect of \\(k\\) on the BFC. Our theory motivates therefore how increasing values of \\(k\\) in random \\(k\\) - SAT would lead to worse oversquashing and performance, even for what would be considered simple problems in terms of \\(\\alpha\\) . To visually understand the aforementioned concepts, we can again refer to Figure 1. As the value of \\(k\\) grows, the gap between the flatter (yellow) and highly negatively curved problems (violet) gets smaller. The same holds for increasing values of \\(\\alpha\\) , as expected. We provide additional visual depictions of this aforementioned explanation in Appendix A.5, where we plot two input graphs for random 3- SAT at small (Figure 5) and large (Figure 6) \\(\\alpha\\) . In the following section, we will show that our results can be empirically confirmed for different GNN- based solvers. ## 4 Experiments Experimental Setting. To validate our theory, we perform different experiments on various datasets, the details of which will be provided in the following subsections. The experiments consist in firstly exploring the behavior of a GNN- based solvers and relating it to the input graph BFC. Based on these\n\n<center>Figure 2: (a) Average BFC as a function of \\(\\alpha\\) for random 3-SAT problems with \\(N = 256\\) . The color is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model [46], with a group labeled as solvable if \\(50\\%\\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c} \\approx 4.267\\) [32]. The average BFC drops monotonically with \\(\\alpha\\) . The small plot in the bottom-left corner provides the model's solution probability curve in terms of \\(\\alpha\\) , where it is possible to clearly notice the algorithmic transition from satisfiable to unsatisfiable problems. (b) Probability of finding a satisfying assignment of the same problems as in (a) with Neuro SAT as a function of the variance and average of the BFC. Notice how as \\(\\alpha\\) grows, the average curvature not only gets more negative, but also concentrates. We can see from the empirical results in (a) that in this case, the model is unable to produce a satisfying assignment. As \\(\\alpha\\) becomes smaller, the input graphs have less negative edges on average, the associated variance naturally grows, and so does the solving probability. Using the first two moments of the BFC, we are able to observe a similar transition-like phenomenon as the small plot in the bottom-left corner of (a). </center> results, we then propose two heuristics to understand how hard a given SAT dataset will be to solve for a GNN- based solver. We will focus on the assignment scenario, as it includes the decision scenario as well. Given that all the datasets we utilize do not come with node features, we make use of learned embeddings in order to effectively explore oversquashing implications. The GNN- based solvers are implemented following the design of Li et al. [28], using Py Torch [41], Py Torch- Geometric [17] and Py Torch Lightning [15]. The networks were trained for 100 epochs using the Adam W optimizer [29], with learning rate \\(\\eta = 0.0001\\) decaying by half after 50 epochs and the gradients clipped at unit norm. The training was done on NVIDIA Titan RTX GPUs. ### 4.1 The Relationship Between Curvature and Satisfiability We start by using numerical simulations to verify the theoretical claims made in Section 3.1. To do this, we generate random 3- SAT instances in Conjunctive Normal Form (CNF) using a custom implementation in the Julia language [4]. We generate problems with \\(\\alpha \\in [3,5]\\) in steps of \\(\\Delta \\alpha = 0.1\\) , capturing both satisfiable and unsatisfiable regimes around the known critical threshold \\(\\alpha_{c} \\approx 4.267\\) . Considering its widespread use and downstream performance, we train the Neuro SAT model [46] to produce a satisfying assignment, while scaling the number of message passing iterations by \\(2N\\) during evaluation, to maximize inference accuracy. We then analyze the performance of the model on problems with \\(N = 256\\) , with the results being summarized in Figure 2. Our results show that by considering the probability of finding a solution at a given \\(\\alpha\\) as a function of the first and second moments of the curvature, we can replicate a SAT/UNSAT phase- transition (Figure 2b). This result presents an important step forward in theoretically understanding the performance of GNN- based solvers.[32]. Similar results hold for random 4- SAT, and can be seen in Figures 7 and 8 in the Appendix. For this higher value of \\(k\\) , we have more negatively curved edges which strongly impact the performance of GNN- based solvers, as will further confirmed in Section 4.2. An interesting observation is that for random 3- SAT, the curvature starts to become highly negative and concentrate close to the estimated dynamical threshold \\(\\alpha_{d} \\approx 3.927\\) [32].\n\nTest- time Rewiring. In order to obtain additional evidence about the previous observations, we put ourselves in a unique scenario: Suppose a GNN- based solver is trained on a dataset of SAT problems, and later tested on a separate testing partition. If we render the said testing partition less curved, would the model perform better without needing to retrain? The purpose behind this experiment is to gain a deeper understanding of the relationship between curvature and problem complexity. For this purpose, we use four different SAT benchmarks proposed by Li et al. [28]: Random 3 and 4 - SAT generated near the (respective) critical threshold \\(\\alpha_{c}\\) , a random \\(k\\) - SAT dataset consisting of mixed \\(k\\) values (SR), and one that mimics the modularity and community structure of industrial problems (CA). The last two datasets are better representatives of real- world problems. We train both a Graph Convolutional Network (GCN)- solver [25] and Neuro SAT on training partitions using the same protocol as before, while the testing partition is rewired using a stochastic discrete Ricci flow procedure, similarly to [47]. The idea behind the rewiring procedure is quite simple: we make the input graph less curved by stochastically deleting edges that have the highest negative curvature, while adding new edges that are less curved. We provide a more detailed explanation of this process, including a schematic algorithm in Appendix A.3. The results are reported in Table 1, where it be observed that that the rewired problems become simpler to solve for both solvers at test- time. A noteworthy observation is that a large improvement happens on 4- SAT problems, while the modular CA dataset reports small improvements. In the following subsection, we make a direct connection of this result with our theory. ### 4.2 A New Hardness Heuristic for GNN-Based Solvers Based on the developed theory and the above observations, we provide, as a practical contribution, two different heuristics that reflect how hard it will be for a GNN- based solver to tackle a dataset. The main motivation behind these heuristics is that if we simply look at the average clause density of a dataset, we can miss out on direct implications of oversquashing. An example of this is the first dataset we presented for the random 3- SAT experiments discussed in Figure 2, which is build at increasing values of \\(\\alpha\\) . Given an input graph \\(G\\) , we define the heuristics as: \\[\\omega (G) = -\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]*\\mathbb{E}[\\alpha ],\\quad \\omega^{*}(G) = \\frac{\\omega(G)}{\\mathbb{V}_{(i\\sim j)}[Ric(i,j)]}, \\quad (11)\\] with the expectations being taken over the edges \\(G\\) . The averages of both heuristics, which we denote by \\(\\bar{\\omega}\\) and \\(\\bar{\\omega}^{*}\\) can then be used to judge the hardness of a given dataset. Intuitively, we provide two non- negative numbers that reveal how dense and curved \\(G\\) is on average \\((\\omega (g))\\) and how much this quantity concentrates \\((\\omega^{*}(G))\\) . Our theoretical insights tell us that these quantities should provide information into the hardness of learning a GNN- based solver. We report the generalization error (1 - testing accuracy) of Neuro SAT on the four previously mentioned benchmarks in Section 4.1, alongside the heuristics in Table 2. A (linear) correlation analysis between the error and the (normalized) heuristics reveals that our curvature- based approach serves as a better predictor of generalization: the respective correlation coefficients are \\(\\rho_{\\bar{\\alpha}} = 0.32\\) , \\(\\rho_{\\bar{\\omega}} = 0.86\\) and \\(\\rho_{\\bar{\\omega}^{*}} = 0.98\\) . These results allow us to formally motivate the performance gains during the test- time rewiring procedure discussed previously. What we observe, is that due to its community structure, the CA dataset has a large clause density, but its average curvature is much lower than that of random 4- SAT problems. This is natural, since a community structure is inherently linked with edges that act as less important bottlenecks for message passing [38]. These results show that the ability of GNN- based solvers to learn representations that can learn long range correlations and generalize well is deeply connected with the curvature of the input data, as discussed thoughtful the paper. ## 5 Conclusions Practical Takeaways and Future Work. In this paper, we have shown that the accuracy of GNN- based SAT solvers is directly related to the input data structure. This relationship is universally prevalent across all machine learning applications and as a result we have different modeling principles for different data. For SAT problems, we have identified that the geometry of the input data is a plausible cause of deficiency, due to its connections with oversquashing. What is extremely fascinating is that most modern GNN- based SAT solvers implement some type of recurrence mechanism [28, 35, 40, 46], and this architectural component has been recently shown to be a great starting point to mitigate oversquashing [3]. The implicit effect of recurrence can be immediately noted by comparing the drop in performance between the GCN and Neuro SAT solvers in Table 1.\n\nTable 1: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers. By increasing the testing set's curvature at test-time through rewiring, both solvers are able to make big leaps in accuracy, especially in more difficult problems. Reducing the curvature of the problem facilitates long range communication and renders problems easier. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface and absolute improvements in parentheses. <table><tr><td>Model</td><td>Variation</td><td>3-SAT</td><td>4-SAT</td><td>Datasets</td><td>SR</td><td>CA</td></tr><tr><td rowspan=\"2\">GCN</td><td>No Rewiring</td><td>0.510 \u00b1 0.012</td><td>0.180 \u00b1 0.048</td><td>0.470 \u00b1 0.031</td><td>0.650 \u00b1 0.016</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.626 \u00b1 0.021 (+0.116)</td><td>0.374 \u00b1 0.045 (+0.194)</td><td>0.696 \u00b1 0.035 (+0.226)</td><td>0.670 \u00b1 0.048 (+0.020)</td><td></td></tr><tr><td rowspan=\"2\">Neuro SAT</td><td>No Rewiring</td><td>0.690 \u00b1 0.022</td><td>0.436 \u00b1 0.032</td><td>0.734 \u00b1 0.017</td><td>0.746 \u00b1 0.018</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.820 \u00b1 0.030 (+0.130)</td><td>0.686 \u00b1 0.029 (+0.250)</td><td>0.902 \u00b1 0.004 (+0.168)</td><td>0.828 \u00b1 0.029 (+0.082)</td><td></td></tr></table> Table 2: Average generalization error (1- testing accuracy) over 5 different runs with the Neuro SAT model on SAT benchmark datasets [28]. The error is reported alongside the average clause density \\(\\bar{\\alpha}\\) and the curvature-based heuristics \\(\\bar{\\omega}\\) and \\(\\bar{\\omega}^{*}\\) . Our heuristics display very strong linear correlation with the generalization error, unlike the average clause density, making it possible to predict how hard each benchmark will be for a GNN-based solver. <table><tr><td rowspan=\"2\">Problem</td><td rowspan=\"2\">Generalization Error</td><td colspan=\"3\">Hardness Heuristic</td></tr><tr><td>\u03b1</td><td>\u03c9</td><td>\u03c9*</td></tr><tr><td>3-SAT</td><td>0.31</td><td>4.59</td><td>4.12</td><td>97.41</td></tr><tr><td>4-SAT</td><td>0.56</td><td>9.08</td><td>9.81</td><td>612.32</td></tr><tr><td>SR</td><td>0.27</td><td>6.09</td><td>5.30</td><td>125.30</td></tr><tr><td>CA</td><td>0.25</td><td>9.73</td><td>6.30</td><td>123.27</td></tr></table> This fact leads us to conjecture that the relationship between input data and model performance is prevalent throughout Neural Combinatorial Optimization (NCO) [48], and different architectural designs are necessary for different problems. Furthermore, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. We report results for some straightforward curvature- aware solvers in Appendix A.4, which do not report consistent improvements. A direct avenue for future work that we consider promising in this field is the application of continuous graph diffusion dynamics for learning[11, 21], which generalize the recurrence mechanism. Finally, regarding the theoretical aspect, we believe that promising avenues for future work that were not directly addressed here are the characterization of both curvature and topological quantities in distribution and not just in the mean. Another direction that is interesting is making a connection between the various critical points of the clause density and the curvature, such that the well- known phase transitions of SAT can be directly related to novel geometric order parameters. Closing Remarks. In conclusion, our results highlight that the limitations of GNN- based SAT solvers cannot be fully understood without considering the geometric properties of the input. Our study presents, to the best of our knowledge, the first attempt at a theoretical understanding of these neural solvers, by establishing a direct connection between their negatively curved graph representations and oversquashing in GNNs. We provide empirical evidence of this connection and verify that it is prevalent on more constrained instances. Beyond SAT, we expect these insights to be valuable for other domains where graph representations of combinatorial problems are employed. Combinatorial Optimization (CO) provides an interesting venue to study the reasoning behavior of NNs, and we hope that this paper makes a case for such studies. In conclusion, we hope that bridging concepts from deep learning, geometry, and physics, will pave the way for principled advances in the design of neural solvers.",
    "introduction": "",
    "acknowledgements_acknowledgements_geri_skenderi_is_funded_by_the_european_union_through_the_next_generation_eu_-_miur_prin_pnrr_2022_grant_p20229pbzr_the_views_and_opinions_expressed_are_however_those_of_the_authors_only_and_do_not_necessarily_reflect_those_of_the_european_union_or_the_european_research_council_executive_agency_neither_the_european_union_nor_the_granting_authority_can_be_held_responsible_references_1_uri_alon_and_eran_yahav_on_the_bottleneck_of_graph_neural_networks_and_its_practical_implications_in_international_conference_on_learning_representations_2021_1_3_2_maria_chiara_angelini_and_federico_ricci-_tersenghi_monte_carlo_algorithms_are_very_effective_in_finding_the_largest_independent_set_in_sparse_random_graphs_physical_review_e_1001_013302_2019_1_5_3_alvaro_arroyo_alessio_gravina_benjamin_gutteridge_federico_barbero_claudio_gallicchio_xiaowen_dong_michael_bronstein_and_pierre_vandergheynst_on_vanishing_gradients_oversmoothing_and_over-_squashing_in_gnns_bridging_recurrent_and_graph_learning_ar_xiv_preprint_ar_xiv250210818_2025_8_4_jeff_bezanson_alan_edelman_stefan_karpinski_and_viral_b_shah_julia_a_fresh_approach_to_numerical_computing_siam_review_59165-_98_2017_url_httpsdoiorg101137141000671_7_5_bhaswar_b_bhattacharya_and_sumit_mukherjee_exact_and_asymptotic_results_on_coarse_ricci_curvature_of_graphs_discrete_mathematics_338123-_42_2015_4_13_15_19_6_armin_biere_marijn_heule_and_hans_van_maaren_handbook_of_satisfiability_volume_185_ios_press_2009_1_3_7_lucas_bordeaux_youssef_hamadi_and_lintao_zhang_propositional_satisfiability_and_constraint_programming_a_comparative_survey_acm_computing_surveys_csur_38412-_es_2006_1_8_a_braunstein_m_mezard_and_r_zecchina_survey_propagation_an_algorithm_for_satisfiability_random_structures_algorithms_272201-_226_2005_doi_httpsdoiorg101002rsa20057_url_httpsonlinelibrarywileycomdoiabs101002rsa20057_1_9_michael_m_bronstein_joan_bruna_yann_le_cun_arthur_szlam_and_pierre_vandergheynst_geometric_deep_learning_going_beyond_euclidean_data_ieee_signal_processing_magazine_34418-_42_2017_3_10_quentin_cappart_didier_chetelat_elias_b_khalil_andrea_lodi_christopher_morris_and_petar_veli\u010dkovi\u0107_combinatorial_optimization_and_reasoning_with_graph_neural_networks_journal_of_machine_learning_research_241301-_61_2023_1_11_ben_chamberlain_james_rowbottom_maria_i_gorinova_michael_bronstein_stefan_webb_and_emanuele_rossi_grand_graph_neural_diffusion_in_international_conference_on_machine_learning_pages_1407-_1418_pmlr_2021_9_12_wenjing_chang_and_wenlong_liu_sat-_gatv2_a_dynamic_attention-_based_graph_neural_network_for_solving_boolean_satisfiability_problem_electronics_143423_2025_1_13_stephen_a_cook_the_complexity_of_theorem-_proving_procedures_in_proceedings_of_the_third_annual_acm_symposium_on_theory_of_computing_stoc_71_page_151-_158_new_york_ny_usa_1971_association_for_computing_machinery_isbn_9781450374644_doi_101145800157805047_url_httpsdoiorg101145800157805047_1_14_gabriele_corso_hannes_stark_stefanie_jegelka_tommi_jaakkola_and_regina_barzilay_graph_neural_networks_nature_reviews_methods_primers_4117_2024_3_15_william_falcon_and_the_py_torch_lightning_team_py_torch_lightning_march_2019_url_httpsgithubcomlightning-_ailightning_7_16_lukas_fesser_and_melanie_weber_effective_structural_encodings_via_local_curvature_profiles_in_international_conference_on_learning_representations_2024_18_17_matthias_fey_and_jan_eric_lenssen_fast_graph_representation_learning_with_pytorch_geometric_ar_xiv_preprint_ar_xiv190302428_2019_7": "[18] Forman. Bochner's method for cell complexes and combinatorial ricci curvature. Discrete & Computational Geometry, 29:323- 374, 2003. 3, 14 [19] Karlis Freivalds and Sergejs Kozlovics. Denoising Diffusion for Sampling SAT Solutions, November 2022. URL http://arxiv.org/abs/2212.00121. ar Xiv:2212.00121 [cs]. 1 [20] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263- 1272. PMLR, 2017. 1, 3 [21] Andi Han, Dai Shi, Lequan Lin, and Junbin Gao. From continuous dynamics to graph neural networks: Neural diffusion and beyond. Transactions on Machine Learning Research, 2024. 9 [22] Masato Inagaki. Spectral convergence of graph laplacians with ricci curvature bounds and in non- collapsed ricci limit spaces. ar Xiv preprint ar Xiv:2506.07427, 2025. 13 [23] Jurgen Jost and Shiping Liu. Ollivier's ricci curvature, local clustering and curvature- dimension inequalities on graphs. Discrete & Computational Geometry, 51(2):300- 322, 2014. 4 [24] Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer computations, pages 85- 103. Springer, 1972. 1 [25] TN Kipf. Semi- supervised classification with graph convolutional networks. ar Xiv preprint ar Xiv:1609.02907, 2016. 8 [26] Florent Krzakala, Andrea Montanari, Federico Ricci- Tersenghi, Guilhem Semerjian, and Lenka Zdeborov\u00e1. Gibbs states and the set of solutions of random constraint satisfaction problems. Proceedings of the National Academy of Sciences, 104(25):10318- 10323, 2007. 1, 2 [27] Qimai Li, Zhichao Han, and Xiao- Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. 1 [28] Zhaoyu Li, Jinpei Guo, and Xujie Si. G4sabench: Benchmarking and advancing sat solving with graph neural networks. Transactions on Machine Learning Research, 2024. 1, 2, 3, 4, 7, 8, 9, 18 [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 7 [30] Raffaele Marino, Giorgio Parisi, and Federico Ricci- Tersenghi. The backtracking survey propagation algorithm for solving random k- sat problems. Nature communications, 7(1):12996, 2016. 1 [31] Joao Marques- Silva. Practical applications of boolean satisfiability. In 2008 9th International Workshop on Discrete Event Systems, pages 74- 80, 2008. doi: 10.1109/WODES.2008.4605925. 1 [32] Stephan Mertens, Marc Mezard, and Riccardo Zecchina. Threshold values of random k- sat from the cavity method. Random Structures & Algorithms, 28(3):340- 373, 2006. doi: https://doi.org/10.1002/rsa.20090. 7, 20, 21 [33] Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009. 1, 2 [34] Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientific Publishing Company, 1987. 2 [35] David Moj\u017ei\u0161ek, Jan \u0171la, Ziwei Li, Ziyu Zhou, and Mikol\u00e1\u0161 Janota. Neural approaches to sat solving: Design choices and interpretability. ar Xiv preprint ar Xiv:2504.01173, 2025. 8 [36] R\u00e9mi Monasson and Riccardo Zecchina. Statistical mechanics of the random k- satisfiability model. Physical Review E, 56(2):1357, 1997. 1 [37] M. Mezard, G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812- 815, 2002. doi: 10.1126/science.1073287. URL https://www.science.org/doi/abs/10.1126/science.1073287. 2 [38] Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 2, 3, 8\n\n[39] Yann Ollivier. Ricci curvature of markov chains on metric spaces. Journal of Functional Analysis, 256(3):810- 864, 2009. 3, 13 [40] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. Goal- Aware Neural SAT Solver. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1- 8, July 2022. doi: 10.1109/IJCNN55064.2022.9892733. URL http://arxiv.org/abs/2106.07162. ar Xiv:2106.07162 [cs]. 1, 4, 8 [41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. Advances in neural information processing systems, 32, 2019. 7 [42] Daniel Paulin. Mixing and concentration by ricci curvature. Journal of Functional Analysis, 270(5):1623- 1662, 2016. 13 [43] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5- 6):355- 607, 2019. 13 [44] Areejit Samal, RP Sreejith, Jiao Gu, Shiping Liu, Emil Saucan, and J\u00fcrgen Jost. Comparative analysis of two discretizations of ricci curvature for complex networks. Scientific reports, 8(1): 8650, 2018. 3, 5, 14 [45] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61- 80, 2008. 1, 3 [46] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, David L. Dill, and Leonardo De Moura. Learning a SAT solver from single- bit supervision. 7th International Conference on Learning Representations, ICLR 2019, pages 1- 11, 2019. ar Xiv: 1802.03685. 1, 4, 7, 8, 20 [47] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. Understanding over- squashing and bottlenecks on graphs via curvature. In International Conference on Learning Representations, 2022. 2, 3, 4, 5, 6, 8, 13, 14, 15, 17 [48] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth Mc Millan, and Risto Miikkulainen. Neuro Comb: Improving SAT Solving with Graph Neural Networks, June 2022. URL http://arxiv.org/abs/2110.14053. ar Xiv:2110.14053 [cs] version: 2. 9 [49] Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. Curvature graph network. In International Conference on Learning Representations, 2019. 18 [50] Lenka Zdeborov\u00e1. Statistical physics of hard optimization problems. Acta Physica Slovaca, 59 (3):169- 303, 2009. 1, 2",
    "a_appendix_in_what_follows_we_provide_supplementary_material_that_complements_the_main_manuscript_the_appendix_is_organized_as_follows_1_appendix_a1_begins_with_background_on_graph_ricci_curvature_where_we_review_both_the_ollivier_and_balanced_forman_discretizations_in_order_to_give_the_reader_an_intuitive_and_formal_foundation_for_subsequent_results_2_in_appendix_a2_we_provide_detailed_proofs_of_the_propositions_and_theorems_discussed_in_the_manuscript_3_appendix_a3_describes_the_test-time_graph_rewiring_procedure_used_in_our_experiments_including_a_full_presentation_of_the_stochastic_curvature-guided_algorithm_4_in_appendix_a4_we_outline_preliminary_ideas_and_implementations_for_curvature-aware_solvers_along_with_empirical_results_that_illustrate_their_potential_5_finally_appendix_a5_contains_additional_plots_and_visualizations_that_further_clarify_the_relationship_between_curvature_problem_hardness_and_solver_behavior_a1_ricci_curvature_of_graphs_in_this_section_we_provide_for_the_sake_of_completeness_both_definitions_and_intuitive_explanations_of_the_two_types_of_graph_ricci_curvature_rc_mentioned_in_this_paper_the_definitions_are_taken_from_bhattacharya_and_mukherjee_5_and_topping_et_al_47_respectively_we_simply_report_them_here_in_a_synthesized_manner_we_kindly_refer_the_reader_to_the_aforementioned_works_for_more_details_a11_ollivier_ricci_curvature_oc_the_formulation_of_ollivier_39_aligns_with_the_intuition_from_differential_geometry_edges_with_negative_curvature_act_as_structural_bottlenecks_separating_dense_regions_and_limiting_smooth_information_propagation_edges_with_positive_curvature_in_the_other_hand_facilitate_smooth_information_propagation_and_are_indicators_of_community_structure_graph_ollivier-_ricci_curvature_oc_is_very_well_studied_and_has_being_linked_to_properties_of_graph_lapacians_and_mixing_times_of_markov_chain_monte_carlo_mcmc_methods_22_42_the_intuitive_idea_of_this_discretization_is_to_directly_implement_the_idea_from_differential_geometry_we_use_a_ratio_between_the_amount_of_mass_moved_around_of_an_edge_neighborhood_with_the_shortest_path_distance_ie_the_graph_geodesic_let_us_formalize_this_concept_for_two_probability_measures_mu_1mu_2_on_a_metric_space_xd_the_the_wasserstein_distance_between_them_is_defined_as_w_1mu_1mu_2_inf_nu_in_mmu_1mu_2int_xtimes_xdxymathrmdnu_xy_quad_12_where_mmu_1mu_2_is_the_collection_of_probability_measures_on_xtimes_x_with_marginals_mu_1_and_mu_2_the_wasserstein_distance_is_the_result_of_the_solution_to_a_famous_problem_called_optimal_transport_43_intuitively_this_distance_measures_the_optimal_cost_to_move_one_pile_of_sand_to_another_one_with_the_same_mass_let_a_metric_measure_space_xdm_be_a_metric_space_xd_with_a_collection_of_probability_measures_m_m_xxin_x_indexed_by_the_points_of_x_the_coarse_ricci_curvature_of_a_metric_measure_space_is_defined_as_follows_definition_a1_ollivier_39_on_any_metric_measure_space_xdm_for_any_two_distinct_points_xyin_x_the_coarse_ricci_curvature_of_xdm_of_xy_is_defined_as_kappa_xycoloneqq_1_-_fracw_1m_xm_ydxy_quad_13_extending_this_definition_to_graphs_requires_some_additional_steps_consider_a_locally_finite_and_possibly_weighted_simple_graph_g_ve_where_each_edge_ijin_e_is_assigned_a_positive_weight_w_ij_w_ji_the_graph_is_equipped_with_the_standard_shortest_path_graph_distance_d_g_that_is_for_ijin_v_d_gij_is_the_length_of_the_shortest_path_in_g_connecting_nodes_i_and_j_for_iin_v": "define the degree \\(d_{i} \\coloneqq \\sum_{(i,j) \\in E} w_{ij}\\) and the neighborhood \\(\\mathcal{N}(i) \\coloneqq \\{j \\in V: (i,j) \\in E\\}\\) . For each \\(i \\in V\\) define a probability measure \\[m_{i}(j) = \\left\\{ \\begin{array}{ll} \\frac{w_{i,j}}{d_{i}}, & \\mathrm{if} j \\in \\mathcal{N}(i) \\\\ 0, & \\mathrm{otherwise.} \\end{array} \\right.\\] Note that these are just the transition probabilities of a weighted random walk on the vertices of \\(G\\) . If \\(m_{G} = \\{m_{i}: i \\in V\\}\\) , then considering the metric measure space \\(\\mathcal{M}_{G} \\coloneqq (V, d_{G}, m_{G})\\) , we can define the OC curvature for any edge \\((i,j) \\in E\\) as \\(\\kappa (\\mathbf{i},\\mathbf{j}) \\coloneqq \\mathbf{1} - \\mathbf{W}_{1}^{G}(\\mathbf{m}_{i}, \\mathbf{m}_{j})\\) , where \\(W_{1}^{G}(m_{i}, m_{j})\\) is obtained by discretizing Equation (12) on \\(\\mathcal{M}_{G}\\) : \\[W_{1}^{G}(m_{i}, m_{j}) = \\inf_{\\nu \\in \\mathcal{A}} \\sum_{z_{1} \\in \\mathcal{N}(i)} \\sum_{z_{2} \\in \\mathcal{N}(j)} \\nu (z_{1}, z_{2}) d(z_{1}, z_{2}). \\quad (14)\\] \\(\\mathcal{A}\\) denotes the set of all \\(d_{i} \\times d_{j}\\) matrices with entries indexed by \\(\\mathcal{N}(i) \\times \\mathcal{N}(j)\\) such that \\(\\nu (i', j') \\geq 0\\) , \\(\\sum_{z \\in \\mathcal{N}(j)} \\nu (i', z) = \\frac{w_{i,i'}}{d_{i}}\\) , and \\(\\sum_{z \\in \\mathcal{N}(i)} \\nu (z, j') = \\frac{w_{j,j'}}{d_{j}}\\) , for all \\(i' \\in \\mathcal{N}(i)\\) and \\(j' \\in \\mathcal{N}(j)\\) . For a matrix \\(\\nu \\in \\mathcal{A}\\) , \\(\\nu (i', j')\\) represents the mass moving from \\(i' \\in \\mathcal{N}(i)\\) to \\(j' \\in \\mathcal{N}(j)\\) . For this reason, the matrix \\(\\nu\\) is often called the transfer plan. ## A.1.2 Balanced Forman curvature (BFC) The Forman- Ricci Curvature (FC) is a discretization of Ricci curvature that holds for a broad class of topological objects, namely so called (regular) cellular (CW) complexes [44]. The original definition [18] is both extremely technical and out of the scope of this paper. Given that graphs edges are 1- dimensional cells (topologically), the definition simplifies greatly making use of very simple graph properties. Consider a simple, unweighted graph for simplicity, then the FC of edge \\((i,j)\\) is defined as \\(\\kappa_{G}^{f}(i,j) = 4 - d_{i} - d_{j}\\) . The main issue of this definition is that it ignores higher order correlations in terms of triangles and 4- cycles, which prove crucial to discriminate positively, flat, and negatively curved graphs. Inspired by these issues, Topping et al. [47] propose an extension of the FC dubbed Balanced Forman Curvature (BFC), such that it is both fast to compute and encodes accurate curvature information. Let us start by defining the sphere and ball of radius \\(r\\) centered at a node \\(i\\) of the graph by: \\[S_{r}(i) \\coloneqq \\{j \\in V: d_{G}(i,j) = r\\} , \\quad B_{r}(i) \\coloneqq \\{j \\in V: d_{G}(i,j) \\leq r\\} . \\quad (15)\\] The BFC is then defined using three combinatorial components: (i) \\(\\sharp_{\\Delta}(i,j)\\) , the number of triangles based at the edge \\(i \\sim j\\) . (ii) \\(\\sharp_{\\square}^{i}(i,j) \\coloneqq\\) , the number of nodes \\(k \\in S_{1}(i)\\) forming a 4- cycle based at \\(i \\sim j\\) without diagonals inside. (iii) \\(Q_{i}(j) \\coloneqq S_{1}(i) \\setminus (\\{j\\} \\cup \\sharp_{\\Delta}(i,j) \\cup \\sharp_{\\square}^{i}(i,j))\\) , simply the complement of the neighbours of \\(i\\) with respect to the sets introduced in (i) and (ii) once we also exclude \\(j\\) . Definition A.2 (Topping et al. [47]). For any edge \\(i \\sim j\\) in a simple, unweighted graph \\(G = (V, E)\\) with adjacency matrix \\(A\\) let: \\(S_{1}(i) \\coloneqq \\{j \\in V: i \\sim j \\in E\\}\\) be the set of 1- hop neighbors of \\(i\\) . \\(\\sharp_{\\Delta}(i,j) \\coloneqq S_{1}(i) \\cap S_{1}(j)\\) be the set of triangles based at \\(i \\sim j\\) . \\(\\sharp_{\\square}^{i}(i,j) \\coloneqq \\{k \\in S_{1}(i) \\setminus S_{1}(j), k \\neq j: \\exists w \\in (S_{1}(k) \\cap S_{1}(j)) \\setminus S_{1}(i)\\}\\) be the neighbors of \\(i\\) forming a 4- cycle based at \\(i \\sim j\\) without diagonals inside. \\(\\gamma_{\\max}(i,j) \\coloneqq \\max \\left\\{\\max_{k \\in \\sharp_{\\square}^{i}} \\{(A_{k} \\cdot (A_{j} - A_{i} \\odot A_{j})) - 1\\} , \\max_{w \\in \\sharp_{\\square}^{i}} \\{(A_{w} \\cdot (A_{i} - A_{j} \\odot A_{i})) - 1\\} \\right\\}\\) , with \\(\\cdot\\) being the dot product and \\(\\odot\\) the elementwise product, be the maximal number of 4- cycles based at \\(i \\sim j\\) traversing a common node. The BFC \\(Ric(i,j)\\) is zero if \\(\\min \\{d_{i}, d_{j}\\} = 1\\) and alternatively: \\[R i c(i,j) \\coloneqq \\frac{2}{d_{i}} + \\frac{2}{d_{j}} -2 + 2\\frac{|\\sharp_{\\Delta}(i,j)|}{\\max \\{d_{i},d_{j}\\}} +\\frac{|\\sharp_{\\Delta}(i,j)|}{\\min \\{d_{i},d_{j}\\}} +\\frac{(\\gamma_{m a x}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|). \\quad (16)\\]",
    "a2_proofs_for_the_sake_of_clarity_and_exposition_we_report_here_both_the_formal_statements_alongside_their_respective_proofs_proposition_31_let_isim_j_be_an_edge_from_the_literal-_clause_graph_lcg_representation_g_of_a_random_k-_sat_problem_with_n_variables_and_m_clauses_with_degree_distributions_given_by_equations_6_and_3_then_mathbbe_isim_jkappa_ijto_0_and_mathbbe_isim_jr_i_cijto_0_as_alpha_fracmnto_0_proof_as_alpha_to_0_the_expected_value_of_the_literal_degree_becomes_lim_alpha_to_0mathbbe_d_isim_pd_id_i_lim_alpha_to_0fraclambda_elambdaelambda_-_1_lim_alpha_to_0elambdacdot_lim_alpha_to_0fraclambdaelambda_-_1_1cdot_lim_alpha_to_0frac1fracelambda_-_1lambda_1_quad_17_where_the_last_equality_is_obtained_due_to_the_fact_that_beginarrayrlim_alpha_to_0lambda_lim_alpha_to_0frac12alpha_k_0_endarray_and_the_limit_formula_beginarrayrlim_xto_0fracax_-_1x_ln_leftaright_endarray_given_the_average_degree_of_the_literals_that_act_as_an_endpoint_of_at_least_one_edge_is_1_in_this_limiting_case_by_definition_of_the_lower_bound_in_equation_8_we_obtain_that_beginarrayrlim_alpha_to_0mathbbe_isim_jbarr_ij_0_endarray_as_a_lower_bound_barr_ij_satisfies_the_following_inequality_5_47_-2_barr_ijleq_r_i_cijleq_kappa_ijleq_0_quad_18_given_that_both_limits_and_expectations_preserve_weak_inequalities_by_sandwiching_we_obtain_beginarrayr_l_underset_alpha_to_0limmathbbe_isim_jbarr_ij_0leq_underset_alpha_to_0limmathbbe_isim_jr_i_cijleq_underset_alpha_to_0limmathbbe_isim_jkappa_ijleq_0_underset_alpha_to_0limmathbbe_isim_jkappa_ij_underset_alpha_to_0limmathbbe_isim_jr_i_cij_0_endarray_quad_20_proposition_32_let_isim_j_be_an_edge_from_the_lcg_representation_g_of_a_random_k-_sat_problem_with_n_variables_and_m_clauses_with_degree_distributions_given_by_equations_6_and_3_then_mathbbe_isim_jbarr_ijto_frac2k_-_2_as_alpha_fracmnto_infty_proof_as_alpha_to_infty_the_expected_value_of_the_literal_degree_becomes_lim_alpha_to_inftymathbbe_d_isim_pd_id_i_lim_alpha_to_inftyfraclambda_elambdaelambda_-_1_lim_alpha_to_inftyfrac1frac1lambda_-_frac1lambda_elambda_infty_quad_21_therefore_we_can_consider_the_lower_bound_as_consisting_only_of_beginarrayrunderliner_ij_frac2d_i_frac2d_j_-_2_endarray_ignoring_the_uninteresting_case_when_k_1_given_that_the_expected_value_is_a_linear_operation_we_obtain_that_lim_alpha_to_inftymathbbe_isim_jbarr_ij_frac2lim_alpha_to_inftymathbbe_d_isim_pd_id_i_frac2k_-2_frac2k_-2_quad_22_theorem_31_let_isim_j_be_an_edge_from_the_lcg_representation_g_of_a_random_k-_sat_problem_with_n_variables_and_m_clauses_with_degree_distributions_given_by_equations_6_and_3_the_bfc_at_isim_j_is_bounded_from_above_by_the_quantity_barr_ijcoloneqq_frac2d_i_frac2d_j_-2_fracd_i_k_-_2max_k_-_1m_-_1_max_d_id_j_quad_23_furthermore_as_beginarrayralpha_fracmnto_infty_mathbbe_isim_jbarr_ijto_frac2k_-_2_endarray_and_therefore_the_average_bfc_over_the_edges_of_g_converges_to_mathbbe_isim_jr_i_cijto_frac2k_-_2": "Proof. We first prove the construction of the upper bound, which we can then use to prove the convergence of the expectation via sandwiching, similarly to the proof of Proposition 3.1. Please note that the definitions for each component of the graph BFC are given in Appendix A.1.2 and we will not be restating them here to avoid repetition. To start, notice that the BFC on \\(G\\) takes a simpler form compared to the more general, original definition A.2: \\[R i c(i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{(\\gamma_{\\mathrm{max}}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|), \\quad (24)\\] This result follows from the fact that bipartite graphs have no triangles, i.e., \\(\\sharp_{\\Delta}(i,j) = 0 \\forall (i,j) \\in E\\) . Our main focus for now will the rightmost term, which constitutes the difference between \\(\\mathrm{Ric}(i,j)\\) and \\(\\underline{{R}}(i,j)\\) (Equation 8). Firstly note that this term is, by definition, non- negative. This implies that \\(\\underline{{R}}(i,j)\\) of \\(\\mathrm{Ric}(i,j)\\) . We can simplify the definitions of the 4- cycle forming neighbors to match the bipartite topology of \\(G\\) : \\[\\begin{array}{r l} & {\\sharp_{\\square}^{i}(i,j)\\coloneqq \\{k\\in S_{1}(i)\\setminus \\{k\\} :\\exists w\\in (S_{1}(k)\\cap S_{1}(j))\\setminus \\{i\\} \\} ,}\\\\ & {\\sharp_{\\square}^{j}(i,j)\\coloneqq \\{k\\in S_{1}(j)\\setminus \\{k\\} :\\exists w\\in (S_{1}(k)\\cap S_{1}(i))\\setminus \\{j\\} \\} .} \\end{array} \\quad (25)\\] By definition of \\(S_{1}(\\cdot)\\) it follows immediately that the cardinality of both sets is bounded above by the node degrees: \\[\\begin{array}{r l} & {|\\sharp_{\\square}^{i}(i,j)|\\leq d_{i} - 1,}\\\\ & {|\\sharp_{\\square}^{j}(i,j)|\\leq d_{j} - 1 = k - 1.} \\end{array} \\quad (28)\\] We now turn to the term \\(\\gamma_{\\mathrm{max}}(i,j)\\) , whose definition can also be simplified, as a consequence of the topology of \\(G\\) . Consider first the symmetric adjacency matrix of a bipartite graph, given by: \\[A = \\left[ \\begin{array}{ll}0 & B\\\\ B^{T} & 0 \\end{array} \\right]\\in \\{0,1\\}^{N + M\\times N + M}, \\quad (29)\\] where \\(B\\) is an \\(N\\times M\\) incidence matrix with \\(B_{i j} = 1\\) if there if \\(i\\sim j\\in E\\) , and \\(B_{i j} = 0\\) otherwise, \\(B^{T}\\) is the transpose of \\(B\\) (which ensures that \\(A\\) is symmetric), and \\(\\mathbf{0}\\) are zero blocks of size \\(N\\times N\\) and \\(M\\times N\\) corresponding to the absence of edges within the partitions. We can thus express \\(\\gamma_{\\mathrm{max}}(i,j)\\) as: \\[\\gamma_{\\mathrm{max}}(i,j)\\coloneqq \\max \\left\\{\\max_{k\\in \\sharp_{\\square}^{i}}\\{A_{k}\\cdot A_{j} - 1\\} ,\\max_{w\\in \\sharp_{\\square}^{j}}\\{A_{w}\\cdot A_{i} - 1\\} \\right\\} , \\quad (30)\\] since \\(A_{i}\\odot A_{j} = \\mathbf{0}\\) . The operation \\(A_{k}\\cdot A_{j} = \\nu \\in \\mathbb{N}_{0}\\) returns the number of common neighbors \\(\\nu\\) that nodes \\(k\\) and \\(j\\) have, with \\(k\\) and \\(j\\) being in the same partition. Therefore, we have the following inequalities that can be used to derive an upper bound for \\(\\gamma_{\\mathrm{max}}(i,j)\\) : \\[\\begin{array}{r l} & {\\max_{k\\in \\sharp_{\\square}^{i}}\\{A_{k}\\cdot A_{j} - 1\\} \\leq k - 1,}\\\\ & {\\max_{w\\in \\sharp_{\\square}^{j}}\\{A_{w}\\cdot A_{i} - 1\\} \\leq M - 1,}\\\\ & {\\gamma_{\\mathrm{max}}(i,j)\\leq \\max \\{k - 1,M - 1\\} .} \\end{array} \\quad (31)\\] Putting everything together we obtain: \\[0\\leq \\frac{(\\gamma_{\\mathrm{max}}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|)\\leq \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\cdot \\max \\{d_{i},d_{j}\\}}, \\quad (34)\\] so that we can write: \\[-2< \\underline{{R}} (i,j)\\leq R i c(i,j)\\leq \\bar{R} (i,j)\\leq 0. \\quad (35)\\] In Proposition 3.2), we have previously that as \\(\\alpha \\to \\infty\\) \\(\\mathbb{E}_{(i\\sim j)}[\\underline{{R}} (i,j)]\\to \\frac{2}{k} - 2\\) , due to the fact that the literal degree becomes a dominant term. Therefore \\(\\max \\{d_{i},d_{j}\\} = d_{i}\\) , and under the same limiting assumption we have that \\(\\max \\{k - 1,M - 1\\} = M - 1\\) . Given that the expected value is a linear operation, we obtain that: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}]} +\\frac{2}{k} -2 + \\frac{1}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[M - 1]} = \\frac{2}{k} -2. \\quad (36)\\]\n\nGiven that both limits and expectations preserve weak inequalities, by sandwiching we obtain: \\[\\frac{2}{k} -2 = \\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\leq \\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\leq \\lim_{\\alpha \\to 0}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{k} -2, \\quad (37)\\] ## A.3 Details on Test-Time Rewiring Graph rewiring is the process of modifying a graph's connectivity by adding, removing, or reweighting edges to optimize information flow. In our case, we make use of the rewiring procedure presented by Topping et al. [47], which consists of a discrete and stochastic Ricci flow, guided by the BFC. At each iteration, the edge with the most negative curvature (i.e., the structurally most \"strained\" connection) is identified. Candidate rewiring edges are then generated between the neighborhoods of the two endpoints of this edge. A new edge is stochastically selected either at random (with probability \\(p\\) ) or by maximizing the curvature improvement obtained from its addition. The selected edge is added to the graph and the corresponding curvature values are updated. By repeating this procedure for a fixed number of iterations, the algorithm progressively increases the average curvature of the graph, yielding a rewired version that contains information bottlenecks that are weaker compared to the input. Algorithm 1 contains the pseudocode for our rewiring algorithm. We would like to stress here that this modifies the constraints of the Boolean Satisfiability Problem (SAT) problem under consideration, but the goal of this procedure is to show that \"flatter\" problems will in fact be easier to solver for Graph Neural Network (GNN)- based solvers. Algorithm 1: Balanced Forman Curvature Stochastic Rewiring Input: Graph \\(G = (V,E)\\) with edge BFC values \\(R i c(i,j),\\forall (i,j)\\in E\\) , probability value \\(p\\in [0,1]\\) , number of iterations \\(N\\in \\mathbb{N}\\) Output: Rewired graph \\(G^{\\prime}\\) with updated BFC values for \\(t\\gets 1\\) to \\(N\\) do Select edge \\((i,j)\\) with most negative curvature \\(R i c(i,j)\\) From the neighbors \\(S_{1}(i)\\) and \\(S_{1}(j)\\) form candidate edge set \\[C = \\{(k,l):k\\in S_{1}(i),l\\in S_{1}(j),k\\neq l,(k,l)\\notin E\\}\\] if \\(C = \\emptyset\\) then L continue With probability \\(p\\) , choose a random edge \\((k,l)\\in C\\) Otherwise, for each \\((k,l)\\in C\\) 1. Compute updated curvature \\(R i c^{\\prime}(i,j)\\) after adding \\((k,l)\\) 2. Evaluate improvement score \\(\\Delta_{k l} = -(R i c(i,j) - R i c^{\\prime}(i,j))\\) Select \\((k,l)\\) with maximum \\(\\Delta_{k l}\\) Add edge \\((k,l)\\) to \\(G\\) and update neighborhood sets; Update curvatures \\(R i c(i,j)\\) and \\(R i c(k,l)\\) accordingly; Finalize bipartite structure of literals and clauses \\((L,C)\\) ensuring no intra- partition edges and set \\(G^{\\prime} = G\\) for \\(t = N\\) return \\(G^{\\prime}\\) ## A.4 Some initial ideas for curvature-aware solvers As discussed during the conclusion, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. In this section, we provide some starting points and experiments for simple implementations of curvature aware solvers that could lead to future improvements. The goal of our implementations was to maintain efficiency and rely on straightforward ideas that could lead to performance improvements. For these purposes, we introduce two simple curvature- aware variants of message passing. The first is an adaptation of Ye et al.\n\nTable 3: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers with different message-passing schemes: 1) Vanilla uses the typical message-passing operation; 2) Curvature Gate learns a gating function for each edge based on its curvature value [49]; 3) Online LCP extends the work of Fesser and Weber [16] and concatenates the local curvature statistics around each nodes as features during each recurrent step; 4) Both uses Curvature Gate and Online LCP. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface. <table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Variation</td><td colspan=\"4\">Datasets</td></tr><tr><td>3-SAT</td><td>4-SAT</td><td>SR</td><td>CA</td></tr><tr><td rowspan=\"3\">GCN</td><td>Vanilla</td><td>0.510 \u00b1 0.012</td><td>0.180 \u00b1 0.048</td><td>0.470 \u00b1 0.031</td><td>0.650 \u00b1 0.016</td></tr><tr><td>+ Curvature Gate</td><td>0.514 \u00b1 0.018</td><td>0.154 \u00b1 0.017</td><td>0.422 \u00b1 0.013</td><td>0.664 \u00b1 0.042</td></tr><tr><td>+ Online LCP</td><td>0.510 \u00b1 0.014</td><td>0.170 \u00b1 0.010</td><td>0.422 \u00b1 0.016</td><td>0.662 \u00b1 0.016</td></tr><tr><td></td><td>+ Both</td><td>0.500 \u00b1 0.012</td><td>0.176 \u00b1 0.031</td><td>0.416 \u00b1 0.027</td><td>0.654 \u00b1 0.027</td></tr><tr><td rowspan=\"3\">Neuro SAT</td><td>Vanilla</td><td>0.690 \u00b1 0.022</td><td>0.436 \u00b1 0.032</td><td>0.734 \u00b1 0.017</td><td>0.746 \u00b1 0.018</td></tr><tr><td>+ Curvature Gate</td><td>0.682 \u00b1 0.030</td><td>0.436 \u00b1 0.015</td><td>0.742 \u00b1 0.027</td><td>0.758 \u00b1 0.016</td></tr><tr><td>+ Online LCP</td><td>0.692 \u00b1 0.020</td><td>0.416 \u00b1 0.046</td><td>0.724 \u00b1 0.022</td><td>0.726 \u00b1 0.059</td></tr><tr><td></td><td>+ Both</td><td>0.664 \u00b1 0.018</td><td>0.438 \u00b1 0.028</td><td>0.742 \u00b1 0.025</td><td>0.758 \u00b1 0.020</td></tr></table> <center>Figure 3: Low dimensional visualization of the literal embeddings produced by Neuro SAT on random 4-SAT with (a) vanilla message passing and (b) Curvature Gate. Even though there is no major change in performance, the learned representations can be linearly separated into truth value assignments in the curvature-aware case, indicating promise for the inclusion of these principles in future GNN-based solver design. </center> [49], where the curvature of an edge is used to learn a gating mechanism that modulates message contributions. The second is a simple recurrent extension to Fesser and Weber [16], where the statistics of the curvature around each node are used as additional features at each recurrent step. Our empirical findings reveal that naively injecting curvature into GNN- based solvers sometimes leads to improved performance, but it does not always provide clear advantages, as seen in Table 3. Furthermore, we also experimented with a contemporary use of both variations. The training protocol and experimental settings are kept identical to previous experiments. The results highlight a subtle but important point: while curvature exposes structural bottlenecks, effective GNN solvers must also learn how to properly use geometric information. A major weakness of both methods is that random \\(k\\) - SAT problems have a lot of regularity, in the sense that the clause partitions will have very similar curvature statistics and thus the learning signal becomes redundant. Nevertheless, we believe that both these implementations provide interesting starting points for future work and research.\n\n<center>(a) Average graph Ricci curvature lower bound (Equation 8) as a function of \\(k\\) and \\(\\alpha\\) . </center> <center>(b) Average graph Ollivier-Ricci curvature as a function of \\(k\\) and \\(\\alpha\\) . </center> Figure 4: Contour plots of the average graph Ricci curvature lower bound (a) and Ollivier- Ricci curvature (b) displaying the changes in average curvature as a function of \\(k\\) and \\(\\alpha\\) . Both quantities behave similarly, especially as \\(\\alpha \\to 0\\) . We can see that in the case of the ORC, the stronger correction towards positive curvature due to the presence of cycles as \\(\\alpha \\to \\infty\\) is in line with the general theory related to this discretization of the Ricci curvature [5]. Notice the difference between the contours of (b) and Figure 1b for large \\(\\alpha\\) and \\(k\\) . Best viewed in color. <center>Figure 5: Visualization of an easy, in terms of clause density \\(\\alpha\\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very small number of long-range interactions, as can clearly be seen in (b). Furthermore, for such very small \\(\\alpha\\) , the average BFC approaches 0, in line with the developed theory. </center> ## A.5 Additional plots In this section we provide additional and miscellaneous plot that help with visualizing various concepts explained in the main manuscript, such as the relationship between curvature and hardness, as well as visualizations of easy and hard problems. More details can be found in Figures 4, 5, 6 and 7.\n\ncompressed into finite- dimensional embeddings. This bottleneck severely restricts the ability of GNNs to model long- range dependencies, and can be thought of as a vanishing gradient problem. Given that the difficulty of SAT can actually be understood intuitively as being proportional to the number of long- range dependencies between nodes, we question whether learning GNN- based solvers on these difficult problems is impacted by oversquashing. Recent geometric analyses propose that oversquashing is directly tied to negative Ricci Curvature (RC) of the underlying graph [38, 47]. These insights invite a fundamental question: Can graph RC serve as a predictive and constructive notion in unraveling novel hardness concepts for GNN- based SAT solvers? In this work, we address this question by studying the typical- case behavior of particular instantiations of graph RC on random \\(k\\) - SAT problems represented as bipartite graphs. Our starting point is the observation that edges of bipartite graphs are always non- positively curved. We show that, on average, the edges become more negatively curved as problems get harder, and less negatively curved as they become easier. Finally, we derive an exact expression for the average Balanced Forman Curvature (BFC) in the limit of unsolvable problems, from which we derive a connection between curvature and oversquashing in GNNs- based SAT solvers, following the theory of Topping et al. [47]. This is, to the best of our knowledge, the first successful attempt at a theoretical characterization of the limitations of GNN- based SAT solvers. To validate our theory, we perform experiments on random 3- and 4- SAT problems, and also on a vast array of datasets coming from the recent benchmark of Li et al. [28]. Firstly, we observe a phase transition- like phenomenon in random 3- SAT solving probability as a function of the mean and variance of the curvature. We further affirm the aforementioned limitations by rewiring only the testing graphs of the benchmarks at test- time to increase their average BFC, and show that these rewired problems become much easier to solve. Finally, we find that heuristics based on the BFC of a dataset correlate extremely well with generalization error, unlike the average clause density, which is typically used to characterize the hardness of a single instance. Overall, our findings suggest that GNN- based SAT solvers have two distinct types of hardness: the hardness of learning representations in negatively curved structures, followed by the well- established algorithmic hardness of SAT. We conclude by relating our findings with modeling principles and design choices of existing approaches. Outline. The remainder of this article is structured as follows: In Section 2, we provide a recap of the most relevant concepts discussed in the paper: random \\(k\\) - SAT, GNNs, and graph RC. Section 3.1 presents the main theoretical results, including an in- depth discussion on how oversquashing affects GNNs- based solvers. We then demonstrate the experimental evidence in Section 4. Finally, we provide a discussion regarding current design principles and future work in Section 5. ## 2 Background This section is only intended to formally introduce the objects studied in the paper and render the material self- contained. We kindly ask the reader to tolerate the occasional whirlwind and abuse of notation, as it will be formalized later in Section 3. ### 2.1 Random \\(k\\) Boolean Satisfiability Problem The random \\(k\\) - SAT (assignment) problem, which is the central object of study in this work, is made up of \\(N\\) variables \\(\\{x_{i}\\}_{i = 1}^{N}\\) that can take binary values \\(x_{i} \\in \\{0,1\\}\\) . Using these variables, one constructs \\(M\\) clauses containing a disjunction of \\(k\\) variables or their negations (called literals). For example, a 3- SAT problem would have clauses of the form \\((x_{i} \\vee \\neg x_{j} \\vee x_{h})\\) . The goal is to assign a value to all literals such that they satisfy the conjunction of all clauses. This logical formula is called a Conjunctive Normal Form (CNF). In the random formulation, it is possible to identify different phases of problem hardness based on a parameter called the clause density \\(\\alpha = M / N\\) . This phenomenon has been actively researched in statistical physics due to the analogies between random \\(k\\) - SAT and spin- glass models [33, 34]. Notable results[26, 37, 50] include the discovery of two transitions for typical instances at a given \\(k\\) , based on the value of \\(\\alpha\\) in the thermodynamic limit \\((M, N \\to \\infty)\\) : As \\(\\alpha\\) increases, the measure over the space of possible solutions first decomposes into an exponential number of clusters at the dynamical transition \\(\\alpha_{d}(k)\\) and subsequently condensates over the largest such states at the critical transition \\(\\alpha_{c}(k)\\) . These phase transitions naturally affect the performances of many algorithms, e.g., when \\(k \\geq 4\\) , going beyond \\(\\alpha_{c}\\) is almost impossible with existing algorithms.\n\n<center>Figure 6: Visualization of a hard, in terms of clause density \\(\\alpha\\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very large number of long-range interactions, as can clearly be seen in (b). Furthermore, for such large \\(\\alpha\\) , the average BFC is strongly negative, in line with the developed theory. </center> <center>Figure 7: Average BFC as a function of \\(\\alpha\\) for random 3 and 4 -SAT problems with \\(N = 256\\) . The size of the blobs is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model [46]. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c}\\) [32]. The average BFC drops monotonically with \\(\\alpha\\) in both cases. For 3-SAT (a), this drop appears to be almost linear, with a substantial amount of variance, as also depicted in Figure 2. 4-SAT on the other hand contains problems where the curvature is substantially more negative and concentrated, which when coupled with the larger number of constraints leads to oversquashing, as discussed in our theory. This can be clearly seen by the fast performance drop-off in (b), which happens much earlier than \\(\\alpha_{c}\\) , indicating the additional hardness phase related to representation learning discussed in the main paper. </center>\n\n<center>Figure 8: Analogue of Figure 2a for 4-SAT. Average BFC as a function of \\(\\alpha\\) for random 4-SAT problems with \\(N = 256\\) . The color is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model, with a group labeled as solvable if \\(50\\%\\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c} \\approx 9.931\\) [32]. The average BFC is negative and drops with \\(\\alpha\\) , but for a small amount. The plot in the top-right corner shows the model's solution probability curve as a function of \\(\\alpha\\) , where it is possible to notice that the GNN-based solver has severe limitations on these problems. Differently from 3-SAT (Figure 2a), the average curvature is negative and concentrated even for problems that would be considered very simple in terms of \\(\\alpha\\) ( \\(\\alpha \\rightarrow 8\\) ). This is due to the larger value of \\(k\\) , the higher number of long-range interactions, and the connection of both factors with oversquashing, as discussed and predicted by our contributions. </center>\n\n### 2.2 Graph Neural Networks GNNs are a subclass of Neural Networks (NNs) that can learn a representation of graph data by locally aggregating information[20, 45]. The main goal of the architecture is to implement inductive biases natural to graph data [9]. An example of such a property is learning graph- level functions invariant to the nodes' ordering. Consider, for simplicity, an unweighted and undirected graph \\(G\\) with \\(N\\) nodes, represented by a symmetric binary adjacency matrix \\(A\\in \\{0,1\\}^{N\\times N}\\) . This setting can be easily extended to deal with more general connectivity structures [14]. By associating a node feature matrix \\(X\\in \\mathbb{R}^{N\\times d}\\) to the graph, we can describe a GNNs as a convolution of the graph signal with \\(A\\) as the shift operator. A generalization of this concept can be obtained by considering the message- passing framework [20]: \\[x_{i}^{(k)} = \\theta^{(k)}\\left(x_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\phi^{(k)}\\left(x_{i}^{(k - 1)},x_{j}^{(k - 1)},e_{j i}\\right)\\right), \\quad (1)\\] where \\(x_{i}^{(k)}\\) denotes node features of node \\(x_{i}\\) at layer \\(k\\) \\(e_{j i}\\) the (optional) edge features from node \\(j\\) to node \\(i\\) \\(\\mathcal{N}(\\cdot)\\) the set of (1- hop) neighbor nodes, \\(\\bigoplus\\) a differentiable, permutation invariant function, (e.g., sum, mean), and \\(\\phi ,\\theta\\) denote differentiable and (optionally) nonlinear functions such as Multi- Layer Perceptrons (MLPs). A CNF formula can be easily translated into a bipartite graph [6], which can then be fed into a GNN- based solver. The particular bipartition we consider in this work is detailed later in Section 3. The application of the above message- passing scheme for SAT problems can be done by applying Equation1 to the clause and literal partitions [28]. Let \\(i\\) be a literal node and \\(j\\) be a clause node, then: \\[\\begin{array}{r l} & {h_{j}^{(k)} = \\theta_{c}^{(k)}\\left(h_{j}^{(k - 1)},\\bigoplus_{i\\in \\mathcal{N}(j)}\\left(\\left\\{\\phi_{i}^{(k)}\\left(h_{i}^{(k - 1)}\\right)\\right\\}\\right)\\right),}\\\\ & {h_{i}^{(k)} = \\theta_{l}^{(k)}\\left(h_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\left(\\left\\{\\phi_{c}^{(k)}\\left(h_{j}^{(k - 1)}\\right)\\right\\}\\right)\\right),} \\end{array} \\quad (2)\\] where the subscripts \\(c\\) and \\(l\\) refer to NNs specialized on the clause and literal partitions respectively. ### 2.3 Ricci Curvature of Graphs In Riemannian geometry, RC quantifies the local deviation of a manifold \\(\\mathcal{M}\\) from flat Euclidean space, as a result of the metric defined on \\(\\mathcal{M}\\) . Intuitively, RC captures how the neighborhoods of two adjacent points relate when moving from one point to the other. On smooth manifolds, it compares how a small ball of mass around a point is distorted when transported along a geodesic to a neighboring point. Extending this notion to more general structures, such as metric spaces or combinatorial complexes, has been an extremely active area of mathematical research, with the works of Ollivier [39] and Forman [18] standing out. In the case of graphs, we ask ourselves how local connectivity either concentrates or disperses. Ollivier [39] implements this idea by comparing probability mass on local neighborhoods, i.e., a random walk distribution on the endpoints of an edge. Given these two distributions, one can compare the ratio between their Wasserstein and shortest path distance, serving as a direct and discrete analogue of geodesic transport. See Appendix A.1.1 for more details. The definition of Forman [18] relies heavily on topology, and thus it takes a combinatorial form. Essentially, given a cell complex, the curvature of a p- cell depends only on the topological structures between the cell and its neighbors. This makes Forman- Ricci Curvature (FC) simpler to compute numerically, since it can avoid the optimization of the optimal transport problem that arises in Ollivier- Ricci Curvature (OC) curvature. Given that RC is directly related to the structure of local neighborhoods, it has emerged as a powerful way of theoretically analyzing limitations of GNNs. In a seminal paper, Topping et al. [47] provide both a balanced version of the FC curvature and show that the oversquashing problem [1] can be directly connected to edges with high negative curvature. This definition, namely the BFC, is central to this paper, therefore please consult Appendix A.1.2 for the definition and additional details. Nguyen et al. [38] have shown that similar results can be derived using the OC curvature. It is worth noting that these notions of curvature are naturally correlated with one another, as shown empirically in a multitude of complex networks by Samal et al. [44].",
    "22_graph_neural_networks_gnns_are_a_subclass_of_neural_networks_nns_that_can_learn_a_representation_of_graph_data_by_locally_aggregating_information20_45_the_main_goal_of_the_architecture_is_to_implement_inductive_biases_natural_to_graph_data_9_an_example_of_such_a_property_is_learning_graph-_level_functions_invariant_to_the_nodes_ordering_consider_for_simplicity_an_unweighted_and_undirected_graph_g_with_n_nodes_represented_by_a_symmetric_binary_adjacency_matrix_ain_01ntimes_n_this_setting_can_be_easily_extended_to_deal_with_more_general_connectivity_structures_14_by_associating_a_node_feature_matrix_xin_mathbbrntimes_d_to_the_graph_we_can_describe_a_gnns_as_a_convolution_of_the_graph_signal_with_a_as_the_shift_operator_a_generalization_of_this_concept_can_be_obtained_by_considering_the_message-_passing_framework_20_x_ik_thetakleftx_ik_-_1bigoplus_jin_mathcalniphikleftx_ik_-_1x_jk_-_1e_j_irightright_quad_1_where_x_ik_denotes_node_features_of_node_x_i_at_layer_k_e_j_i_the_optional_edge_features_from_node_j_to_node_i_mathcalncdot_the_set_of_1-_hop_neighbor_nodes_bigoplus_a_differentiable_permutation_invariant_function_eg_sum_mean_and_phi_theta_denote_differentiable_and_optionally_nonlinear_functions_such_as_multi-_layer_perceptrons_mlps_a_cnf_formula_can_be_easily_translated_into_a_bipartite_graph_6_which_can_then_be_fed_into_a_gnn-_based_solver_the_particular_bipartition_we_consider_in_this_work_is_detailed_later_in_section_3_the_application_of_the_above_message-_passing_scheme_for_sat_problems_can_be_done_by_applying_equation1_to_the_clause_and_literal_partitions_28_let_i_be_a_literal_node_and_j_be_a_clause_node_then_beginarrayr_l_h_jk_theta_cklefth_jk_-_1bigoplus_iin_mathcalnjleftleftphi_iklefth_ik_-_1rightrightrightright_h_ik_theta_lklefth_ik_-_1bigoplus_jin_mathcalnileftleftphi_cklefth_jk_-_1rightrightrightright_endarray_quad_2_where_the_subscripts_c_and_l_refer_to_nns_specialized_on_the_clause_and_literal_partitions_respectively_23_ricci_curvature_of_graphs_in_riemannian_geometry_rc_quantifies_the_local_deviation_of_a_manifold_mathcalm_from_flat_euclidean_space_as_a_result_of_the_metric_defined_on_mathcalm_intuitively_rc_captures_how_the_neighborhoods_of_two_adjacent_points_relate_when_moving_from_one_point_to_the_other_on_smooth_manifolds_it_compares_how_a_small_ball_of_mass_around_a_point_is_distorted_when_transported_along_a_geodesic_to_a_neighboring_point_extending_this_notion_to_more_general_structures_such_as_metric_spaces_or_combinatorial_complexes_has_been_an_extremely_active_area_of_mathematical_research_with_the_works_of_ollivier_39_and_forman_18_standing_out_in_the_case_of_graphs_we_ask_ourselves_how_local_connectivity_either_concentrates_or_disperses_ollivier_39_implements_this_idea_by_comparing_probability_mass_on_local_neighborhoods_ie_a_random_walk_distribution_on_the_endpoints_of_an_edge_given_these_two_distributions_one_can_compare_the_ratio_between_their_wasserstein_and_shortest_path_distance_serving_as_a_direct_and_discrete_analogue_of_geodesic_transport_see_appendix_a11_for_more_details_the_definition_of_forman_18_relies_heavily_on_topology_and_thus_it_takes_a_combinatorial_form_essentially_given_a_cell_complex_the_curvature_of_a_p-_cell_depends_only_on_the_topological_structures_between_the_cell_and_its_neighbors_this_makes_forman-_ricci_curvature_fc_simpler_to_compute_numerically_since_it_can_avoid_the_optimization_of_the_optimal_transport_problem_that_arises_in_ollivier-_ricci_curvature_oc_curvature_given_that_rc_is_directly_related_to_the_structure_of_local_neighborhoods_it_has_emerged_as_a_powerful_way_of_theoretically_analyzing_limitations_of_gnns_in_a_seminal_paper_topping_et_al_47_provide_both_a_balanced_version_of_the_fc_curvature_and_show_that_the_oversquashing_problem_1_can_be_directly_connected_to_edges_with_high_negative_curvature_this_definition_namely_the_bfc_is_central_to_this_paper_therefore_please_consult_appendix_a12_for_the_definition_and_additional_details_nguyen_et_al_38_have_shown_that_similar_results_can_be_derived_using_the_oc_curvature_it_is_worth_noting_that_these_notions_of_curvature_are_naturally_correlated_with_one_another_as_shown_empirically_in_a_multitude_of_complex_networks_by_samal_et_al_44": "",
    "3_curvature_of_random_k-sat_problems_and_its_relationship_with_gnns_setting_and_notation_we_consider_random_k_-_sat_problems_with_n_variables_and_m_clauses_with_alpha_m_n_and_knmin_mathbbn_these_problems_are_represented_through_a_simple_bipartite_graph_g_ve_where_the_node_set_is_a_literal-_clause_bipartition_v_lcup_c_with_lcap_c_emptyset_and_l_2nc_m_the_edge_set_takes_the_form_e_ijin_vtimes_visim_jiin_ljin_c_where_isim_j_indicates_a_connection_between_nodes_given_vin_v_we_denote_its_degree_by_d_v_finally_we_denote_the_expected_value_of_the_random_variable_x_with_probability_distribution_mathrmp_by_mathbbe_px_and_mathbbe_psim_px_the_expectation_over_samples_drawn_from_p_unless_noted_otherwise_when_we_refer_to_the_expected_value_in_simulations_we_imply_its_estimate_via_the_sample_mean_statistic_data_model_our_bipartite_formulation_is_a_simplification_of_the_input_graphs_considered_in_many_gnn-_based_solvers_40_46_recent_literature_28_refers_to_this_data_structure_as_a_literal-_clause_graph_lcg_following_an_erd\u0151sr\u00e9nyi-_like_procedure_each_clause_is_assigned_k_literals_independently_at_random_with_probability_p_assuming_that_all_literals_are_equally_likely_to_appear_in_a_given_clause_we_obtain_the_following_degree_distributions_beginarraycpd_j_h_delta_h_-_k_pd_i_h_binommh_ph1_-_pm_-_h_endarray_quad_3_where_delta_cdot_represents_the_dirac_delta_function_binomcdot_the_binomial_coefficient_and_pcoloneqq_frack2n_in_the_limit_mnto_infty_we_can_approximate_the_binomial_form_of_the_literal_degree_distribution_with_a_poisson_distribution_pd_i_h_fraclambdahe-lambdah_quad_5_with_lambda_m_p_textstyle_frac12alpha_k_we_will_be_interested_in_using_this_approximation_to_calculate_properties_of_the_graph_rc_which_is_an_edge_level_property_therefore_the_case_d_i_0_should_be_truncated_this_fact_leads_us_to_propose_an_alternative_probability_mass_function_based_on_zero-_truncated_poisson_distribution_the_for_the_literal_degrees_pd_i_h_pd_i_hhgeq_1_fracpd_i_h1_-_pd_i_0_fraclambdahhelambda_-_1_quad_6_characterizing_average_curvature_we_will_now_proceed_by_showing_that_the_above_bipartite_representation_of_sat_problems_has_significant_implications_on_the_average_rc_most_importantly_we_will_precisely_characterize_the_average_behavior_of_a_specific_definition_curvature_namely_the_bfc_which_in_turn_has_strong_implications_on_oversquashing_in_gnns_the_proofs_of_all_statements_are_deferred_to_appendix_a2_we_start_from_an_established_lower_bound_of_the_oc_definition_31_oc_lower_bound_in_bipartite_graphs_5_23_for_any_edge_isim_j_in_a_simple_unweighted_bipartite_graph_g_we_have_that_that_underlinekappaij_-2cdot_max_01_-_frac1d_i_-frac1d_j_leq_kappa_ijleq_0_quad_7_where_kappa_is_the_oc_the_above_lower_bound_can_be_redefined_by_alternatively_stating_the_condition_encoded_inside_the_maximum_function_in_equation_7_underlinerij_left_beginarrayl_l0_mathrmifmind_id_j_1_frac2d_i_frac2d_j_-2_mathrmotherwise_endarray_right_quad_8_this_alternative_definition_shows_that_equation_8_is_also_a_lower_bound_for_the_bfc_st_underlinerijleq_r_i_cij_this_statement_is_a_direct_consequence_of_the_definition_of_the_bfc_and_a_proof_is_given_inside_the_proof_of_theorem_31_furthermore_we_also_have_that_-_2_r_i_cijleq_kappa_ijleq_0_which_is_a_direct_consequence_of_the_facts_that_kappa_ijgeq_r_i_cij_47_and_equation_7_starting_from_these_statements_we_can_proceed_to_show_that_the_average_behavior_of_the_graph_rc_is_naturally_dictated_by_average_degree_of_the_literals_given_that_d_j_k_always_holds_the_literal_degree_distribution_is_a_zero-_truncated_poisson_distribution_from_which_the_average_degree_of_a_literal_i_can_be_calculated_as_mathbbe_d_isim_pd_id_i_fraclambda_elambdaelambda_-_1_quad_9": "Proposition 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\to 0\\) and \\(\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]\\to 0\\) as \\(\\alpha = \\frac{M}{N}\\to 0\\) . Proposition 3.1 tells us that as problems get easier, i.e., they are in the satisfiable phase, their graph representations get (Ricci) flatter. This statement implies a relationship between very simple problems and their bipartite topology, which is that each clause is made of distinct literals. In this scenario, producing a satisfying assignment becomes trivial since assigning truth values to these unique literals does not affect other clauses. This implication aligns well with common knowledge, therefore the next thing to check is whether there is an opposite behavior when dealing with very difficult problems. This later case is important because it is close to the unsatisfiable phase where the faults of existing algorithms emerge [2]. Through a similar argument as before, we can formalize this intuition and characterize the average lower bound in the case when problems are surely unsatisfiable. Proposition 3.2. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) . In this scenario, the average lower bound of the graph RC will tend to be maximally negative, therefore we cannot directly speak on the average curvature by sandwiching as before. Nevertheless, Proposition 3.2 provides an extremely interesting insight into the interplay between \\(\\alpha\\) and \\(k\\) . As the problems get harder, the number of constraints (i.e., \\(k\\) ) becomes a decisive factor on the curvature. A larger value of \\(k\\) implies more constraints and thus many long range interactions between the literals, but it also implies that edges will become more negatively curved. In turn, this implies the existence of bottlenecks in the graph, which makes long range communication becomes difficult [47]. While it is not possible to connect the exact graph OC to this result, it is possible to extend it to the average graph BFC: Theorem 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. The BFC at \\(i\\sim j\\) is bounded from above by the quantity: \\[\\bar{R} (i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\max \\{d_{i},d_{j}\\}}. \\quad (10)\\] Furthermore, as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) , \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) and therefore the average BFC over the edges of \\(G\\) converges to \\(\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]\\to \\frac{2}{k} - 2\\) . Theorem 3.1 contains a crucial result in its characterization of the typical- case BFC, which is that it is connected to both \\(\\alpha\\) and \\(k\\) . While the connection to \\(\\alpha\\) might seem obvious, the one with \\(k\\) is quite insightful. One can observe that as problems become harder to satisfy (large values of \\(\\alpha\\) ), \\(k\\) plays an important role in how negatively curved the graph edges can become. This phenomenon can be seen in Figure 1, where for smaller \\(k\\) , larger values of \\(\\alpha\\) are required to have highly negatively curved edges. This latter point is crucial in developing a more profound understanding of the limitations of GNN- based solvers, and we will expand upon it in the following subsection. Another interesting consequence of our results, is that due to their shared lower bound and the results in Propositions 3.1 and 3.2, we confirm a tight link between graph OC and BFC. In general, the correlation between graph OC and FC has been studied in great detail on an array of complex networks by Samal et al. [44]. In our case, we confirm that both OC and BFC discretizations behave identically for simpler problems and similarly in relation to \\(\\alpha\\) and \\(k\\) , a phenomenon that can also be verified empirically through Figures 1 and 4 (Appendix). ### 3.1 Message-Passing Bottlenecks and Downstream Performance The results presented in the previous section allow us to proceed with a principled way of understanding performance limitations in GNN- based SAT solvers. This is due to the direct connection between the result in Theorem 3.1 to Theorem 4 of Topping et al. [47], which establishes that \"edges with high negative curvature are those causing the graph bottleneck and thus leading to the oversquashing phenomenon\". This seminal result states that if the gradients of the message passing functions ( \\(\\theta\\) and \\(\\phi\\) in Equation 1) are bounded, and there exists a sufficiently negatively curved edge compared to the degrees of its endpoints, then the derivative of the learned node representations around that edge vanishes. Intuitively, this can be understood as a difficulty of propagating the information in nodes at a reachable distance due to the fact that the graph structure limits the pathways where\n\n<center>Figure 1: Average graph Ricci Curvature lower bound (a) and Balanced Forman Curvature (b) as a function of \\(\\alpha\\) and \\(k\\) . Both quantities behave very similarly, both in terms of the smooth transition from flat to negative curvature and their magnitude, especially as \\(\\alpha \\to 0\\) and \\(\\alpha \\to \\infty\\) , in line with the developed theory. An alternative version of this figure displaying the average graph OC in (b) is presented in Figure 4 of the Appendix. Best viewed in color. </center> information can travel. Simply stated, nodes in different neighborhoods need to pass all messages through the same edge(s), leading to a difficulty in learning fixed- length representations that can hold information on long range correlations. Formally, for large values of \\(k\\) , as the clause density \\(\\alpha \\to \\infty\\) , any infinitesimally small value \\(\\delta > 0\\) could be used in Theorem 4 of Topping et al. [47] such that \\(Ric(i,j) \\leq - 2 + \\delta\\) , leading to an exponentially decaying Jacobian of the node representations around \\(i \\sim j\\) . This result leads us to the conclusion that GNN- based solvers are limited by both these parameters and suffer from two distinct hardness types: the algorithmic hardness inherent to SAT and the hardness of learning representations for long range communication. The interplay between \\(k\\) and \\(\\alpha\\) in Theorem 3.1 provides additional insights. Indeed, for problems with large values of \\(k\\) or large values of \\(\\alpha\\) , highly negatively curved edges are guaranteed to exist on average, and this quantity will concentrate. On the other hand, for large values of \\(\\alpha\\) and relatively small values of \\(k\\) , the latter becomes crucial in deciding how well a GNN- based solver will be able to learn, i.e., it should be easier to learn a solver for smaller values of \\(k\\) . We confirm this fact empirically in Section 4. The intuition we provide for a more complete understanding of this crucial result is the following: At increasing connectivity, literals become very distant on the interaction network, i.e., the number of long- range codependencies increases. In this scenario, the GNN will not be able to learn a fixed length representation that can \"remember\" the information of reachable, but not directly adjacent nodes. This means that the ability to learn a solver is compromised by an oversquashing phenomenon. For large values of \\(k\\) , this problem becomes prevalent even before the hardness of exploring the solution space, due to the effect of \\(k\\) on the BFC. Our theory motivates therefore how increasing values of \\(k\\) in random \\(k\\) - SAT would lead to worse oversquashing and performance, even for what would be considered simple problems in terms of \\(\\alpha\\) . To visually understand the aforementioned concepts, we can again refer to Figure 1. As the value of \\(k\\) grows, the gap between the flatter (yellow) and highly negatively curved problems (violet) gets smaller. The same holds for increasing values of \\(\\alpha\\) , as expected. We provide additional visual depictions of this aforementioned explanation in Appendix A.5, where we plot two input graphs for random 3- SAT at small (Figure 5) and large (Figure 6) \\(\\alpha\\) . In the following section, we will show that our results can be empirically confirmed for different GNN- based solvers. ## 4 Experiments Experimental Setting. To validate our theory, we perform different experiments on various datasets, the details of which will be provided in the following subsections. The experiments consist in firstly exploring the behavior of a GNN- based solvers and relating it to the input graph BFC. Based on these\n\n<center>Figure 2: (a) Average BFC as a function of \\(\\alpha\\) for random 3-SAT problems with \\(N = 256\\) . The color is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model [46], with a group labeled as solvable if \\(50\\%\\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c} \\approx 4.267\\) [32]. The average BFC drops monotonically with \\(\\alpha\\) . The small plot in the bottom-left corner provides the model's solution probability curve in terms of \\(\\alpha\\) , where it is possible to clearly notice the algorithmic transition from satisfiable to unsatisfiable problems. (b) Probability of finding a satisfying assignment of the same problems as in (a) with Neuro SAT as a function of the variance and average of the BFC. Notice how as \\(\\alpha\\) grows, the average curvature not only gets more negative, but also concentrates. We can see from the empirical results in (a) that in this case, the model is unable to produce a satisfying assignment. As \\(\\alpha\\) becomes smaller, the input graphs have less negative edges on average, the associated variance naturally grows, and so does the solving probability. Using the first two moments of the BFC, we are able to observe a similar transition-like phenomenon as the small plot in the bottom-left corner of (a). </center> results, we then propose two heuristics to understand how hard a given SAT dataset will be to solve for a GNN- based solver. We will focus on the assignment scenario, as it includes the decision scenario as well. Given that all the datasets we utilize do not come with node features, we make use of learned embeddings in order to effectively explore oversquashing implications. The GNN- based solvers are implemented following the design of Li et al. [28], using Py Torch [41], Py Torch- Geometric [17] and Py Torch Lightning [15]. The networks were trained for 100 epochs using the Adam W optimizer [29], with learning rate \\(\\eta = 0.0001\\) decaying by half after 50 epochs and the gradients clipped at unit norm. The training was done on NVIDIA Titan RTX GPUs. ### 4.1 The Relationship Between Curvature and Satisfiability We start by using numerical simulations to verify the theoretical claims made in Section 3.1. To do this, we generate random 3- SAT instances in Conjunctive Normal Form (CNF) using a custom implementation in the Julia language [4]. We generate problems with \\(\\alpha \\in [3,5]\\) in steps of \\(\\Delta \\alpha = 0.1\\) , capturing both satisfiable and unsatisfiable regimes around the known critical threshold \\(\\alpha_{c} \\approx 4.267\\) . Considering its widespread use and downstream performance, we train the Neuro SAT model [46] to produce a satisfying assignment, while scaling the number of message passing iterations by \\(2N\\) during evaluation, to maximize inference accuracy. We then analyze the performance of the model on problems with \\(N = 256\\) , with the results being summarized in Figure 2. Our results show that by considering the probability of finding a solution at a given \\(\\alpha\\) as a function of the first and second moments of the curvature, we can replicate a SAT/UNSAT phase- transition (Figure 2b). This result presents an important step forward in theoretically understanding the performance of GNN- based solvers.[32]. Similar results hold for random 4- SAT, and can be seen in Figures 7 and 8 in the Appendix. For this higher value of \\(k\\) , we have more negatively curved edges which strongly impact the performance of GNN- based solvers, as will further confirmed in Section 4.2. An interesting observation is that for random 3- SAT, the curvature starts to become highly negative and concentrate close to the estimated dynamical threshold \\(\\alpha_{d} \\approx 3.927\\) [32].\n\nTest- time Rewiring. In order to obtain additional evidence about the previous observations, we put ourselves in a unique scenario: Suppose a GNN- based solver is trained on a dataset of SAT problems, and later tested on a separate testing partition. If we render the said testing partition less curved, would the model perform better without needing to retrain? The purpose behind this experiment is to gain a deeper understanding of the relationship between curvature and problem complexity. For this purpose, we use four different SAT benchmarks proposed by Li et al. [28]: Random 3 and 4 - SAT generated near the (respective) critical threshold \\(\\alpha_{c}\\) , a random \\(k\\) - SAT dataset consisting of mixed \\(k\\) values (SR), and one that mimics the modularity and community structure of industrial problems (CA). The last two datasets are better representatives of real- world problems. We train both a Graph Convolutional Network (GCN)- solver [25] and Neuro SAT on training partitions using the same protocol as before, while the testing partition is rewired using a stochastic discrete Ricci flow procedure, similarly to [47]. The idea behind the rewiring procedure is quite simple: we make the input graph less curved by stochastically deleting edges that have the highest negative curvature, while adding new edges that are less curved. We provide a more detailed explanation of this process, including a schematic algorithm in Appendix A.3. The results are reported in Table 1, where it be observed that that the rewired problems become simpler to solve for both solvers at test- time. A noteworthy observation is that a large improvement happens on 4- SAT problems, while the modular CA dataset reports small improvements. In the following subsection, we make a direct connection of this result with our theory. ### 4.2 A New Hardness Heuristic for GNN-Based Solvers Based on the developed theory and the above observations, we provide, as a practical contribution, two different heuristics that reflect how hard it will be for a GNN- based solver to tackle a dataset. The main motivation behind these heuristics is that if we simply look at the average clause density of a dataset, we can miss out on direct implications of oversquashing. An example of this is the first dataset we presented for the random 3- SAT experiments discussed in Figure 2, which is build at increasing values of \\(\\alpha\\) . Given an input graph \\(G\\) , we define the heuristics as: \\[\\omega (G) = -\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]*\\mathbb{E}[\\alpha ],\\quad \\omega^{*}(G) = \\frac{\\omega(G)}{\\mathbb{V}_{(i\\sim j)}[Ric(i,j)]}, \\quad (11)\\] with the expectations being taken over the edges \\(G\\) . The averages of both heuristics, which we denote by \\(\\bar{\\omega}\\) and \\(\\bar{\\omega}^{*}\\) can then be used to judge the hardness of a given dataset. Intuitively, we provide two non- negative numbers that reveal how dense and curved \\(G\\) is on average \\((\\omega (g))\\) and how much this quantity concentrates \\((\\omega^{*}(G))\\) . Our theoretical insights tell us that these quantities should provide information into the hardness of learning a GNN- based solver. We report the generalization error (1 - testing accuracy) of Neuro SAT on the four previously mentioned benchmarks in Section 4.1, alongside the heuristics in Table 2. A (linear) correlation analysis between the error and the (normalized) heuristics reveals that our curvature- based approach serves as a better predictor of generalization: the respective correlation coefficients are \\(\\rho_{\\bar{\\alpha}} = 0.32\\) , \\(\\rho_{\\bar{\\omega}} = 0.86\\) and \\(\\rho_{\\bar{\\omega}^{*}} = 0.98\\) . These results allow us to formally motivate the performance gains during the test- time rewiring procedure discussed previously. What we observe, is that due to its community structure, the CA dataset has a large clause density, but its average curvature is much lower than that of random 4- SAT problems. This is natural, since a community structure is inherently linked with edges that act as less important bottlenecks for message passing [38]. These results show that the ability of GNN- based solvers to learn representations that can learn long range correlations and generalize well is deeply connected with the curvature of the input data, as discussed thoughtful the paper. ## 5 Conclusions Practical Takeaways and Future Work. In this paper, we have shown that the accuracy of GNN- based SAT solvers is directly related to the input data structure. This relationship is universally prevalent across all machine learning applications and as a result we have different modeling principles for different data. For SAT problems, we have identified that the geometry of the input data is a plausible cause of deficiency, due to its connections with oversquashing. What is extremely fascinating is that most modern GNN- based SAT solvers implement some type of recurrence mechanism [28, 35, 40, 46], and this architectural component has been recently shown to be a great starting point to mitigate oversquashing [3]. The implicit effect of recurrence can be immediately noted by comparing the drop in performance between the GCN and Neuro SAT solvers in Table 1.\n\nTable 1: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers. By increasing the testing set's curvature at test-time through rewiring, both solvers are able to make big leaps in accuracy, especially in more difficult problems. Reducing the curvature of the problem facilitates long range communication and renders problems easier. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface and absolute improvements in parentheses. <table><tr><td>Model</td><td>Variation</td><td>3-SAT</td><td>4-SAT</td><td>Datasets</td><td>SR</td><td>CA</td></tr><tr><td rowspan=\"2\">GCN</td><td>No Rewiring</td><td>0.510 \u00b1 0.012</td><td>0.180 \u00b1 0.048</td><td>0.470 \u00b1 0.031</td><td>0.650 \u00b1 0.016</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.626 \u00b1 0.021 (+0.116)</td><td>0.374 \u00b1 0.045 (+0.194)</td><td>0.696 \u00b1 0.035 (+0.226)</td><td>0.670 \u00b1 0.048 (+0.020)</td><td></td></tr><tr><td rowspan=\"2\">Neuro SAT</td><td>No Rewiring</td><td>0.690 \u00b1 0.022</td><td>0.436 \u00b1 0.032</td><td>0.734 \u00b1 0.017</td><td>0.746 \u00b1 0.018</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.820 \u00b1 0.030 (+0.130)</td><td>0.686 \u00b1 0.029 (+0.250)</td><td>0.902 \u00b1 0.004 (+0.168)</td><td>0.828 \u00b1 0.029 (+0.082)</td><td></td></tr></table> Table 2: Average generalization error (1- testing accuracy) over 5 different runs with the Neuro SAT model on SAT benchmark datasets [28]. The error is reported alongside the average clause density \\(\\bar{\\alpha}\\) and the curvature-based heuristics \\(\\bar{\\omega}\\) and \\(\\bar{\\omega}^{*}\\) . Our heuristics display very strong linear correlation with the generalization error, unlike the average clause density, making it possible to predict how hard each benchmark will be for a GNN-based solver. <table><tr><td rowspan=\"2\">Problem</td><td rowspan=\"2\">Generalization Error</td><td colspan=\"3\">Hardness Heuristic</td></tr><tr><td>\u03b1</td><td>\u03c9</td><td>\u03c9*</td></tr><tr><td>3-SAT</td><td>0.31</td><td>4.59</td><td>4.12</td><td>97.41</td></tr><tr><td>4-SAT</td><td>0.56</td><td>9.08</td><td>9.81</td><td>612.32</td></tr><tr><td>SR</td><td>0.27</td><td>6.09</td><td>5.30</td><td>125.30</td></tr><tr><td>CA</td><td>0.25</td><td>9.73</td><td>6.30</td><td>123.27</td></tr></table> This fact leads us to conjecture that the relationship between input data and model performance is prevalent throughout Neural Combinatorial Optimization (NCO) [48], and different architectural designs are necessary for different problems. Furthermore, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. We report results for some straightforward curvature- aware solvers in Appendix A.4, which do not report consistent improvements. A direct avenue for future work that we consider promising in this field is the application of continuous graph diffusion dynamics for learning[11, 21], which generalize the recurrence mechanism. Finally, regarding the theoretical aspect, we believe that promising avenues for future work that were not directly addressed here are the characterization of both curvature and topological quantities in distribution and not just in the mean. Another direction that is interesting is making a connection between the various critical points of the clause density and the curvature, such that the well- known phase transitions of SAT can be directly related to novel geometric order parameters. Closing Remarks. In conclusion, our results highlight that the limitations of GNN- based SAT solvers cannot be fully understood without considering the geometric properties of the input. Our study presents, to the best of our knowledge, the first attempt at a theoretical understanding of these neural solvers, by establishing a direct connection between their negatively curved graph representations and oversquashing in GNNs. We provide empirical evidence of this connection and verify that it is prevalent on more constrained instances. Beyond SAT, we expect these insights to be valuable for other domains where graph representations of combinatorial problems are employed. Combinatorial Optimization (CO) provides an interesting venue to study the reasoning behavior of NNs, and we hope that this paper makes a case for such studies. In conclusion, we hope that bridging concepts from deep learning, geometry, and physics, will pave the way for principled advances in the design of neural solvers."
  },
  "section_objects": [
    {
      "heading": "On the Hardness of Learning GNN based SAT Solvers",
      "content": "## Introduction\n\n\n## Acknowledgements Acknowledgements Geri Skenderi is funded by the European Union through the Next Generation EU - MIUR PRIN PNRR 2022 Grant P20229PBZR. The views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible. ## References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. 1, 3 [2] Maria Chiara Angelini and Federico Ricci- Tersenghi. Monte carlo algorithms are very effective in finding the largest independent set in sparse random graphs. Physical Review E, 100(1): 013302, 2019. 1, 5 [3] Alvaro Arroyo, Alessio Gravina, Benjamin Gutteridge, Federico Barbero, Claudio Gallicchio, Xiaowen Dong, Michael Bronstein, and Pierre Vandergheynst. On vanishing gradients, oversmoothing, and over- squashing in gnns: Bridging recurrent and graph learning. ar Xiv preprint ar Xiv:2502.10818, 2025. 8 [4] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing. SIAM review, 59(1):65- 98, 2017. URL https://doi.org/10.1137/141000671. 7 [5] Bhaswar B Bhattacharya and Sumit Mukherjee. Exact and asymptotic results on coarse ricci curvature of graphs. Discrete Mathematics, 338(1):23- 42, 2015. 4, 13, 15, 19 [6] Armin Biere, Marijn Heule, and Hans van Maaren. Handbook of satisfiability, volume 185. IOS press, 2009. 1, 3 [7] Lucas Bordeaux, Youssef Hamadi, and Lintao Zhang. Propositional satisfiability and constraint programming: A comparative survey. ACM Computing Surveys (CSUR), 38(4):12- es, 2006. 1 [8] A. Braunstein, M. Mezard, and R. Zecchina. Survey propagation: An algorithm for satisfiability. Random Structures & Algorithms, 27(2):201- 226, 2005. doi: https://doi.org/10.1002/rsa.20057. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.20057. 1 [9] Michael M Bronstein, Joan Bruna, Yann Le Cun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18- 42, 2017. 3 [10] Quentin Cappart, Didier Chetelat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Veli\u010dkovi\u0107. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 1 [11] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele Rossi. Grand: Graph neural diffusion. In International conference on machine learning, pages 1407- 1418. PMLR, 2021. 9 [12] Wenjing Chang and Wenlong Liu. Sat- gatv2: A dynamic attention- based graph neural network for solving boolean satisfiability problem. Electronics, 14(3):423, 2025. 1 [13] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, page 151- 158, New York, NY, USA, 1971. Association for Computing Machinery. ISBN 9781450374644. doi: 10.1145/800157.805047. URL https://doi.org/10.1145/800157.805047. 1 [14] Gabriele Corso, Hannes Stark, Stefanie Jegelka, Tommi Jaakkola, and Regina Barzilay. Graph neural networks. Nature Reviews Methods Primers, 4(1):17, 2024. 3 [15] William Falcon and The Py Torch Lightning team. Py Torch Lightning, March 2019. URL https://github.com/Lightning- AI/lightning. 7 [16] Lukas Fesser and Melanie Weber. Effective structural encodings via local curvature profiles. In International Conference on Learning Representations, 2024. 18 [17] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. ar Xiv preprint ar Xiv:1903.02428, 2019. 7\n\n[18] Forman. Bochner's method for cell complexes and combinatorial ricci curvature. Discrete & Computational Geometry, 29:323- 374, 2003. 3, 14 [19] Karlis Freivalds and Sergejs Kozlovics. Denoising Diffusion for Sampling SAT Solutions, November 2022. URL http://arxiv.org/abs/2212.00121. ar Xiv:2212.00121 [cs]. 1 [20] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263- 1272. PMLR, 2017. 1, 3 [21] Andi Han, Dai Shi, Lequan Lin, and Junbin Gao. From continuous dynamics to graph neural networks: Neural diffusion and beyond. Transactions on Machine Learning Research, 2024. 9 [22] Masato Inagaki. Spectral convergence of graph laplacians with ricci curvature bounds and in non- collapsed ricci limit spaces. ar Xiv preprint ar Xiv:2506.07427, 2025. 13 [23] Jurgen Jost and Shiping Liu. Ollivier's ricci curvature, local clustering and curvature- dimension inequalities on graphs. Discrete & Computational Geometry, 51(2):300- 322, 2014. 4 [24] Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer computations, pages 85- 103. Springer, 1972. 1 [25] TN Kipf. Semi- supervised classification with graph convolutional networks. ar Xiv preprint ar Xiv:1609.02907, 2016. 8 [26] Florent Krzakala, Andrea Montanari, Federico Ricci- Tersenghi, Guilhem Semerjian, and Lenka Zdeborov\u00e1. Gibbs states and the set of solutions of random constraint satisfaction problems. Proceedings of the National Academy of Sciences, 104(25):10318- 10323, 2007. 1, 2 [27] Qimai Li, Zhichao Han, and Xiao- Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. 1 [28] Zhaoyu Li, Jinpei Guo, and Xujie Si. G4sabench: Benchmarking and advancing sat solving with graph neural networks. Transactions on Machine Learning Research, 2024. 1, 2, 3, 4, 7, 8, 9, 18 [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 7 [30] Raffaele Marino, Giorgio Parisi, and Federico Ricci- Tersenghi. The backtracking survey propagation algorithm for solving random k- sat problems. Nature communications, 7(1):12996, 2016. 1 [31] Joao Marques- Silva. Practical applications of boolean satisfiability. In 2008 9th International Workshop on Discrete Event Systems, pages 74- 80, 2008. doi: 10.1109/WODES.2008.4605925. 1 [32] Stephan Mertens, Marc Mezard, and Riccardo Zecchina. Threshold values of random k- sat from the cavity method. Random Structures & Algorithms, 28(3):340- 373, 2006. doi: https://doi.org/10.1002/rsa.20090. 7, 20, 21 [33] Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009. 1, 2 [34] Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientific Publishing Company, 1987. 2 [35] David Moj\u017ei\u0161ek, Jan \u0171la, Ziwei Li, Ziyu Zhou, and Mikol\u00e1\u0161 Janota. Neural approaches to sat solving: Design choices and interpretability. ar Xiv preprint ar Xiv:2504.01173, 2025. 8 [36] R\u00e9mi Monasson and Riccardo Zecchina. Statistical mechanics of the random k- satisfiability model. Physical Review E, 56(2):1357, 1997. 1 [37] M. Mezard, G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812- 815, 2002. doi: 10.1126/science.1073287. URL https://www.science.org/doi/abs/10.1126/science.1073287. 2 [38] Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 2, 3, 8\n\n[39] Yann Ollivier. Ricci curvature of markov chains on metric spaces. Journal of Functional Analysis, 256(3):810- 864, 2009. 3, 13 [40] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. Goal- Aware Neural SAT Solver. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1- 8, July 2022. doi: 10.1109/IJCNN55064.2022.9892733. URL http://arxiv.org/abs/2106.07162. ar Xiv:2106.07162 [cs]. 1, 4, 8 [41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. Advances in neural information processing systems, 32, 2019. 7 [42] Daniel Paulin. Mixing and concentration by ricci curvature. Journal of Functional Analysis, 270(5):1623- 1662, 2016. 13 [43] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5- 6):355- 607, 2019. 13 [44] Areejit Samal, RP Sreejith, Jiao Gu, Shiping Liu, Emil Saucan, and J\u00fcrgen Jost. Comparative analysis of two discretizations of ricci curvature for complex networks. Scientific reports, 8(1): 8650, 2018. 3, 5, 14 [45] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61- 80, 2008. 1, 3 [46] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, David L. Dill, and Leonardo De Moura. Learning a SAT solver from single- bit supervision. 7th International Conference on Learning Representations, ICLR 2019, pages 1- 11, 2019. ar Xiv: 1802.03685. 1, 4, 7, 8, 20 [47] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. Understanding over- squashing and bottlenecks on graphs via curvature. In International Conference on Learning Representations, 2022. 2, 3, 4, 5, 6, 8, 13, 14, 15, 17 [48] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth Mc Millan, and Risto Miikkulainen. Neuro Comb: Improving SAT Solving with Graph Neural Networks, June 2022. URL http://arxiv.org/abs/2110.14053. ar Xiv:2110.14053 [cs] version: 2. 9 [49] Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. Curvature graph network. In International Conference on Learning Representations, 2019. 18 [50] Lenka Zdeborov\u00e1. Statistical physics of hard optimization problems. Acta Physica Slovaca, 59 (3):169- 303, 2009. 1, 2\n\n## A Appendix In what follows, we provide supplementary material that complements the main manuscript. The appendix is organized as follows: 1. Appendix A.1 begins with background on graph Ricci curvature, where we review both the Ollivier and Balanced Forman discretizations in order to give the reader an intuitive and formal foundation for subsequent results. 2. In Appendix A.2, we provide detailed proofs of the propositions and theorems discussed in the manuscript. 3. Appendix A.3 describes the test-time graph rewiring procedure used in our experiments, including a full presentation of the stochastic curvature-guided algorithm. 4. In Appendix A.4, we outline preliminary ideas and implementations for curvature-aware solvers, along with empirical results that illustrate their potential. 5. Finally, Appendix A.5 contains additional plots and visualizations that further clarify the relationship between curvature, problem hardness, and solver behavior. ## A.1 Ricci curvature of graphs In this section, we provide, for the sake of completeness, both definitions and intuitive explanations of the two types of graph Ricci Curvature (RC) mentioned in this paper. The definitions are taken from Bhattacharya and Mukherjee [5] and Topping et al. [47] respectively, we simply report them here in a synthesized manner. We kindly refer the reader to the aforementioned works for more details. ## A.1.1 Ollivier Ricci curvature (OC) The formulation of Ollivier [39] aligns with the intuition from differential geometry: edges with negative curvature act as structural bottlenecks, separating dense regions and limiting smooth information propagation. Edges with positive curvature in the other hand facilitate smooth information propagation and are indicators of community structure. Graph Ollivier- Ricci Curvature (OC) is very well studied and has being linked to properties of graph Lapacians and mixing times of Markov Chain Monte Carlo (MCMC) methods [22, 42]. The intuitive idea of this discretization is to directly implement the idea from differential geometry: we use a ratio between the amount of \"mass\" moved around of an edge neighborhood with the shortest path distance, i.e., the graph geodesic. Let us formalize this concept. For two probability measures \\(\\mu_{1},\\mu_{2}\\) on a metric space \\((X,d)\\) , the the Wasserstein distance between them is defined as \\[W_{1}(\\mu_{1},\\mu_{2}) = \\inf_{\\nu \\in M(\\mu_{1},\\mu_{2})}\\int_{X\\times X}d(x,y)\\mathrm{d}\\nu (x,y), \\quad (12)\\] where \\(M(\\mu_{1},\\mu_{2})\\) is the collection of probability measures on \\(X\\times X\\) with marginals \\(\\mu_{1}\\) and \\(\\mu_{2}\\) . The Wasserstein distance is the result of the solution to a famous problem called Optimal Transport [43]. Intuitively, this distance measures the optimal cost to move one pile of sand to another one with the same mass. Let a metric measure space \\((X,d,m)\\) be a metric space \\((X,d)\\) , with a collection of probability measures \\(m = \\{m_{x}:x\\in X\\}\\) indexed by the points of \\(X\\) . The (coarse) Ricci curvature of a metric measure space is defined as follows: Definition A.1 (Ollivier [39]). On any metric measure space \\((X,d,m)\\) , for any two distinct points \\(x,y\\in X\\) , the (coarse) Ricci curvature of \\((X,d,m)\\) of \\((x,y)\\) is defined as: \\[\\kappa (x,y)\\coloneqq 1 - \\frac{W_{1}(m_{x},m_{y})}{d(x,y)} \\quad (13)\\] Extending this definition to graphs requires some additional steps. Consider a locally finite and possibly weighted simple graph \\(G = (V,E)\\) , where each edge \\((i,j)\\in E\\) is assigned a positive weight \\(w_{ij} = w_{ji}\\) . The graph is equipped with the standard shortest path graph distance \\(d_{G}\\) , that is, for \\(i,j\\in V\\) , \\(d_{G}(i,j)\\) is the length of the shortest path in \\(G\\) connecting nodes \\(i\\) and \\(j\\) . For \\(i\\in V\\)\n\ndefine the degree \\(d_{i} \\coloneqq \\sum_{(i,j) \\in E} w_{ij}\\) and the neighborhood \\(\\mathcal{N}(i) \\coloneqq \\{j \\in V: (i,j) \\in E\\}\\) . For each \\(i \\in V\\) define a probability measure \\[m_{i}(j) = \\left\\{ \\begin{array}{ll} \\frac{w_{i,j}}{d_{i}}, & \\mathrm{if} j \\in \\mathcal{N}(i) \\\\ 0, & \\mathrm{otherwise.} \\end{array} \\right.\\] Note that these are just the transition probabilities of a weighted random walk on the vertices of \\(G\\) . If \\(m_{G} = \\{m_{i}: i \\in V\\}\\) , then considering the metric measure space \\(\\mathcal{M}_{G} \\coloneqq (V, d_{G}, m_{G})\\) , we can define the OC curvature for any edge \\((i,j) \\in E\\) as \\(\\kappa (\\mathbf{i},\\mathbf{j}) \\coloneqq \\mathbf{1} - \\mathbf{W}_{1}^{G}(\\mathbf{m}_{i}, \\mathbf{m}_{j})\\) , where \\(W_{1}^{G}(m_{i}, m_{j})\\) is obtained by discretizing Equation (12) on \\(\\mathcal{M}_{G}\\) : \\[W_{1}^{G}(m_{i}, m_{j}) = \\inf_{\\nu \\in \\mathcal{A}} \\sum_{z_{1} \\in \\mathcal{N}(i)} \\sum_{z_{2} \\in \\mathcal{N}(j)} \\nu (z_{1}, z_{2}) d(z_{1}, z_{2}). \\quad (14)\\] \\(\\mathcal{A}\\) denotes the set of all \\(d_{i} \\times d_{j}\\) matrices with entries indexed by \\(\\mathcal{N}(i) \\times \\mathcal{N}(j)\\) such that \\(\\nu (i', j') \\geq 0\\) , \\(\\sum_{z \\in \\mathcal{N}(j)} \\nu (i', z) = \\frac{w_{i,i'}}{d_{i}}\\) , and \\(\\sum_{z \\in \\mathcal{N}(i)} \\nu (z, j') = \\frac{w_{j,j'}}{d_{j}}\\) , for all \\(i' \\in \\mathcal{N}(i)\\) and \\(j' \\in \\mathcal{N}(j)\\) . For a matrix \\(\\nu \\in \\mathcal{A}\\) , \\(\\nu (i', j')\\) represents the mass moving from \\(i' \\in \\mathcal{N}(i)\\) to \\(j' \\in \\mathcal{N}(j)\\) . For this reason, the matrix \\(\\nu\\) is often called the transfer plan. ## A.1.2 Balanced Forman curvature (BFC) The Forman- Ricci Curvature (FC) is a discretization of Ricci curvature that holds for a broad class of topological objects, namely so called (regular) cellular (CW) complexes [44]. The original definition [18] is both extremely technical and out of the scope of this paper. Given that graphs edges are 1- dimensional cells (topologically), the definition simplifies greatly making use of very simple graph properties. Consider a simple, unweighted graph for simplicity, then the FC of edge \\((i,j)\\) is defined as \\(\\kappa_{G}^{f}(i,j) = 4 - d_{i} - d_{j}\\) . The main issue of this definition is that it ignores higher order correlations in terms of triangles and 4- cycles, which prove crucial to discriminate positively, flat, and negatively curved graphs. Inspired by these issues, Topping et al. [47] propose an extension of the FC dubbed Balanced Forman Curvature (BFC), such that it is both fast to compute and encodes accurate curvature information. Let us start by defining the sphere and ball of radius \\(r\\) centered at a node \\(i\\) of the graph by: \\[S_{r}(i) \\coloneqq \\{j \\in V: d_{G}(i,j) = r\\} , \\quad B_{r}(i) \\coloneqq \\{j \\in V: d_{G}(i,j) \\leq r\\} . \\quad (15)\\] The BFC is then defined using three combinatorial components: (i) \\(\\sharp_{\\Delta}(i,j)\\) , the number of triangles based at the edge \\(i \\sim j\\) . (ii) \\(\\sharp_{\\square}^{i}(i,j) \\coloneqq\\) , the number of nodes \\(k \\in S_{1}(i)\\) forming a 4- cycle based at \\(i \\sim j\\) without diagonals inside. (iii) \\(Q_{i}(j) \\coloneqq S_{1}(i) \\setminus (\\{j\\} \\cup \\sharp_{\\Delta}(i,j) \\cup \\sharp_{\\square}^{i}(i,j))\\) , simply the complement of the neighbours of \\(i\\) with respect to the sets introduced in (i) and (ii) once we also exclude \\(j\\) . Definition A.2 (Topping et al. [47]). For any edge \\(i \\sim j\\) in a simple, unweighted graph \\(G = (V, E)\\) with adjacency matrix \\(A\\) let: \\(S_{1}(i) \\coloneqq \\{j \\in V: i \\sim j \\in E\\}\\) be the set of 1- hop neighbors of \\(i\\) . \\(\\sharp_{\\Delta}(i,j) \\coloneqq S_{1}(i) \\cap S_{1}(j)\\) be the set of triangles based at \\(i \\sim j\\) . \\(\\sharp_{\\square}^{i}(i,j) \\coloneqq \\{k \\in S_{1}(i) \\setminus S_{1}(j), k \\neq j: \\exists w \\in (S_{1}(k) \\cap S_{1}(j)) \\setminus S_{1}(i)\\}\\) be the neighbors of \\(i\\) forming a 4- cycle based at \\(i \\sim j\\) without diagonals inside. \\(\\gamma_{\\max}(i,j) \\coloneqq \\max \\left\\{\\max_{k \\in \\sharp_{\\square}^{i}} \\{(A_{k} \\cdot (A_{j} - A_{i} \\odot A_{j})) - 1\\} , \\max_{w \\in \\sharp_{\\square}^{i}} \\{(A_{w} \\cdot (A_{i} - A_{j} \\odot A_{i})) - 1\\} \\right\\}\\) , with \\(\\cdot\\) being the dot product and \\(\\odot\\) the elementwise product, be the maximal number of 4- cycles based at \\(i \\sim j\\) traversing a common node. The BFC \\(Ric(i,j)\\) is zero if \\(\\min \\{d_{i}, d_{j}\\} = 1\\) and alternatively: \\[R i c(i,j) \\coloneqq \\frac{2}{d_{i}} + \\frac{2}{d_{j}} -2 + 2\\frac{|\\sharp_{\\Delta}(i,j)|}{\\max \\{d_{i},d_{j}\\}} +\\frac{|\\sharp_{\\Delta}(i,j)|}{\\min \\{d_{i},d_{j}\\}} +\\frac{(\\gamma_{m a x}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|). \\quad (16)\\]\n\n## A.2 Proofs For the sake of clarity and exposition, we report here both the formal statements alongside their respective proofs. Proposition 3.1. Let \\(i\\sim j\\) be an edge from the Literal- Clause Graph (LCG) representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\to 0\\) and \\(\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\to 0\\) as \\(\\alpha = \\frac{M}{N}\\to 0\\) Proof. As \\(\\alpha \\to 0\\) the expected value of the literal degree becomes: \\[\\lim_{\\alpha \\to 0}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\lim_{\\alpha \\to 0}\\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} = \\lim_{\\alpha \\to 0}e^{\\lambda}\\cdot \\lim_{\\alpha \\to 0}\\frac{\\lambda}{e^{\\lambda} - 1} = 1\\cdot \\lim_{\\alpha \\to 0}\\frac{1}{\\frac{e^{\\lambda} - 1}{\\lambda}} = 1, \\quad (17)\\] where the last equality is obtained due to the fact that \\(\\begin{array}{r}{\\lim_{\\alpha \\to 0}\\lambda = \\lim_{\\alpha \\to 0}\\frac{1}{2}\\alpha k = 0} \\end{array}\\) and the limit formula \\(\\begin{array}{r}{\\lim_{x\\to 0}\\frac{a^{x} - 1}{x} = \\ln \\left(a\\right)} \\end{array}\\) Given the average degree of the literals that act as an endpoint of at least one edge is 1 in this limiting case, by definition of the lower bound in Equation 8, we obtain that \\(\\begin{array}{r}{\\lim_{\\alpha \\to 0}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = 0} \\end{array}\\) . As a lower bound, \\(\\bar{R} (i,j)\\) satisfies the following inequality [5, 47]: \\[-2< \\bar{R} (i,j)\\leq R i c(i,j)\\leq \\kappa (i,j)\\leq 0 \\quad (18)\\] Given that both limits and expectations preserve weak inequalities, by sandwiching we obtain: \\[\\begin{array}{r l} & {\\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = 0\\leq \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\leq \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\leq 0}\\\\ & {\\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)] = \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)] = 0} \\end{array} \\quad (20)\\] Proposition 3.2. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) Proof. As \\(\\alpha \\to \\infty\\) the expected value of the literal degree becomes: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\lim_{\\alpha \\to \\infty}\\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} = \\lim_{\\alpha \\to \\infty}\\frac{1}{\\frac{1}{\\lambda} - \\frac{1}{\\lambda e^{\\lambda}}} = \\infty , \\quad (21)\\] therefore we can consider the lower bound as consisting only of \\(\\begin{array}{r}{\\underline{{R}} (i,j)] = \\frac{2}{d_{i}} +\\frac{2}{d_{j}} - 2} \\end{array}\\) (ignoring the uninteresting case when \\(k = 1\\) ). Given that the expected value is a linear operation, we obtain that: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}]} +\\frac{2}{k} -2 = \\frac{2}{k} -2 \\quad (22)\\] Theorem 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. The BFC at \\(i\\sim j\\) is bounded from above by the quantity: \\[\\bar{R} (i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\max \\{d_{i},d_{j}\\}}. \\quad (23)\\] Furthermore, as \\(\\begin{array}{r}{\\alpha = \\frac{M}{N}\\to \\infty ,\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2} \\end{array}\\) and therefore the average BFC over the edges of \\(G\\) converges to \\(\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\to \\frac{2}{k} - 2\\)\n\nProof. We first prove the construction of the upper bound, which we can then use to prove the convergence of the expectation via sandwiching, similarly to the proof of Proposition 3.1. Please note that the definitions for each component of the graph BFC are given in Appendix A.1.2 and we will not be restating them here to avoid repetition. To start, notice that the BFC on \\(G\\) takes a simpler form compared to the more general, original definition A.2: \\[R i c(i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{(\\gamma_{\\mathrm{max}}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|), \\quad (24)\\] This result follows from the fact that bipartite graphs have no triangles, i.e., \\(\\sharp_{\\Delta}(i,j) = 0 \\forall (i,j) \\in E\\) . Our main focus for now will the rightmost term, which constitutes the difference between \\(\\mathrm{Ric}(i,j)\\) and \\(\\underline{{R}}(i,j)\\) (Equation 8). Firstly note that this term is, by definition, non- negative. This implies that \\(\\underline{{R}}(i,j)\\) of \\(\\mathrm{Ric}(i,j)\\) . We can simplify the definitions of the 4- cycle forming neighbors to match the bipartite topology of \\(G\\) : \\[\\begin{array}{r l} & {\\sharp_{\\square}^{i}(i,j)\\coloneqq \\{k\\in S_{1}(i)\\setminus \\{k\\} :\\exists w\\in (S_{1}(k)\\cap S_{1}(j))\\setminus \\{i\\} \\} ,}\\\\ & {\\sharp_{\\square}^{j}(i,j)\\coloneqq \\{k\\in S_{1}(j)\\setminus \\{k\\} :\\exists w\\in (S_{1}(k)\\cap S_{1}(i))\\setminus \\{j\\} \\} .} \\end{array} \\quad (25)\\] By definition of \\(S_{1}(\\cdot)\\) it follows immediately that the cardinality of both sets is bounded above by the node degrees: \\[\\begin{array}{r l} & {|\\sharp_{\\square}^{i}(i,j)|\\leq d_{i} - 1,}\\\\ & {|\\sharp_{\\square}^{j}(i,j)|\\leq d_{j} - 1 = k - 1.} \\end{array} \\quad (28)\\] We now turn to the term \\(\\gamma_{\\mathrm{max}}(i,j)\\) , whose definition can also be simplified, as a consequence of the topology of \\(G\\) . Consider first the symmetric adjacency matrix of a bipartite graph, given by: \\[A = \\left[ \\begin{array}{ll}0 & B\\\\ B^{T} & 0 \\end{array} \\right]\\in \\{0,1\\}^{N + M\\times N + M}, \\quad (29)\\] where \\(B\\) is an \\(N\\times M\\) incidence matrix with \\(B_{i j} = 1\\) if there if \\(i\\sim j\\in E\\) , and \\(B_{i j} = 0\\) otherwise, \\(B^{T}\\) is the transpose of \\(B\\) (which ensures that \\(A\\) is symmetric), and \\(\\mathbf{0}\\) are zero blocks of size \\(N\\times N\\) and \\(M\\times N\\) corresponding to the absence of edges within the partitions. We can thus express \\(\\gamma_{\\mathrm{max}}(i,j)\\) as: \\[\\gamma_{\\mathrm{max}}(i,j)\\coloneqq \\max \\left\\{\\max_{k\\in \\sharp_{\\square}^{i}}\\{A_{k}\\cdot A_{j} - 1\\} ,\\max_{w\\in \\sharp_{\\square}^{j}}\\{A_{w}\\cdot A_{i} - 1\\} \\right\\} , \\quad (30)\\] since \\(A_{i}\\odot A_{j} = \\mathbf{0}\\) . The operation \\(A_{k}\\cdot A_{j} = \\nu \\in \\mathbb{N}_{0}\\) returns the number of common neighbors \\(\\nu\\) that nodes \\(k\\) and \\(j\\) have, with \\(k\\) and \\(j\\) being in the same partition. Therefore, we have the following inequalities that can be used to derive an upper bound for \\(\\gamma_{\\mathrm{max}}(i,j)\\) : \\[\\begin{array}{r l} & {\\max_{k\\in \\sharp_{\\square}^{i}}\\{A_{k}\\cdot A_{j} - 1\\} \\leq k - 1,}\\\\ & {\\max_{w\\in \\sharp_{\\square}^{j}}\\{A_{w}\\cdot A_{i} - 1\\} \\leq M - 1,}\\\\ & {\\gamma_{\\mathrm{max}}(i,j)\\leq \\max \\{k - 1,M - 1\\} .} \\end{array} \\quad (31)\\] Putting everything together we obtain: \\[0\\leq \\frac{(\\gamma_{\\mathrm{max}}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|)\\leq \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\cdot \\max \\{d_{i},d_{j}\\}}, \\quad (34)\\] so that we can write: \\[-2< \\underline{{R}} (i,j)\\leq R i c(i,j)\\leq \\bar{R} (i,j)\\leq 0. \\quad (35)\\] In Proposition 3.2), we have previously that as \\(\\alpha \\to \\infty\\) \\(\\mathbb{E}_{(i\\sim j)}[\\underline{{R}} (i,j)]\\to \\frac{2}{k} - 2\\) , due to the fact that the literal degree becomes a dominant term. Therefore \\(\\max \\{d_{i},d_{j}\\} = d_{i}\\) , and under the same limiting assumption we have that \\(\\max \\{k - 1,M - 1\\} = M - 1\\) . Given that the expected value is a linear operation, we obtain that: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}]} +\\frac{2}{k} -2 + \\frac{1}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[M - 1]} = \\frac{2}{k} -2. \\quad (36)\\]\n\nGiven that both limits and expectations preserve weak inequalities, by sandwiching we obtain: \\[\\frac{2}{k} -2 = \\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\leq \\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\leq \\lim_{\\alpha \\to 0}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{k} -2, \\quad (37)\\] ## A.3 Details on Test-Time Rewiring Graph rewiring is the process of modifying a graph's connectivity by adding, removing, or reweighting edges to optimize information flow. In our case, we make use of the rewiring procedure presented by Topping et al. [47], which consists of a discrete and stochastic Ricci flow, guided by the BFC. At each iteration, the edge with the most negative curvature (i.e., the structurally most \"strained\" connection) is identified. Candidate rewiring edges are then generated between the neighborhoods of the two endpoints of this edge. A new edge is stochastically selected either at random (with probability \\(p\\) ) or by maximizing the curvature improvement obtained from its addition. The selected edge is added to the graph and the corresponding curvature values are updated. By repeating this procedure for a fixed number of iterations, the algorithm progressively increases the average curvature of the graph, yielding a rewired version that contains information bottlenecks that are weaker compared to the input. Algorithm 1 contains the pseudocode for our rewiring algorithm. We would like to stress here that this modifies the constraints of the Boolean Satisfiability Problem (SAT) problem under consideration, but the goal of this procedure is to show that \"flatter\" problems will in fact be easier to solver for Graph Neural Network (GNN)- based solvers. Algorithm 1: Balanced Forman Curvature Stochastic Rewiring Input: Graph \\(G = (V,E)\\) with edge BFC values \\(R i c(i,j),\\forall (i,j)\\in E\\) , probability value \\(p\\in [0,1]\\) , number of iterations \\(N\\in \\mathbb{N}\\) Output: Rewired graph \\(G^{\\prime}\\) with updated BFC values for \\(t\\gets 1\\) to \\(N\\) do Select edge \\((i,j)\\) with most negative curvature \\(R i c(i,j)\\) From the neighbors \\(S_{1}(i)\\) and \\(S_{1}(j)\\) form candidate edge set \\[C = \\{(k,l):k\\in S_{1}(i),l\\in S_{1}(j),k\\neq l,(k,l)\\notin E\\}\\] if \\(C = \\emptyset\\) then L continue With probability \\(p\\) , choose a random edge \\((k,l)\\in C\\) Otherwise, for each \\((k,l)\\in C\\) 1. Compute updated curvature \\(R i c^{\\prime}(i,j)\\) after adding \\((k,l)\\) 2. Evaluate improvement score \\(\\Delta_{k l} = -(R i c(i,j) - R i c^{\\prime}(i,j))\\) Select \\((k,l)\\) with maximum \\(\\Delta_{k l}\\) Add edge \\((k,l)\\) to \\(G\\) and update neighborhood sets; Update curvatures \\(R i c(i,j)\\) and \\(R i c(k,l)\\) accordingly; Finalize bipartite structure of literals and clauses \\((L,C)\\) ensuring no intra- partition edges and set \\(G^{\\prime} = G\\) for \\(t = N\\) return \\(G^{\\prime}\\) ## A.4 Some initial ideas for curvature-aware solvers As discussed during the conclusion, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. In this section, we provide some starting points and experiments for simple implementations of curvature aware solvers that could lead to future improvements. The goal of our implementations was to maintain efficiency and rely on straightforward ideas that could lead to performance improvements. For these purposes, we introduce two simple curvature- aware variants of message passing. The first is an adaptation of Ye et al.\n\nTable 3: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers with different message-passing schemes: 1) Vanilla uses the typical message-passing operation; 2) Curvature Gate learns a gating function for each edge based on its curvature value [49]; 3) Online LCP extends the work of Fesser and Weber [16] and concatenates the local curvature statistics around each nodes as features during each recurrent step; 4) Both uses Curvature Gate and Online LCP. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface. <table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Variation</td><td colspan=\"4\">Datasets</td></tr><tr><td>3-SAT</td><td>4-SAT</td><td>SR</td><td>CA</td></tr><tr><td rowspan=\"3\">GCN</td><td>Vanilla</td><td>0.510 \u00b1 0.012</td><td>0.180 \u00b1 0.048</td><td>0.470 \u00b1 0.031</td><td>0.650 \u00b1 0.016</td></tr><tr><td>+ Curvature Gate</td><td>0.514 \u00b1 0.018</td><td>0.154 \u00b1 0.017</td><td>0.422 \u00b1 0.013</td><td>0.664 \u00b1 0.042</td></tr><tr><td>+ Online LCP</td><td>0.510 \u00b1 0.014</td><td>0.170 \u00b1 0.010</td><td>0.422 \u00b1 0.016</td><td>0.662 \u00b1 0.016</td></tr><tr><td></td><td>+ Both</td><td>0.500 \u00b1 0.012</td><td>0.176 \u00b1 0.031</td><td>0.416 \u00b1 0.027</td><td>0.654 \u00b1 0.027</td></tr><tr><td rowspan=\"3\">Neuro SAT</td><td>Vanilla</td><td>0.690 \u00b1 0.022</td><td>0.436 \u00b1 0.032</td><td>0.734 \u00b1 0.017</td><td>0.746 \u00b1 0.018</td></tr><tr><td>+ Curvature Gate</td><td>0.682 \u00b1 0.030</td><td>0.436 \u00b1 0.015</td><td>0.742 \u00b1 0.027</td><td>0.758 \u00b1 0.016</td></tr><tr><td>+ Online LCP</td><td>0.692 \u00b1 0.020</td><td>0.416 \u00b1 0.046</td><td>0.724 \u00b1 0.022</td><td>0.726 \u00b1 0.059</td></tr><tr><td></td><td>+ Both</td><td>0.664 \u00b1 0.018</td><td>0.438 \u00b1 0.028</td><td>0.742 \u00b1 0.025</td><td>0.758 \u00b1 0.020</td></tr></table> <center>Figure 3: Low dimensional visualization of the literal embeddings produced by Neuro SAT on random 4-SAT with (a) vanilla message passing and (b) Curvature Gate. Even though there is no major change in performance, the learned representations can be linearly separated into truth value assignments in the curvature-aware case, indicating promise for the inclusion of these principles in future GNN-based solver design. </center> [49], where the curvature of an edge is used to learn a gating mechanism that modulates message contributions. The second is a simple recurrent extension to Fesser and Weber [16], where the statistics of the curvature around each node are used as additional features at each recurrent step. Our empirical findings reveal that naively injecting curvature into GNN- based solvers sometimes leads to improved performance, but it does not always provide clear advantages, as seen in Table 3. Furthermore, we also experimented with a contemporary use of both variations. The training protocol and experimental settings are kept identical to previous experiments. The results highlight a subtle but important point: while curvature exposes structural bottlenecks, effective GNN solvers must also learn how to properly use geometric information. A major weakness of both methods is that random \\(k\\) - SAT problems have a lot of regularity, in the sense that the clause partitions will have very similar curvature statistics and thus the learning signal becomes redundant. Nevertheless, we believe that both these implementations provide interesting starting points for future work and research.\n\n<center>(a) Average graph Ricci curvature lower bound (Equation 8) as a function of \\(k\\) and \\(\\alpha\\) . </center> <center>(b) Average graph Ollivier-Ricci curvature as a function of \\(k\\) and \\(\\alpha\\) . </center> Figure 4: Contour plots of the average graph Ricci curvature lower bound (a) and Ollivier- Ricci curvature (b) displaying the changes in average curvature as a function of \\(k\\) and \\(\\alpha\\) . Both quantities behave similarly, especially as \\(\\alpha \\to 0\\) . We can see that in the case of the ORC, the stronger correction towards positive curvature due to the presence of cycles as \\(\\alpha \\to \\infty\\) is in line with the general theory related to this discretization of the Ricci curvature [5]. Notice the difference between the contours of (b) and Figure 1b for large \\(\\alpha\\) and \\(k\\) . Best viewed in color. <center>Figure 5: Visualization of an easy, in terms of clause density \\(\\alpha\\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very small number of long-range interactions, as can clearly be seen in (b). Furthermore, for such very small \\(\\alpha\\) , the average BFC approaches 0, in line with the developed theory. </center> ## A.5 Additional plots In this section we provide additional and miscellaneous plot that help with visualizing various concepts explained in the main manuscript, such as the relationship between curvature and hardness, as well as visualizations of easy and hard problems. More details can be found in Figures 4, 5, 6 and 7.\n\ncompressed into finite- dimensional embeddings. This bottleneck severely restricts the ability of GNNs to model long- range dependencies, and can be thought of as a vanishing gradient problem. Given that the difficulty of SAT can actually be understood intuitively as being proportional to the number of long- range dependencies between nodes, we question whether learning GNN- based solvers on these difficult problems is impacted by oversquashing. Recent geometric analyses propose that oversquashing is directly tied to negative Ricci Curvature (RC) of the underlying graph [38, 47]. These insights invite a fundamental question: Can graph RC serve as a predictive and constructive notion in unraveling novel hardness concepts for GNN- based SAT solvers? In this work, we address this question by studying the typical- case behavior of particular instantiations of graph RC on random \\(k\\) - SAT problems represented as bipartite graphs. Our starting point is the observation that edges of bipartite graphs are always non- positively curved. We show that, on average, the edges become more negatively curved as problems get harder, and less negatively curved as they become easier. Finally, we derive an exact expression for the average Balanced Forman Curvature (BFC) in the limit of unsolvable problems, from which we derive a connection between curvature and oversquashing in GNNs- based SAT solvers, following the theory of Topping et al. [47]. This is, to the best of our knowledge, the first successful attempt at a theoretical characterization of the limitations of GNN- based SAT solvers. To validate our theory, we perform experiments on random 3- and 4- SAT problems, and also on a vast array of datasets coming from the recent benchmark of Li et al. [28]. Firstly, we observe a phase transition- like phenomenon in random 3- SAT solving probability as a function of the mean and variance of the curvature. We further affirm the aforementioned limitations by rewiring only the testing graphs of the benchmarks at test- time to increase their average BFC, and show that these rewired problems become much easier to solve. Finally, we find that heuristics based on the BFC of a dataset correlate extremely well with generalization error, unlike the average clause density, which is typically used to characterize the hardness of a single instance. Overall, our findings suggest that GNN- based SAT solvers have two distinct types of hardness: the hardness of learning representations in negatively curved structures, followed by the well- established algorithmic hardness of SAT. We conclude by relating our findings with modeling principles and design choices of existing approaches. Outline. The remainder of this article is structured as follows: In Section 2, we provide a recap of the most relevant concepts discussed in the paper: random \\(k\\) - SAT, GNNs, and graph RC. Section 3.1 presents the main theoretical results, including an in- depth discussion on how oversquashing affects GNNs- based solvers. We then demonstrate the experimental evidence in Section 4. Finally, we provide a discussion regarding current design principles and future work in Section 5. ## 2 Background This section is only intended to formally introduce the objects studied in the paper and render the material self- contained. We kindly ask the reader to tolerate the occasional whirlwind and abuse of notation, as it will be formalized later in Section 3. ### 2.1 Random \\(k\\) Boolean Satisfiability Problem The random \\(k\\) - SAT (assignment) problem, which is the central object of study in this work, is made up of \\(N\\) variables \\(\\{x_{i}\\}_{i = 1}^{N}\\) that can take binary values \\(x_{i} \\in \\{0,1\\}\\) . Using these variables, one constructs \\(M\\) clauses containing a disjunction of \\(k\\) variables or their negations (called literals). For example, a 3- SAT problem would have clauses of the form \\((x_{i} \\vee \\neg x_{j} \\vee x_{h})\\) . The goal is to assign a value to all literals such that they satisfy the conjunction of all clauses. This logical formula is called a Conjunctive Normal Form (CNF). In the random formulation, it is possible to identify different phases of problem hardness based on a parameter called the clause density \\(\\alpha = M / N\\) . This phenomenon has been actively researched in statistical physics due to the analogies between random \\(k\\) - SAT and spin- glass models [33, 34]. Notable results[26, 37, 50] include the discovery of two transitions for typical instances at a given \\(k\\) , based on the value of \\(\\alpha\\) in the thermodynamic limit \\((M, N \\to \\infty)\\) : As \\(\\alpha\\) increases, the measure over the space of possible solutions first decomposes into an exponential number of clusters at the dynamical transition \\(\\alpha_{d}(k)\\) and subsequently condensates over the largest such states at the critical transition \\(\\alpha_{c}(k)\\) . These phase transitions naturally affect the performances of many algorithms, e.g., when \\(k \\geq 4\\) , going beyond \\(\\alpha_{c}\\) is almost impossible with existing algorithms.\n\n<center>Figure 6: Visualization of a hard, in terms of clause density \\(\\alpha\\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very large number of long-range interactions, as can clearly be seen in (b). Furthermore, for such large \\(\\alpha\\) , the average BFC is strongly negative, in line with the developed theory. </center> <center>Figure 7: Average BFC as a function of \\(\\alpha\\) for random 3 and 4 -SAT problems with \\(N = 256\\) . The size of the blobs is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model [46]. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c}\\) [32]. The average BFC drops monotonically with \\(\\alpha\\) in both cases. For 3-SAT (a), this drop appears to be almost linear, with a substantial amount of variance, as also depicted in Figure 2. 4-SAT on the other hand contains problems where the curvature is substantially more negative and concentrated, which when coupled with the larger number of constraints leads to oversquashing, as discussed in our theory. This can be clearly seen by the fast performance drop-off in (b), which happens much earlier than \\(\\alpha_{c}\\) , indicating the additional hardness phase related to representation learning discussed in the main paper. </center>\n\n<center>Figure 8: Analogue of Figure 2a for 4-SAT. Average BFC as a function of \\(\\alpha\\) for random 4-SAT problems with \\(N = 256\\) . The color is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model, with a group labeled as solvable if \\(50\\%\\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c} \\approx 9.931\\) [32]. The average BFC is negative and drops with \\(\\alpha\\) , but for a small amount. The plot in the top-right corner shows the model's solution probability curve as a function of \\(\\alpha\\) , where it is possible to notice that the GNN-based solver has severe limitations on these problems. Differently from 3-SAT (Figure 2a), the average curvature is negative and concentrated even for problems that would be considered very simple in terms of \\(\\alpha\\) ( \\(\\alpha \\rightarrow 8\\) ). This is due to the larger value of \\(k\\) , the higher number of long-range interactions, and the connection of both factors with oversquashing, as discussed and predicted by our contributions. </center>\n\n### 2.2 Graph Neural Networks GNNs are a subclass of Neural Networks (NNs) that can learn a representation of graph data by locally aggregating information[20, 45]. The main goal of the architecture is to implement inductive biases natural to graph data [9]. An example of such a property is learning graph- level functions invariant to the nodes' ordering. Consider, for simplicity, an unweighted and undirected graph \\(G\\) with \\(N\\) nodes, represented by a symmetric binary adjacency matrix \\(A\\in \\{0,1\\}^{N\\times N}\\) . This setting can be easily extended to deal with more general connectivity structures [14]. By associating a node feature matrix \\(X\\in \\mathbb{R}^{N\\times d}\\) to the graph, we can describe a GNNs as a convolution of the graph signal with \\(A\\) as the shift operator. A generalization of this concept can be obtained by considering the message- passing framework [20]: \\[x_{i}^{(k)} = \\theta^{(k)}\\left(x_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\phi^{(k)}\\left(x_{i}^{(k - 1)},x_{j}^{(k - 1)},e_{j i}\\right)\\right), \\quad (1)\\] where \\(x_{i}^{(k)}\\) denotes node features of node \\(x_{i}\\) at layer \\(k\\) \\(e_{j i}\\) the (optional) edge features from node \\(j\\) to node \\(i\\) \\(\\mathcal{N}(\\cdot)\\) the set of (1- hop) neighbor nodes, \\(\\bigoplus\\) a differentiable, permutation invariant function, (e.g., sum, mean), and \\(\\phi ,\\theta\\) denote differentiable and (optionally) nonlinear functions such as Multi- Layer Perceptrons (MLPs). A CNF formula can be easily translated into a bipartite graph [6], which can then be fed into a GNN- based solver. The particular bipartition we consider in this work is detailed later in Section 3. The application of the above message- passing scheme for SAT problems can be done by applying Equation1 to the clause and literal partitions [28]. Let \\(i\\) be a literal node and \\(j\\) be a clause node, then: \\[\\begin{array}{r l} & {h_{j}^{(k)} = \\theta_{c}^{(k)}\\left(h_{j}^{(k - 1)},\\bigoplus_{i\\in \\mathcal{N}(j)}\\left(\\left\\{\\phi_{i}^{(k)}\\left(h_{i}^{(k - 1)}\\right)\\right\\}\\right)\\right),}\\\\ & {h_{i}^{(k)} = \\theta_{l}^{(k)}\\left(h_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\left(\\left\\{\\phi_{c}^{(k)}\\left(h_{j}^{(k - 1)}\\right)\\right\\}\\right)\\right),} \\end{array} \\quad (2)\\] where the subscripts \\(c\\) and \\(l\\) refer to NNs specialized on the clause and literal partitions respectively. ### 2.3 Ricci Curvature of Graphs In Riemannian geometry, RC quantifies the local deviation of a manifold \\(\\mathcal{M}\\) from flat Euclidean space, as a result of the metric defined on \\(\\mathcal{M}\\) . Intuitively, RC captures how the neighborhoods of two adjacent points relate when moving from one point to the other. On smooth manifolds, it compares how a small ball of mass around a point is distorted when transported along a geodesic to a neighboring point. Extending this notion to more general structures, such as metric spaces or combinatorial complexes, has been an extremely active area of mathematical research, with the works of Ollivier [39] and Forman [18] standing out. In the case of graphs, we ask ourselves how local connectivity either concentrates or disperses. Ollivier [39] implements this idea by comparing probability mass on local neighborhoods, i.e., a random walk distribution on the endpoints of an edge. Given these two distributions, one can compare the ratio between their Wasserstein and shortest path distance, serving as a direct and discrete analogue of geodesic transport. See Appendix A.1.1 for more details. The definition of Forman [18] relies heavily on topology, and thus it takes a combinatorial form. Essentially, given a cell complex, the curvature of a p- cell depends only on the topological structures between the cell and its neighbors. This makes Forman- Ricci Curvature (FC) simpler to compute numerically, since it can avoid the optimization of the optimal transport problem that arises in Ollivier- Ricci Curvature (OC) curvature. Given that RC is directly related to the structure of local neighborhoods, it has emerged as a powerful way of theoretically analyzing limitations of GNNs. In a seminal paper, Topping et al. [47] provide both a balanced version of the FC curvature and show that the oversquashing problem [1] can be directly connected to edges with high negative curvature. This definition, namely the BFC, is central to this paper, therefore please consult Appendix A.1.2 for the definition and additional details. Nguyen et al. [38] have shown that similar results can be derived using the OC curvature. It is worth noting that these notions of curvature are naturally correlated with one another, as shown empirically in a multitude of complex networks by Samal et al. [44].\n\n## 3 Curvature of Random k-SAT Problems and Its Relationship with GNNs Setting and Notation. We consider random \\(k\\) - SAT problems with \\(N\\) variables and \\(M\\) clauses, with \\(\\alpha = M / N\\) and \\(k,N,M\\in \\mathbb{N}\\) . These problems are represented through a simple bipartite graph \\(G = (V,E)\\) , where the node set is a literal- clause bipartition \\(V = L\\cup C\\) , with \\(L\\cap C = \\emptyset\\) and \\(|L| = 2N,|C| = M\\) . The edge set takes the form \\(E = \\{(i,j)\\in V\\times V:i\\sim j,i\\in L,j\\in C\\}\\) where \\(i\\sim j\\) indicates a connection between nodes. Given \\(v\\in V\\) , we denote its degree by \\(d_{v}\\) . Finally, we denote the expected value of the random variable \\(X\\) with probability distribution \\(\\mathrm{P}\\) by \\(\\mathbb{E}_{P}[X]\\) and \\(\\mathbb{E}_{p\\sim P}[X]\\) the expectation over samples drawn from \\(P\\) . Unless noted otherwise, when we refer to the expected value in simulations, we imply its estimate via the sample mean statistic. Data Model. Our bipartite formulation is a simplification of the input graphs considered in many GNN- based solvers [40, 46]. Recent literature [28] refers to this data structure as a Literal- Clause Graph (LCG). Following an Erd\u0151s\u2013R\u00e9nyi- like procedure, each clause is assigned \\(k\\) literals independently at random with probability \\(p\\) . Assuming that all literals are equally likely to appear in a given clause, we obtain the following degree distributions: \\[\\begin{array}{c}{P(d_{j} = h) = \\delta (h - k)}\\\\ {P(d_{i} = h) = \\binom{M}{h} p^{h}(1 - p)^{M - h},} \\end{array} \\quad (3)\\] where \\(\\delta (\\cdot)\\) represents the Dirac delta function, \\(\\binom{\\cdot}{}\\) the binomial coefficient, and \\(p\\coloneqq \\frac{k}{2N}\\) . In the limit \\(M,N\\to \\infty\\) , we can approximate the Binomial form of the literal degree distribution with a Poisson distribution: \\[P(d_{i} = h) = \\frac{\\lambda^{h}e^{-\\lambda}}{h!}, \\quad (5)\\] with \\(\\lambda = M p = \\textstyle {\\frac{1}{2}}\\alpha k\\) . We will be interested in using this approximation to calculate properties of the graph RC, which is an edge level property, therefore the case \\(d_{i} = 0\\) should be truncated. This fact leads us to propose an alternative probability mass function based on zero- truncated Poisson distribution the for the literal degrees: \\[P^{*}(d_{i} = h) = P(d_{i} = h:h\\geq 1) = \\frac{P(d_{i} = h)}{1 - P(d_{i} = 0)} = \\frac{\\lambda^{h}}{h!(e^{\\lambda} - 1)}. \\quad (6)\\] Characterizing Average Curvature. We will now proceed by showing that the above bipartite representation of SAT problems has significant implications on the average RC. Most importantly, we will precisely characterize the average behavior of a specific definition curvature, namely the BFC, which in turn has strong implications on oversquashing in GNNs. The proofs of all statements are deferred to Appendix A.2. We start from an established lower bound of the OC. Definition 3.1 (OC lower bound in bipartite graphs [5, 23]). For any edge \\(i\\sim j\\) in a simple, unweighted bipartite graph \\(G\\) , we have that that: \\[\\underline{{\\kappa}}(i,j) = -2\\cdot \\max \\{0,1 - \\frac{1}{d_{i}} -\\frac{1}{d_{j}}\\} \\leq \\kappa (i,j)\\leq 0, \\quad (7)\\] where \\(\\kappa\\) is the OC. The above lower bound can be redefined by alternatively stating the condition encoded inside the maximum function in Equation 7: \\[\\underline{{R}}(i,j) = \\left\\{ \\begin{array}{l l}{0} & {\\mathrm{if~min}\\{d_{i},d_{j}\\} = 1}\\\\ {\\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2} & {\\mathrm{otherwise}} \\end{array} \\right. \\quad (8)\\] This alternative definition shows that Equation 8 is also a lower bound for the BFC s.t., \\(\\underline{{R}}(i,j)\\leq\\) \\(R i c(i,j)\\) . This statement is a direct consequence of the definition of the BFC, and a proof is given inside the proof of Theorem 3.1. Furthermore, we also have that \\(- 2< R i c(i,j)\\leq \\kappa (i,j)\\leq 0\\) , which is a direct consequence of the facts that \\(\\kappa (i,j)\\geq R i c(i,j)\\) [47] and Equation 7. Starting from these statements, we can proceed to show that the average behavior of the graph RC is naturally dictated by average degree of the literals, given that \\(d_{j} = k\\) always holds. The literal degree distribution is a zero- truncated Poisson distribution, from which the average degree of a literal \\(i\\) can be calculated as: \\[\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} \\quad (9)\\]\n\nProposition 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\to 0\\) and \\(\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]\\to 0\\) as \\(\\alpha = \\frac{M}{N}\\to 0\\) . Proposition 3.1 tells us that as problems get easier, i.e., they are in the satisfiable phase, their graph representations get (Ricci) flatter. This statement implies a relationship between very simple problems and their bipartite topology, which is that each clause is made of distinct literals. In this scenario, producing a satisfying assignment becomes trivial since assigning truth values to these unique literals does not affect other clauses. This implication aligns well with common knowledge, therefore the next thing to check is whether there is an opposite behavior when dealing with very difficult problems. This later case is important because it is close to the unsatisfiable phase where the faults of existing algorithms emerge [2]. Through a similar argument as before, we can formalize this intuition and characterize the average lower bound in the case when problems are surely unsatisfiable. Proposition 3.2. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) . In this scenario, the average lower bound of the graph RC will tend to be maximally negative, therefore we cannot directly speak on the average curvature by sandwiching as before. Nevertheless, Proposition 3.2 provides an extremely interesting insight into the interplay between \\(\\alpha\\) and \\(k\\) . As the problems get harder, the number of constraints (i.e., \\(k\\) ) becomes a decisive factor on the curvature. A larger value of \\(k\\) implies more constraints and thus many long range interactions between the literals, but it also implies that edges will become more negatively curved. In turn, this implies the existence of bottlenecks in the graph, which makes long range communication becomes difficult [47]. While it is not possible to connect the exact graph OC to this result, it is possible to extend it to the average graph BFC: Theorem 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. The BFC at \\(i\\sim j\\) is bounded from above by the quantity: \\[\\bar{R} (i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\max \\{d_{i},d_{j}\\}}. \\quad (10)\\] Furthermore, as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) , \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) and therefore the average BFC over the edges of \\(G\\) converges to \\(\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]\\to \\frac{2}{k} - 2\\) . Theorem 3.1 contains a crucial result in its characterization of the typical- case BFC, which is that it is connected to both \\(\\alpha\\) and \\(k\\) . While the connection to \\(\\alpha\\) might seem obvious, the one with \\(k\\) is quite insightful. One can observe that as problems become harder to satisfy (large values of \\(\\alpha\\) ), \\(k\\) plays an important role in how negatively curved the graph edges can become. This phenomenon can be seen in Figure 1, where for smaller \\(k\\) , larger values of \\(\\alpha\\) are required to have highly negatively curved edges. This latter point is crucial in developing a more profound understanding of the limitations of GNN- based solvers, and we will expand upon it in the following subsection. Another interesting consequence of our results, is that due to their shared lower bound and the results in Propositions 3.1 and 3.2, we confirm a tight link between graph OC and BFC. In general, the correlation between graph OC and FC has been studied in great detail on an array of complex networks by Samal et al. [44]. In our case, we confirm that both OC and BFC discretizations behave identically for simpler problems and similarly in relation to \\(\\alpha\\) and \\(k\\) , a phenomenon that can also be verified empirically through Figures 1 and 4 (Appendix). ### 3.1 Message-Passing Bottlenecks and Downstream Performance The results presented in the previous section allow us to proceed with a principled way of understanding performance limitations in GNN- based SAT solvers. This is due to the direct connection between the result in Theorem 3.1 to Theorem 4 of Topping et al. [47], which establishes that \"edges with high negative curvature are those causing the graph bottleneck and thus leading to the oversquashing phenomenon\". This seminal result states that if the gradients of the message passing functions ( \\(\\theta\\) and \\(\\phi\\) in Equation 1) are bounded, and there exists a sufficiently negatively curved edge compared to the degrees of its endpoints, then the derivative of the learned node representations around that edge vanishes. Intuitively, this can be understood as a difficulty of propagating the information in nodes at a reachable distance due to the fact that the graph structure limits the pathways where\n\n<center>Figure 1: Average graph Ricci Curvature lower bound (a) and Balanced Forman Curvature (b) as a function of \\(\\alpha\\) and \\(k\\) . Both quantities behave very similarly, both in terms of the smooth transition from flat to negative curvature and their magnitude, especially as \\(\\alpha \\to 0\\) and \\(\\alpha \\to \\infty\\) , in line with the developed theory. An alternative version of this figure displaying the average graph OC in (b) is presented in Figure 4 of the Appendix. Best viewed in color. </center> information can travel. Simply stated, nodes in different neighborhoods need to pass all messages through the same edge(s), leading to a difficulty in learning fixed- length representations that can hold information on long range correlations. Formally, for large values of \\(k\\) , as the clause density \\(\\alpha \\to \\infty\\) , any infinitesimally small value \\(\\delta > 0\\) could be used in Theorem 4 of Topping et al. [47] such that \\(Ric(i,j) \\leq - 2 + \\delta\\) , leading to an exponentially decaying Jacobian of the node representations around \\(i \\sim j\\) . This result leads us to the conclusion that GNN- based solvers are limited by both these parameters and suffer from two distinct hardness types: the algorithmic hardness inherent to SAT and the hardness of learning representations for long range communication. The interplay between \\(k\\) and \\(\\alpha\\) in Theorem 3.1 provides additional insights. Indeed, for problems with large values of \\(k\\) or large values of \\(\\alpha\\) , highly negatively curved edges are guaranteed to exist on average, and this quantity will concentrate. On the other hand, for large values of \\(\\alpha\\) and relatively small values of \\(k\\) , the latter becomes crucial in deciding how well a GNN- based solver will be able to learn, i.e., it should be easier to learn a solver for smaller values of \\(k\\) . We confirm this fact empirically in Section 4. The intuition we provide for a more complete understanding of this crucial result is the following: At increasing connectivity, literals become very distant on the interaction network, i.e., the number of long- range codependencies increases. In this scenario, the GNN will not be able to learn a fixed length representation that can \"remember\" the information of reachable, but not directly adjacent nodes. This means that the ability to learn a solver is compromised by an oversquashing phenomenon. For large values of \\(k\\) , this problem becomes prevalent even before the hardness of exploring the solution space, due to the effect of \\(k\\) on the BFC. Our theory motivates therefore how increasing values of \\(k\\) in random \\(k\\) - SAT would lead to worse oversquashing and performance, even for what would be considered simple problems in terms of \\(\\alpha\\) . To visually understand the aforementioned concepts, we can again refer to Figure 1. As the value of \\(k\\) grows, the gap between the flatter (yellow) and highly negatively curved problems (violet) gets smaller. The same holds for increasing values of \\(\\alpha\\) , as expected. We provide additional visual depictions of this aforementioned explanation in Appendix A.5, where we plot two input graphs for random 3- SAT at small (Figure 5) and large (Figure 6) \\(\\alpha\\) . In the following section, we will show that our results can be empirically confirmed for different GNN- based solvers. ## 4 Experiments Experimental Setting. To validate our theory, we perform different experiments on various datasets, the details of which will be provided in the following subsections. The experiments consist in firstly exploring the behavior of a GNN- based solvers and relating it to the input graph BFC. Based on these\n\n<center>Figure 2: (a) Average BFC as a function of \\(\\alpha\\) for random 3-SAT problems with \\(N = 256\\) . The color is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model [46], with a group labeled as solvable if \\(50\\%\\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c} \\approx 4.267\\) [32]. The average BFC drops monotonically with \\(\\alpha\\) . The small plot in the bottom-left corner provides the model's solution probability curve in terms of \\(\\alpha\\) , where it is possible to clearly notice the algorithmic transition from satisfiable to unsatisfiable problems. (b) Probability of finding a satisfying assignment of the same problems as in (a) with Neuro SAT as a function of the variance and average of the BFC. Notice how as \\(\\alpha\\) grows, the average curvature not only gets more negative, but also concentrates. We can see from the empirical results in (a) that in this case, the model is unable to produce a satisfying assignment. As \\(\\alpha\\) becomes smaller, the input graphs have less negative edges on average, the associated variance naturally grows, and so does the solving probability. Using the first two moments of the BFC, we are able to observe a similar transition-like phenomenon as the small plot in the bottom-left corner of (a). </center> results, we then propose two heuristics to understand how hard a given SAT dataset will be to solve for a GNN- based solver. We will focus on the assignment scenario, as it includes the decision scenario as well. Given that all the datasets we utilize do not come with node features, we make use of learned embeddings in order to effectively explore oversquashing implications. The GNN- based solvers are implemented following the design of Li et al. [28], using Py Torch [41], Py Torch- Geometric [17] and Py Torch Lightning [15]. The networks were trained for 100 epochs using the Adam W optimizer [29], with learning rate \\(\\eta = 0.0001\\) decaying by half after 50 epochs and the gradients clipped at unit norm. The training was done on NVIDIA Titan RTX GPUs. ### 4.1 The Relationship Between Curvature and Satisfiability We start by using numerical simulations to verify the theoretical claims made in Section 3.1. To do this, we generate random 3- SAT instances in Conjunctive Normal Form (CNF) using a custom implementation in the Julia language [4]. We generate problems with \\(\\alpha \\in [3,5]\\) in steps of \\(\\Delta \\alpha = 0.1\\) , capturing both satisfiable and unsatisfiable regimes around the known critical threshold \\(\\alpha_{c} \\approx 4.267\\) . Considering its widespread use and downstream performance, we train the Neuro SAT model [46] to produce a satisfying assignment, while scaling the number of message passing iterations by \\(2N\\) during evaluation, to maximize inference accuracy. We then analyze the performance of the model on problems with \\(N = 256\\) , with the results being summarized in Figure 2. Our results show that by considering the probability of finding a solution at a given \\(\\alpha\\) as a function of the first and second moments of the curvature, we can replicate a SAT/UNSAT phase- transition (Figure 2b). This result presents an important step forward in theoretically understanding the performance of GNN- based solvers.[32]. Similar results hold for random 4- SAT, and can be seen in Figures 7 and 8 in the Appendix. For this higher value of \\(k\\) , we have more negatively curved edges which strongly impact the performance of GNN- based solvers, as will further confirmed in Section 4.2. An interesting observation is that for random 3- SAT, the curvature starts to become highly negative and concentrate close to the estimated dynamical threshold \\(\\alpha_{d} \\approx 3.927\\) [32].\n\nTest- time Rewiring. In order to obtain additional evidence about the previous observations, we put ourselves in a unique scenario: Suppose a GNN- based solver is trained on a dataset of SAT problems, and later tested on a separate testing partition. If we render the said testing partition less curved, would the model perform better without needing to retrain? The purpose behind this experiment is to gain a deeper understanding of the relationship between curvature and problem complexity. For this purpose, we use four different SAT benchmarks proposed by Li et al. [28]: Random 3 and 4 - SAT generated near the (respective) critical threshold \\(\\alpha_{c}\\) , a random \\(k\\) - SAT dataset consisting of mixed \\(k\\) values (SR), and one that mimics the modularity and community structure of industrial problems (CA). The last two datasets are better representatives of real- world problems. We train both a Graph Convolutional Network (GCN)- solver [25] and Neuro SAT on training partitions using the same protocol as before, while the testing partition is rewired using a stochastic discrete Ricci flow procedure, similarly to [47]. The idea behind the rewiring procedure is quite simple: we make the input graph less curved by stochastically deleting edges that have the highest negative curvature, while adding new edges that are less curved. We provide a more detailed explanation of this process, including a schematic algorithm in Appendix A.3. The results are reported in Table 1, where it be observed that that the rewired problems become simpler to solve for both solvers at test- time. A noteworthy observation is that a large improvement happens on 4- SAT problems, while the modular CA dataset reports small improvements. In the following subsection, we make a direct connection of this result with our theory. ### 4.2 A New Hardness Heuristic for GNN-Based Solvers Based on the developed theory and the above observations, we provide, as a practical contribution, two different heuristics that reflect how hard it will be for a GNN- based solver to tackle a dataset. The main motivation behind these heuristics is that if we simply look at the average clause density of a dataset, we can miss out on direct implications of oversquashing. An example of this is the first dataset we presented for the random 3- SAT experiments discussed in Figure 2, which is build at increasing values of \\(\\alpha\\) . Given an input graph \\(G\\) , we define the heuristics as: \\[\\omega (G) = -\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]*\\mathbb{E}[\\alpha ],\\quad \\omega^{*}(G) = \\frac{\\omega(G)}{\\mathbb{V}_{(i\\sim j)}[Ric(i,j)]}, \\quad (11)\\] with the expectations being taken over the edges \\(G\\) . The averages of both heuristics, which we denote by \\(\\bar{\\omega}\\) and \\(\\bar{\\omega}^{*}\\) can then be used to judge the hardness of a given dataset. Intuitively, we provide two non- negative numbers that reveal how dense and curved \\(G\\) is on average \\((\\omega (g))\\) and how much this quantity concentrates \\((\\omega^{*}(G))\\) . Our theoretical insights tell us that these quantities should provide information into the hardness of learning a GNN- based solver. We report the generalization error (1 - testing accuracy) of Neuro SAT on the four previously mentioned benchmarks in Section 4.1, alongside the heuristics in Table 2. A (linear) correlation analysis between the error and the (normalized) heuristics reveals that our curvature- based approach serves as a better predictor of generalization: the respective correlation coefficients are \\(\\rho_{\\bar{\\alpha}} = 0.32\\) , \\(\\rho_{\\bar{\\omega}} = 0.86\\) and \\(\\rho_{\\bar{\\omega}^{*}} = 0.98\\) . These results allow us to formally motivate the performance gains during the test- time rewiring procedure discussed previously. What we observe, is that due to its community structure, the CA dataset has a large clause density, but its average curvature is much lower than that of random 4- SAT problems. This is natural, since a community structure is inherently linked with edges that act as less important bottlenecks for message passing [38]. These results show that the ability of GNN- based solvers to learn representations that can learn long range correlations and generalize well is deeply connected with the curvature of the input data, as discussed thoughtful the paper. ## 5 Conclusions Practical Takeaways and Future Work. In this paper, we have shown that the accuracy of GNN- based SAT solvers is directly related to the input data structure. This relationship is universally prevalent across all machine learning applications and as a result we have different modeling principles for different data. For SAT problems, we have identified that the geometry of the input data is a plausible cause of deficiency, due to its connections with oversquashing. What is extremely fascinating is that most modern GNN- based SAT solvers implement some type of recurrence mechanism [28, 35, 40, 46], and this architectural component has been recently shown to be a great starting point to mitigate oversquashing [3]. The implicit effect of recurrence can be immediately noted by comparing the drop in performance between the GCN and Neuro SAT solvers in Table 1.\n\nTable 1: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers. By increasing the testing set's curvature at test-time through rewiring, both solvers are able to make big leaps in accuracy, especially in more difficult problems. Reducing the curvature of the problem facilitates long range communication and renders problems easier. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface and absolute improvements in parentheses. <table><tr><td>Model</td><td>Variation</td><td>3-SAT</td><td>4-SAT</td><td>Datasets</td><td>SR</td><td>CA</td></tr><tr><td rowspan=\"2\">GCN</td><td>No Rewiring</td><td>0.510 \u00b1 0.012</td><td>0.180 \u00b1 0.048</td><td>0.470 \u00b1 0.031</td><td>0.650 \u00b1 0.016</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.626 \u00b1 0.021 (+0.116)</td><td>0.374 \u00b1 0.045 (+0.194)</td><td>0.696 \u00b1 0.035 (+0.226)</td><td>0.670 \u00b1 0.048 (+0.020)</td><td></td></tr><tr><td rowspan=\"2\">Neuro SAT</td><td>No Rewiring</td><td>0.690 \u00b1 0.022</td><td>0.436 \u00b1 0.032</td><td>0.734 \u00b1 0.017</td><td>0.746 \u00b1 0.018</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.820 \u00b1 0.030 (+0.130)</td><td>0.686 \u00b1 0.029 (+0.250)</td><td>0.902 \u00b1 0.004 (+0.168)</td><td>0.828 \u00b1 0.029 (+0.082)</td><td></td></tr></table> Table 2: Average generalization error (1- testing accuracy) over 5 different runs with the Neuro SAT model on SAT benchmark datasets [28]. The error is reported alongside the average clause density \\(\\bar{\\alpha}\\) and the curvature-based heuristics \\(\\bar{\\omega}\\) and \\(\\bar{\\omega}^{*}\\) . Our heuristics display very strong linear correlation with the generalization error, unlike the average clause density, making it possible to predict how hard each benchmark will be for a GNN-based solver. <table><tr><td rowspan=\"2\">Problem</td><td rowspan=\"2\">Generalization Error</td><td colspan=\"3\">Hardness Heuristic</td></tr><tr><td>\u03b1</td><td>\u03c9</td><td>\u03c9*</td></tr><tr><td>3-SAT</td><td>0.31</td><td>4.59</td><td>4.12</td><td>97.41</td></tr><tr><td>4-SAT</td><td>0.56</td><td>9.08</td><td>9.81</td><td>612.32</td></tr><tr><td>SR</td><td>0.27</td><td>6.09</td><td>5.30</td><td>125.30</td></tr><tr><td>CA</td><td>0.25</td><td>9.73</td><td>6.30</td><td>123.27</td></tr></table> This fact leads us to conjecture that the relationship between input data and model performance is prevalent throughout Neural Combinatorial Optimization (NCO) [48], and different architectural designs are necessary for different problems. Furthermore, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. We report results for some straightforward curvature- aware solvers in Appendix A.4, which do not report consistent improvements. A direct avenue for future work that we consider promising in this field is the application of continuous graph diffusion dynamics for learning[11, 21], which generalize the recurrence mechanism. Finally, regarding the theoretical aspect, we believe that promising avenues for future work that were not directly addressed here are the characterization of both curvature and topological quantities in distribution and not just in the mean. Another direction that is interesting is making a connection between the various critical points of the clause density and the curvature, such that the well- known phase transitions of SAT can be directly related to novel geometric order parameters. Closing Remarks. In conclusion, our results highlight that the limitations of GNN- based SAT solvers cannot be fully understood without considering the geometric properties of the input. Our study presents, to the best of our knowledge, the first attempt at a theoretical understanding of these neural solvers, by establishing a direct connection between their negatively curved graph representations and oversquashing in GNNs. We provide empirical evidence of this connection and verify that it is prevalent on more constrained instances. Beyond SAT, we expect these insights to be valuable for other domains where graph representations of combinatorial problems are employed. Combinatorial Optimization (CO) provides an interesting venue to study the reasoning behavior of NNs, and we hope that this paper makes a case for such studies. In conclusion, we hope that bridging concepts from deep learning, geometry, and physics, will pave the way for principled advances in the design of neural solvers.",
      "level": 1,
      "line_start": 1,
      "line_end": 45
    },
    {
      "heading": "Introduction",
      "content": "",
      "level": 2,
      "line_start": 4,
      "line_end": 6
    },
    {
      "heading": "Acknowledgements Acknowledgements Geri Skenderi is funded by the European Union through the Next Generation EU - MIUR PRIN PNRR 2022 Grant P20229PBZR. The views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible. ## References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. 1, 3 [2] Maria Chiara Angelini and Federico Ricci- Tersenghi. Monte carlo algorithms are very effective in finding the largest independent set in sparse random graphs. Physical Review E, 100(1): 013302, 2019. 1, 5 [3] Alvaro Arroyo, Alessio Gravina, Benjamin Gutteridge, Federico Barbero, Claudio Gallicchio, Xiaowen Dong, Michael Bronstein, and Pierre Vandergheynst. On vanishing gradients, oversmoothing, and over- squashing in gnns: Bridging recurrent and graph learning. ar Xiv preprint ar Xiv:2502.10818, 2025. 8 [4] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing. SIAM review, 59(1):65- 98, 2017. URL https://doi.org/10.1137/141000671. 7 [5] Bhaswar B Bhattacharya and Sumit Mukherjee. Exact and asymptotic results on coarse ricci curvature of graphs. Discrete Mathematics, 338(1):23- 42, 2015. 4, 13, 15, 19 [6] Armin Biere, Marijn Heule, and Hans van Maaren. Handbook of satisfiability, volume 185. IOS press, 2009. 1, 3 [7] Lucas Bordeaux, Youssef Hamadi, and Lintao Zhang. Propositional satisfiability and constraint programming: A comparative survey. ACM Computing Surveys (CSUR), 38(4):12- es, 2006. 1 [8] A. Braunstein, M. Mezard, and R. Zecchina. Survey propagation: An algorithm for satisfiability. Random Structures & Algorithms, 27(2):201- 226, 2005. doi: https://doi.org/10.1002/rsa.20057. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.20057. 1 [9] Michael M Bronstein, Joan Bruna, Yann Le Cun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18- 42, 2017. 3 [10] Quentin Cappart, Didier Chetelat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Veli\u010dkovi\u0107. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):1- 61, 2023. 1 [11] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele Rossi. Grand: Graph neural diffusion. In International conference on machine learning, pages 1407- 1418. PMLR, 2021. 9 [12] Wenjing Chang and Wenlong Liu. Sat- gatv2: A dynamic attention- based graph neural network for solving boolean satisfiability problem. Electronics, 14(3):423, 2025. 1 [13] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, page 151- 158, New York, NY, USA, 1971. Association for Computing Machinery. ISBN 9781450374644. doi: 10.1145/800157.805047. URL https://doi.org/10.1145/800157.805047. 1 [14] Gabriele Corso, Hannes Stark, Stefanie Jegelka, Tommi Jaakkola, and Regina Barzilay. Graph neural networks. Nature Reviews Methods Primers, 4(1):17, 2024. 3 [15] William Falcon and The Py Torch Lightning team. Py Torch Lightning, March 2019. URL https://github.com/Lightning- AI/lightning. 7 [16] Lukas Fesser and Melanie Weber. Effective structural encodings via local curvature profiles. In International Conference on Learning Representations, 2024. 18 [17] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. ar Xiv preprint ar Xiv:1903.02428, 2019. 7",
      "content": "[18] Forman. Bochner's method for cell complexes and combinatorial ricci curvature. Discrete & Computational Geometry, 29:323- 374, 2003. 3, 14 [19] Karlis Freivalds and Sergejs Kozlovics. Denoising Diffusion for Sampling SAT Solutions, November 2022. URL http://arxiv.org/abs/2212.00121. ar Xiv:2212.00121 [cs]. 1 [20] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263- 1272. PMLR, 2017. 1, 3 [21] Andi Han, Dai Shi, Lequan Lin, and Junbin Gao. From continuous dynamics to graph neural networks: Neural diffusion and beyond. Transactions on Machine Learning Research, 2024. 9 [22] Masato Inagaki. Spectral convergence of graph laplacians with ricci curvature bounds and in non- collapsed ricci limit spaces. ar Xiv preprint ar Xiv:2506.07427, 2025. 13 [23] Jurgen Jost and Shiping Liu. Ollivier's ricci curvature, local clustering and curvature- dimension inequalities on graphs. Discrete & Computational Geometry, 51(2):300- 322, 2014. 4 [24] Richard M Karp. Reducibility among combinatorial problems. In Complexity of computer computations, pages 85- 103. Springer, 1972. 1 [25] TN Kipf. Semi- supervised classification with graph convolutional networks. ar Xiv preprint ar Xiv:1609.02907, 2016. 8 [26] Florent Krzakala, Andrea Montanari, Federico Ricci- Tersenghi, Guilhem Semerjian, and Lenka Zdeborov\u00e1. Gibbs states and the set of solutions of random constraint satisfaction problems. Proceedings of the National Academy of Sciences, 104(25):10318- 10323, 2007. 1, 2 [27] Qimai Li, Zhichao Han, and Xiao- Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. 1 [28] Zhaoyu Li, Jinpei Guo, and Xujie Si. G4sabench: Benchmarking and advancing sat solving with graph neural networks. Transactions on Machine Learning Research, 2024. 1, 2, 3, 4, 7, 8, 9, 18 [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 7 [30] Raffaele Marino, Giorgio Parisi, and Federico Ricci- Tersenghi. The backtracking survey propagation algorithm for solving random k- sat problems. Nature communications, 7(1):12996, 2016. 1 [31] Joao Marques- Silva. Practical applications of boolean satisfiability. In 2008 9th International Workshop on Discrete Event Systems, pages 74- 80, 2008. doi: 10.1109/WODES.2008.4605925. 1 [32] Stephan Mertens, Marc Mezard, and Riccardo Zecchina. Threshold values of random k- sat from the cavity method. Random Structures & Algorithms, 28(3):340- 373, 2006. doi: https://doi.org/10.1002/rsa.20090. 7, 20, 21 [33] Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009. 1, 2 [34] Marc Mezard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientific Publishing Company, 1987. 2 [35] David Moj\u017ei\u0161ek, Jan \u0171la, Ziwei Li, Ziyu Zhou, and Mikol\u00e1\u0161 Janota. Neural approaches to sat solving: Design choices and interpretability. ar Xiv preprint ar Xiv:2504.01173, 2025. 8 [36] R\u00e9mi Monasson and Riccardo Zecchina. Statistical mechanics of the random k- satisfiability model. Physical Review E, 56(2):1357, 1997. 1 [37] M. Mezard, G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812- 815, 2002. doi: 10.1126/science.1073287. URL https://www.science.org/doi/abs/10.1126/science.1073287. 2 [38] Khang Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, Nhat Ho, Stanley Osher, and Tan Minh Nguyen. Revisiting over- smoothing and over- squashing using ollivier- ricci curvature. In International Conference on Machine Learning, pages 25956- 25979. PMLR, 2023. 2, 3, 8\n\n[39] Yann Ollivier. Ricci curvature of markov chains on metric spaces. Journal of Functional Analysis, 256(3):810- 864, 2009. 3, 13 [40] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. Goal- Aware Neural SAT Solver. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1- 8, July 2022. doi: 10.1109/IJCNN55064.2022.9892733. URL http://arxiv.org/abs/2106.07162. ar Xiv:2106.07162 [cs]. 1, 4, 8 [41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high- performance deep learning library. Advances in neural information processing systems, 32, 2019. 7 [42] Daniel Paulin. Mixing and concentration by ricci curvature. Journal of Functional Analysis, 270(5):1623- 1662, 2016. 13 [43] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5- 6):355- 607, 2019. 13 [44] Areejit Samal, RP Sreejith, Jiao Gu, Shiping Liu, Emil Saucan, and J\u00fcrgen Jost. Comparative analysis of two discretizations of ricci curvature for complex networks. Scientific reports, 8(1): 8650, 2018. 3, 5, 14 [45] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61- 80, 2008. 1, 3 [46] Daniel Selsam, Matthew Lamm, Benedikt B\u00fcnz, Percy Liang, David L. Dill, and Leonardo De Moura. Learning a SAT solver from single- bit supervision. 7th International Conference on Learning Representations, ICLR 2019, pages 1- 11, 2019. ar Xiv: 1802.03685. 1, 4, 7, 8, 20 [47] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. Understanding over- squashing and bottlenecks on graphs via curvature. In International Conference on Learning Representations, 2022. 2, 3, 4, 5, 6, 8, 13, 14, 15, 17 [48] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth Mc Millan, and Risto Miikkulainen. Neuro Comb: Improving SAT Solving with Graph Neural Networks, June 2022. URL http://arxiv.org/abs/2110.14053. ar Xiv:2110.14053 [cs] version: 2. 9 [49] Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. Curvature graph network. In International Conference on Learning Representations, 2019. 18 [50] Lenka Zdeborov\u00e1. Statistical physics of hard optimization problems. Acta Physica Slovaca, 59 (3):169- 303, 2009. 1, 2",
      "level": 2,
      "line_start": 7,
      "line_end": 12
    },
    {
      "heading": "A Appendix In what follows, we provide supplementary material that complements the main manuscript. The appendix is organized as follows: 1. Appendix A.1 begins with background on graph Ricci curvature, where we review both the Ollivier and Balanced Forman discretizations in order to give the reader an intuitive and formal foundation for subsequent results. 2. In Appendix A.2, we provide detailed proofs of the propositions and theorems discussed in the manuscript. 3. Appendix A.3 describes the test-time graph rewiring procedure used in our experiments, including a full presentation of the stochastic curvature-guided algorithm. 4. In Appendix A.4, we outline preliminary ideas and implementations for curvature-aware solvers, along with empirical results that illustrate their potential. 5. Finally, Appendix A.5 contains additional plots and visualizations that further clarify the relationship between curvature, problem hardness, and solver behavior. ## A.1 Ricci curvature of graphs In this section, we provide, for the sake of completeness, both definitions and intuitive explanations of the two types of graph Ricci Curvature (RC) mentioned in this paper. The definitions are taken from Bhattacharya and Mukherjee [5] and Topping et al. [47] respectively, we simply report them here in a synthesized manner. We kindly refer the reader to the aforementioned works for more details. ## A.1.1 Ollivier Ricci curvature (OC) The formulation of Ollivier [39] aligns with the intuition from differential geometry: edges with negative curvature act as structural bottlenecks, separating dense regions and limiting smooth information propagation. Edges with positive curvature in the other hand facilitate smooth information propagation and are indicators of community structure. Graph Ollivier- Ricci Curvature (OC) is very well studied and has being linked to properties of graph Lapacians and mixing times of Markov Chain Monte Carlo (MCMC) methods [22, 42]. The intuitive idea of this discretization is to directly implement the idea from differential geometry: we use a ratio between the amount of \"mass\" moved around of an edge neighborhood with the shortest path distance, i.e., the graph geodesic. Let us formalize this concept. For two probability measures \\(\\mu_{1},\\mu_{2}\\) on a metric space \\((X,d)\\) , the the Wasserstein distance between them is defined as \\[W_{1}(\\mu_{1},\\mu_{2}) = \\inf_{\\nu \\in M(\\mu_{1},\\mu_{2})}\\int_{X\\times X}d(x,y)\\mathrm{d}\\nu (x,y), \\quad (12)\\] where \\(M(\\mu_{1},\\mu_{2})\\) is the collection of probability measures on \\(X\\times X\\) with marginals \\(\\mu_{1}\\) and \\(\\mu_{2}\\) . The Wasserstein distance is the result of the solution to a famous problem called Optimal Transport [43]. Intuitively, this distance measures the optimal cost to move one pile of sand to another one with the same mass. Let a metric measure space \\((X,d,m)\\) be a metric space \\((X,d)\\) , with a collection of probability measures \\(m = \\{m_{x}:x\\in X\\}\\) indexed by the points of \\(X\\) . The (coarse) Ricci curvature of a metric measure space is defined as follows: Definition A.1 (Ollivier [39]). On any metric measure space \\((X,d,m)\\) , for any two distinct points \\(x,y\\in X\\) , the (coarse) Ricci curvature of \\((X,d,m)\\) of \\((x,y)\\) is defined as: \\[\\kappa (x,y)\\coloneqq 1 - \\frac{W_{1}(m_{x},m_{y})}{d(x,y)} \\quad (13)\\] Extending this definition to graphs requires some additional steps. Consider a locally finite and possibly weighted simple graph \\(G = (V,E)\\) , where each edge \\((i,j)\\in E\\) is assigned a positive weight \\(w_{ij} = w_{ji}\\) . The graph is equipped with the standard shortest path graph distance \\(d_{G}\\) , that is, for \\(i,j\\in V\\) , \\(d_{G}(i,j)\\) is the length of the shortest path in \\(G\\) connecting nodes \\(i\\) and \\(j\\) . For \\(i\\in V\\)",
      "content": "define the degree \\(d_{i} \\coloneqq \\sum_{(i,j) \\in E} w_{ij}\\) and the neighborhood \\(\\mathcal{N}(i) \\coloneqq \\{j \\in V: (i,j) \\in E\\}\\) . For each \\(i \\in V\\) define a probability measure \\[m_{i}(j) = \\left\\{ \\begin{array}{ll} \\frac{w_{i,j}}{d_{i}}, & \\mathrm{if} j \\in \\mathcal{N}(i) \\\\ 0, & \\mathrm{otherwise.} \\end{array} \\right.\\] Note that these are just the transition probabilities of a weighted random walk on the vertices of \\(G\\) . If \\(m_{G} = \\{m_{i}: i \\in V\\}\\) , then considering the metric measure space \\(\\mathcal{M}_{G} \\coloneqq (V, d_{G}, m_{G})\\) , we can define the OC curvature for any edge \\((i,j) \\in E\\) as \\(\\kappa (\\mathbf{i},\\mathbf{j}) \\coloneqq \\mathbf{1} - \\mathbf{W}_{1}^{G}(\\mathbf{m}_{i}, \\mathbf{m}_{j})\\) , where \\(W_{1}^{G}(m_{i}, m_{j})\\) is obtained by discretizing Equation (12) on \\(\\mathcal{M}_{G}\\) : \\[W_{1}^{G}(m_{i}, m_{j}) = \\inf_{\\nu \\in \\mathcal{A}} \\sum_{z_{1} \\in \\mathcal{N}(i)} \\sum_{z_{2} \\in \\mathcal{N}(j)} \\nu (z_{1}, z_{2}) d(z_{1}, z_{2}). \\quad (14)\\] \\(\\mathcal{A}\\) denotes the set of all \\(d_{i} \\times d_{j}\\) matrices with entries indexed by \\(\\mathcal{N}(i) \\times \\mathcal{N}(j)\\) such that \\(\\nu (i', j') \\geq 0\\) , \\(\\sum_{z \\in \\mathcal{N}(j)} \\nu (i', z) = \\frac{w_{i,i'}}{d_{i}}\\) , and \\(\\sum_{z \\in \\mathcal{N}(i)} \\nu (z, j') = \\frac{w_{j,j'}}{d_{j}}\\) , for all \\(i' \\in \\mathcal{N}(i)\\) and \\(j' \\in \\mathcal{N}(j)\\) . For a matrix \\(\\nu \\in \\mathcal{A}\\) , \\(\\nu (i', j')\\) represents the mass moving from \\(i' \\in \\mathcal{N}(i)\\) to \\(j' \\in \\mathcal{N}(j)\\) . For this reason, the matrix \\(\\nu\\) is often called the transfer plan. ## A.1.2 Balanced Forman curvature (BFC) The Forman- Ricci Curvature (FC) is a discretization of Ricci curvature that holds for a broad class of topological objects, namely so called (regular) cellular (CW) complexes [44]. The original definition [18] is both extremely technical and out of the scope of this paper. Given that graphs edges are 1- dimensional cells (topologically), the definition simplifies greatly making use of very simple graph properties. Consider a simple, unweighted graph for simplicity, then the FC of edge \\((i,j)\\) is defined as \\(\\kappa_{G}^{f}(i,j) = 4 - d_{i} - d_{j}\\) . The main issue of this definition is that it ignores higher order correlations in terms of triangles and 4- cycles, which prove crucial to discriminate positively, flat, and negatively curved graphs. Inspired by these issues, Topping et al. [47] propose an extension of the FC dubbed Balanced Forman Curvature (BFC), such that it is both fast to compute and encodes accurate curvature information. Let us start by defining the sphere and ball of radius \\(r\\) centered at a node \\(i\\) of the graph by: \\[S_{r}(i) \\coloneqq \\{j \\in V: d_{G}(i,j) = r\\} , \\quad B_{r}(i) \\coloneqq \\{j \\in V: d_{G}(i,j) \\leq r\\} . \\quad (15)\\] The BFC is then defined using three combinatorial components: (i) \\(\\sharp_{\\Delta}(i,j)\\) , the number of triangles based at the edge \\(i \\sim j\\) . (ii) \\(\\sharp_{\\square}^{i}(i,j) \\coloneqq\\) , the number of nodes \\(k \\in S_{1}(i)\\) forming a 4- cycle based at \\(i \\sim j\\) without diagonals inside. (iii) \\(Q_{i}(j) \\coloneqq S_{1}(i) \\setminus (\\{j\\} \\cup \\sharp_{\\Delta}(i,j) \\cup \\sharp_{\\square}^{i}(i,j))\\) , simply the complement of the neighbours of \\(i\\) with respect to the sets introduced in (i) and (ii) once we also exclude \\(j\\) . Definition A.2 (Topping et al. [47]). For any edge \\(i \\sim j\\) in a simple, unweighted graph \\(G = (V, E)\\) with adjacency matrix \\(A\\) let: \\(S_{1}(i) \\coloneqq \\{j \\in V: i \\sim j \\in E\\}\\) be the set of 1- hop neighbors of \\(i\\) . \\(\\sharp_{\\Delta}(i,j) \\coloneqq S_{1}(i) \\cap S_{1}(j)\\) be the set of triangles based at \\(i \\sim j\\) . \\(\\sharp_{\\square}^{i}(i,j) \\coloneqq \\{k \\in S_{1}(i) \\setminus S_{1}(j), k \\neq j: \\exists w \\in (S_{1}(k) \\cap S_{1}(j)) \\setminus S_{1}(i)\\}\\) be the neighbors of \\(i\\) forming a 4- cycle based at \\(i \\sim j\\) without diagonals inside. \\(\\gamma_{\\max}(i,j) \\coloneqq \\max \\left\\{\\max_{k \\in \\sharp_{\\square}^{i}} \\{(A_{k} \\cdot (A_{j} - A_{i} \\odot A_{j})) - 1\\} , \\max_{w \\in \\sharp_{\\square}^{i}} \\{(A_{w} \\cdot (A_{i} - A_{j} \\odot A_{i})) - 1\\} \\right\\}\\) , with \\(\\cdot\\) being the dot product and \\(\\odot\\) the elementwise product, be the maximal number of 4- cycles based at \\(i \\sim j\\) traversing a common node. The BFC \\(Ric(i,j)\\) is zero if \\(\\min \\{d_{i}, d_{j}\\} = 1\\) and alternatively: \\[R i c(i,j) \\coloneqq \\frac{2}{d_{i}} + \\frac{2}{d_{j}} -2 + 2\\frac{|\\sharp_{\\Delta}(i,j)|}{\\max \\{d_{i},d_{j}\\}} +\\frac{|\\sharp_{\\Delta}(i,j)|}{\\min \\{d_{i},d_{j}\\}} +\\frac{(\\gamma_{m a x}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|). \\quad (16)\\]",
      "level": 2,
      "line_start": 13,
      "line_end": 16
    },
    {
      "heading": "A.2 Proofs For the sake of clarity and exposition, we report here both the formal statements alongside their respective proofs. Proposition 3.1. Let \\(i\\sim j\\) be an edge from the Literal- Clause Graph (LCG) representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\to 0\\) and \\(\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\to 0\\) as \\(\\alpha = \\frac{M}{N}\\to 0\\) Proof. As \\(\\alpha \\to 0\\) the expected value of the literal degree becomes: \\[\\lim_{\\alpha \\to 0}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\lim_{\\alpha \\to 0}\\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} = \\lim_{\\alpha \\to 0}e^{\\lambda}\\cdot \\lim_{\\alpha \\to 0}\\frac{\\lambda}{e^{\\lambda} - 1} = 1\\cdot \\lim_{\\alpha \\to 0}\\frac{1}{\\frac{e^{\\lambda} - 1}{\\lambda}} = 1, \\quad (17)\\] where the last equality is obtained due to the fact that \\(\\begin{array}{r}{\\lim_{\\alpha \\to 0}\\lambda = \\lim_{\\alpha \\to 0}\\frac{1}{2}\\alpha k = 0} \\end{array}\\) and the limit formula \\(\\begin{array}{r}{\\lim_{x\\to 0}\\frac{a^{x} - 1}{x} = \\ln \\left(a\\right)} \\end{array}\\) Given the average degree of the literals that act as an endpoint of at least one edge is 1 in this limiting case, by definition of the lower bound in Equation 8, we obtain that \\(\\begin{array}{r}{\\lim_{\\alpha \\to 0}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = 0} \\end{array}\\) . As a lower bound, \\(\\bar{R} (i,j)\\) satisfies the following inequality [5, 47]: \\[-2< \\bar{R} (i,j)\\leq R i c(i,j)\\leq \\kappa (i,j)\\leq 0 \\quad (18)\\] Given that both limits and expectations preserve weak inequalities, by sandwiching we obtain: \\[\\begin{array}{r l} & {\\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = 0\\leq \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\leq \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\leq 0}\\\\ & {\\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)] = \\underset {\\alpha \\to 0}{\\lim}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)] = 0} \\end{array} \\quad (20)\\] Proposition 3.2. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) Proof. As \\(\\alpha \\to \\infty\\) the expected value of the literal degree becomes: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\lim_{\\alpha \\to \\infty}\\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} = \\lim_{\\alpha \\to \\infty}\\frac{1}{\\frac{1}{\\lambda} - \\frac{1}{\\lambda e^{\\lambda}}} = \\infty , \\quad (21)\\] therefore we can consider the lower bound as consisting only of \\(\\begin{array}{r}{\\underline{{R}} (i,j)] = \\frac{2}{d_{i}} +\\frac{2}{d_{j}} - 2} \\end{array}\\) (ignoring the uninteresting case when \\(k = 1\\) ). Given that the expected value is a linear operation, we obtain that: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}]} +\\frac{2}{k} -2 = \\frac{2}{k} -2 \\quad (22)\\] Theorem 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. The BFC at \\(i\\sim j\\) is bounded from above by the quantity: \\[\\bar{R} (i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\max \\{d_{i},d_{j}\\}}. \\quad (23)\\] Furthermore, as \\(\\begin{array}{r}{\\alpha = \\frac{M}{N}\\to \\infty ,\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2} \\end{array}\\) and therefore the average BFC over the edges of \\(G\\) converges to \\(\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\to \\frac{2}{k} - 2\\)",
      "content": "Proof. We first prove the construction of the upper bound, which we can then use to prove the convergence of the expectation via sandwiching, similarly to the proof of Proposition 3.1. Please note that the definitions for each component of the graph BFC are given in Appendix A.1.2 and we will not be restating them here to avoid repetition. To start, notice that the BFC on \\(G\\) takes a simpler form compared to the more general, original definition A.2: \\[R i c(i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{(\\gamma_{\\mathrm{max}}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|), \\quad (24)\\] This result follows from the fact that bipartite graphs have no triangles, i.e., \\(\\sharp_{\\Delta}(i,j) = 0 \\forall (i,j) \\in E\\) . Our main focus for now will the rightmost term, which constitutes the difference between \\(\\mathrm{Ric}(i,j)\\) and \\(\\underline{{R}}(i,j)\\) (Equation 8). Firstly note that this term is, by definition, non- negative. This implies that \\(\\underline{{R}}(i,j)\\) of \\(\\mathrm{Ric}(i,j)\\) . We can simplify the definitions of the 4- cycle forming neighbors to match the bipartite topology of \\(G\\) : \\[\\begin{array}{r l} & {\\sharp_{\\square}^{i}(i,j)\\coloneqq \\{k\\in S_{1}(i)\\setminus \\{k\\} :\\exists w\\in (S_{1}(k)\\cap S_{1}(j))\\setminus \\{i\\} \\} ,}\\\\ & {\\sharp_{\\square}^{j}(i,j)\\coloneqq \\{k\\in S_{1}(j)\\setminus \\{k\\} :\\exists w\\in (S_{1}(k)\\cap S_{1}(i))\\setminus \\{j\\} \\} .} \\end{array} \\quad (25)\\] By definition of \\(S_{1}(\\cdot)\\) it follows immediately that the cardinality of both sets is bounded above by the node degrees: \\[\\begin{array}{r l} & {|\\sharp_{\\square}^{i}(i,j)|\\leq d_{i} - 1,}\\\\ & {|\\sharp_{\\square}^{j}(i,j)|\\leq d_{j} - 1 = k - 1.} \\end{array} \\quad (28)\\] We now turn to the term \\(\\gamma_{\\mathrm{max}}(i,j)\\) , whose definition can also be simplified, as a consequence of the topology of \\(G\\) . Consider first the symmetric adjacency matrix of a bipartite graph, given by: \\[A = \\left[ \\begin{array}{ll}0 & B\\\\ B^{T} & 0 \\end{array} \\right]\\in \\{0,1\\}^{N + M\\times N + M}, \\quad (29)\\] where \\(B\\) is an \\(N\\times M\\) incidence matrix with \\(B_{i j} = 1\\) if there if \\(i\\sim j\\in E\\) , and \\(B_{i j} = 0\\) otherwise, \\(B^{T}\\) is the transpose of \\(B\\) (which ensures that \\(A\\) is symmetric), and \\(\\mathbf{0}\\) are zero blocks of size \\(N\\times N\\) and \\(M\\times N\\) corresponding to the absence of edges within the partitions. We can thus express \\(\\gamma_{\\mathrm{max}}(i,j)\\) as: \\[\\gamma_{\\mathrm{max}}(i,j)\\coloneqq \\max \\left\\{\\max_{k\\in \\sharp_{\\square}^{i}}\\{A_{k}\\cdot A_{j} - 1\\} ,\\max_{w\\in \\sharp_{\\square}^{j}}\\{A_{w}\\cdot A_{i} - 1\\} \\right\\} , \\quad (30)\\] since \\(A_{i}\\odot A_{j} = \\mathbf{0}\\) . The operation \\(A_{k}\\cdot A_{j} = \\nu \\in \\mathbb{N}_{0}\\) returns the number of common neighbors \\(\\nu\\) that nodes \\(k\\) and \\(j\\) have, with \\(k\\) and \\(j\\) being in the same partition. Therefore, we have the following inequalities that can be used to derive an upper bound for \\(\\gamma_{\\mathrm{max}}(i,j)\\) : \\[\\begin{array}{r l} & {\\max_{k\\in \\sharp_{\\square}^{i}}\\{A_{k}\\cdot A_{j} - 1\\} \\leq k - 1,}\\\\ & {\\max_{w\\in \\sharp_{\\square}^{j}}\\{A_{w}\\cdot A_{i} - 1\\} \\leq M - 1,}\\\\ & {\\gamma_{\\mathrm{max}}(i,j)\\leq \\max \\{k - 1,M - 1\\} .} \\end{array} \\quad (31)\\] Putting everything together we obtain: \\[0\\leq \\frac{(\\gamma_{\\mathrm{max}}(i,j))^{-1}}{\\max \\{d_{i},d_{j}\\}} (|\\sharp_{\\square}^{i}| + |\\sharp_{\\square}^{j}|)\\leq \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\cdot \\max \\{d_{i},d_{j}\\}}, \\quad (34)\\] so that we can write: \\[-2< \\underline{{R}} (i,j)\\leq R i c(i,j)\\leq \\bar{R} (i,j)\\leq 0. \\quad (35)\\] In Proposition 3.2), we have previously that as \\(\\alpha \\to \\infty\\) \\(\\mathbb{E}_{(i\\sim j)}[\\underline{{R}} (i,j)]\\to \\frac{2}{k} - 2\\) , due to the fact that the literal degree becomes a dominant term. Therefore \\(\\max \\{d_{i},d_{j}\\} = d_{i}\\) , and under the same limiting assumption we have that \\(\\max \\{k - 1,M - 1\\} = M - 1\\) . Given that the expected value is a linear operation, we obtain that: \\[\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}]} +\\frac{2}{k} -2 + \\frac{1}{\\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[M - 1]} = \\frac{2}{k} -2. \\quad (36)\\]\n\nGiven that both limits and expectations preserve weak inequalities, by sandwiching we obtain: \\[\\frac{2}{k} -2 = \\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\leq \\lim_{\\alpha \\to \\infty}\\mathbb{E}_{(i\\sim j)}[R i c(i,j)]\\leq \\lim_{\\alpha \\to 0}\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)] = \\frac{2}{k} -2, \\quad (37)\\] ## A.3 Details on Test-Time Rewiring Graph rewiring is the process of modifying a graph's connectivity by adding, removing, or reweighting edges to optimize information flow. In our case, we make use of the rewiring procedure presented by Topping et al. [47], which consists of a discrete and stochastic Ricci flow, guided by the BFC. At each iteration, the edge with the most negative curvature (i.e., the structurally most \"strained\" connection) is identified. Candidate rewiring edges are then generated between the neighborhoods of the two endpoints of this edge. A new edge is stochastically selected either at random (with probability \\(p\\) ) or by maximizing the curvature improvement obtained from its addition. The selected edge is added to the graph and the corresponding curvature values are updated. By repeating this procedure for a fixed number of iterations, the algorithm progressively increases the average curvature of the graph, yielding a rewired version that contains information bottlenecks that are weaker compared to the input. Algorithm 1 contains the pseudocode for our rewiring algorithm. We would like to stress here that this modifies the constraints of the Boolean Satisfiability Problem (SAT) problem under consideration, but the goal of this procedure is to show that \"flatter\" problems will in fact be easier to solver for Graph Neural Network (GNN)- based solvers. Algorithm 1: Balanced Forman Curvature Stochastic Rewiring Input: Graph \\(G = (V,E)\\) with edge BFC values \\(R i c(i,j),\\forall (i,j)\\in E\\) , probability value \\(p\\in [0,1]\\) , number of iterations \\(N\\in \\mathbb{N}\\) Output: Rewired graph \\(G^{\\prime}\\) with updated BFC values for \\(t\\gets 1\\) to \\(N\\) do Select edge \\((i,j)\\) with most negative curvature \\(R i c(i,j)\\) From the neighbors \\(S_{1}(i)\\) and \\(S_{1}(j)\\) form candidate edge set \\[C = \\{(k,l):k\\in S_{1}(i),l\\in S_{1}(j),k\\neq l,(k,l)\\notin E\\}\\] if \\(C = \\emptyset\\) then L continue With probability \\(p\\) , choose a random edge \\((k,l)\\in C\\) Otherwise, for each \\((k,l)\\in C\\) 1. Compute updated curvature \\(R i c^{\\prime}(i,j)\\) after adding \\((k,l)\\) 2. Evaluate improvement score \\(\\Delta_{k l} = -(R i c(i,j) - R i c^{\\prime}(i,j))\\) Select \\((k,l)\\) with maximum \\(\\Delta_{k l}\\) Add edge \\((k,l)\\) to \\(G\\) and update neighborhood sets; Update curvatures \\(R i c(i,j)\\) and \\(R i c(k,l)\\) accordingly; Finalize bipartite structure of literals and clauses \\((L,C)\\) ensuring no intra- partition edges and set \\(G^{\\prime} = G\\) for \\(t = N\\) return \\(G^{\\prime}\\) ## A.4 Some initial ideas for curvature-aware solvers As discussed during the conclusion, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. In this section, we provide some starting points and experiments for simple implementations of curvature aware solvers that could lead to future improvements. The goal of our implementations was to maintain efficiency and rely on straightforward ideas that could lead to performance improvements. For these purposes, we introduce two simple curvature- aware variants of message passing. The first is an adaptation of Ye et al.\n\nTable 3: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers with different message-passing schemes: 1) Vanilla uses the typical message-passing operation; 2) Curvature Gate learns a gating function for each edge based on its curvature value [49]; 3) Online LCP extends the work of Fesser and Weber [16] and concatenates the local curvature statistics around each nodes as features during each recurrent step; 4) Both uses Curvature Gate and Online LCP. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface. <table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Variation</td><td colspan=\"4\">Datasets</td></tr><tr><td>3-SAT</td><td>4-SAT</td><td>SR</td><td>CA</td></tr><tr><td rowspan=\"3\">GCN</td><td>Vanilla</td><td>0.510 \u00b1 0.012</td><td>0.180 \u00b1 0.048</td><td>0.470 \u00b1 0.031</td><td>0.650 \u00b1 0.016</td></tr><tr><td>+ Curvature Gate</td><td>0.514 \u00b1 0.018</td><td>0.154 \u00b1 0.017</td><td>0.422 \u00b1 0.013</td><td>0.664 \u00b1 0.042</td></tr><tr><td>+ Online LCP</td><td>0.510 \u00b1 0.014</td><td>0.170 \u00b1 0.010</td><td>0.422 \u00b1 0.016</td><td>0.662 \u00b1 0.016</td></tr><tr><td></td><td>+ Both</td><td>0.500 \u00b1 0.012</td><td>0.176 \u00b1 0.031</td><td>0.416 \u00b1 0.027</td><td>0.654 \u00b1 0.027</td></tr><tr><td rowspan=\"3\">Neuro SAT</td><td>Vanilla</td><td>0.690 \u00b1 0.022</td><td>0.436 \u00b1 0.032</td><td>0.734 \u00b1 0.017</td><td>0.746 \u00b1 0.018</td></tr><tr><td>+ Curvature Gate</td><td>0.682 \u00b1 0.030</td><td>0.436 \u00b1 0.015</td><td>0.742 \u00b1 0.027</td><td>0.758 \u00b1 0.016</td></tr><tr><td>+ Online LCP</td><td>0.692 \u00b1 0.020</td><td>0.416 \u00b1 0.046</td><td>0.724 \u00b1 0.022</td><td>0.726 \u00b1 0.059</td></tr><tr><td></td><td>+ Both</td><td>0.664 \u00b1 0.018</td><td>0.438 \u00b1 0.028</td><td>0.742 \u00b1 0.025</td><td>0.758 \u00b1 0.020</td></tr></table> <center>Figure 3: Low dimensional visualization of the literal embeddings produced by Neuro SAT on random 4-SAT with (a) vanilla message passing and (b) Curvature Gate. Even though there is no major change in performance, the learned representations can be linearly separated into truth value assignments in the curvature-aware case, indicating promise for the inclusion of these principles in future GNN-based solver design. </center> [49], where the curvature of an edge is used to learn a gating mechanism that modulates message contributions. The second is a simple recurrent extension to Fesser and Weber [16], where the statistics of the curvature around each node are used as additional features at each recurrent step. Our empirical findings reveal that naively injecting curvature into GNN- based solvers sometimes leads to improved performance, but it does not always provide clear advantages, as seen in Table 3. Furthermore, we also experimented with a contemporary use of both variations. The training protocol and experimental settings are kept identical to previous experiments. The results highlight a subtle but important point: while curvature exposes structural bottlenecks, effective GNN solvers must also learn how to properly use geometric information. A major weakness of both methods is that random \\(k\\) - SAT problems have a lot of regularity, in the sense that the clause partitions will have very similar curvature statistics and thus the learning signal becomes redundant. Nevertheless, we believe that both these implementations provide interesting starting points for future work and research.\n\n<center>(a) Average graph Ricci curvature lower bound (Equation 8) as a function of \\(k\\) and \\(\\alpha\\) . </center> <center>(b) Average graph Ollivier-Ricci curvature as a function of \\(k\\) and \\(\\alpha\\) . </center> Figure 4: Contour plots of the average graph Ricci curvature lower bound (a) and Ollivier- Ricci curvature (b) displaying the changes in average curvature as a function of \\(k\\) and \\(\\alpha\\) . Both quantities behave similarly, especially as \\(\\alpha \\to 0\\) . We can see that in the case of the ORC, the stronger correction towards positive curvature due to the presence of cycles as \\(\\alpha \\to \\infty\\) is in line with the general theory related to this discretization of the Ricci curvature [5]. Notice the difference between the contours of (b) and Figure 1b for large \\(\\alpha\\) and \\(k\\) . Best viewed in color. <center>Figure 5: Visualization of an easy, in terms of clause density \\(\\alpha\\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very small number of long-range interactions, as can clearly be seen in (b). Furthermore, for such very small \\(\\alpha\\) , the average BFC approaches 0, in line with the developed theory. </center> ## A.5 Additional plots In this section we provide additional and miscellaneous plot that help with visualizing various concepts explained in the main manuscript, such as the relationship between curvature and hardness, as well as visualizations of easy and hard problems. More details can be found in Figures 4, 5, 6 and 7.\n\ncompressed into finite- dimensional embeddings. This bottleneck severely restricts the ability of GNNs to model long- range dependencies, and can be thought of as a vanishing gradient problem. Given that the difficulty of SAT can actually be understood intuitively as being proportional to the number of long- range dependencies between nodes, we question whether learning GNN- based solvers on these difficult problems is impacted by oversquashing. Recent geometric analyses propose that oversquashing is directly tied to negative Ricci Curvature (RC) of the underlying graph [38, 47]. These insights invite a fundamental question: Can graph RC serve as a predictive and constructive notion in unraveling novel hardness concepts for GNN- based SAT solvers? In this work, we address this question by studying the typical- case behavior of particular instantiations of graph RC on random \\(k\\) - SAT problems represented as bipartite graphs. Our starting point is the observation that edges of bipartite graphs are always non- positively curved. We show that, on average, the edges become more negatively curved as problems get harder, and less negatively curved as they become easier. Finally, we derive an exact expression for the average Balanced Forman Curvature (BFC) in the limit of unsolvable problems, from which we derive a connection between curvature and oversquashing in GNNs- based SAT solvers, following the theory of Topping et al. [47]. This is, to the best of our knowledge, the first successful attempt at a theoretical characterization of the limitations of GNN- based SAT solvers. To validate our theory, we perform experiments on random 3- and 4- SAT problems, and also on a vast array of datasets coming from the recent benchmark of Li et al. [28]. Firstly, we observe a phase transition- like phenomenon in random 3- SAT solving probability as a function of the mean and variance of the curvature. We further affirm the aforementioned limitations by rewiring only the testing graphs of the benchmarks at test- time to increase their average BFC, and show that these rewired problems become much easier to solve. Finally, we find that heuristics based on the BFC of a dataset correlate extremely well with generalization error, unlike the average clause density, which is typically used to characterize the hardness of a single instance. Overall, our findings suggest that GNN- based SAT solvers have two distinct types of hardness: the hardness of learning representations in negatively curved structures, followed by the well- established algorithmic hardness of SAT. We conclude by relating our findings with modeling principles and design choices of existing approaches. Outline. The remainder of this article is structured as follows: In Section 2, we provide a recap of the most relevant concepts discussed in the paper: random \\(k\\) - SAT, GNNs, and graph RC. Section 3.1 presents the main theoretical results, including an in- depth discussion on how oversquashing affects GNNs- based solvers. We then demonstrate the experimental evidence in Section 4. Finally, we provide a discussion regarding current design principles and future work in Section 5. ## 2 Background This section is only intended to formally introduce the objects studied in the paper and render the material self- contained. We kindly ask the reader to tolerate the occasional whirlwind and abuse of notation, as it will be formalized later in Section 3. ### 2.1 Random \\(k\\) Boolean Satisfiability Problem The random \\(k\\) - SAT (assignment) problem, which is the central object of study in this work, is made up of \\(N\\) variables \\(\\{x_{i}\\}_{i = 1}^{N}\\) that can take binary values \\(x_{i} \\in \\{0,1\\}\\) . Using these variables, one constructs \\(M\\) clauses containing a disjunction of \\(k\\) variables or their negations (called literals). For example, a 3- SAT problem would have clauses of the form \\((x_{i} \\vee \\neg x_{j} \\vee x_{h})\\) . The goal is to assign a value to all literals such that they satisfy the conjunction of all clauses. This logical formula is called a Conjunctive Normal Form (CNF). In the random formulation, it is possible to identify different phases of problem hardness based on a parameter called the clause density \\(\\alpha = M / N\\) . This phenomenon has been actively researched in statistical physics due to the analogies between random \\(k\\) - SAT and spin- glass models [33, 34]. Notable results[26, 37, 50] include the discovery of two transitions for typical instances at a given \\(k\\) , based on the value of \\(\\alpha\\) in the thermodynamic limit \\((M, N \\to \\infty)\\) : As \\(\\alpha\\) increases, the measure over the space of possible solutions first decomposes into an exponential number of clusters at the dynamical transition \\(\\alpha_{d}(k)\\) and subsequently condensates over the largest such states at the critical transition \\(\\alpha_{c}(k)\\) . These phase transitions naturally affect the performances of many algorithms, e.g., when \\(k \\geq 4\\) , going beyond \\(\\alpha_{c}\\) is almost impossible with existing algorithms.\n\n<center>Figure 6: Visualization of a hard, in terms of clause density \\(\\alpha\\) , random 3-SAT problem with 256 variables in (a) bipartite and (b) circular layouts. There is a very large number of long-range interactions, as can clearly be seen in (b). Furthermore, for such large \\(\\alpha\\) , the average BFC is strongly negative, in line with the developed theory. </center> <center>Figure 7: Average BFC as a function of \\(\\alpha\\) for random 3 and 4 -SAT problems with \\(N = 256\\) . The size of the blobs is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model [46]. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c}\\) [32]. The average BFC drops monotonically with \\(\\alpha\\) in both cases. For 3-SAT (a), this drop appears to be almost linear, with a substantial amount of variance, as also depicted in Figure 2. 4-SAT on the other hand contains problems where the curvature is substantially more negative and concentrated, which when coupled with the larger number of constraints leads to oversquashing, as discussed in our theory. This can be clearly seen by the fast performance drop-off in (b), which happens much earlier than \\(\\alpha_{c}\\) , indicating the additional hardness phase related to representation learning discussed in the main paper. </center>\n\n<center>Figure 8: Analogue of Figure 2a for 4-SAT. Average BFC as a function of \\(\\alpha\\) for random 4-SAT problems with \\(N = 256\\) . The color is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model, with a group labeled as solvable if \\(50\\%\\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c} \\approx 9.931\\) [32]. The average BFC is negative and drops with \\(\\alpha\\) , but for a small amount. The plot in the top-right corner shows the model's solution probability curve as a function of \\(\\alpha\\) , where it is possible to notice that the GNN-based solver has severe limitations on these problems. Differently from 3-SAT (Figure 2a), the average curvature is negative and concentrated even for problems that would be considered very simple in terms of \\(\\alpha\\) ( \\(\\alpha \\rightarrow 8\\) ). This is due to the larger value of \\(k\\) , the higher number of long-range interactions, and the connection of both factors with oversquashing, as discussed and predicted by our contributions. </center>\n\n### 2.2 Graph Neural Networks GNNs are a subclass of Neural Networks (NNs) that can learn a representation of graph data by locally aggregating information[20, 45]. The main goal of the architecture is to implement inductive biases natural to graph data [9]. An example of such a property is learning graph- level functions invariant to the nodes' ordering. Consider, for simplicity, an unweighted and undirected graph \\(G\\) with \\(N\\) nodes, represented by a symmetric binary adjacency matrix \\(A\\in \\{0,1\\}^{N\\times N}\\) . This setting can be easily extended to deal with more general connectivity structures [14]. By associating a node feature matrix \\(X\\in \\mathbb{R}^{N\\times d}\\) to the graph, we can describe a GNNs as a convolution of the graph signal with \\(A\\) as the shift operator. A generalization of this concept can be obtained by considering the message- passing framework [20]: \\[x_{i}^{(k)} = \\theta^{(k)}\\left(x_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\phi^{(k)}\\left(x_{i}^{(k - 1)},x_{j}^{(k - 1)},e_{j i}\\right)\\right), \\quad (1)\\] where \\(x_{i}^{(k)}\\) denotes node features of node \\(x_{i}\\) at layer \\(k\\) \\(e_{j i}\\) the (optional) edge features from node \\(j\\) to node \\(i\\) \\(\\mathcal{N}(\\cdot)\\) the set of (1- hop) neighbor nodes, \\(\\bigoplus\\) a differentiable, permutation invariant function, (e.g., sum, mean), and \\(\\phi ,\\theta\\) denote differentiable and (optionally) nonlinear functions such as Multi- Layer Perceptrons (MLPs). A CNF formula can be easily translated into a bipartite graph [6], which can then be fed into a GNN- based solver. The particular bipartition we consider in this work is detailed later in Section 3. The application of the above message- passing scheme for SAT problems can be done by applying Equation1 to the clause and literal partitions [28]. Let \\(i\\) be a literal node and \\(j\\) be a clause node, then: \\[\\begin{array}{r l} & {h_{j}^{(k)} = \\theta_{c}^{(k)}\\left(h_{j}^{(k - 1)},\\bigoplus_{i\\in \\mathcal{N}(j)}\\left(\\left\\{\\phi_{i}^{(k)}\\left(h_{i}^{(k - 1)}\\right)\\right\\}\\right)\\right),}\\\\ & {h_{i}^{(k)} = \\theta_{l}^{(k)}\\left(h_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\left(\\left\\{\\phi_{c}^{(k)}\\left(h_{j}^{(k - 1)}\\right)\\right\\}\\right)\\right),} \\end{array} \\quad (2)\\] where the subscripts \\(c\\) and \\(l\\) refer to NNs specialized on the clause and literal partitions respectively. ### 2.3 Ricci Curvature of Graphs In Riemannian geometry, RC quantifies the local deviation of a manifold \\(\\mathcal{M}\\) from flat Euclidean space, as a result of the metric defined on \\(\\mathcal{M}\\) . Intuitively, RC captures how the neighborhoods of two adjacent points relate when moving from one point to the other. On smooth manifolds, it compares how a small ball of mass around a point is distorted when transported along a geodesic to a neighboring point. Extending this notion to more general structures, such as metric spaces or combinatorial complexes, has been an extremely active area of mathematical research, with the works of Ollivier [39] and Forman [18] standing out. In the case of graphs, we ask ourselves how local connectivity either concentrates or disperses. Ollivier [39] implements this idea by comparing probability mass on local neighborhoods, i.e., a random walk distribution on the endpoints of an edge. Given these two distributions, one can compare the ratio between their Wasserstein and shortest path distance, serving as a direct and discrete analogue of geodesic transport. See Appendix A.1.1 for more details. The definition of Forman [18] relies heavily on topology, and thus it takes a combinatorial form. Essentially, given a cell complex, the curvature of a p- cell depends only on the topological structures between the cell and its neighbors. This makes Forman- Ricci Curvature (FC) simpler to compute numerically, since it can avoid the optimization of the optimal transport problem that arises in Ollivier- Ricci Curvature (OC) curvature. Given that RC is directly related to the structure of local neighborhoods, it has emerged as a powerful way of theoretically analyzing limitations of GNNs. In a seminal paper, Topping et al. [47] provide both a balanced version of the FC curvature and show that the oversquashing problem [1] can be directly connected to edges with high negative curvature. This definition, namely the BFC, is central to this paper, therefore please consult Appendix A.1.2 for the definition and additional details. Nguyen et al. [38] have shown that similar results can be derived using the OC curvature. It is worth noting that these notions of curvature are naturally correlated with one another, as shown empirically in a multitude of complex networks by Samal et al. [44].",
      "level": 2,
      "line_start": 17,
      "line_end": 34
    },
    {
      "heading": "2.2 Graph Neural Networks GNNs are a subclass of Neural Networks (NNs) that can learn a representation of graph data by locally aggregating information[20, 45]. The main goal of the architecture is to implement inductive biases natural to graph data [9]. An example of such a property is learning graph- level functions invariant to the nodes' ordering. Consider, for simplicity, an unweighted and undirected graph \\(G\\) with \\(N\\) nodes, represented by a symmetric binary adjacency matrix \\(A\\in \\{0,1\\}^{N\\times N}\\) . This setting can be easily extended to deal with more general connectivity structures [14]. By associating a node feature matrix \\(X\\in \\mathbb{R}^{N\\times d}\\) to the graph, we can describe a GNNs as a convolution of the graph signal with \\(A\\) as the shift operator. A generalization of this concept can be obtained by considering the message- passing framework [20]: \\[x_{i}^{(k)} = \\theta^{(k)}\\left(x_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\phi^{(k)}\\left(x_{i}^{(k - 1)},x_{j}^{(k - 1)},e_{j i}\\right)\\right), \\quad (1)\\] where \\(x_{i}^{(k)}\\) denotes node features of node \\(x_{i}\\) at layer \\(k\\) \\(e_{j i}\\) the (optional) edge features from node \\(j\\) to node \\(i\\) \\(\\mathcal{N}(\\cdot)\\) the set of (1- hop) neighbor nodes, \\(\\bigoplus\\) a differentiable, permutation invariant function, (e.g., sum, mean), and \\(\\phi ,\\theta\\) denote differentiable and (optionally) nonlinear functions such as Multi- Layer Perceptrons (MLPs). A CNF formula can be easily translated into a bipartite graph [6], which can then be fed into a GNN- based solver. The particular bipartition we consider in this work is detailed later in Section 3. The application of the above message- passing scheme for SAT problems can be done by applying Equation1 to the clause and literal partitions [28]. Let \\(i\\) be a literal node and \\(j\\) be a clause node, then: \\[\\begin{array}{r l} & {h_{j}^{(k)} = \\theta_{c}^{(k)}\\left(h_{j}^{(k - 1)},\\bigoplus_{i\\in \\mathcal{N}(j)}\\left(\\left\\{\\phi_{i}^{(k)}\\left(h_{i}^{(k - 1)}\\right)\\right\\}\\right)\\right),}\\\\ & {h_{i}^{(k)} = \\theta_{l}^{(k)}\\left(h_{i}^{(k - 1)},\\bigoplus_{j\\in \\mathcal{N}(i)}\\left(\\left\\{\\phi_{c}^{(k)}\\left(h_{j}^{(k - 1)}\\right)\\right\\}\\right)\\right),} \\end{array} \\quad (2)\\] where the subscripts \\(c\\) and \\(l\\) refer to NNs specialized on the clause and literal partitions respectively. ### 2.3 Ricci Curvature of Graphs In Riemannian geometry, RC quantifies the local deviation of a manifold \\(\\mathcal{M}\\) from flat Euclidean space, as a result of the metric defined on \\(\\mathcal{M}\\) . Intuitively, RC captures how the neighborhoods of two adjacent points relate when moving from one point to the other. On smooth manifolds, it compares how a small ball of mass around a point is distorted when transported along a geodesic to a neighboring point. Extending this notion to more general structures, such as metric spaces or combinatorial complexes, has been an extremely active area of mathematical research, with the works of Ollivier [39] and Forman [18] standing out. In the case of graphs, we ask ourselves how local connectivity either concentrates or disperses. Ollivier [39] implements this idea by comparing probability mass on local neighborhoods, i.e., a random walk distribution on the endpoints of an edge. Given these two distributions, one can compare the ratio between their Wasserstein and shortest path distance, serving as a direct and discrete analogue of geodesic transport. See Appendix A.1.1 for more details. The definition of Forman [18] relies heavily on topology, and thus it takes a combinatorial form. Essentially, given a cell complex, the curvature of a p- cell depends only on the topological structures between the cell and its neighbors. This makes Forman- Ricci Curvature (FC) simpler to compute numerically, since it can avoid the optimization of the optimal transport problem that arises in Ollivier- Ricci Curvature (OC) curvature. Given that RC is directly related to the structure of local neighborhoods, it has emerged as a powerful way of theoretically analyzing limitations of GNNs. In a seminal paper, Topping et al. [47] provide both a balanced version of the FC curvature and show that the oversquashing problem [1] can be directly connected to edges with high negative curvature. This definition, namely the BFC, is central to this paper, therefore please consult Appendix A.1.2 for the definition and additional details. Nguyen et al. [38] have shown that similar results can be derived using the OC curvature. It is worth noting that these notions of curvature are naturally correlated with one another, as shown empirically in a multitude of complex networks by Samal et al. [44].",
      "content": "",
      "level": 3,
      "line_start": 33,
      "line_end": 34
    },
    {
      "heading": "3 Curvature of Random k-SAT Problems and Its Relationship with GNNs Setting and Notation. We consider random \\(k\\) - SAT problems with \\(N\\) variables and \\(M\\) clauses, with \\(\\alpha = M / N\\) and \\(k,N,M\\in \\mathbb{N}\\) . These problems are represented through a simple bipartite graph \\(G = (V,E)\\) , where the node set is a literal- clause bipartition \\(V = L\\cup C\\) , with \\(L\\cap C = \\emptyset\\) and \\(|L| = 2N,|C| = M\\) . The edge set takes the form \\(E = \\{(i,j)\\in V\\times V:i\\sim j,i\\in L,j\\in C\\}\\) where \\(i\\sim j\\) indicates a connection between nodes. Given \\(v\\in V\\) , we denote its degree by \\(d_{v}\\) . Finally, we denote the expected value of the random variable \\(X\\) with probability distribution \\(\\mathrm{P}\\) by \\(\\mathbb{E}_{P}[X]\\) and \\(\\mathbb{E}_{p\\sim P}[X]\\) the expectation over samples drawn from \\(P\\) . Unless noted otherwise, when we refer to the expected value in simulations, we imply its estimate via the sample mean statistic. Data Model. Our bipartite formulation is a simplification of the input graphs considered in many GNN- based solvers [40, 46]. Recent literature [28] refers to this data structure as a Literal- Clause Graph (LCG). Following an Erd\u0151s\u2013R\u00e9nyi- like procedure, each clause is assigned \\(k\\) literals independently at random with probability \\(p\\) . Assuming that all literals are equally likely to appear in a given clause, we obtain the following degree distributions: \\[\\begin{array}{c}{P(d_{j} = h) = \\delta (h - k)}\\\\ {P(d_{i} = h) = \\binom{M}{h} p^{h}(1 - p)^{M - h},} \\end{array} \\quad (3)\\] where \\(\\delta (\\cdot)\\) represents the Dirac delta function, \\(\\binom{\\cdot}{}\\) the binomial coefficient, and \\(p\\coloneqq \\frac{k}{2N}\\) . In the limit \\(M,N\\to \\infty\\) , we can approximate the Binomial form of the literal degree distribution with a Poisson distribution: \\[P(d_{i} = h) = \\frac{\\lambda^{h}e^{-\\lambda}}{h!}, \\quad (5)\\] with \\(\\lambda = M p = \\textstyle {\\frac{1}{2}}\\alpha k\\) . We will be interested in using this approximation to calculate properties of the graph RC, which is an edge level property, therefore the case \\(d_{i} = 0\\) should be truncated. This fact leads us to propose an alternative probability mass function based on zero- truncated Poisson distribution the for the literal degrees: \\[P^{*}(d_{i} = h) = P(d_{i} = h:h\\geq 1) = \\frac{P(d_{i} = h)}{1 - P(d_{i} = 0)} = \\frac{\\lambda^{h}}{h!(e^{\\lambda} - 1)}. \\quad (6)\\] Characterizing Average Curvature. We will now proceed by showing that the above bipartite representation of SAT problems has significant implications on the average RC. Most importantly, we will precisely characterize the average behavior of a specific definition curvature, namely the BFC, which in turn has strong implications on oversquashing in GNNs. The proofs of all statements are deferred to Appendix A.2. We start from an established lower bound of the OC. Definition 3.1 (OC lower bound in bipartite graphs [5, 23]). For any edge \\(i\\sim j\\) in a simple, unweighted bipartite graph \\(G\\) , we have that that: \\[\\underline{{\\kappa}}(i,j) = -2\\cdot \\max \\{0,1 - \\frac{1}{d_{i}} -\\frac{1}{d_{j}}\\} \\leq \\kappa (i,j)\\leq 0, \\quad (7)\\] where \\(\\kappa\\) is the OC. The above lower bound can be redefined by alternatively stating the condition encoded inside the maximum function in Equation 7: \\[\\underline{{R}}(i,j) = \\left\\{ \\begin{array}{l l}{0} & {\\mathrm{if~min}\\{d_{i},d_{j}\\} = 1}\\\\ {\\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2} & {\\mathrm{otherwise}} \\end{array} \\right. \\quad (8)\\] This alternative definition shows that Equation 8 is also a lower bound for the BFC s.t., \\(\\underline{{R}}(i,j)\\leq\\) \\(R i c(i,j)\\) . This statement is a direct consequence of the definition of the BFC, and a proof is given inside the proof of Theorem 3.1. Furthermore, we also have that \\(- 2< R i c(i,j)\\leq \\kappa (i,j)\\leq 0\\) , which is a direct consequence of the facts that \\(\\kappa (i,j)\\geq R i c(i,j)\\) [47] and Equation 7. Starting from these statements, we can proceed to show that the average behavior of the graph RC is naturally dictated by average degree of the literals, given that \\(d_{j} = k\\) always holds. The literal degree distribution is a zero- truncated Poisson distribution, from which the average degree of a literal \\(i\\) can be calculated as: \\[\\mathbb{E}_{d_{i}\\sim P^{*}(d_{i})}[d_{i}] = \\frac{\\lambda e^{\\lambda}}{e^{\\lambda} - 1} \\quad (9)\\]",
      "content": "Proposition 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\kappa (i,j)]\\to 0\\) and \\(\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]\\to 0\\) as \\(\\alpha = \\frac{M}{N}\\to 0\\) . Proposition 3.1 tells us that as problems get easier, i.e., they are in the satisfiable phase, their graph representations get (Ricci) flatter. This statement implies a relationship between very simple problems and their bipartite topology, which is that each clause is made of distinct literals. In this scenario, producing a satisfying assignment becomes trivial since assigning truth values to these unique literals does not affect other clauses. This implication aligns well with common knowledge, therefore the next thing to check is whether there is an opposite behavior when dealing with very difficult problems. This later case is important because it is close to the unsatisfiable phase where the faults of existing algorithms emerge [2]. Through a similar argument as before, we can formalize this intuition and characterize the average lower bound in the case when problems are surely unsatisfiable. Proposition 3.2. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. Then \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) . In this scenario, the average lower bound of the graph RC will tend to be maximally negative, therefore we cannot directly speak on the average curvature by sandwiching as before. Nevertheless, Proposition 3.2 provides an extremely interesting insight into the interplay between \\(\\alpha\\) and \\(k\\) . As the problems get harder, the number of constraints (i.e., \\(k\\) ) becomes a decisive factor on the curvature. A larger value of \\(k\\) implies more constraints and thus many long range interactions between the literals, but it also implies that edges will become more negatively curved. In turn, this implies the existence of bottlenecks in the graph, which makes long range communication becomes difficult [47]. While it is not possible to connect the exact graph OC to this result, it is possible to extend it to the average graph BFC: Theorem 3.1. Let \\(i\\sim j\\) be an edge from the LCG representation \\(G\\) of a random k- SAT problem with \\(N\\) variables and \\(M\\) clauses, with degree distributions given by Equations 6 and 3. The BFC at \\(i\\sim j\\) is bounded from above by the quantity: \\[\\bar{R} (i,j)\\coloneqq \\frac{2}{d_{i}} +\\frac{2}{d_{j}} -2 + \\frac{d_{i} + k - 2}{\\max \\{k - 1,M - 1\\} \\max \\{d_{i},d_{j}\\}}. \\quad (10)\\] Furthermore, as \\(\\alpha = \\frac{M}{N}\\to \\infty\\) , \\(\\mathbb{E}_{(i\\sim j)}[\\bar{R} (i,j)]\\to \\frac{2}{k} - 2\\) and therefore the average BFC over the edges of \\(G\\) converges to \\(\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]\\to \\frac{2}{k} - 2\\) . Theorem 3.1 contains a crucial result in its characterization of the typical- case BFC, which is that it is connected to both \\(\\alpha\\) and \\(k\\) . While the connection to \\(\\alpha\\) might seem obvious, the one with \\(k\\) is quite insightful. One can observe that as problems become harder to satisfy (large values of \\(\\alpha\\) ), \\(k\\) plays an important role in how negatively curved the graph edges can become. This phenomenon can be seen in Figure 1, where for smaller \\(k\\) , larger values of \\(\\alpha\\) are required to have highly negatively curved edges. This latter point is crucial in developing a more profound understanding of the limitations of GNN- based solvers, and we will expand upon it in the following subsection. Another interesting consequence of our results, is that due to their shared lower bound and the results in Propositions 3.1 and 3.2, we confirm a tight link between graph OC and BFC. In general, the correlation between graph OC and FC has been studied in great detail on an array of complex networks by Samal et al. [44]. In our case, we confirm that both OC and BFC discretizations behave identically for simpler problems and similarly in relation to \\(\\alpha\\) and \\(k\\) , a phenomenon that can also be verified empirically through Figures 1 and 4 (Appendix). ### 3.1 Message-Passing Bottlenecks and Downstream Performance The results presented in the previous section allow us to proceed with a principled way of understanding performance limitations in GNN- based SAT solvers. This is due to the direct connection between the result in Theorem 3.1 to Theorem 4 of Topping et al. [47], which establishes that \"edges with high negative curvature are those causing the graph bottleneck and thus leading to the oversquashing phenomenon\". This seminal result states that if the gradients of the message passing functions ( \\(\\theta\\) and \\(\\phi\\) in Equation 1) are bounded, and there exists a sufficiently negatively curved edge compared to the degrees of its endpoints, then the derivative of the learned node representations around that edge vanishes. Intuitively, this can be understood as a difficulty of propagating the information in nodes at a reachable distance due to the fact that the graph structure limits the pathways where\n\n<center>Figure 1: Average graph Ricci Curvature lower bound (a) and Balanced Forman Curvature (b) as a function of \\(\\alpha\\) and \\(k\\) . Both quantities behave very similarly, both in terms of the smooth transition from flat to negative curvature and their magnitude, especially as \\(\\alpha \\to 0\\) and \\(\\alpha \\to \\infty\\) , in line with the developed theory. An alternative version of this figure displaying the average graph OC in (b) is presented in Figure 4 of the Appendix. Best viewed in color. </center> information can travel. Simply stated, nodes in different neighborhoods need to pass all messages through the same edge(s), leading to a difficulty in learning fixed- length representations that can hold information on long range correlations. Formally, for large values of \\(k\\) , as the clause density \\(\\alpha \\to \\infty\\) , any infinitesimally small value \\(\\delta > 0\\) could be used in Theorem 4 of Topping et al. [47] such that \\(Ric(i,j) \\leq - 2 + \\delta\\) , leading to an exponentially decaying Jacobian of the node representations around \\(i \\sim j\\) . This result leads us to the conclusion that GNN- based solvers are limited by both these parameters and suffer from two distinct hardness types: the algorithmic hardness inherent to SAT and the hardness of learning representations for long range communication. The interplay between \\(k\\) and \\(\\alpha\\) in Theorem 3.1 provides additional insights. Indeed, for problems with large values of \\(k\\) or large values of \\(\\alpha\\) , highly negatively curved edges are guaranteed to exist on average, and this quantity will concentrate. On the other hand, for large values of \\(\\alpha\\) and relatively small values of \\(k\\) , the latter becomes crucial in deciding how well a GNN- based solver will be able to learn, i.e., it should be easier to learn a solver for smaller values of \\(k\\) . We confirm this fact empirically in Section 4. The intuition we provide for a more complete understanding of this crucial result is the following: At increasing connectivity, literals become very distant on the interaction network, i.e., the number of long- range codependencies increases. In this scenario, the GNN will not be able to learn a fixed length representation that can \"remember\" the information of reachable, but not directly adjacent nodes. This means that the ability to learn a solver is compromised by an oversquashing phenomenon. For large values of \\(k\\) , this problem becomes prevalent even before the hardness of exploring the solution space, due to the effect of \\(k\\) on the BFC. Our theory motivates therefore how increasing values of \\(k\\) in random \\(k\\) - SAT would lead to worse oversquashing and performance, even for what would be considered simple problems in terms of \\(\\alpha\\) . To visually understand the aforementioned concepts, we can again refer to Figure 1. As the value of \\(k\\) grows, the gap between the flatter (yellow) and highly negatively curved problems (violet) gets smaller. The same holds for increasing values of \\(\\alpha\\) , as expected. We provide additional visual depictions of this aforementioned explanation in Appendix A.5, where we plot two input graphs for random 3- SAT at small (Figure 5) and large (Figure 6) \\(\\alpha\\) . In the following section, we will show that our results can be empirically confirmed for different GNN- based solvers. ## 4 Experiments Experimental Setting. To validate our theory, we perform different experiments on various datasets, the details of which will be provided in the following subsections. The experiments consist in firstly exploring the behavior of a GNN- based solvers and relating it to the input graph BFC. Based on these\n\n<center>Figure 2: (a) Average BFC as a function of \\(\\alpha\\) for random 3-SAT problems with \\(N = 256\\) . The color is used as a representation for the average solvability of the problems at a given \\(\\alpha\\) by the Neuro SAT model [46], with a group labeled as solvable if \\(50\\%\\) or more of the problems get a satisfying assignment. The vertical red line represents the analytical SAT-UNSAT critical threshold \\(\\alpha_{c} \\approx 4.267\\) [32]. The average BFC drops monotonically with \\(\\alpha\\) . The small plot in the bottom-left corner provides the model's solution probability curve in terms of \\(\\alpha\\) , where it is possible to clearly notice the algorithmic transition from satisfiable to unsatisfiable problems. (b) Probability of finding a satisfying assignment of the same problems as in (a) with Neuro SAT as a function of the variance and average of the BFC. Notice how as \\(\\alpha\\) grows, the average curvature not only gets more negative, but also concentrates. We can see from the empirical results in (a) that in this case, the model is unable to produce a satisfying assignment. As \\(\\alpha\\) becomes smaller, the input graphs have less negative edges on average, the associated variance naturally grows, and so does the solving probability. Using the first two moments of the BFC, we are able to observe a similar transition-like phenomenon as the small plot in the bottom-left corner of (a). </center> results, we then propose two heuristics to understand how hard a given SAT dataset will be to solve for a GNN- based solver. We will focus on the assignment scenario, as it includes the decision scenario as well. Given that all the datasets we utilize do not come with node features, we make use of learned embeddings in order to effectively explore oversquashing implications. The GNN- based solvers are implemented following the design of Li et al. [28], using Py Torch [41], Py Torch- Geometric [17] and Py Torch Lightning [15]. The networks were trained for 100 epochs using the Adam W optimizer [29], with learning rate \\(\\eta = 0.0001\\) decaying by half after 50 epochs and the gradients clipped at unit norm. The training was done on NVIDIA Titan RTX GPUs. ### 4.1 The Relationship Between Curvature and Satisfiability We start by using numerical simulations to verify the theoretical claims made in Section 3.1. To do this, we generate random 3- SAT instances in Conjunctive Normal Form (CNF) using a custom implementation in the Julia language [4]. We generate problems with \\(\\alpha \\in [3,5]\\) in steps of \\(\\Delta \\alpha = 0.1\\) , capturing both satisfiable and unsatisfiable regimes around the known critical threshold \\(\\alpha_{c} \\approx 4.267\\) . Considering its widespread use and downstream performance, we train the Neuro SAT model [46] to produce a satisfying assignment, while scaling the number of message passing iterations by \\(2N\\) during evaluation, to maximize inference accuracy. We then analyze the performance of the model on problems with \\(N = 256\\) , with the results being summarized in Figure 2. Our results show that by considering the probability of finding a solution at a given \\(\\alpha\\) as a function of the first and second moments of the curvature, we can replicate a SAT/UNSAT phase- transition (Figure 2b). This result presents an important step forward in theoretically understanding the performance of GNN- based solvers.[32]. Similar results hold for random 4- SAT, and can be seen in Figures 7 and 8 in the Appendix. For this higher value of \\(k\\) , we have more negatively curved edges which strongly impact the performance of GNN- based solvers, as will further confirmed in Section 4.2. An interesting observation is that for random 3- SAT, the curvature starts to become highly negative and concentrate close to the estimated dynamical threshold \\(\\alpha_{d} \\approx 3.927\\) [32].\n\nTest- time Rewiring. In order to obtain additional evidence about the previous observations, we put ourselves in a unique scenario: Suppose a GNN- based solver is trained on a dataset of SAT problems, and later tested on a separate testing partition. If we render the said testing partition less curved, would the model perform better without needing to retrain? The purpose behind this experiment is to gain a deeper understanding of the relationship between curvature and problem complexity. For this purpose, we use four different SAT benchmarks proposed by Li et al. [28]: Random 3 and 4 - SAT generated near the (respective) critical threshold \\(\\alpha_{c}\\) , a random \\(k\\) - SAT dataset consisting of mixed \\(k\\) values (SR), and one that mimics the modularity and community structure of industrial problems (CA). The last two datasets are better representatives of real- world problems. We train both a Graph Convolutional Network (GCN)- solver [25] and Neuro SAT on training partitions using the same protocol as before, while the testing partition is rewired using a stochastic discrete Ricci flow procedure, similarly to [47]. The idea behind the rewiring procedure is quite simple: we make the input graph less curved by stochastically deleting edges that have the highest negative curvature, while adding new edges that are less curved. We provide a more detailed explanation of this process, including a schematic algorithm in Appendix A.3. The results are reported in Table 1, where it be observed that that the rewired problems become simpler to solve for both solvers at test- time. A noteworthy observation is that a large improvement happens on 4- SAT problems, while the modular CA dataset reports small improvements. In the following subsection, we make a direct connection of this result with our theory. ### 4.2 A New Hardness Heuristic for GNN-Based Solvers Based on the developed theory and the above observations, we provide, as a practical contribution, two different heuristics that reflect how hard it will be for a GNN- based solver to tackle a dataset. The main motivation behind these heuristics is that if we simply look at the average clause density of a dataset, we can miss out on direct implications of oversquashing. An example of this is the first dataset we presented for the random 3- SAT experiments discussed in Figure 2, which is build at increasing values of \\(\\alpha\\) . Given an input graph \\(G\\) , we define the heuristics as: \\[\\omega (G) = -\\mathbb{E}_{(i\\sim j)}[Ric(i,j)]*\\mathbb{E}[\\alpha ],\\quad \\omega^{*}(G) = \\frac{\\omega(G)}{\\mathbb{V}_{(i\\sim j)}[Ric(i,j)]}, \\quad (11)\\] with the expectations being taken over the edges \\(G\\) . The averages of both heuristics, which we denote by \\(\\bar{\\omega}\\) and \\(\\bar{\\omega}^{*}\\) can then be used to judge the hardness of a given dataset. Intuitively, we provide two non- negative numbers that reveal how dense and curved \\(G\\) is on average \\((\\omega (g))\\) and how much this quantity concentrates \\((\\omega^{*}(G))\\) . Our theoretical insights tell us that these quantities should provide information into the hardness of learning a GNN- based solver. We report the generalization error (1 - testing accuracy) of Neuro SAT on the four previously mentioned benchmarks in Section 4.1, alongside the heuristics in Table 2. A (linear) correlation analysis between the error and the (normalized) heuristics reveals that our curvature- based approach serves as a better predictor of generalization: the respective correlation coefficients are \\(\\rho_{\\bar{\\alpha}} = 0.32\\) , \\(\\rho_{\\bar{\\omega}} = 0.86\\) and \\(\\rho_{\\bar{\\omega}^{*}} = 0.98\\) . These results allow us to formally motivate the performance gains during the test- time rewiring procedure discussed previously. What we observe, is that due to its community structure, the CA dataset has a large clause density, but its average curvature is much lower than that of random 4- SAT problems. This is natural, since a community structure is inherently linked with edges that act as less important bottlenecks for message passing [38]. These results show that the ability of GNN- based solvers to learn representations that can learn long range correlations and generalize well is deeply connected with the curvature of the input data, as discussed thoughtful the paper. ## 5 Conclusions Practical Takeaways and Future Work. In this paper, we have shown that the accuracy of GNN- based SAT solvers is directly related to the input data structure. This relationship is universally prevalent across all machine learning applications and as a result we have different modeling principles for different data. For SAT problems, we have identified that the geometry of the input data is a plausible cause of deficiency, due to its connections with oversquashing. What is extremely fascinating is that most modern GNN- based SAT solvers implement some type of recurrence mechanism [28, 35, 40, 46], and this architectural component has been recently shown to be a great starting point to mitigate oversquashing [3]. The implicit effect of recurrence can be immediately noted by comparing the drop in performance between the GCN and Neuro SAT solvers in Table 1.\n\nTable 1: Accuracy of producing satisfying assignments on SAT benchmark datasets [28] of two different GNN-based solvers. By increasing the testing set's curvature at test-time through rewiring, both solvers are able to make big leaps in accuracy, especially in more difficult problems. Reducing the curvature of the problem facilitates long range communication and renders problems easier. The reported results represent the average and standard deviation over 5 different runs, with the best results (for each model) in boldface and absolute improvements in parentheses. <table><tr><td>Model</td><td>Variation</td><td>3-SAT</td><td>4-SAT</td><td>Datasets</td><td>SR</td><td>CA</td></tr><tr><td rowspan=\"2\">GCN</td><td>No Rewiring</td><td>0.510 \u00b1 0.012</td><td>0.180 \u00b1 0.048</td><td>0.470 \u00b1 0.031</td><td>0.650 \u00b1 0.016</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.626 \u00b1 0.021 (+0.116)</td><td>0.374 \u00b1 0.045 (+0.194)</td><td>0.696 \u00b1 0.035 (+0.226)</td><td>0.670 \u00b1 0.048 (+0.020)</td><td></td></tr><tr><td rowspan=\"2\">Neuro SAT</td><td>No Rewiring</td><td>0.690 \u00b1 0.022</td><td>0.436 \u00b1 0.032</td><td>0.734 \u00b1 0.017</td><td>0.746 \u00b1 0.018</td><td></td></tr><tr><td>Test-time Rewiring</td><td>0.820 \u00b1 0.030 (+0.130)</td><td>0.686 \u00b1 0.029 (+0.250)</td><td>0.902 \u00b1 0.004 (+0.168)</td><td>0.828 \u00b1 0.029 (+0.082)</td><td></td></tr></table> Table 2: Average generalization error (1- testing accuracy) over 5 different runs with the Neuro SAT model on SAT benchmark datasets [28]. The error is reported alongside the average clause density \\(\\bar{\\alpha}\\) and the curvature-based heuristics \\(\\bar{\\omega}\\) and \\(\\bar{\\omega}^{*}\\) . Our heuristics display very strong linear correlation with the generalization error, unlike the average clause density, making it possible to predict how hard each benchmark will be for a GNN-based solver. <table><tr><td rowspan=\"2\">Problem</td><td rowspan=\"2\">Generalization Error</td><td colspan=\"3\">Hardness Heuristic</td></tr><tr><td>\u03b1</td><td>\u03c9</td><td>\u03c9*</td></tr><tr><td>3-SAT</td><td>0.31</td><td>4.59</td><td>4.12</td><td>97.41</td></tr><tr><td>4-SAT</td><td>0.56</td><td>9.08</td><td>9.81</td><td>612.32</td></tr><tr><td>SR</td><td>0.27</td><td>6.09</td><td>5.30</td><td>125.30</td></tr><tr><td>CA</td><td>0.25</td><td>9.73</td><td>6.30</td><td>123.27</td></tr></table> This fact leads us to conjecture that the relationship between input data and model performance is prevalent throughout Neural Combinatorial Optimization (NCO) [48], and different architectural designs are necessary for different problems. Furthermore, while we were able to identify a cause for the hardness of learning GNN- based SAT solvers and also provided heuristics to identify the hardness of a given dataset, we did not propose new modeling principles. We report results for some straightforward curvature- aware solvers in Appendix A.4, which do not report consistent improvements. A direct avenue for future work that we consider promising in this field is the application of continuous graph diffusion dynamics for learning[11, 21], which generalize the recurrence mechanism. Finally, regarding the theoretical aspect, we believe that promising avenues for future work that were not directly addressed here are the characterization of both curvature and topological quantities in distribution and not just in the mean. Another direction that is interesting is making a connection between the various critical points of the clause density and the curvature, such that the well- known phase transitions of SAT can be directly related to novel geometric order parameters. Closing Remarks. In conclusion, our results highlight that the limitations of GNN- based SAT solvers cannot be fully understood without considering the geometric properties of the input. Our study presents, to the best of our knowledge, the first attempt at a theoretical understanding of these neural solvers, by establishing a direct connection between their negatively curved graph representations and oversquashing in GNNs. We provide empirical evidence of this connection and verify that it is prevalent on more constrained instances. Beyond SAT, we expect these insights to be valuable for other domains where graph representations of combinatorial problems are employed. Combinatorial Optimization (CO) provides an interesting venue to study the reasoning behavior of NNs, and we hope that this paper makes a case for such studies. In conclusion, we hope that bridging concepts from deep learning, geometry, and physics, will pave the way for principled advances in the design of neural solvers.",
      "level": 2,
      "line_start": 35,
      "line_end": 45
    }
  ]
}