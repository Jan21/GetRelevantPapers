{
  "sections": {
    "hypersat_unsupervised_hypergraph_neural_networks_f": "## Introduction\n\n\nThe neural- guided solvers, on the other hand, integrate neural networks with traditional search frameworks to improve both the efficiency and solution quality of SAT solvers. Along this line, Neuro Core (Selsam & Bj\u00f8rner, 2019) leveraged Graph Neural Networks (GNNs) to predict unsatisfiable cores in SAT instances and further applied this prediction to periodically update the activity score of each variable in the Conflict- Driven Clause Learning (CDCL) solver. NLocal SAT (Zhang et al., 2021) employed a gated graph convolutional network (GGCN) to guide the initialization of assignments in the Stochastic Local Search (SLS) solver. Recently, Neuro Back (Wang et al., 2024) proposed a new approach by utilizing a graph transformer architecture to make offline neural predictions on backbone variable phases for refining the phase selection heuristic in CDCL solvers. For the Max SAT problem, (Liu et al., 2023) was the pioneering work in exploring the use of GNNs for solving the Max SAT problem. This work represented the Max SAT problem as two kinds of factor graphs and studied the capability of typical GNN models in solving the Max SAT problem from a theoretical perspective. Overall, it can be observed that GNNs have shown promising performance in solving both SAT and Max SAT problems, however, the extension of these methods to the Weighted Max SAT problem remains underdeveloped to our best knowledge. The main challenge comes from the uneven weight distribution in the Weighted Max SAT problem. The addition of weights increases the complexity of the search space and also makes the design of effective heuristics more difficult. The neural model needs to learn how to prioritize high- weight clauses over others. Moreover, since a small change in the assignment could result in a significant change in the weighted sum, the hidden structural patterns within Weighted Max SAT instances are much harder to learn. In this work, we consider the problem of designing a neural network solver for Weighted Max SAT problems. Considering the intrinsic difficulties in learning the non- linear dependency and sensitive objective function in the Weighted Max SAT problem, we formulate our learning- based framework Hyper SAT by integrating hypergraph representation, cross- attention mechanism, and multi- objective loss design. We model the Weighted Max SAT instance as a hypergraph. The positive and negative literals of a variable are represented as separate nodes, and each clause is represented as a hyperedge, with an associated weight equal to the weight of the clause. The hypergraph convolutional network (Hyper GCN) is utilized as the learning model, and a specific cross- attention mechanism is designed to capture the logical interplay between the positive and negative literal node features of the same variable. In addition, we design an unsupervised multi- objective loss function to optimize the learning model. Besides the intrinsic optimization objective of the Weighted Max SAT problem, a shared representation constraint ob jective is introduced to ensure that the representations of positive and negative literals of a variable are as distinct as possible in the feature space. We have tested our model on several random Weighted Max SAT datasets in different settings. The experimental results demonstrate that our model outperforms baseline methods across various datasets, achieving significantly better performance. This work provides a new perspective on solving the Weighted Max SAT problem using learning- based methods, with the hope that the results can offer preliminary knowledge into the capability of neural networks for solving Weighted Max SAT problems in the future. In summary, we make the following contributions: - We propose Hyper SAT, an innovative neural approach that uses an unsupervised hypergraph neural network model to solve Weighted Max SAT problems. This is the first work to predict the solution of the Weighted Max SAT problem with Hyper GCN in an end-to-end fashion.- We propose a hypergraph representation for Weighted Max SAT instances and design a cross-attention mechanism along with a shared representation constraint loss function to capture the logical relationships between positive and negative literal nodes within the hypergraph.- We conduct an extensive evaluation of the proposed Hyper SAT on multiple Weighted Max SAT datasets with different distributions. The experimental results demonstrate the superior performance of Hyper SAT. ## 2. Preliminaries We now briefly introduce some relevant preliminaries on Weighted Max SAT and hypergraph neural networks that will be leveraged in later sections. ### 2.1. Weighted Max SAT A Weighted Max SAT instance is represented by a triple \\(\\phi = (\\mathcal{X}, \\mathcal{C}, \\mathbf{w})\\) . Here, the set of variables is denoted by \\(\\mathcal{X} = \\{x_1, x_2, \\ldots , x_n\\}\\) , where each \\(x_i \\in \\{0, 1\\}\\) is a Boolean variable and \\(n\\) is the total number of variables involved in the instance. The set of clauses is given by \\(\\mathcal{C} = \\{C_1, C_2, \\ldots , C_m\\}\\) , where each \\(C_j \\in \\mathcal{C}\\) is a disjunction of literals and \\(m\\) is the number of clauses. The weight vector is given by \\(\\mathbf{w} = (w_1, w_2, \\ldots , w_m)\\) where \\(w_j\\) represents the weight of \\(C_j\\) . Take the clause \\(C_j \\in \\mathcal{C}\\) for example. Suppose it contains \\(k_j\\) literals. Then, it can be represented by \\(C_j = l_{j1} \\lor l_{j2} \\lor \\dots \\lor l_{jk_j}\\) , where each literal \\(l_{ji}\\) is either a variable or the negation of a variable in \\(\\mathcal{X}\\) . The clause \\(C_j\\) is satisfied if at least one of its literals evaluates to true. The clause \\(C_j\\) is unsatisfied if all its literals evaluate to false.\n\n<center>Figure 1. An overview of Hyper SAT framework. </center> An assignment \\(\\boldsymbol {A} = \\{a_{1},a_{2},\\ldots ,a_{n}\\}\\) is to assign each variable \\(x_{i}\\in \\mathcal{X}\\) with a value \\(a_{i}\\in \\{0,1\\}\\) . Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , the objective is to find an assignment \\(\\mathbf{A}\\) for the Boolean variables \\(\\mathcal{X}\\) such that the total weight of satisfied clauses is maximized. The optimization problem is given by \\[\\max_{A} \\sum_{j = 1}^{m} w_{j} \\cdot \\mathbf{1}(C_{j}(\\mathbf{A}))\\] \\[\\mathrm{s.t.} \\mathbf{A} \\in \\{0,1\\}^{n}.\\] Here, \\[\\mathbf{1}(C_{j}(\\mathbf{A})) = \\left\\{ \\begin{array}{ll}1, & \\mathrm{if} C_{j}\\mathrm{is~satisfied~by~}\\mathbf{A},\\\\ 0, & \\mathrm{otherwise}. \\end{array} \\right.\\] ### 2.2. Hypergraph Neural Networks A hypergraph is defined as \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) which includes a set of nodes \\(\\mathcal{V} = \\{v_{1},v_{2},\\dots ,v_{|\\mathcal{V}|}\\}\\) and a set of hyperedges \\(\\mathcal{E} = \\{e_{1},e_{2},\\dots ,e_{|\\mathcal{E}|}\\}\\) . \\(\\mathbf{W}\\) is a diagonal matrix of edge weights which assigns a weight to each hyperedge. Each hyperedge \\(e_{j}\\) is a non- empty subset of nodes (i.e., \\(\\emptyset \\neq e_{j}\\subseteq \\mathcal{V}\\) ). The hypergraph \\(\\mathcal{G}\\) can be represented as a \\(|\\mathcal{V}|\\times |\\mathcal{E}|\\) incidence matrix \\(\\boldsymbol{H}\\) , where \\(\\boldsymbol{H}_{i,j} = 1\\) if \\(v_{i}\\in e_{j}\\) and 0 otherwise. For a vertex \\(v_{i}\\in \\mathcal{V}\\) , its degree is defined as \\(d(v_{i}) = \\sum_{j = 1}^{|\\mathcal{E}|}\\boldsymbol{H}_{i,j}\\boldsymbol{W}_{j,j}\\) . For an edge \\(e_{j}\\in \\mathcal{E}\\) , its degree is defined as \\(\\delta (e_{j}) = \\sum_{i = 1}^{|\\mathcal{V}|}\\boldsymbol{H}_{i,j}\\) . Additionally, \\(\\boldsymbol{D}_{v}\\) and \\(\\boldsymbol{D}_{e}\\) denote the diagonal matrices of the vertex degrees and the edge degrees, respectively. Hypergraph Neural Networks (HGNN) (Feng et al., 2019) perform data representation learning by utilizing a hypergraph structure to capture higher- order dependencies between nodes. In HGNN, hyperedge convolution operations are used to extract features by leveraging the hypergraph Laplacian for spectral convolution. To reduce computational complexity, Chebyshev polynomials are applied to approximate the spectral convolution and avoid the need to compute high- order eigenvectors explicitly. Through the hyperedge convolution operation, the \\(l\\) - th layer of HGNN can be formulated by \\[\\boldsymbol{X}^{(l + 1)} = \\sigma \\left(\\boldsymbol{D}_{v}^{-1 / 2}\\boldsymbol {H}\\boldsymbol {W}\\boldsymbol{D}_{e}^{-1}\\boldsymbol {H}^{\\top}\\boldsymbol{D}_{v}^{-1 / 2}\\boldsymbol {X}^{(l)}\\boldsymbol {\\Theta}^{(l)}\\right),\\] where \\(\\boldsymbol{X}^{(l)}\\in \\mathbb{R}^{|\\mathcal{V}|\\times d_{l}}\\) is the signal of the hypergraph at \\(l\\) layer with \\(|\\mathcal{V}|\\) nodes and \\(d_{l}\\) dimensional features, \\(\\boldsymbol{X}^{(0)}\\) is the original signal of the hypergraph. \\(\\Theta^{(l)}\\) is the learnable filter parameter and \\(\\sigma\\) denotes the nonlinear activation function. ## 3. Hyper SAT In this section, we propose Hyper SAT, a neural approach for Weighted Max SAT problems. Figure 1 illustrates the workflow of Hyper SAT. It mainly consists of three modules: the hypergraph modeling module, the neural network module, and the probabilistic mapping module. Given a Weighted Max SAT instance, Hyper SAT first applies its hypergraph modeling component to represent the instance as a hypergraph. Then, the hypergraph is solved by the neural network module, which is responsible for processing and optimizing the hypergraph. Finally, through a mapping operation, the probabilistic output of the neural network module is mapped into Boolean values, which serves as a solution to the Weighted Max SAT instance. Note that the uneven weight of clauses increases the nonlinear dependency and sensitivity among variables, the neural network is required to learn how to prioritize the contributions of different clauses. To this end, we propose a specific cross- attention mechanism and introduce an unsupervised multi- objective loss function to capture the logical interplay between the positive and negative literal node representations. These two mechanisms are integrated with the hypergraph convolutional network to form the neural network module of our Hyper SAT.\n\n### 3.1. Hypergraph Modeling Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , we construct the hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) as follows: For the construction of the vertex set \\(\\nu\\) , each variable \\(x_{i}\\in \\mathcal{X}\\) is represented by two nodes \\(v_{i}\\) and \\(v_{n + i}\\) , which correspond to the positive and negative literals of the variable \\(x_{i}\\) (i.e., \\(x_{i}\\) and \\(\\neg x_{i}\\) ), respectively. For the construction of the hyperedge set \\(\\mathcal{E}\\) , each clause \\(C_{j}\\in \\mathcal{C}\\) is represented by a hyperedge \\(e_{j}\\in \\mathcal{E}\\) , which connects the nodes corresponding to the literals in \\(C_{j}\\) . Specifically, if clause \\(C_{j}\\) consists of the literals \\(l_{j1},l_{j2},\\ldots ,l_{jk_{j}}\\) , then the corresponding hyperedge \\(e_{j}\\) connects the nodes corresponding to these literals. For the construction of the weight matrix \\(\\mathbf{W}\\) , the weight of each hyperedge is equal to the weight of the corresponding clause, i.e., \\(W_{j,j} = w_{j}\\) . Through the above modeling process, the Weighted Max SAT instance can be uniquely represented by a hypergraph, where each literal is a node, and each clause is a hyperedge connecting the literals involved. The weight of each hyperedge is the weight of the corresponding clause in the instance. Figure 2 gives an illustration of the hypergraph modeling process. <center>Figure 2. Hypergraph Modeling of a Weighted Max SAT instance \\((\\neg x_{1}\\lor x_{2}\\lor \\neg x_{3})\\land (x_{1}\\lor x_{2})\\land (\\neg x_{2}\\lor x_{3}\\lor x_{4})\\land (\\neg x_{1}\\lor x_{3}\\lor \\neg x_{4})\\) with 8 literals, 4 clauses and weights \\(\\{w_{1},w_{2},w_{3},w_{4}\\}\\) . </center> In contrast to existing methods that represent Conjunctive Normal Form (CNF) formulas as factor graphs (Guo et al., 2023), which limit their ability to model relationships beyond pairwise connections, we construct Weighted Max SAT instances as hypergraphs. These hypergraphs can encode higher- order variable dependencies through their degree- free hyperedges, and thus offer more powerful representational capacity and enable more efficient handling of higher- order relationships in combinatorial optimization problems. In particular, we treat literals rather than variables as nodes, which addresses a major issue present in Hyp Op (Heydaribeni et al., 2024). Since the logical relationships between positive and negative literals are central to the Weighted Max SAT problem, it is essential to treat them as distinct nodes, rather than merging them into a single node. In addition, we encode the weights of the clauses in the Weighted Max SAT problem as the weights of the corresponding hyperedges in the hypergraph. ### 3.2. Neural Network Architecture The neural network architecture of our proposed Hyper SAT comprises the Hyper GCN, a transformer module with the cross- attention mechanism, and a softmax layer. #### 3.2.1. HYPERGRAPH CONVOLUTIONAL NETWORKS In Hyper SAT, we introduce a \\(T\\) - layer Hyper GCN for message passing among different nodes. The operation at the \\(l\\) - th layer of the Hyper GCN is formally expressed as follows: \\[\\begin{array}{r}{\\pmb{L}^{(l + 1)^{\\prime}} = \\sigma \\left(\\pmb{D}_{v}^{-\\frac{1}{2}}\\widetilde{\\pmb{Q}}\\pmb{D}_{v}^{-\\frac{1}{2}}\\pmb{L}^{(l)}\\pmb{R}^{(l)}\\right).} \\end{array} \\quad (2)\\] In this formulation, the matrix \\(\\pmb{L}^{(l + 1)^{\\prime}}\\) represents the output of the \\(l\\) - th layer; \\(\\pmb{D}_{v}\\) is the diagonal matrix of the vertex degrees in the hypergraph; \\(\\pmb{L}^{(l)}\\in \\mathbb{R}^{2n\\times d_{l}}\\) is the matrix of node representations at the \\(l\\) - th layer, where \\(d_{l}\\) is the dimension of the \\(l\\) - th layer node representations; \\(\\pmb{R}^{(l)}\\in\\) \\(\\mathbb{R}^{d_{l}\\times d_{l + 1}}\\) is the \\(l\\) - th layer learnable weight matrix; and \\(\\widetilde{\\pmb{Q}}\\) is given by \\[\\widetilde{\\pmb{Q}} = \\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top} - \\mathrm{diag}(\\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top}), \\quad (3)\\] where \\(\\pmb{H}\\) is the hypergraph incidence matrix, and \\(\\widetilde{\\pmb{D}}_{e} =\\) \\(\\pmb{D}_{e} - \\pmb{I}\\) . Specifically, \\(\\sigma\\) denotes the nonlinear activation function and \\(\\pmb{L}^{(0)}\\) is a learnable input embedding of Hyper GCN. Compared to the updating rule in Eq. (1) in traditional HGNN, our Hyper GCN focuses the convolutional layer's computation more on the influence of adjacent nodes by removing the diagonal elements of \\(\\widetilde{\\pmb{Q}}\\) . This adjustment allows the representation updates of each node to better align with the higher- order relationships of the adjacency structure. #### 3.2.2. CROSS-ATTENTION MECHANISM The core of the Weighted Max SAT problem lies in the logical constraints among variables. Positive and negative literal nodes (e.g., \\(x\\) and \\(\\neg x\\) ) are logically mutually exclusive and strongly correlated, and their relationships directly reflect the underlying structural characteristics of the problem.\n\nConsidering this, we leverage a cross- attention mechanism to dynamically assign importance weights to the relationships between each pair of complementary literals. This mechanism enables the model to adaptively capture critical logical properties such as mutual exclusivity, dependency, and the process of information exchange between positive and negative literal nodes. Therefore, after updating each node representation in the hypergraph convolution layer, a cross- attention layer is introduced to enhance the representations of positive and negative literal nodes. Specifically, given the \\(l\\) - th layer output of the Hyper GCN \\(\\pmb{L}^{(l + 1)}\\) , we divide it into two parts: \\(\\pmb{L}^{(l + 1)^{\\prime}} =\\) \\(\\left[\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\right]\\) , where \\(\\pmb{L}_{+}^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the positive literal node representations and \\(\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the negative literal node representations. The cross- attention mechanism is mathematically represented as follows: \\[\\begin{array}{r l} & {\\pmb{L}_{+}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{+}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{-}^{(l + 1)},}\\\\ & {\\pmb{L}_{-}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{-}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{+}^{(l + 1)}.} \\end{array} \\quad (4)\\] In this formulation, \\[\\begin{array}{r l} & {\\pmb{Q}_{+}^{(l + 1)} = \\pmb{W}_{Q}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{Q}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{K}_{+}^{(l + 1)} = \\pmb{W}_{K}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{K}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{V}_{+}^{(l + 1)} = \\pmb{W}_{V}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{V}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},} \\end{array} \\quad (5)\\] where \\(\\pmb{W}_{Q}^{(l + 1)}\\) , \\(\\pmb{W}_{K}^{(l + 1)}\\) , \\(\\pmb{W}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrices for the positive literal nodes at the \\(l\\) - th layer, and \\(\\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrix for the negative literal nodes at the \\(l\\) - th layer. The final node representation at the \\(l\\) - th layer is obtained as \\(\\pmb{L}^{(l + 1)} = [\\pmb{L}_{+}^{(l + 1)},\\pmb{L}_{- }^{(l + 1)}]\\) . The cross- attention mechanism explicitly constructs the interaction between positive and negative literal nodes by enabling each node to incorporate the features of its counterpart. This facilitates a more comprehensive representation of node features for Weighted Max SAT problems. Building upon this, the transformer module in Hyper SAT incorporates the previously discussed cross- attention layer, along with other components to further enhance the modeling capability. Inspired by the recent advancements in Vision Transformer architecture (Vi T- 22B (Dehghani et al., 2023)), the transformer module in Hyper SAT includes a Layer Norm layer (Ba, 2016), followed by a combination of a cross- attention layer and a Feed- Forward Network (FFN) layer, along with a residual connection and an additional Layer Norm layer. The FFN and cross- attention layers operate in parallel to enhance the efficiency. The residual connection is introduced by adding the outputs from the cross- attention layer, the FFN layer, and the Layer Norm layer. The resulting sum is then passed through another Layer Norm layer to stabilize the representations before proceeding to the next layer. The final layer of the network is a softmax layer. We reshape the iterated \\(\\pmb{L}^{(T)^{\\prime}}\\in \\mathbb{R}^{2n\\times 1}\\) into \\(\\hat{\\pmb{L}}^{(T)^{\\prime}}\\in \\mathbb{R}^{n\\times 2}\\) , which serves as the input to the softmax layer. After applying the softmax function, we obtain \\(\\hat{\\pmb{L}} = \\mathrm{softmax}(\\hat{\\pmb{L}}^{(T)^{\\prime}})\\) , which provides the soft node assignments for each literal, interpreted as class probabilities. The probability of assigning a variable to a truth value is recorded by \\(\\pmb{Y} = \\hat{\\pmb{L}}_{- 1}\\) , which is the first column of \\(\\hat{\\pmb{L}}\\) . #### 3.2.3. LOSS FUNCTION Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) and its hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) , we relax the Boolean variables \\(\\mathcal{X}\\) into continuous probability parameters \\(\\mathbf{Y}(\\gamma)\\) where \\(\\gamma = (\\pmb {R},\\pmb {L}^{(0)},\\pmb {W}_{Q},\\pmb {W}_{K},\\pmb {W}_{V},\\widetilde{\\pmb{W}}_{Q},\\widetilde{\\pmb{W}}_{K},\\widetilde{\\pmb{W}}_{V})\\) represent the learnable parameters. The relaxation is defined as follows: \\[\\mathcal{X}\\in \\{0,1\\}^{n}\\longrightarrow \\mathbf{Y}(\\gamma)\\in [0,1]^{n}. \\quad (6)\\] With this relaxation, we can design a differentiable loss function to optimize the learnable parameters \\(\\gamma\\) , enabling gradient- based optimization. In this paper, we propose an unsupervised multi- objective loss function that consists of two components: a primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) and a shared representation constraint loss \\(\\mathcal{L}_{\\mathrm{shared}}\\) . This unsupervised loss function obviates the necessity for large, labeled training datasets, which are conventionally indispensable in supervised learning paradigms. The specific form is as follows: \\[\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{task}} + \\lambda \\mathcal{L}_{\\mathrm{shared}}, \\quad (7)\\] where \\(\\lambda \\geq 0\\) is a balancing hyperparameter. The primary task loss function is the relaxed optimization objective of the Weighted Max SAT problem, as shown below: \\[\\mathcal{L}_{\\mathrm{task}}(\\mathbf{Y}) = \\sum_{j = 1}^{m}w_{j}V_{j}(\\mathbf{Y}), \\quad (8)\\] where \\[V_{j}(\\mathbf{Y}) = 1 - \\prod_{i\\in C_{j}^{+}}(1 - y_{i})\\prod_{i\\in C_{j}^{-}}y_{i}. \\quad (9)\\] Here, \\(C_{j}^{+}\\) and \\(C_{j}^{- }\\) are the index sets of variables appearing in the clause \\(C_{j}\\) in the positive and negative form, respectively. The term \\(V_{j}(\\mathbf{Y})\\) represents the satisfaction of clause \\(C_{j}\\) , where a value of 1 indicates that the clause is satisfied, and 0 indicates it is not. The weight \\(w_{j}\\) ensures that\n\nmore important clauses are prioritized during optimization. Minimizing the primary task loss function corresponds to maximizing the weighted sum of satisfied clauses in the Weighted Max SAT problem. The shared representation constraint loss function focuses on the representations of positive and negative literal nodes in the penultimate layer of the Hyper SAT network. Its form is given by \\[\\mathcal{L}_{\\mathrm{shared}} = \\left\\| \\pmb{L}_{+}^{(T - 1)} + \\pmb{L}_{-}^{(T - 1)}\\right\\|_{F}^{2}, \\quad (10)\\] where \\(\\left\\| \\cdot \\right\\|_{F}\\) denotes the Frobenius norm. By applying the shared representation constraint loss function, the network encourages the positive and negative literal nodes to develop distinct feature representations in the learning process. This constraint enhances the separation of the two types of nodes in the feature space. It should be noted that most neural network- based SAT methods employ supervised learning. However, supervised learning approaches are not particularly well- suited for solving the Weighted Max SAT problem. On the one hand, a Weighted Max SAT instance may have multiple satisfying assignments. On the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the Weighted Max SAT instance. From this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances. ### 3.3. Probabilistic Mapping The neural network module takes the hypergraph as input and produces a probability vector \\(\\mathbf{Y}\\) as output, which indicates the probability of each variable taking the truth value. Accordingly, to convert the probability vector \\(\\mathbf{Y}\\) into a Boolean assignment, we transform \\(\\mathbf{Y}\\) into \\(n\\) Bernoulli distributions, where the \\(i\\) - th distribution \\(B(y_{i})\\) is over the discrete values \\(\\{0,1\\}\\) . Finally, the node assignments are generated by sampling with the corresponding probability distributions. ## 4. Experiment ### 4.1. Experimental Settings We compare the performance of Hyper SAT against GNN- based methods for solving Weighted Max SAT problems. Baseline Algorithms. We compare our proposed Hyper SAT against GNN- based methods. The following algorithms are considered as baselines: (i) Hyp Op (Heydaribeni et al., 2024): an advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs; (ii) (Liu et al., 2023): an innovative supervised GNN- based approach that predicts solutions in an end- to- end manner by transforming CNF formulas into factor graphs. We apply these baselines to solve Weighted Max SAT problems. Datasets. We evaluate the algorithms using random 3- SAT benchmarks from the SATLIB dataset (Hoos & St\u00fctzle, 2000). The SATLIB dataset is one of the standard datasets for evaluating SAT solvers. We utilize a range of datasets with varying distributions, from which SAT and UNSAT instances are generated for each distribution. The number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. More details on the datasets are provided in Table 1. In particular, to generate the required Weighted Max SAT instances, we assign weights to the clauses in each CNF file within the dataset. These weights are sampled from the integers in the range \\([1,10]\\) uniformly at random. Table 1. The parameters of the datasets. <table><tr><td>DATASET</td><td>INSTANCE</td><td>VARS</td><td>CLAUSES</td><td>TYPES</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>SAT</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>UNSAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>SAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>UNSAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>SAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>UNSAT</td></tr></table> Model Settings. Hyper GCN is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. The dimension of the cross- attention layer is the square root of the number of variables. The dropout used in the cross- attention layer is 0.1. The model is optimized using the Adam optimizer (Kingma, 2014) with a learning rate of \\(7 \\times 10^{- 2}\\) . The balancing hyperparameter of loss function \\(\\lambda\\) is \\(2 \\times 10^{- 3}\\) . The FFN consists of two linear transformations and a Re LU activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. An early stopping strategy is employed, with a tolerance of \\(10^{- 4}\\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. Finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained. Evaluation Configuration. All experiments are conducted on an NVIDIA A100- SXM4- 40GB GPU, using Python 3.9.30 and Py Torch 1.13.0.\n\n### 4.2. Analytical Experiment We first evaluate the performance of unsupervised methods, Hyper SAT and Hyp Op, with a focus on their convergence. We evaluate the convergence using the primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) from Eq. (8), which represents the sum of the weights of unsatisfied clauses. We illustrate the evolution curves of loss for Hyper SAT and Hyp Op on the uuf250- 1065 dataset in Figure 3 as an example. All models can converge within 300 epochs. Moreover, the loss of Hyper SAT is around 52, while the loss of Hyp Op is around 139. We can observe that Hyper SAT achieves better performance than Hyp Op. Specifically, Hyper SAT decreases the loss more quickly and achieves a lower loss value. The experimental results demonstrate that Hyper SAT can be used in learning to solve Weighted Max SAT problems. <center>Figure 3. The evolution of loss for Hyper SAT and Hyp Op during an inference process of 300 epochs on the uuf250-1065 dataset. </center> ### 4.3. Result We evaluate the performance of Hyper SAT against baseline algorithms Hyp Op and (Liu et al., 2023) on various datasets. The primary evaluation metric is the average weighted sum of unsatisfied clauses. The experiments are conducted on datasets with the number of variables ranging from 100 to 250, and the number of clauses varying between 430 and 1065. The results are shown in Table 2. As presented in the table, Hyper SAT consistently achieves lower values for the average weighted sum of unsatisfied clauses compared to the baseline algorithms (Liu et al., 2023) and Hyp Op across multiple datasets. For example, on the uf100- 430 dataset, Hyper SAT significantly outperforms both baselines with a result of 15.64. This result represents a substantial improvement over the results of (Liu et al., 2023) (32.48) and Hyp Op (99.15). This trend is observed across all datasets, where Hyper SAT always exceeds the Table 2. The average weighted sum of unsatisfied clauses of Weighted Max SAT problems. <table><tr><td>DATASET</td><td>LIU ET AL. (2023)</td><td>HYPOP</td><td>HYPERSAT</td></tr><tr><td>UF100-430</td><td>32.48</td><td>99.15</td><td>15.64</td></tr><tr><td>UUF100-430</td><td>41.65</td><td>102.44</td><td>20.46</td></tr><tr><td>UF200-860</td><td>67.38</td><td>158.46</td><td>28.98</td></tr><tr><td>UUF200-860</td><td>81.68</td><td>171.34</td><td>35.55</td></tr><tr><td>UF250-1065</td><td>79.06</td><td>170.60</td><td>33.24</td></tr><tr><td>UUF250-1065</td><td>100.04</td><td>182.39</td><td>41.64</td></tr></table> performance of the baselines. The average weight of unsatisfied clauses is reduced by approximately \\(50\\%\\) compared to (Liu et al., 2023) and over \\(80\\%\\) compared to Hyp Op. On the uf200- 860 and uuf200- 860 datasets, Hyper SAT achieves reductions of \\(56.99\\%\\) and \\(56.48\\%\\) compared to (Liu et al., 2023), respectively, and reductions of \\(81.71\\%\\) and \\(79.25\\%\\) compared to Hyp Op. Similarly, on the uf250- 1065 and uuf250- 1065 datasets, Hyper SAT continues to outperform the baselines. The reductions compared to (Liu et al., 2023) are \\(57.96\\%\\) and \\(58.38\\%\\) , respectively, while the reductions compared to Hyp Op are \\(80.52\\%\\) and \\(77.17\\%\\) . Importantly, even with the larger datasets, Hyper SAT maintains or even enhances its efficacy. This demonstrates that Hyper SAT scales well and performs better. These results show that Hyper SAT consistently provides substantial improvements over both baseline algorithms across a range of datasets, including those with larger problem sizes. As a result, it validates its robustness and effectiveness in reducing the weighted sum of unsatisfied clauses. ### 4.4. Ablation Study We conduct an ablation study to evaluate the contribution of each key component in our proposed model. By systematically removing components, we analyze their impact on performance and highlight the importance of each component. The experiments are designed to isolate the impact of the following components: (i) the hypergraph modeling of literal nodes rather than variable nodes; (ii) the transformer module with the cross- attention mechanism; (iii) the shared representation constraint loss. The results of the ablation study are shown in Table 3. Effect of Hypergraph Modeling of Literal Nodes: The performance drops by \\(52.77\\%\\) when the hypergraph modeling of variable nodes is used instead of literal nodes. The results demonstrate the importance and superiority of modeling the Weighted Max SAT instance as a hypergraph with literal nodes. Effect of Transformer with Cross- Attention: We disable the transformer module with the cross- attention mechanism to assess its importance. Without this module, the model experiences a performance drop of \\(11.54\\%\\) . This signifi\n\nTable 3. The results of the ablation study. We consider three components: (i) HGM-L: the hypergraph modeling of literal nodes rather than variable nodes; (ii) Transformer: the transformer module with the cross-attention mechanism; (iii) SRCL: the shared representation constraint loss. Specifically, the first row in the table represents the transformation of the Weighted Max SAT instance into the hypergraph of variables. <table><tr><td>HGM-L</td><td>TRANSFORMER</td><td>SRCL</td><td>RESULT</td></tr><tr><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td><td>182.39</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u00d7</td><td>86.14</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u00d7</td><td>64.08</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u221a</td><td>47.07</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u221a</td><td>41.64</td></tr></table> cantly reduces its ability to effectively capture dependencies across each pair of complementary literals, leading to lower overall accuracy. Effect of Share Representation Constraint Loss: We further remove the shared representation constraint loss and optimize the model with the primary task loss function to investigate the role of the unsupervised multi- objective loss. The results show that without the shared representation constraint loss, the model achieves a performance of 64.08. This performance is lower than that of the original configuration. Therefore, it highlights the importance of the shared representation constraint loss, which encourages the positive and negative literal nodes to develop distinct feature representations. In summary, the full model consistently outperforms the ablated versions, demonstrating the synergistic effect of integrating the hypergraph modeling of literal nodes, cross- attention mechanism, and shared representation constraint loss design. ## 5. Conclusion In this work, we propose Hyper SAT, a novel neural approach for Weighted Max SAT problems, addressing the challenges associated with learning the complex non- linear dependencies and sensitive objective function of this NP- hard problem. By modeling Weighted Max SAT instances as hypergraphs, a hypergraph convolutional network is introduced as the learning model, coupled with a cross- attention mechanism to capture the logical relationships between positive and negative literals. The proposed framework incorporates an unsupervised multi- objective loss design, which not only optimizes the intrinsic objectives of the Weighted Max SAT problem but also ensures that the representations of positive and negative literals remain distinct in the feature space. Extensive experiments demonstrate the potential of Hyper SAT. The experimental results show that Hyper SAT is effective in solving Weighted Max SAT instances and outperforms state- of- the- art competitors in terms of performance. This work offers a fresh perspective on solving the Weighted Max SAT problem through learning- based methods. Future work will focus on further integrating the model with heuristic solvers, improving the design of well- crafted solvers, and exploring its applications in other complex combinatorial problems. ## Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ## References Allouche, D., Traor\u00e9, S., Andr\u00e9, I., De Givry, S., Katsirelos, G., Barbe, S., and Schiex, T. Computational protein design as a cost function network optimization problem. In International Conference on Principles and Practice of Constraint Programming, pp. 840- 849. Springer, 2012. Audemard, G. and Simon, L. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27 (1):1- 25, 2018. Ba, J. L. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Biere, A. and Fleury, M. Gimsatul, isasat and kissat entering the sat competition 2022. Proceedings of SAT Competition, pp. 10- 11, 2022. Cai, S. and Zhang, X. Deep cooperation of cdc1 and local search for sat. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, pp. 64- 81. Springer, 2021. Cameron, C., Chen, R., Hartford, J., and Leyton- Brown, K. Predicting propositional satisfiability via end- to- end learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3324- 3331, 2020. Clarke, E., Biere, A., Raimi, R., and Zhu, Y. Bounded model checking using satisfiability solving. Formal methods in system design, 19:7- 34, 2001. Cook, S. A. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, pp. 151- 158, 1971. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers\n\nto 22 billion parameters. In International Conference on Machine Learning, pp. 7480- 7512. PMLR, 2023. Feng, Y., You, H., Zhang, Z., Ji, R., and Gao, Y. Hypergraph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 3558- 3565, 2019. Guo, W., Zhen, H.- L., Li, X., Luo, W., Yuan, M., Jin, Y., and Yan, J. Machine learning methods in solving the boolean satisfiability problem. Machine Intelligence Research, 20 (5):640- 655, 2023. Heydaribeni, N., Zhan, X., Zhang, R., Eliassi- Rad, T., and Koushanfar, F. Distributed constrained combinatorial optimization leveraging hypergraph neural networks. Nature Machine Intelligence, pp. 1- 9, 2024. Hoos, H. H. and St\u00fctzle, T. SATLIB: An online resource for research on sat. Sat, 2000:283- 292, 2000. Kautz, H. A., Selman, B., et al. Planning as satisfiability. In ECAI, volume 92, pp. 359- 363. Citeseer, 1992. Kingma, D. P. Adam: A method for stochastic optimization. ar Xiv preprint ar Xiv:1412.6980, 2014. Liu, M., Huang, P., Jia, F., Zhang, F., Sun, Y., Cai, S., Ma, F., and Zhang, J. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 16264- 16265, 2023. Ozolins, E., Freivalds, K., Draguns, A., Gaile, E., Zakovskis, R., and Kozlovics, S. Goal- aware neural sat solver. In 2022 International Joint Conference on Neural Networks, pp. 1- 8. IEEE, 2022. Selsam, D. and Bj\u00f8rner, N. Guiding high- performance sat solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, pp. 336- 353. Springer, 2019. Selsam, D., Lamm, M., B\u00fcnz, B., Liang, P., de Moura, L., and Dill, D. L. Learning a sat solver from single- bit supervision. In 7th International Conference on Learning Representations, 2019. Sun, L., Gerault, D., Benamira, A., and Peyrin, T. Neurogift: Using a machine learning based sat solver for cryptanalysis. In Cyber Security Cryptography and Machine Learning: Fourth International Symposium, Proceedings 4, pp. 62- 84. Springer, 2020. Thornton, J. and Sattar, A. Dynamic constraint weighting for over- constrained problems. In Pacific Rim International Conference on Artificial Intelligence, pp. 377- 388. Springer, 1998. Timm, N. and Botha, J. Synthesis of cost- optimal multiagent systems for resource allocation. ar Xiv preprint ar Xiv:2209.09473, 2022. Wang, W., Hu, Y., Tiwari, M., Khurshid, S., Mc Millan, K., and Miikkulainen, R. Neuroback: Improving CDCL SAT solving using graph neural networks. In The Twelfth International Conference on Learning Representations, 2024. Yang, Q., Wu, K., and Jiang, Y. Learning action models from plan examples using weighted max- sat. Artificial Intelligence, 171(2- 3):107- 143, 2007. Zhang, W., Sun, Z., Zhu, Q., Li, G., Cai, S., Xiong, Y., and Zhang, L. Nlocalsat: boosting local search with solution prediction. In Proceedings of the Twenty- Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 1177- 1183, 2021.",
    "introduction": "The neural- guided solvers, on the other hand, integrate neural networks with traditional search frameworks to improve both the efficiency and solution quality of SAT solvers. Along this line, Neuro Core (Selsam & Bj\u00f8rner, 2019) leveraged Graph Neural Networks (GNNs) to predict unsatisfiable cores in SAT instances and further applied this prediction to periodically update the activity score of each variable in the Conflict- Driven Clause Learning (CDCL) solver. NLocal SAT (Zhang et al., 2021) employed a gated graph convolutional network (GGCN) to guide the initialization of assignments in the Stochastic Local Search (SLS) solver. Recently, Neuro Back (Wang et al., 2024) proposed a new approach by utilizing a graph transformer architecture to make offline neural predictions on backbone variable phases for refining the phase selection heuristic in CDCL solvers. For the Max SAT problem, (Liu et al., 2023) was the pioneering work in exploring the use of GNNs for solving the Max SAT problem. This work represented the Max SAT problem as two kinds of factor graphs and studied the capability of typical GNN models in solving the Max SAT problem from a theoretical perspective. Overall, it can be observed that GNNs have shown promising performance in solving both SAT and Max SAT problems, however, the extension of these methods to the Weighted Max SAT problem remains underdeveloped to our best knowledge. The main challenge comes from the uneven weight distribution in the Weighted Max SAT problem. The addition of weights increases the complexity of the search space and also makes the design of effective heuristics more difficult. The neural model needs to learn how to prioritize high- weight clauses over others. Moreover, since a small change in the assignment could result in a significant change in the weighted sum, the hidden structural patterns within Weighted Max SAT instances are much harder to learn. In this work, we consider the problem of designing a neural network solver for Weighted Max SAT problems. Considering the intrinsic difficulties in learning the non- linear dependency and sensitive objective function in the Weighted Max SAT problem, we formulate our learning- based framework Hyper SAT by integrating hypergraph representation, cross- attention mechanism, and multi- objective loss design. We model the Weighted Max SAT instance as a hypergraph. The positive and negative literals of a variable are represented as separate nodes, and each clause is represented as a hyperedge, with an associated weight equal to the weight of the clause. The hypergraph convolutional network (Hyper GCN) is utilized as the learning model, and a specific cross- attention mechanism is designed to capture the logical interplay between the positive and negative literal node features of the same variable. In addition, we design an unsupervised multi- objective loss function to optimize the learning model. Besides the intrinsic optimization objective of the Weighted Max SAT problem, a shared representation constraint ob jective is introduced to ensure that the representations of positive and negative literals of a variable are as distinct as possible in the feature space. We have tested our model on several random Weighted Max SAT datasets in different settings. The experimental results demonstrate that our model outperforms baseline methods across various datasets, achieving significantly better performance. This work provides a new perspective on solving the Weighted Max SAT problem using learning- based methods, with the hope that the results can offer preliminary knowledge into the capability of neural networks for solving Weighted Max SAT problems in the future. In summary, we make the following contributions: - We propose Hyper SAT, an innovative neural approach that uses an unsupervised hypergraph neural network model to solve Weighted Max SAT problems. This is the first work to predict the solution of the Weighted Max SAT problem with Hyper GCN in an end-to-end fashion.- We propose a hypergraph representation for Weighted Max SAT instances and design a cross-attention mechanism along with a shared representation constraint loss function to capture the logical relationships between positive and negative literal nodes within the hypergraph.- We conduct an extensive evaluation of the proposed Hyper SAT on multiple Weighted Max SAT datasets with different distributions. The experimental results demonstrate the superior performance of Hyper SAT. ## 2. Preliminaries We now briefly introduce some relevant preliminaries on Weighted Max SAT and hypergraph neural networks that will be leveraged in later sections. ### 2.1. Weighted Max SAT A Weighted Max SAT instance is represented by a triple \\(\\phi = (\\mathcal{X}, \\mathcal{C}, \\mathbf{w})\\) . Here, the set of variables is denoted by \\(\\mathcal{X} = \\{x_1, x_2, \\ldots , x_n\\}\\) , where each \\(x_i \\in \\{0, 1\\}\\) is a Boolean variable and \\(n\\) is the total number of variables involved in the instance. The set of clauses is given by \\(\\mathcal{C} = \\{C_1, C_2, \\ldots , C_m\\}\\) , where each \\(C_j \\in \\mathcal{C}\\) is a disjunction of literals and \\(m\\) is the number of clauses. The weight vector is given by \\(\\mathbf{w} = (w_1, w_2, \\ldots , w_m)\\) where \\(w_j\\) represents the weight of \\(C_j\\) . Take the clause \\(C_j \\in \\mathcal{C}\\) for example. Suppose it contains \\(k_j\\) literals. Then, it can be represented by \\(C_j = l_{j1} \\lor l_{j2} \\lor \\dots \\lor l_{jk_j}\\) , where each literal \\(l_{ji}\\) is either a variable or the negation of a variable in \\(\\mathcal{X}\\) . The clause \\(C_j\\) is satisfied if at least one of its literals evaluates to true. The clause \\(C_j\\) is unsatisfied if all its literals evaluate to false.\n\n<center>Figure 1. An overview of Hyper SAT framework. </center> An assignment \\(\\boldsymbol {A} = \\{a_{1},a_{2},\\ldots ,a_{n}\\}\\) is to assign each variable \\(x_{i}\\in \\mathcal{X}\\) with a value \\(a_{i}\\in \\{0,1\\}\\) . Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , the objective is to find an assignment \\(\\mathbf{A}\\) for the Boolean variables \\(\\mathcal{X}\\) such that the total weight of satisfied clauses is maximized. The optimization problem is given by \\[\\max_{A} \\sum_{j = 1}^{m} w_{j} \\cdot \\mathbf{1}(C_{j}(\\mathbf{A}))\\] \\[\\mathrm{s.t.} \\mathbf{A} \\in \\{0,1\\}^{n}.\\] Here, \\[\\mathbf{1}(C_{j}(\\mathbf{A})) = \\left\\{ \\begin{array}{ll}1, & \\mathrm{if} C_{j}\\mathrm{is~satisfied~by~}\\mathbf{A},\\\\ 0, & \\mathrm{otherwise}. \\end{array} \\right.\\] ### 2.2. Hypergraph Neural Networks A hypergraph is defined as \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) which includes a set of nodes \\(\\mathcal{V} = \\{v_{1},v_{2},\\dots ,v_{|\\mathcal{V}|}\\}\\) and a set of hyperedges \\(\\mathcal{E} = \\{e_{1},e_{2},\\dots ,e_{|\\mathcal{E}|}\\}\\) . \\(\\mathbf{W}\\) is a diagonal matrix of edge weights which assigns a weight to each hyperedge. Each hyperedge \\(e_{j}\\) is a non- empty subset of nodes (i.e., \\(\\emptyset \\neq e_{j}\\subseteq \\mathcal{V}\\) ). The hypergraph \\(\\mathcal{G}\\) can be represented as a \\(|\\mathcal{V}|\\times |\\mathcal{E}|\\) incidence matrix \\(\\boldsymbol{H}\\) , where \\(\\boldsymbol{H}_{i,j} = 1\\) if \\(v_{i}\\in e_{j}\\) and 0 otherwise. For a vertex \\(v_{i}\\in \\mathcal{V}\\) , its degree is defined as \\(d(v_{i}) = \\sum_{j = 1}^{|\\mathcal{E}|}\\boldsymbol{H}_{i,j}\\boldsymbol{W}_{j,j}\\) . For an edge \\(e_{j}\\in \\mathcal{E}\\) , its degree is defined as \\(\\delta (e_{j}) = \\sum_{i = 1}^{|\\mathcal{V}|}\\boldsymbol{H}_{i,j}\\) . Additionally, \\(\\boldsymbol{D}_{v}\\) and \\(\\boldsymbol{D}_{e}\\) denote the diagonal matrices of the vertex degrees and the edge degrees, respectively. Hypergraph Neural Networks (HGNN) (Feng et al., 2019) perform data representation learning by utilizing a hypergraph structure to capture higher- order dependencies between nodes. In HGNN, hyperedge convolution operations are used to extract features by leveraging the hypergraph Laplacian for spectral convolution. To reduce computational complexity, Chebyshev polynomials are applied to approximate the spectral convolution and avoid the need to compute high- order eigenvectors explicitly. Through the hyperedge convolution operation, the \\(l\\) - th layer of HGNN can be formulated by \\[\\boldsymbol{X}^{(l + 1)} = \\sigma \\left(\\boldsymbol{D}_{v}^{-1 / 2}\\boldsymbol {H}\\boldsymbol {W}\\boldsymbol{D}_{e}^{-1}\\boldsymbol {H}^{\\top}\\boldsymbol{D}_{v}^{-1 / 2}\\boldsymbol {X}^{(l)}\\boldsymbol {\\Theta}^{(l)}\\right),\\] where \\(\\boldsymbol{X}^{(l)}\\in \\mathbb{R}^{|\\mathcal{V}|\\times d_{l}}\\) is the signal of the hypergraph at \\(l\\) layer with \\(|\\mathcal{V}|\\) nodes and \\(d_{l}\\) dimensional features, \\(\\boldsymbol{X}^{(0)}\\) is the original signal of the hypergraph. \\(\\Theta^{(l)}\\) is the learnable filter parameter and \\(\\sigma\\) denotes the nonlinear activation function. ## 3. Hyper SAT In this section, we propose Hyper SAT, a neural approach for Weighted Max SAT problems. Figure 1 illustrates the workflow of Hyper SAT. It mainly consists of three modules: the hypergraph modeling module, the neural network module, and the probabilistic mapping module. Given a Weighted Max SAT instance, Hyper SAT first applies its hypergraph modeling component to represent the instance as a hypergraph. Then, the hypergraph is solved by the neural network module, which is responsible for processing and optimizing the hypergraph. Finally, through a mapping operation, the probabilistic output of the neural network module is mapped into Boolean values, which serves as a solution to the Weighted Max SAT instance. Note that the uneven weight of clauses increases the nonlinear dependency and sensitivity among variables, the neural network is required to learn how to prioritize the contributions of different clauses. To this end, we propose a specific cross- attention mechanism and introduce an unsupervised multi- objective loss function to capture the logical interplay between the positive and negative literal node representations. These two mechanisms are integrated with the hypergraph convolutional network to form the neural network module of our Hyper SAT.\n\n### 3.1. Hypergraph Modeling Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , we construct the hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) as follows: For the construction of the vertex set \\(\\nu\\) , each variable \\(x_{i}\\in \\mathcal{X}\\) is represented by two nodes \\(v_{i}\\) and \\(v_{n + i}\\) , which correspond to the positive and negative literals of the variable \\(x_{i}\\) (i.e., \\(x_{i}\\) and \\(\\neg x_{i}\\) ), respectively. For the construction of the hyperedge set \\(\\mathcal{E}\\) , each clause \\(C_{j}\\in \\mathcal{C}\\) is represented by a hyperedge \\(e_{j}\\in \\mathcal{E}\\) , which connects the nodes corresponding to the literals in \\(C_{j}\\) . Specifically, if clause \\(C_{j}\\) consists of the literals \\(l_{j1},l_{j2},\\ldots ,l_{jk_{j}}\\) , then the corresponding hyperedge \\(e_{j}\\) connects the nodes corresponding to these literals. For the construction of the weight matrix \\(\\mathbf{W}\\) , the weight of each hyperedge is equal to the weight of the corresponding clause, i.e., \\(W_{j,j} = w_{j}\\) . Through the above modeling process, the Weighted Max SAT instance can be uniquely represented by a hypergraph, where each literal is a node, and each clause is a hyperedge connecting the literals involved. The weight of each hyperedge is the weight of the corresponding clause in the instance. Figure 2 gives an illustration of the hypergraph modeling process. <center>Figure 2. Hypergraph Modeling of a Weighted Max SAT instance \\((\\neg x_{1}\\lor x_{2}\\lor \\neg x_{3})\\land (x_{1}\\lor x_{2})\\land (\\neg x_{2}\\lor x_{3}\\lor x_{4})\\land (\\neg x_{1}\\lor x_{3}\\lor \\neg x_{4})\\) with 8 literals, 4 clauses and weights \\(\\{w_{1},w_{2},w_{3},w_{4}\\}\\) . </center> In contrast to existing methods that represent Conjunctive Normal Form (CNF) formulas as factor graphs (Guo et al., 2023), which limit their ability to model relationships beyond pairwise connections, we construct Weighted Max SAT instances as hypergraphs. These hypergraphs can encode higher- order variable dependencies through their degree- free hyperedges, and thus offer more powerful representational capacity and enable more efficient handling of higher- order relationships in combinatorial optimization problems. In particular, we treat literals rather than variables as nodes, which addresses a major issue present in Hyp Op (Heydaribeni et al., 2024). Since the logical relationships between positive and negative literals are central to the Weighted Max SAT problem, it is essential to treat them as distinct nodes, rather than merging them into a single node. In addition, we encode the weights of the clauses in the Weighted Max SAT problem as the weights of the corresponding hyperedges in the hypergraph. ### 3.2. Neural Network Architecture The neural network architecture of our proposed Hyper SAT comprises the Hyper GCN, a transformer module with the cross- attention mechanism, and a softmax layer. #### 3.2.1. HYPERGRAPH CONVOLUTIONAL NETWORKS In Hyper SAT, we introduce a \\(T\\) - layer Hyper GCN for message passing among different nodes. The operation at the \\(l\\) - th layer of the Hyper GCN is formally expressed as follows: \\[\\begin{array}{r}{\\pmb{L}^{(l + 1)^{\\prime}} = \\sigma \\left(\\pmb{D}_{v}^{-\\frac{1}{2}}\\widetilde{\\pmb{Q}}\\pmb{D}_{v}^{-\\frac{1}{2}}\\pmb{L}^{(l)}\\pmb{R}^{(l)}\\right).} \\end{array} \\quad (2)\\] In this formulation, the matrix \\(\\pmb{L}^{(l + 1)^{\\prime}}\\) represents the output of the \\(l\\) - th layer; \\(\\pmb{D}_{v}\\) is the diagonal matrix of the vertex degrees in the hypergraph; \\(\\pmb{L}^{(l)}\\in \\mathbb{R}^{2n\\times d_{l}}\\) is the matrix of node representations at the \\(l\\) - th layer, where \\(d_{l}\\) is the dimension of the \\(l\\) - th layer node representations; \\(\\pmb{R}^{(l)}\\in\\) \\(\\mathbb{R}^{d_{l}\\times d_{l + 1}}\\) is the \\(l\\) - th layer learnable weight matrix; and \\(\\widetilde{\\pmb{Q}}\\) is given by \\[\\widetilde{\\pmb{Q}} = \\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top} - \\mathrm{diag}(\\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top}), \\quad (3)\\] where \\(\\pmb{H}\\) is the hypergraph incidence matrix, and \\(\\widetilde{\\pmb{D}}_{e} =\\) \\(\\pmb{D}_{e} - \\pmb{I}\\) . Specifically, \\(\\sigma\\) denotes the nonlinear activation function and \\(\\pmb{L}^{(0)}\\) is a learnable input embedding of Hyper GCN. Compared to the updating rule in Eq. (1) in traditional HGNN, our Hyper GCN focuses the convolutional layer's computation more on the influence of adjacent nodes by removing the diagonal elements of \\(\\widetilde{\\pmb{Q}}\\) . This adjustment allows the representation updates of each node to better align with the higher- order relationships of the adjacency structure. #### 3.2.2. CROSS-ATTENTION MECHANISM The core of the Weighted Max SAT problem lies in the logical constraints among variables. Positive and negative literal nodes (e.g., \\(x\\) and \\(\\neg x\\) ) are logically mutually exclusive and strongly correlated, and their relationships directly reflect the underlying structural characteristics of the problem.\n\nConsidering this, we leverage a cross- attention mechanism to dynamically assign importance weights to the relationships between each pair of complementary literals. This mechanism enables the model to adaptively capture critical logical properties such as mutual exclusivity, dependency, and the process of information exchange between positive and negative literal nodes. Therefore, after updating each node representation in the hypergraph convolution layer, a cross- attention layer is introduced to enhance the representations of positive and negative literal nodes. Specifically, given the \\(l\\) - th layer output of the Hyper GCN \\(\\pmb{L}^{(l + 1)}\\) , we divide it into two parts: \\(\\pmb{L}^{(l + 1)^{\\prime}} =\\) \\(\\left[\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\right]\\) , where \\(\\pmb{L}_{+}^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the positive literal node representations and \\(\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the negative literal node representations. The cross- attention mechanism is mathematically represented as follows: \\[\\begin{array}{r l} & {\\pmb{L}_{+}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{+}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{-}^{(l + 1)},}\\\\ & {\\pmb{L}_{-}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{-}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{+}^{(l + 1)}.} \\end{array} \\quad (4)\\] In this formulation, \\[\\begin{array}{r l} & {\\pmb{Q}_{+}^{(l + 1)} = \\pmb{W}_{Q}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{Q}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{K}_{+}^{(l + 1)} = \\pmb{W}_{K}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{K}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{V}_{+}^{(l + 1)} = \\pmb{W}_{V}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{V}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},} \\end{array} \\quad (5)\\] where \\(\\pmb{W}_{Q}^{(l + 1)}\\) , \\(\\pmb{W}_{K}^{(l + 1)}\\) , \\(\\pmb{W}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrices for the positive literal nodes at the \\(l\\) - th layer, and \\(\\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrix for the negative literal nodes at the \\(l\\) - th layer. The final node representation at the \\(l\\) - th layer is obtained as \\(\\pmb{L}^{(l + 1)} = [\\pmb{L}_{+}^{(l + 1)},\\pmb{L}_{- }^{(l + 1)}]\\) . The cross- attention mechanism explicitly constructs the interaction between positive and negative literal nodes by enabling each node to incorporate the features of its counterpart. This facilitates a more comprehensive representation of node features for Weighted Max SAT problems. Building upon this, the transformer module in Hyper SAT incorporates the previously discussed cross- attention layer, along with other components to further enhance the modeling capability. Inspired by the recent advancements in Vision Transformer architecture (Vi T- 22B (Dehghani et al., 2023)), the transformer module in Hyper SAT includes a Layer Norm layer (Ba, 2016), followed by a combination of a cross- attention layer and a Feed- Forward Network (FFN) layer, along with a residual connection and an additional Layer Norm layer. The FFN and cross- attention layers operate in parallel to enhance the efficiency. The residual connection is introduced by adding the outputs from the cross- attention layer, the FFN layer, and the Layer Norm layer. The resulting sum is then passed through another Layer Norm layer to stabilize the representations before proceeding to the next layer. The final layer of the network is a softmax layer. We reshape the iterated \\(\\pmb{L}^{(T)^{\\prime}}\\in \\mathbb{R}^{2n\\times 1}\\) into \\(\\hat{\\pmb{L}}^{(T)^{\\prime}}\\in \\mathbb{R}^{n\\times 2}\\) , which serves as the input to the softmax layer. After applying the softmax function, we obtain \\(\\hat{\\pmb{L}} = \\mathrm{softmax}(\\hat{\\pmb{L}}^{(T)^{\\prime}})\\) , which provides the soft node assignments for each literal, interpreted as class probabilities. The probability of assigning a variable to a truth value is recorded by \\(\\pmb{Y} = \\hat{\\pmb{L}}_{- 1}\\) , which is the first column of \\(\\hat{\\pmb{L}}\\) . #### 3.2.3. LOSS FUNCTION Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) and its hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) , we relax the Boolean variables \\(\\mathcal{X}\\) into continuous probability parameters \\(\\mathbf{Y}(\\gamma)\\) where \\(\\gamma = (\\pmb {R},\\pmb {L}^{(0)},\\pmb {W}_{Q},\\pmb {W}_{K},\\pmb {W}_{V},\\widetilde{\\pmb{W}}_{Q},\\widetilde{\\pmb{W}}_{K},\\widetilde{\\pmb{W}}_{V})\\) represent the learnable parameters. The relaxation is defined as follows: \\[\\mathcal{X}\\in \\{0,1\\}^{n}\\longrightarrow \\mathbf{Y}(\\gamma)\\in [0,1]^{n}. \\quad (6)\\] With this relaxation, we can design a differentiable loss function to optimize the learnable parameters \\(\\gamma\\) , enabling gradient- based optimization. In this paper, we propose an unsupervised multi- objective loss function that consists of two components: a primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) and a shared representation constraint loss \\(\\mathcal{L}_{\\mathrm{shared}}\\) . This unsupervised loss function obviates the necessity for large, labeled training datasets, which are conventionally indispensable in supervised learning paradigms. The specific form is as follows: \\[\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{task}} + \\lambda \\mathcal{L}_{\\mathrm{shared}}, \\quad (7)\\] where \\(\\lambda \\geq 0\\) is a balancing hyperparameter. The primary task loss function is the relaxed optimization objective of the Weighted Max SAT problem, as shown below: \\[\\mathcal{L}_{\\mathrm{task}}(\\mathbf{Y}) = \\sum_{j = 1}^{m}w_{j}V_{j}(\\mathbf{Y}), \\quad (8)\\] where \\[V_{j}(\\mathbf{Y}) = 1 - \\prod_{i\\in C_{j}^{+}}(1 - y_{i})\\prod_{i\\in C_{j}^{-}}y_{i}. \\quad (9)\\] Here, \\(C_{j}^{+}\\) and \\(C_{j}^{- }\\) are the index sets of variables appearing in the clause \\(C_{j}\\) in the positive and negative form, respectively. The term \\(V_{j}(\\mathbf{Y})\\) represents the satisfaction of clause \\(C_{j}\\) , where a value of 1 indicates that the clause is satisfied, and 0 indicates it is not. The weight \\(w_{j}\\) ensures that\n\nmore important clauses are prioritized during optimization. Minimizing the primary task loss function corresponds to maximizing the weighted sum of satisfied clauses in the Weighted Max SAT problem. The shared representation constraint loss function focuses on the representations of positive and negative literal nodes in the penultimate layer of the Hyper SAT network. Its form is given by \\[\\mathcal{L}_{\\mathrm{shared}} = \\left\\| \\pmb{L}_{+}^{(T - 1)} + \\pmb{L}_{-}^{(T - 1)}\\right\\|_{F}^{2}, \\quad (10)\\] where \\(\\left\\| \\cdot \\right\\|_{F}\\) denotes the Frobenius norm. By applying the shared representation constraint loss function, the network encourages the positive and negative literal nodes to develop distinct feature representations in the learning process. This constraint enhances the separation of the two types of nodes in the feature space. It should be noted that most neural network- based SAT methods employ supervised learning. However, supervised learning approaches are not particularly well- suited for solving the Weighted Max SAT problem. On the one hand, a Weighted Max SAT instance may have multiple satisfying assignments. On the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the Weighted Max SAT instance. From this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances. ### 3.3. Probabilistic Mapping The neural network module takes the hypergraph as input and produces a probability vector \\(\\mathbf{Y}\\) as output, which indicates the probability of each variable taking the truth value. Accordingly, to convert the probability vector \\(\\mathbf{Y}\\) into a Boolean assignment, we transform \\(\\mathbf{Y}\\) into \\(n\\) Bernoulli distributions, where the \\(i\\) - th distribution \\(B(y_{i})\\) is over the discrete values \\(\\{0,1\\}\\) . Finally, the node assignments are generated by sampling with the corresponding probability distributions. ## 4. Experiment ### 4.1. Experimental Settings We compare the performance of Hyper SAT against GNN- based methods for solving Weighted Max SAT problems. Baseline Algorithms. We compare our proposed Hyper SAT against GNN- based methods. The following algorithms are considered as baselines: (i) Hyp Op (Heydaribeni et al., 2024): an advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs; (ii) (Liu et al., 2023): an innovative supervised GNN- based approach that predicts solutions in an end- to- end manner by transforming CNF formulas into factor graphs. We apply these baselines to solve Weighted Max SAT problems. Datasets. We evaluate the algorithms using random 3- SAT benchmarks from the SATLIB dataset (Hoos & St\u00fctzle, 2000). The SATLIB dataset is one of the standard datasets for evaluating SAT solvers. We utilize a range of datasets with varying distributions, from which SAT and UNSAT instances are generated for each distribution. The number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. More details on the datasets are provided in Table 1. In particular, to generate the required Weighted Max SAT instances, we assign weights to the clauses in each CNF file within the dataset. These weights are sampled from the integers in the range \\([1,10]\\) uniformly at random. Table 1. The parameters of the datasets. <table><tr><td>DATASET</td><td>INSTANCE</td><td>VARS</td><td>CLAUSES</td><td>TYPES</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>SAT</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>UNSAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>SAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>UNSAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>SAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>UNSAT</td></tr></table> Model Settings. Hyper GCN is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. The dimension of the cross- attention layer is the square root of the number of variables. The dropout used in the cross- attention layer is 0.1. The model is optimized using the Adam optimizer (Kingma, 2014) with a learning rate of \\(7 \\times 10^{- 2}\\) . The balancing hyperparameter of loss function \\(\\lambda\\) is \\(2 \\times 10^{- 3}\\) . The FFN consists of two linear transformations and a Re LU activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. An early stopping strategy is employed, with a tolerance of \\(10^{- 4}\\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. Finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained. Evaluation Configuration. All experiments are conducted on an NVIDIA A100- SXM4- 40GB GPU, using Python 3.9.30 and Py Torch 1.13.0.\n\n### 4.2. Analytical Experiment We first evaluate the performance of unsupervised methods, Hyper SAT and Hyp Op, with a focus on their convergence. We evaluate the convergence using the primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) from Eq. (8), which represents the sum of the weights of unsatisfied clauses. We illustrate the evolution curves of loss for Hyper SAT and Hyp Op on the uuf250- 1065 dataset in Figure 3 as an example. All models can converge within 300 epochs. Moreover, the loss of Hyper SAT is around 52, while the loss of Hyp Op is around 139. We can observe that Hyper SAT achieves better performance than Hyp Op. Specifically, Hyper SAT decreases the loss more quickly and achieves a lower loss value. The experimental results demonstrate that Hyper SAT can be used in learning to solve Weighted Max SAT problems. <center>Figure 3. The evolution of loss for Hyper SAT and Hyp Op during an inference process of 300 epochs on the uuf250-1065 dataset. </center> ### 4.3. Result We evaluate the performance of Hyper SAT against baseline algorithms Hyp Op and (Liu et al., 2023) on various datasets. The primary evaluation metric is the average weighted sum of unsatisfied clauses. The experiments are conducted on datasets with the number of variables ranging from 100 to 250, and the number of clauses varying between 430 and 1065. The results are shown in Table 2. As presented in the table, Hyper SAT consistently achieves lower values for the average weighted sum of unsatisfied clauses compared to the baseline algorithms (Liu et al., 2023) and Hyp Op across multiple datasets. For example, on the uf100- 430 dataset, Hyper SAT significantly outperforms both baselines with a result of 15.64. This result represents a substantial improvement over the results of (Liu et al., 2023) (32.48) and Hyp Op (99.15). This trend is observed across all datasets, where Hyper SAT always exceeds the Table 2. The average weighted sum of unsatisfied clauses of Weighted Max SAT problems. <table><tr><td>DATASET</td><td>LIU ET AL. (2023)</td><td>HYPOP</td><td>HYPERSAT</td></tr><tr><td>UF100-430</td><td>32.48</td><td>99.15</td><td>15.64</td></tr><tr><td>UUF100-430</td><td>41.65</td><td>102.44</td><td>20.46</td></tr><tr><td>UF200-860</td><td>67.38</td><td>158.46</td><td>28.98</td></tr><tr><td>UUF200-860</td><td>81.68</td><td>171.34</td><td>35.55</td></tr><tr><td>UF250-1065</td><td>79.06</td><td>170.60</td><td>33.24</td></tr><tr><td>UUF250-1065</td><td>100.04</td><td>182.39</td><td>41.64</td></tr></table> performance of the baselines. The average weight of unsatisfied clauses is reduced by approximately \\(50\\%\\) compared to (Liu et al., 2023) and over \\(80\\%\\) compared to Hyp Op. On the uf200- 860 and uuf200- 860 datasets, Hyper SAT achieves reductions of \\(56.99\\%\\) and \\(56.48\\%\\) compared to (Liu et al., 2023), respectively, and reductions of \\(81.71\\%\\) and \\(79.25\\%\\) compared to Hyp Op. Similarly, on the uf250- 1065 and uuf250- 1065 datasets, Hyper SAT continues to outperform the baselines. The reductions compared to (Liu et al., 2023) are \\(57.96\\%\\) and \\(58.38\\%\\) , respectively, while the reductions compared to Hyp Op are \\(80.52\\%\\) and \\(77.17\\%\\) . Importantly, even with the larger datasets, Hyper SAT maintains or even enhances its efficacy. This demonstrates that Hyper SAT scales well and performs better. These results show that Hyper SAT consistently provides substantial improvements over both baseline algorithms across a range of datasets, including those with larger problem sizes. As a result, it validates its robustness and effectiveness in reducing the weighted sum of unsatisfied clauses. ### 4.4. Ablation Study We conduct an ablation study to evaluate the contribution of each key component in our proposed model. By systematically removing components, we analyze their impact on performance and highlight the importance of each component. The experiments are designed to isolate the impact of the following components: (i) the hypergraph modeling of literal nodes rather than variable nodes; (ii) the transformer module with the cross- attention mechanism; (iii) the shared representation constraint loss. The results of the ablation study are shown in Table 3. Effect of Hypergraph Modeling of Literal Nodes: The performance drops by \\(52.77\\%\\) when the hypergraph modeling of variable nodes is used instead of literal nodes. The results demonstrate the importance and superiority of modeling the Weighted Max SAT instance as a hypergraph with literal nodes. Effect of Transformer with Cross- Attention: We disable the transformer module with the cross- attention mechanism to assess its importance. Without this module, the model experiences a performance drop of \\(11.54\\%\\) . This signifi\n\nTable 3. The results of the ablation study. We consider three components: (i) HGM-L: the hypergraph modeling of literal nodes rather than variable nodes; (ii) Transformer: the transformer module with the cross-attention mechanism; (iii) SRCL: the shared representation constraint loss. Specifically, the first row in the table represents the transformation of the Weighted Max SAT instance into the hypergraph of variables. <table><tr><td>HGM-L</td><td>TRANSFORMER</td><td>SRCL</td><td>RESULT</td></tr><tr><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td><td>182.39</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u00d7</td><td>86.14</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u00d7</td><td>64.08</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u221a</td><td>47.07</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u221a</td><td>41.64</td></tr></table> cantly reduces its ability to effectively capture dependencies across each pair of complementary literals, leading to lower overall accuracy. Effect of Share Representation Constraint Loss: We further remove the shared representation constraint loss and optimize the model with the primary task loss function to investigate the role of the unsupervised multi- objective loss. The results show that without the shared representation constraint loss, the model achieves a performance of 64.08. This performance is lower than that of the original configuration. Therefore, it highlights the importance of the shared representation constraint loss, which encourages the positive and negative literal nodes to develop distinct feature representations. In summary, the full model consistently outperforms the ablated versions, demonstrating the synergistic effect of integrating the hypergraph modeling of literal nodes, cross- attention mechanism, and shared representation constraint loss design. ## 5. Conclusion In this work, we propose Hyper SAT, a novel neural approach for Weighted Max SAT problems, addressing the challenges associated with learning the complex non- linear dependencies and sensitive objective function of this NP- hard problem. By modeling Weighted Max SAT instances as hypergraphs, a hypergraph convolutional network is introduced as the learning model, coupled with a cross- attention mechanism to capture the logical relationships between positive and negative literals. The proposed framework incorporates an unsupervised multi- objective loss design, which not only optimizes the intrinsic objectives of the Weighted Max SAT problem but also ensures that the representations of positive and negative literals remain distinct in the feature space. Extensive experiments demonstrate the potential of Hyper SAT. The experimental results show that Hyper SAT is effective in solving Weighted Max SAT instances and outperforms state- of- the- art competitors in terms of performance. This work offers a fresh perspective on solving the Weighted Max SAT problem through learning- based methods. Future work will focus on further integrating the model with heuristic solvers, improving the design of well- crafted solvers, and exploring its applications in other complex combinatorial problems. ## Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ## References Allouche, D., Traor\u00e9, S., Andr\u00e9, I., De Givry, S., Katsirelos, G., Barbe, S., and Schiex, T. Computational protein design as a cost function network optimization problem. In International Conference on Principles and Practice of Constraint Programming, pp. 840- 849. Springer, 2012. Audemard, G. and Simon, L. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27 (1):1- 25, 2018. Ba, J. L. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Biere, A. and Fleury, M. Gimsatul, isasat and kissat entering the sat competition 2022. Proceedings of SAT Competition, pp. 10- 11, 2022. Cai, S. and Zhang, X. Deep cooperation of cdc1 and local search for sat. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, pp. 64- 81. Springer, 2021. Cameron, C., Chen, R., Hartford, J., and Leyton- Brown, K. Predicting propositional satisfiability via end- to- end learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3324- 3331, 2020. Clarke, E., Biere, A., Raimi, R., and Zhu, Y. Bounded model checking using satisfiability solving. Formal methods in system design, 19:7- 34, 2001. Cook, S. A. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, pp. 151- 158, 1971. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers\n\nto 22 billion parameters. In International Conference on Machine Learning, pp. 7480- 7512. PMLR, 2023. Feng, Y., You, H., Zhang, Z., Ji, R., and Gao, Y. Hypergraph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 3558- 3565, 2019. Guo, W., Zhen, H.- L., Li, X., Luo, W., Yuan, M., Jin, Y., and Yan, J. Machine learning methods in solving the boolean satisfiability problem. Machine Intelligence Research, 20 (5):640- 655, 2023. Heydaribeni, N., Zhan, X., Zhang, R., Eliassi- Rad, T., and Koushanfar, F. Distributed constrained combinatorial optimization leveraging hypergraph neural networks. Nature Machine Intelligence, pp. 1- 9, 2024. Hoos, H. H. and St\u00fctzle, T. SATLIB: An online resource for research on sat. Sat, 2000:283- 292, 2000. Kautz, H. A., Selman, B., et al. Planning as satisfiability. In ECAI, volume 92, pp. 359- 363. Citeseer, 1992. Kingma, D. P. Adam: A method for stochastic optimization. ar Xiv preprint ar Xiv:1412.6980, 2014. Liu, M., Huang, P., Jia, F., Zhang, F., Sun, Y., Cai, S., Ma, F., and Zhang, J. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 16264- 16265, 2023. Ozolins, E., Freivalds, K., Draguns, A., Gaile, E., Zakovskis, R., and Kozlovics, S. Goal- aware neural sat solver. In 2022 International Joint Conference on Neural Networks, pp. 1- 8. IEEE, 2022. Selsam, D. and Bj\u00f8rner, N. Guiding high- performance sat solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, pp. 336- 353. Springer, 2019. Selsam, D., Lamm, M., B\u00fcnz, B., Liang, P., de Moura, L., and Dill, D. L. Learning a sat solver from single- bit supervision. In 7th International Conference on Learning Representations, 2019. Sun, L., Gerault, D., Benamira, A., and Peyrin, T. Neurogift: Using a machine learning based sat solver for cryptanalysis. In Cyber Security Cryptography and Machine Learning: Fourth International Symposium, Proceedings 4, pp. 62- 84. Springer, 2020. Thornton, J. and Sattar, A. Dynamic constraint weighting for over- constrained problems. In Pacific Rim International Conference on Artificial Intelligence, pp. 377- 388. Springer, 1998. Timm, N. and Botha, J. Synthesis of cost- optimal multiagent systems for resource allocation. ar Xiv preprint ar Xiv:2209.09473, 2022. Wang, W., Hu, Y., Tiwari, M., Khurshid, S., Mc Millan, K., and Miikkulainen, R. Neuroback: Improving CDCL SAT solving using graph neural networks. In The Twelfth International Conference on Learning Representations, 2024. Yang, Q., Wu, K., and Jiang, Y. Learning action models from plan examples using weighted max- sat. Artificial Intelligence, 171(2- 3):107- 143, 2007. Zhang, W., Sun, Z., Zhu, Q., Li, G., Cai, S., Xiong, Y., and Zhang, L. Nlocalsat: boosting local search with solution prediction. In Proceedings of the Twenty- Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 1177- 1183, 2021.",
    "31_hypergraph_modeling_given_a_weighted_max_sat_instance_phi_mathcalxmathcalcmathbfw_we_construct_the_hypergraph_mathcalg_mathcalvmathcalemathbfw_as_follows_for_the_construction_of_the_vertex_set_nu_each_variable_x_iin_mathcalx_is_represented_by_two_nodes_v_i_and_v_n_i_which_correspond_to_the_positive_and_negative_literals_of_the_variable_x_i_ie_x_i_and_neg_x_i_respectively_for_the_construction_of_the_hyperedge_set_mathcale_each_clause_c_jin_mathcalc_is_represented_by_a_hyperedge_e_jin_mathcale_which_connects_the_nodes_corresponding_to_the_literals_in_c_j_specifically_if_clause_c_j_consists_of_the_literals_l_j1l_j2ldots_l_jk_j_then_the_corresponding_hyperedge_e_j_connects_the_nodes_corresponding_to_these_literals_for_the_construction_of_the_weight_matrix_mathbfw_the_weight_of_each_hyperedge_is_equal_to_the_weight_of_the_corresponding_clause_ie_w_jj_w_j_through_the_above_modeling_process_the_weighted_max_sat_instance_can_be_uniquely_represented_by_a_hypergraph_where_each_literal_is_a_node_and_each_clause_is_a_hyperedge_connecting_the_literals_involved_the_weight_of_each_hyperedge_is_the_weight_of_the_corresponding_clause_in_the_instance_figure_2_gives_an_illustration_of_the_hypergraph_modeling_process_centerfigure_2_hypergraph_modeling_of_a_weighted_max_sat_instance_neg_x_1lor_x_2lor_neg_x_3land_x_1lor_x_2land_neg_x_2lor_x_3lor_x_4land_neg_x_1lor_x_3lor_neg_x_4_with_8_literals_4_clauses_and_weights_w_1w_2w_3w_4_center_in_contrast_to_existing_methods_that_represent_conjunctive_normal_form_cnf_formulas_as_factor_graphs_guo_et_al_2023_which_limit_their_ability_to_model_relationships_beyond_pairwise_connections_we_construct_weighted_max_sat_instances_as_hypergraphs_these_hypergraphs_can_encode_higher-_order_variable_dependencies_through_their_degree-_free_hyperedges_and_thus_offer_more_powerful_representational_capacity_and_enable_more_efficient_handling_of_higher-_order_relationships_in_combinatorial_optimization_problems_in_particular_we_treat_literals_rather_than_variables_as_nodes_which_addresses_a_major_issue_present_in_hyp_op_heydaribeni_et_al_2024_since_the_logical_relationships_between_positive_and_negative_literals_are_central_to_the_weighted_max_sat_problem_it_is_essential_to_treat_them_as_distinct_nodes_rather_than_merging_them_into_a_single_node_in_addition_we_encode_the_weights_of_the_clauses_in_the_weighted_max_sat_problem_as_the_weights_of_the_corresponding_hyperedges_in_the_hypergraph_32_neural_network_architecture_the_neural_network_architecture_of_our_proposed_hyper_sat_comprises_the_hyper_gcn_a_transformer_module_with_the_cross-_attention_mechanism_and_a_softmax_layer_321_hypergraph_convolutional_networks_in_hyper_sat_we_introduce_a_t_-_layer_hyper_gcn_for_message_passing_among_different_nodes_the_operation_at_the_l_-_th_layer_of_the_hyper_gcn_is_formally_expressed_as_follows_beginarrayrpmbll_1prime_sigma_leftpmbd_v-frac12widetildepmbqpmbd_v-frac12pmbllpmbrlright_endarray_quad_2_in_this_formulation_the_matrix_pmbll_1prime_represents_the_output_of_the_l_-_th_layer_pmbd_v_is_the_diagonal_matrix_of_the_vertex_degrees_in_the_hypergraph_pmbllin_mathbbr2ntimes_d_l_is_the_matrix_of_node_representations_at_the_l_-_th_layer_where_d_l_is_the_dimension_of_the_l_-_th_layer_node_representations_pmbrlin_mathbbrd_ltimes_d_l_1_is_the_l_-_th_layer_learnable_weight_matrix_and_widetildepmbq_is_given_by_widetildepmbq_pmb_hwidetildepmbd_e-1pmb_htop_-_mathrmdiagpmb_hwidetildepmbd_e-1pmb_htop_quad_3_where_pmbh_is_the_hypergraph_incidence_matrix_and_widetildepmbd_e_pmbd_e_-_pmbi_specifically_sigma_denotes_the_nonlinear_activation_function_and_pmbl0_is_a_learnable_input_embedding_of_hyper_gcn_compared_to_the_updating_rule_in_eq_1_in_traditional_hgnn_our_hyper_gcn_focuses_the_convolutional_layers_computation_more_on_the_influence_of_adjacent_nodes_by_removing_the_diagonal_elements_of_widetildepmbq_this_adjustment_allows_the_representation_updates_of_each_node_to_better_align_with_the_higher-_order_relationships_of_the_adjacency_structure_322_cross-attention_mechanism_the_core_of_the_weighted_max_sat_problem_lies_in_the_logical_constraints_among_variables_positive_and_negative_literal_nodes_eg_x_and_neg_x_are_logically_mutually_exclusive_and_strongly_correlated_and_their_relationships_directly_reflect_the_underlying_structural_characteristics_of_the_problem": "Considering this, we leverage a cross- attention mechanism to dynamically assign importance weights to the relationships between each pair of complementary literals. This mechanism enables the model to adaptively capture critical logical properties such as mutual exclusivity, dependency, and the process of information exchange between positive and negative literal nodes. Therefore, after updating each node representation in the hypergraph convolution layer, a cross- attention layer is introduced to enhance the representations of positive and negative literal nodes. Specifically, given the \\(l\\) - th layer output of the Hyper GCN \\(\\pmb{L}^{(l + 1)}\\) , we divide it into two parts: \\(\\pmb{L}^{(l + 1)^{\\prime}} =\\) \\(\\left[\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\right]\\) , where \\(\\pmb{L}_{+}^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the positive literal node representations and \\(\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the negative literal node representations. The cross- attention mechanism is mathematically represented as follows: \\[\\begin{array}{r l} & {\\pmb{L}_{+}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{+}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{-}^{(l + 1)},}\\\\ & {\\pmb{L}_{-}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{-}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{+}^{(l + 1)}.} \\end{array} \\quad (4)\\] In this formulation, \\[\\begin{array}{r l} & {\\pmb{Q}_{+}^{(l + 1)} = \\pmb{W}_{Q}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{Q}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{K}_{+}^{(l + 1)} = \\pmb{W}_{K}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{K}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{V}_{+}^{(l + 1)} = \\pmb{W}_{V}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{V}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},} \\end{array} \\quad (5)\\] where \\(\\pmb{W}_{Q}^{(l + 1)}\\) , \\(\\pmb{W}_{K}^{(l + 1)}\\) , \\(\\pmb{W}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrices for the positive literal nodes at the \\(l\\) - th layer, and \\(\\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrix for the negative literal nodes at the \\(l\\) - th layer. The final node representation at the \\(l\\) - th layer is obtained as \\(\\pmb{L}^{(l + 1)} = [\\pmb{L}_{+}^{(l + 1)},\\pmb{L}_{- }^{(l + 1)}]\\) . The cross- attention mechanism explicitly constructs the interaction between positive and negative literal nodes by enabling each node to incorporate the features of its counterpart. This facilitates a more comprehensive representation of node features for Weighted Max SAT problems. Building upon this, the transformer module in Hyper SAT incorporates the previously discussed cross- attention layer, along with other components to further enhance the modeling capability. Inspired by the recent advancements in Vision Transformer architecture (Vi T- 22B (Dehghani et al., 2023)), the transformer module in Hyper SAT includes a Layer Norm layer (Ba, 2016), followed by a combination of a cross- attention layer and a Feed- Forward Network (FFN) layer, along with a residual connection and an additional Layer Norm layer. The FFN and cross- attention layers operate in parallel to enhance the efficiency. The residual connection is introduced by adding the outputs from the cross- attention layer, the FFN layer, and the Layer Norm layer. The resulting sum is then passed through another Layer Norm layer to stabilize the representations before proceeding to the next layer. The final layer of the network is a softmax layer. We reshape the iterated \\(\\pmb{L}^{(T)^{\\prime}}\\in \\mathbb{R}^{2n\\times 1}\\) into \\(\\hat{\\pmb{L}}^{(T)^{\\prime}}\\in \\mathbb{R}^{n\\times 2}\\) , which serves as the input to the softmax layer. After applying the softmax function, we obtain \\(\\hat{\\pmb{L}} = \\mathrm{softmax}(\\hat{\\pmb{L}}^{(T)^{\\prime}})\\) , which provides the soft node assignments for each literal, interpreted as class probabilities. The probability of assigning a variable to a truth value is recorded by \\(\\pmb{Y} = \\hat{\\pmb{L}}_{- 1}\\) , which is the first column of \\(\\hat{\\pmb{L}}\\) . #### 3.2.3. LOSS FUNCTION Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) and its hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) , we relax the Boolean variables \\(\\mathcal{X}\\) into continuous probability parameters \\(\\mathbf{Y}(\\gamma)\\) where \\(\\gamma = (\\pmb {R},\\pmb {L}^{(0)},\\pmb {W}_{Q},\\pmb {W}_{K},\\pmb {W}_{V},\\widetilde{\\pmb{W}}_{Q},\\widetilde{\\pmb{W}}_{K},\\widetilde{\\pmb{W}}_{V})\\) represent the learnable parameters. The relaxation is defined as follows: \\[\\mathcal{X}\\in \\{0,1\\}^{n}\\longrightarrow \\mathbf{Y}(\\gamma)\\in [0,1]^{n}. \\quad (6)\\] With this relaxation, we can design a differentiable loss function to optimize the learnable parameters \\(\\gamma\\) , enabling gradient- based optimization. In this paper, we propose an unsupervised multi- objective loss function that consists of two components: a primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) and a shared representation constraint loss \\(\\mathcal{L}_{\\mathrm{shared}}\\) . This unsupervised loss function obviates the necessity for large, labeled training datasets, which are conventionally indispensable in supervised learning paradigms. The specific form is as follows: \\[\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{task}} + \\lambda \\mathcal{L}_{\\mathrm{shared}}, \\quad (7)\\] where \\(\\lambda \\geq 0\\) is a balancing hyperparameter. The primary task loss function is the relaxed optimization objective of the Weighted Max SAT problem, as shown below: \\[\\mathcal{L}_{\\mathrm{task}}(\\mathbf{Y}) = \\sum_{j = 1}^{m}w_{j}V_{j}(\\mathbf{Y}), \\quad (8)\\] where \\[V_{j}(\\mathbf{Y}) = 1 - \\prod_{i\\in C_{j}^{+}}(1 - y_{i})\\prod_{i\\in C_{j}^{-}}y_{i}. \\quad (9)\\] Here, \\(C_{j}^{+}\\) and \\(C_{j}^{- }\\) are the index sets of variables appearing in the clause \\(C_{j}\\) in the positive and negative form, respectively. The term \\(V_{j}(\\mathbf{Y})\\) represents the satisfaction of clause \\(C_{j}\\) , where a value of 1 indicates that the clause is satisfied, and 0 indicates it is not. The weight \\(w_{j}\\) ensures that\n\nmore important clauses are prioritized during optimization. Minimizing the primary task loss function corresponds to maximizing the weighted sum of satisfied clauses in the Weighted Max SAT problem. The shared representation constraint loss function focuses on the representations of positive and negative literal nodes in the penultimate layer of the Hyper SAT network. Its form is given by \\[\\mathcal{L}_{\\mathrm{shared}} = \\left\\| \\pmb{L}_{+}^{(T - 1)} + \\pmb{L}_{-}^{(T - 1)}\\right\\|_{F}^{2}, \\quad (10)\\] where \\(\\left\\| \\cdot \\right\\|_{F}\\) denotes the Frobenius norm. By applying the shared representation constraint loss function, the network encourages the positive and negative literal nodes to develop distinct feature representations in the learning process. This constraint enhances the separation of the two types of nodes in the feature space. It should be noted that most neural network- based SAT methods employ supervised learning. However, supervised learning approaches are not particularly well- suited for solving the Weighted Max SAT problem. On the one hand, a Weighted Max SAT instance may have multiple satisfying assignments. On the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the Weighted Max SAT instance. From this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances. ### 3.3. Probabilistic Mapping The neural network module takes the hypergraph as input and produces a probability vector \\(\\mathbf{Y}\\) as output, which indicates the probability of each variable taking the truth value. Accordingly, to convert the probability vector \\(\\mathbf{Y}\\) into a Boolean assignment, we transform \\(\\mathbf{Y}\\) into \\(n\\) Bernoulli distributions, where the \\(i\\) - th distribution \\(B(y_{i})\\) is over the discrete values \\(\\{0,1\\}\\) . Finally, the node assignments are generated by sampling with the corresponding probability distributions. ## 4. Experiment ### 4.1. Experimental Settings We compare the performance of Hyper SAT against GNN- based methods for solving Weighted Max SAT problems. Baseline Algorithms. We compare our proposed Hyper SAT against GNN- based methods. The following algorithms are considered as baselines: (i) Hyp Op (Heydaribeni et al., 2024): an advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs; (ii) (Liu et al., 2023): an innovative supervised GNN- based approach that predicts solutions in an end- to- end manner by transforming CNF formulas into factor graphs. We apply these baselines to solve Weighted Max SAT problems. Datasets. We evaluate the algorithms using random 3- SAT benchmarks from the SATLIB dataset (Hoos & St\u00fctzle, 2000). The SATLIB dataset is one of the standard datasets for evaluating SAT solvers. We utilize a range of datasets with varying distributions, from which SAT and UNSAT instances are generated for each distribution. The number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. More details on the datasets are provided in Table 1. In particular, to generate the required Weighted Max SAT instances, we assign weights to the clauses in each CNF file within the dataset. These weights are sampled from the integers in the range \\([1,10]\\) uniformly at random. Table 1. The parameters of the datasets. <table><tr><td>DATASET</td><td>INSTANCE</td><td>VARS</td><td>CLAUSES</td><td>TYPES</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>SAT</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>UNSAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>SAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>UNSAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>SAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>UNSAT</td></tr></table> Model Settings. Hyper GCN is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. The dimension of the cross- attention layer is the square root of the number of variables. The dropout used in the cross- attention layer is 0.1. The model is optimized using the Adam optimizer (Kingma, 2014) with a learning rate of \\(7 \\times 10^{- 2}\\) . The balancing hyperparameter of loss function \\(\\lambda\\) is \\(2 \\times 10^{- 3}\\) . The FFN consists of two linear transformations and a Re LU activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. An early stopping strategy is employed, with a tolerance of \\(10^{- 4}\\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. Finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained. Evaluation Configuration. All experiments are conducted on an NVIDIA A100- SXM4- 40GB GPU, using Python 3.9.30 and Py Torch 1.13.0.",
    "42_analytical_experiment_we_first_evaluate_the_performance_of_unsupervised_methods_hyper_sat_and_hyp_op_with_a_focus_on_their_convergence_we_evaluate_the_convergence_using_the_primary_task_loss_mathcall_mathrmtask_from_eq_8_which_represents_the_sum_of_the_weights_of_unsatisfied_clauses_we_illustrate_the_evolution_curves_of_loss_for_hyper_sat_and_hyp_op_on_the_uuf250-_1065_dataset_in_figure_3_as_an_example_all_models_can_converge_within_300_epochs_moreover_the_loss_of_hyper_sat_is_around_52_while_the_loss_of_hyp_op_is_around_139_we_can_observe_that_hyper_sat_achieves_better_performance_than_hyp_op_specifically_hyper_sat_decreases_the_loss_more_quickly_and_achieves_a_lower_loss_value_the_experimental_results_demonstrate_that_hyper_sat_can_be_used_in_learning_to_solve_weighted_max_sat_problems_centerfigure_3_the_evolution_of_loss_for_hyper_sat_and_hyp_op_during_an_inference_process_of_300_epochs_on_the_uuf250-1065_dataset_center_43_result_we_evaluate_the_performance_of_hyper_sat_against_baseline_algorithms_hyp_op_and_liu_et_al_2023_on_various_datasets_the_primary_evaluation_metric_is_the_average_weighted_sum_of_unsatisfied_clauses_the_experiments_are_conducted_on_datasets_with_the_number_of_variables_ranging_from_100_to_250_and_the_number_of_clauses_varying_between_430_and_1065_the_results_are_shown_in_table_2_as_presented_in_the_table_hyper_sat_consistently_achieves_lower_values_for_the_average_weighted_sum_of_unsatisfied_clauses_compared_to_the_baseline_algorithms_liu_et_al_2023_and_hyp_op_across_multiple_datasets_for_example_on_the_uf100-_430_dataset_hyper_sat_significantly_outperforms_both_baselines_with_a_result_of_1564_this_result_represents_a_substantial_improvement_over_the_results_of_liu_et_al_2023_3248_and_hyp_op_9915_this_trend_is_observed_across_all_datasets_where_hyper_sat_always_exceeds_the_table_2_the_average_weighted_sum_of_unsatisfied_clauses_of_weighted_max_sat_problems_tabletrtddatasettdtdliu_et_al_2023tdtdhypoptdtdhypersattdtrtrtduf100-430tdtd3248tdtd9915tdtd1564tdtrtrtduuf100-430tdtd4165tdtd10244tdtd2046tdtrtrtduf200-860tdtd6738tdtd15846tdtd2898tdtrtrtduuf200-860tdtd8168tdtd17134tdtd3555tdtrtrtduf250-1065tdtd7906tdtd17060tdtd3324tdtrtrtduuf250-1065tdtd10004tdtd18239tdtd4164tdtrtable_performance_of_the_baselines_the_average_weight_of_unsatisfied_clauses_is_reduced_by_approximately_50_compared_to_liu_et_al_2023_and_over_80_compared_to_hyp_op_on_the_uf200-_860_and_uuf200-_860_datasets_hyper_sat_achieves_reductions_of_5699_and_5648_compared_to_liu_et_al_2023_respectively_and_reductions_of_8171_and_7925_compared_to_hyp_op_similarly_on_the_uf250-_1065_and_uuf250-_1065_datasets_hyper_sat_continues_to_outperform_the_baselines_the_reductions_compared_to_liu_et_al_2023_are_5796_and_5838_respectively_while_the_reductions_compared_to_hyp_op_are_8052_and_7717_importantly_even_with_the_larger_datasets_hyper_sat_maintains_or_even_enhances_its_efficacy_this_demonstrates_that_hyper_sat_scales_well_and_performs_better_these_results_show_that_hyper_sat_consistently_provides_substantial_improvements_over_both_baseline_algorithms_across_a_range_of_datasets_including_those_with_larger_problem_sizes_as_a_result_it_validates_its_robustness_and_effectiveness_in_reducing_the_weighted_sum_of_unsatisfied_clauses_44_ablation_study_we_conduct_an_ablation_study_to_evaluate_the_contribution_of_each_key_component_in_our_proposed_model_by_systematically_removing_components_we_analyze_their_impact_on_performance_and_highlight_the_importance_of_each_component_the_experiments_are_designed_to_isolate_the_impact_of_the_following_components_i_the_hypergraph_modeling_of_literal_nodes_rather_than_variable_nodes_ii_the_transformer_module_with_the_cross-_attention_mechanism_iii_the_shared_representation_constraint_loss_the_results_of_the_ablation_study_are_shown_in_table_3_effect_of_hypergraph_modeling_of_literal_nodes_the_performance_drops_by_5277_when_the_hypergraph_modeling_of_variable_nodes_is_used_instead_of_literal_nodes_the_results_demonstrate_the_importance_and_superiority_of_modeling_the_weighted_max_sat_instance_as_a_hypergraph_with_literal_nodes_effect_of_transformer_with_cross-_attention_we_disable_the_transformer_module_with_the_cross-_attention_mechanism_to_assess_its_importance_without_this_module_the_model_experiences_a_performance_drop_of_1154_this_signifi": "Table 3. The results of the ablation study. We consider three components: (i) HGM-L: the hypergraph modeling of literal nodes rather than variable nodes; (ii) Transformer: the transformer module with the cross-attention mechanism; (iii) SRCL: the shared representation constraint loss. Specifically, the first row in the table represents the transformation of the Weighted Max SAT instance into the hypergraph of variables. <table><tr><td>HGM-L</td><td>TRANSFORMER</td><td>SRCL</td><td>RESULT</td></tr><tr><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td><td>182.39</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u00d7</td><td>86.14</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u00d7</td><td>64.08</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u221a</td><td>47.07</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u221a</td><td>41.64</td></tr></table> cantly reduces its ability to effectively capture dependencies across each pair of complementary literals, leading to lower overall accuracy. Effect of Share Representation Constraint Loss: We further remove the shared representation constraint loss and optimize the model with the primary task loss function to investigate the role of the unsupervised multi- objective loss. The results show that without the shared representation constraint loss, the model achieves a performance of 64.08. This performance is lower than that of the original configuration. Therefore, it highlights the importance of the shared representation constraint loss, which encourages the positive and negative literal nodes to develop distinct feature representations. In summary, the full model consistently outperforms the ablated versions, demonstrating the synergistic effect of integrating the hypergraph modeling of literal nodes, cross- attention mechanism, and shared representation constraint loss design. ## 5. Conclusion In this work, we propose Hyper SAT, a novel neural approach for Weighted Max SAT problems, addressing the challenges associated with learning the complex non- linear dependencies and sensitive objective function of this NP- hard problem. By modeling Weighted Max SAT instances as hypergraphs, a hypergraph convolutional network is introduced as the learning model, coupled with a cross- attention mechanism to capture the logical relationships between positive and negative literals. The proposed framework incorporates an unsupervised multi- objective loss design, which not only optimizes the intrinsic objectives of the Weighted Max SAT problem but also ensures that the representations of positive and negative literals remain distinct in the feature space. Extensive experiments demonstrate the potential of Hyper SAT. The experimental results show that Hyper SAT is effective in solving Weighted Max SAT instances and outperforms state- of- the- art competitors in terms of performance. This work offers a fresh perspective on solving the Weighted Max SAT problem through learning- based methods. Future work will focus on further integrating the model with heuristic solvers, improving the design of well- crafted solvers, and exploring its applications in other complex combinatorial problems. ## Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ## References Allouche, D., Traor\u00e9, S., Andr\u00e9, I., De Givry, S., Katsirelos, G., Barbe, S., and Schiex, T. Computational protein design as a cost function network optimization problem. In International Conference on Principles and Practice of Constraint Programming, pp. 840- 849. Springer, 2012. Audemard, G. and Simon, L. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27 (1):1- 25, 2018. Ba, J. L. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Biere, A. and Fleury, M. Gimsatul, isasat and kissat entering the sat competition 2022. Proceedings of SAT Competition, pp. 10- 11, 2022. Cai, S. and Zhang, X. Deep cooperation of cdc1 and local search for sat. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, pp. 64- 81. Springer, 2021. Cameron, C., Chen, R., Hartford, J., and Leyton- Brown, K. Predicting propositional satisfiability via end- to- end learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3324- 3331, 2020. Clarke, E., Biere, A., Raimi, R., and Zhu, Y. Bounded model checking using satisfiability solving. Formal methods in system design, 19:7- 34, 2001. Cook, S. A. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, pp. 151- 158, 1971. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers\n\nto 22 billion parameters. In International Conference on Machine Learning, pp. 7480- 7512. PMLR, 2023. Feng, Y., You, H., Zhang, Z., Ji, R., and Gao, Y. Hypergraph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 3558- 3565, 2019. Guo, W., Zhen, H.- L., Li, X., Luo, W., Yuan, M., Jin, Y., and Yan, J. Machine learning methods in solving the boolean satisfiability problem. Machine Intelligence Research, 20 (5):640- 655, 2023. Heydaribeni, N., Zhan, X., Zhang, R., Eliassi- Rad, T., and Koushanfar, F. Distributed constrained combinatorial optimization leveraging hypergraph neural networks. Nature Machine Intelligence, pp. 1- 9, 2024. Hoos, H. H. and St\u00fctzle, T. SATLIB: An online resource for research on sat. Sat, 2000:283- 292, 2000. Kautz, H. A., Selman, B., et al. Planning as satisfiability. In ECAI, volume 92, pp. 359- 363. Citeseer, 1992. Kingma, D. P. Adam: A method for stochastic optimization. ar Xiv preprint ar Xiv:1412.6980, 2014. Liu, M., Huang, P., Jia, F., Zhang, F., Sun, Y., Cai, S., Ma, F., and Zhang, J. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 16264- 16265, 2023. Ozolins, E., Freivalds, K., Draguns, A., Gaile, E., Zakovskis, R., and Kozlovics, S. Goal- aware neural sat solver. In 2022 International Joint Conference on Neural Networks, pp. 1- 8. IEEE, 2022. Selsam, D. and Bj\u00f8rner, N. Guiding high- performance sat solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, pp. 336- 353. Springer, 2019. Selsam, D., Lamm, M., B\u00fcnz, B., Liang, P., de Moura, L., and Dill, D. L. Learning a sat solver from single- bit supervision. In 7th International Conference on Learning Representations, 2019. Sun, L., Gerault, D., Benamira, A., and Peyrin, T. Neurogift: Using a machine learning based sat solver for cryptanalysis. In Cyber Security Cryptography and Machine Learning: Fourth International Symposium, Proceedings 4, pp. 62- 84. Springer, 2020. Thornton, J. and Sattar, A. Dynamic constraint weighting for over- constrained problems. In Pacific Rim International Conference on Artificial Intelligence, pp. 377- 388. Springer, 1998. Timm, N. and Botha, J. Synthesis of cost- optimal multiagent systems for resource allocation. ar Xiv preprint ar Xiv:2209.09473, 2022. Wang, W., Hu, Y., Tiwari, M., Khurshid, S., Mc Millan, K., and Miikkulainen, R. Neuroback: Improving CDCL SAT solving using graph neural networks. In The Twelfth International Conference on Learning Representations, 2024. Yang, Q., Wu, K., and Jiang, Y. Learning action models from plan examples using weighted max- sat. Artificial Intelligence, 171(2- 3):107- 143, 2007. Zhang, W., Sun, Z., Zhu, Q., Li, G., Cai, S., Xiong, Y., and Zhang, L. Nlocalsat: boosting local search with solution prediction. In Proceedings of the Twenty- Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 1177- 1183, 2021."
  },
  "section_objects": [
    {
      "heading": "HyperSAT Unsupervised Hypergraph Neural Networks f",
      "content": "## Introduction\n\n\nThe neural- guided solvers, on the other hand, integrate neural networks with traditional search frameworks to improve both the efficiency and solution quality of SAT solvers. Along this line, Neuro Core (Selsam & Bj\u00f8rner, 2019) leveraged Graph Neural Networks (GNNs) to predict unsatisfiable cores in SAT instances and further applied this prediction to periodically update the activity score of each variable in the Conflict- Driven Clause Learning (CDCL) solver. NLocal SAT (Zhang et al., 2021) employed a gated graph convolutional network (GGCN) to guide the initialization of assignments in the Stochastic Local Search (SLS) solver. Recently, Neuro Back (Wang et al., 2024) proposed a new approach by utilizing a graph transformer architecture to make offline neural predictions on backbone variable phases for refining the phase selection heuristic in CDCL solvers. For the Max SAT problem, (Liu et al., 2023) was the pioneering work in exploring the use of GNNs for solving the Max SAT problem. This work represented the Max SAT problem as two kinds of factor graphs and studied the capability of typical GNN models in solving the Max SAT problem from a theoretical perspective. Overall, it can be observed that GNNs have shown promising performance in solving both SAT and Max SAT problems, however, the extension of these methods to the Weighted Max SAT problem remains underdeveloped to our best knowledge. The main challenge comes from the uneven weight distribution in the Weighted Max SAT problem. The addition of weights increases the complexity of the search space and also makes the design of effective heuristics more difficult. The neural model needs to learn how to prioritize high- weight clauses over others. Moreover, since a small change in the assignment could result in a significant change in the weighted sum, the hidden structural patterns within Weighted Max SAT instances are much harder to learn. In this work, we consider the problem of designing a neural network solver for Weighted Max SAT problems. Considering the intrinsic difficulties in learning the non- linear dependency and sensitive objective function in the Weighted Max SAT problem, we formulate our learning- based framework Hyper SAT by integrating hypergraph representation, cross- attention mechanism, and multi- objective loss design. We model the Weighted Max SAT instance as a hypergraph. The positive and negative literals of a variable are represented as separate nodes, and each clause is represented as a hyperedge, with an associated weight equal to the weight of the clause. The hypergraph convolutional network (Hyper GCN) is utilized as the learning model, and a specific cross- attention mechanism is designed to capture the logical interplay between the positive and negative literal node features of the same variable. In addition, we design an unsupervised multi- objective loss function to optimize the learning model. Besides the intrinsic optimization objective of the Weighted Max SAT problem, a shared representation constraint ob jective is introduced to ensure that the representations of positive and negative literals of a variable are as distinct as possible in the feature space. We have tested our model on several random Weighted Max SAT datasets in different settings. The experimental results demonstrate that our model outperforms baseline methods across various datasets, achieving significantly better performance. This work provides a new perspective on solving the Weighted Max SAT problem using learning- based methods, with the hope that the results can offer preliminary knowledge into the capability of neural networks for solving Weighted Max SAT problems in the future. In summary, we make the following contributions: - We propose Hyper SAT, an innovative neural approach that uses an unsupervised hypergraph neural network model to solve Weighted Max SAT problems. This is the first work to predict the solution of the Weighted Max SAT problem with Hyper GCN in an end-to-end fashion.- We propose a hypergraph representation for Weighted Max SAT instances and design a cross-attention mechanism along with a shared representation constraint loss function to capture the logical relationships between positive and negative literal nodes within the hypergraph.- We conduct an extensive evaluation of the proposed Hyper SAT on multiple Weighted Max SAT datasets with different distributions. The experimental results demonstrate the superior performance of Hyper SAT. ## 2. Preliminaries We now briefly introduce some relevant preliminaries on Weighted Max SAT and hypergraph neural networks that will be leveraged in later sections. ### 2.1. Weighted Max SAT A Weighted Max SAT instance is represented by a triple \\(\\phi = (\\mathcal{X}, \\mathcal{C}, \\mathbf{w})\\) . Here, the set of variables is denoted by \\(\\mathcal{X} = \\{x_1, x_2, \\ldots , x_n\\}\\) , where each \\(x_i \\in \\{0, 1\\}\\) is a Boolean variable and \\(n\\) is the total number of variables involved in the instance. The set of clauses is given by \\(\\mathcal{C} = \\{C_1, C_2, \\ldots , C_m\\}\\) , where each \\(C_j \\in \\mathcal{C}\\) is a disjunction of literals and \\(m\\) is the number of clauses. The weight vector is given by \\(\\mathbf{w} = (w_1, w_2, \\ldots , w_m)\\) where \\(w_j\\) represents the weight of \\(C_j\\) . Take the clause \\(C_j \\in \\mathcal{C}\\) for example. Suppose it contains \\(k_j\\) literals. Then, it can be represented by \\(C_j = l_{j1} \\lor l_{j2} \\lor \\dots \\lor l_{jk_j}\\) , where each literal \\(l_{ji}\\) is either a variable or the negation of a variable in \\(\\mathcal{X}\\) . The clause \\(C_j\\) is satisfied if at least one of its literals evaluates to true. The clause \\(C_j\\) is unsatisfied if all its literals evaluate to false.\n\n<center>Figure 1. An overview of Hyper SAT framework. </center> An assignment \\(\\boldsymbol {A} = \\{a_{1},a_{2},\\ldots ,a_{n}\\}\\) is to assign each variable \\(x_{i}\\in \\mathcal{X}\\) with a value \\(a_{i}\\in \\{0,1\\}\\) . Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , the objective is to find an assignment \\(\\mathbf{A}\\) for the Boolean variables \\(\\mathcal{X}\\) such that the total weight of satisfied clauses is maximized. The optimization problem is given by \\[\\max_{A} \\sum_{j = 1}^{m} w_{j} \\cdot \\mathbf{1}(C_{j}(\\mathbf{A}))\\] \\[\\mathrm{s.t.} \\mathbf{A} \\in \\{0,1\\}^{n}.\\] Here, \\[\\mathbf{1}(C_{j}(\\mathbf{A})) = \\left\\{ \\begin{array}{ll}1, & \\mathrm{if} C_{j}\\mathrm{is~satisfied~by~}\\mathbf{A},\\\\ 0, & \\mathrm{otherwise}. \\end{array} \\right.\\] ### 2.2. Hypergraph Neural Networks A hypergraph is defined as \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) which includes a set of nodes \\(\\mathcal{V} = \\{v_{1},v_{2},\\dots ,v_{|\\mathcal{V}|}\\}\\) and a set of hyperedges \\(\\mathcal{E} = \\{e_{1},e_{2},\\dots ,e_{|\\mathcal{E}|}\\}\\) . \\(\\mathbf{W}\\) is a diagonal matrix of edge weights which assigns a weight to each hyperedge. Each hyperedge \\(e_{j}\\) is a non- empty subset of nodes (i.e., \\(\\emptyset \\neq e_{j}\\subseteq \\mathcal{V}\\) ). The hypergraph \\(\\mathcal{G}\\) can be represented as a \\(|\\mathcal{V}|\\times |\\mathcal{E}|\\) incidence matrix \\(\\boldsymbol{H}\\) , where \\(\\boldsymbol{H}_{i,j} = 1\\) if \\(v_{i}\\in e_{j}\\) and 0 otherwise. For a vertex \\(v_{i}\\in \\mathcal{V}\\) , its degree is defined as \\(d(v_{i}) = \\sum_{j = 1}^{|\\mathcal{E}|}\\boldsymbol{H}_{i,j}\\boldsymbol{W}_{j,j}\\) . For an edge \\(e_{j}\\in \\mathcal{E}\\) , its degree is defined as \\(\\delta (e_{j}) = \\sum_{i = 1}^{|\\mathcal{V}|}\\boldsymbol{H}_{i,j}\\) . Additionally, \\(\\boldsymbol{D}_{v}\\) and \\(\\boldsymbol{D}_{e}\\) denote the diagonal matrices of the vertex degrees and the edge degrees, respectively. Hypergraph Neural Networks (HGNN) (Feng et al., 2019) perform data representation learning by utilizing a hypergraph structure to capture higher- order dependencies between nodes. In HGNN, hyperedge convolution operations are used to extract features by leveraging the hypergraph Laplacian for spectral convolution. To reduce computational complexity, Chebyshev polynomials are applied to approximate the spectral convolution and avoid the need to compute high- order eigenvectors explicitly. Through the hyperedge convolution operation, the \\(l\\) - th layer of HGNN can be formulated by \\[\\boldsymbol{X}^{(l + 1)} = \\sigma \\left(\\boldsymbol{D}_{v}^{-1 / 2}\\boldsymbol {H}\\boldsymbol {W}\\boldsymbol{D}_{e}^{-1}\\boldsymbol {H}^{\\top}\\boldsymbol{D}_{v}^{-1 / 2}\\boldsymbol {X}^{(l)}\\boldsymbol {\\Theta}^{(l)}\\right),\\] where \\(\\boldsymbol{X}^{(l)}\\in \\mathbb{R}^{|\\mathcal{V}|\\times d_{l}}\\) is the signal of the hypergraph at \\(l\\) layer with \\(|\\mathcal{V}|\\) nodes and \\(d_{l}\\) dimensional features, \\(\\boldsymbol{X}^{(0)}\\) is the original signal of the hypergraph. \\(\\Theta^{(l)}\\) is the learnable filter parameter and \\(\\sigma\\) denotes the nonlinear activation function. ## 3. Hyper SAT In this section, we propose Hyper SAT, a neural approach for Weighted Max SAT problems. Figure 1 illustrates the workflow of Hyper SAT. It mainly consists of three modules: the hypergraph modeling module, the neural network module, and the probabilistic mapping module. Given a Weighted Max SAT instance, Hyper SAT first applies its hypergraph modeling component to represent the instance as a hypergraph. Then, the hypergraph is solved by the neural network module, which is responsible for processing and optimizing the hypergraph. Finally, through a mapping operation, the probabilistic output of the neural network module is mapped into Boolean values, which serves as a solution to the Weighted Max SAT instance. Note that the uneven weight of clauses increases the nonlinear dependency and sensitivity among variables, the neural network is required to learn how to prioritize the contributions of different clauses. To this end, we propose a specific cross- attention mechanism and introduce an unsupervised multi- objective loss function to capture the logical interplay between the positive and negative literal node representations. These two mechanisms are integrated with the hypergraph convolutional network to form the neural network module of our Hyper SAT.\n\n### 3.1. Hypergraph Modeling Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , we construct the hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) as follows: For the construction of the vertex set \\(\\nu\\) , each variable \\(x_{i}\\in \\mathcal{X}\\) is represented by two nodes \\(v_{i}\\) and \\(v_{n + i}\\) , which correspond to the positive and negative literals of the variable \\(x_{i}\\) (i.e., \\(x_{i}\\) and \\(\\neg x_{i}\\) ), respectively. For the construction of the hyperedge set \\(\\mathcal{E}\\) , each clause \\(C_{j}\\in \\mathcal{C}\\) is represented by a hyperedge \\(e_{j}\\in \\mathcal{E}\\) , which connects the nodes corresponding to the literals in \\(C_{j}\\) . Specifically, if clause \\(C_{j}\\) consists of the literals \\(l_{j1},l_{j2},\\ldots ,l_{jk_{j}}\\) , then the corresponding hyperedge \\(e_{j}\\) connects the nodes corresponding to these literals. For the construction of the weight matrix \\(\\mathbf{W}\\) , the weight of each hyperedge is equal to the weight of the corresponding clause, i.e., \\(W_{j,j} = w_{j}\\) . Through the above modeling process, the Weighted Max SAT instance can be uniquely represented by a hypergraph, where each literal is a node, and each clause is a hyperedge connecting the literals involved. The weight of each hyperedge is the weight of the corresponding clause in the instance. Figure 2 gives an illustration of the hypergraph modeling process. <center>Figure 2. Hypergraph Modeling of a Weighted Max SAT instance \\((\\neg x_{1}\\lor x_{2}\\lor \\neg x_{3})\\land (x_{1}\\lor x_{2})\\land (\\neg x_{2}\\lor x_{3}\\lor x_{4})\\land (\\neg x_{1}\\lor x_{3}\\lor \\neg x_{4})\\) with 8 literals, 4 clauses and weights \\(\\{w_{1},w_{2},w_{3},w_{4}\\}\\) . </center> In contrast to existing methods that represent Conjunctive Normal Form (CNF) formulas as factor graphs (Guo et al., 2023), which limit their ability to model relationships beyond pairwise connections, we construct Weighted Max SAT instances as hypergraphs. These hypergraphs can encode higher- order variable dependencies through their degree- free hyperedges, and thus offer more powerful representational capacity and enable more efficient handling of higher- order relationships in combinatorial optimization problems. In particular, we treat literals rather than variables as nodes, which addresses a major issue present in Hyp Op (Heydaribeni et al., 2024). Since the logical relationships between positive and negative literals are central to the Weighted Max SAT problem, it is essential to treat them as distinct nodes, rather than merging them into a single node. In addition, we encode the weights of the clauses in the Weighted Max SAT problem as the weights of the corresponding hyperedges in the hypergraph. ### 3.2. Neural Network Architecture The neural network architecture of our proposed Hyper SAT comprises the Hyper GCN, a transformer module with the cross- attention mechanism, and a softmax layer. #### 3.2.1. HYPERGRAPH CONVOLUTIONAL NETWORKS In Hyper SAT, we introduce a \\(T\\) - layer Hyper GCN for message passing among different nodes. The operation at the \\(l\\) - th layer of the Hyper GCN is formally expressed as follows: \\[\\begin{array}{r}{\\pmb{L}^{(l + 1)^{\\prime}} = \\sigma \\left(\\pmb{D}_{v}^{-\\frac{1}{2}}\\widetilde{\\pmb{Q}}\\pmb{D}_{v}^{-\\frac{1}{2}}\\pmb{L}^{(l)}\\pmb{R}^{(l)}\\right).} \\end{array} \\quad (2)\\] In this formulation, the matrix \\(\\pmb{L}^{(l + 1)^{\\prime}}\\) represents the output of the \\(l\\) - th layer; \\(\\pmb{D}_{v}\\) is the diagonal matrix of the vertex degrees in the hypergraph; \\(\\pmb{L}^{(l)}\\in \\mathbb{R}^{2n\\times d_{l}}\\) is the matrix of node representations at the \\(l\\) - th layer, where \\(d_{l}\\) is the dimension of the \\(l\\) - th layer node representations; \\(\\pmb{R}^{(l)}\\in\\) \\(\\mathbb{R}^{d_{l}\\times d_{l + 1}}\\) is the \\(l\\) - th layer learnable weight matrix; and \\(\\widetilde{\\pmb{Q}}\\) is given by \\[\\widetilde{\\pmb{Q}} = \\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top} - \\mathrm{diag}(\\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top}), \\quad (3)\\] where \\(\\pmb{H}\\) is the hypergraph incidence matrix, and \\(\\widetilde{\\pmb{D}}_{e} =\\) \\(\\pmb{D}_{e} - \\pmb{I}\\) . Specifically, \\(\\sigma\\) denotes the nonlinear activation function and \\(\\pmb{L}^{(0)}\\) is a learnable input embedding of Hyper GCN. Compared to the updating rule in Eq. (1) in traditional HGNN, our Hyper GCN focuses the convolutional layer's computation more on the influence of adjacent nodes by removing the diagonal elements of \\(\\widetilde{\\pmb{Q}}\\) . This adjustment allows the representation updates of each node to better align with the higher- order relationships of the adjacency structure. #### 3.2.2. CROSS-ATTENTION MECHANISM The core of the Weighted Max SAT problem lies in the logical constraints among variables. Positive and negative literal nodes (e.g., \\(x\\) and \\(\\neg x\\) ) are logically mutually exclusive and strongly correlated, and their relationships directly reflect the underlying structural characteristics of the problem.\n\nConsidering this, we leverage a cross- attention mechanism to dynamically assign importance weights to the relationships between each pair of complementary literals. This mechanism enables the model to adaptively capture critical logical properties such as mutual exclusivity, dependency, and the process of information exchange between positive and negative literal nodes. Therefore, after updating each node representation in the hypergraph convolution layer, a cross- attention layer is introduced to enhance the representations of positive and negative literal nodes. Specifically, given the \\(l\\) - th layer output of the Hyper GCN \\(\\pmb{L}^{(l + 1)}\\) , we divide it into two parts: \\(\\pmb{L}^{(l + 1)^{\\prime}} =\\) \\(\\left[\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\right]\\) , where \\(\\pmb{L}_{+}^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the positive literal node representations and \\(\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the negative literal node representations. The cross- attention mechanism is mathematically represented as follows: \\[\\begin{array}{r l} & {\\pmb{L}_{+}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{+}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{-}^{(l + 1)},}\\\\ & {\\pmb{L}_{-}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{-}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{+}^{(l + 1)}.} \\end{array} \\quad (4)\\] In this formulation, \\[\\begin{array}{r l} & {\\pmb{Q}_{+}^{(l + 1)} = \\pmb{W}_{Q}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{Q}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{K}_{+}^{(l + 1)} = \\pmb{W}_{K}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{K}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{V}_{+}^{(l + 1)} = \\pmb{W}_{V}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{V}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},} \\end{array} \\quad (5)\\] where \\(\\pmb{W}_{Q}^{(l + 1)}\\) , \\(\\pmb{W}_{K}^{(l + 1)}\\) , \\(\\pmb{W}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrices for the positive literal nodes at the \\(l\\) - th layer, and \\(\\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrix for the negative literal nodes at the \\(l\\) - th layer. The final node representation at the \\(l\\) - th layer is obtained as \\(\\pmb{L}^{(l + 1)} = [\\pmb{L}_{+}^{(l + 1)},\\pmb{L}_{- }^{(l + 1)}]\\) . The cross- attention mechanism explicitly constructs the interaction between positive and negative literal nodes by enabling each node to incorporate the features of its counterpart. This facilitates a more comprehensive representation of node features for Weighted Max SAT problems. Building upon this, the transformer module in Hyper SAT incorporates the previously discussed cross- attention layer, along with other components to further enhance the modeling capability. Inspired by the recent advancements in Vision Transformer architecture (Vi T- 22B (Dehghani et al., 2023)), the transformer module in Hyper SAT includes a Layer Norm layer (Ba, 2016), followed by a combination of a cross- attention layer and a Feed- Forward Network (FFN) layer, along with a residual connection and an additional Layer Norm layer. The FFN and cross- attention layers operate in parallel to enhance the efficiency. The residual connection is introduced by adding the outputs from the cross- attention layer, the FFN layer, and the Layer Norm layer. The resulting sum is then passed through another Layer Norm layer to stabilize the representations before proceeding to the next layer. The final layer of the network is a softmax layer. We reshape the iterated \\(\\pmb{L}^{(T)^{\\prime}}\\in \\mathbb{R}^{2n\\times 1}\\) into \\(\\hat{\\pmb{L}}^{(T)^{\\prime}}\\in \\mathbb{R}^{n\\times 2}\\) , which serves as the input to the softmax layer. After applying the softmax function, we obtain \\(\\hat{\\pmb{L}} = \\mathrm{softmax}(\\hat{\\pmb{L}}^{(T)^{\\prime}})\\) , which provides the soft node assignments for each literal, interpreted as class probabilities. The probability of assigning a variable to a truth value is recorded by \\(\\pmb{Y} = \\hat{\\pmb{L}}_{- 1}\\) , which is the first column of \\(\\hat{\\pmb{L}}\\) . #### 3.2.3. LOSS FUNCTION Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) and its hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) , we relax the Boolean variables \\(\\mathcal{X}\\) into continuous probability parameters \\(\\mathbf{Y}(\\gamma)\\) where \\(\\gamma = (\\pmb {R},\\pmb {L}^{(0)},\\pmb {W}_{Q},\\pmb {W}_{K},\\pmb {W}_{V},\\widetilde{\\pmb{W}}_{Q},\\widetilde{\\pmb{W}}_{K},\\widetilde{\\pmb{W}}_{V})\\) represent the learnable parameters. The relaxation is defined as follows: \\[\\mathcal{X}\\in \\{0,1\\}^{n}\\longrightarrow \\mathbf{Y}(\\gamma)\\in [0,1]^{n}. \\quad (6)\\] With this relaxation, we can design a differentiable loss function to optimize the learnable parameters \\(\\gamma\\) , enabling gradient- based optimization. In this paper, we propose an unsupervised multi- objective loss function that consists of two components: a primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) and a shared representation constraint loss \\(\\mathcal{L}_{\\mathrm{shared}}\\) . This unsupervised loss function obviates the necessity for large, labeled training datasets, which are conventionally indispensable in supervised learning paradigms. The specific form is as follows: \\[\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{task}} + \\lambda \\mathcal{L}_{\\mathrm{shared}}, \\quad (7)\\] where \\(\\lambda \\geq 0\\) is a balancing hyperparameter. The primary task loss function is the relaxed optimization objective of the Weighted Max SAT problem, as shown below: \\[\\mathcal{L}_{\\mathrm{task}}(\\mathbf{Y}) = \\sum_{j = 1}^{m}w_{j}V_{j}(\\mathbf{Y}), \\quad (8)\\] where \\[V_{j}(\\mathbf{Y}) = 1 - \\prod_{i\\in C_{j}^{+}}(1 - y_{i})\\prod_{i\\in C_{j}^{-}}y_{i}. \\quad (9)\\] Here, \\(C_{j}^{+}\\) and \\(C_{j}^{- }\\) are the index sets of variables appearing in the clause \\(C_{j}\\) in the positive and negative form, respectively. The term \\(V_{j}(\\mathbf{Y})\\) represents the satisfaction of clause \\(C_{j}\\) , where a value of 1 indicates that the clause is satisfied, and 0 indicates it is not. The weight \\(w_{j}\\) ensures that\n\nmore important clauses are prioritized during optimization. Minimizing the primary task loss function corresponds to maximizing the weighted sum of satisfied clauses in the Weighted Max SAT problem. The shared representation constraint loss function focuses on the representations of positive and negative literal nodes in the penultimate layer of the Hyper SAT network. Its form is given by \\[\\mathcal{L}_{\\mathrm{shared}} = \\left\\| \\pmb{L}_{+}^{(T - 1)} + \\pmb{L}_{-}^{(T - 1)}\\right\\|_{F}^{2}, \\quad (10)\\] where \\(\\left\\| \\cdot \\right\\|_{F}\\) denotes the Frobenius norm. By applying the shared representation constraint loss function, the network encourages the positive and negative literal nodes to develop distinct feature representations in the learning process. This constraint enhances the separation of the two types of nodes in the feature space. It should be noted that most neural network- based SAT methods employ supervised learning. However, supervised learning approaches are not particularly well- suited for solving the Weighted Max SAT problem. On the one hand, a Weighted Max SAT instance may have multiple satisfying assignments. On the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the Weighted Max SAT instance. From this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances. ### 3.3. Probabilistic Mapping The neural network module takes the hypergraph as input and produces a probability vector \\(\\mathbf{Y}\\) as output, which indicates the probability of each variable taking the truth value. Accordingly, to convert the probability vector \\(\\mathbf{Y}\\) into a Boolean assignment, we transform \\(\\mathbf{Y}\\) into \\(n\\) Bernoulli distributions, where the \\(i\\) - th distribution \\(B(y_{i})\\) is over the discrete values \\(\\{0,1\\}\\) . Finally, the node assignments are generated by sampling with the corresponding probability distributions. ## 4. Experiment ### 4.1. Experimental Settings We compare the performance of Hyper SAT against GNN- based methods for solving Weighted Max SAT problems. Baseline Algorithms. We compare our proposed Hyper SAT against GNN- based methods. The following algorithms are considered as baselines: (i) Hyp Op (Heydaribeni et al., 2024): an advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs; (ii) (Liu et al., 2023): an innovative supervised GNN- based approach that predicts solutions in an end- to- end manner by transforming CNF formulas into factor graphs. We apply these baselines to solve Weighted Max SAT problems. Datasets. We evaluate the algorithms using random 3- SAT benchmarks from the SATLIB dataset (Hoos & St\u00fctzle, 2000). The SATLIB dataset is one of the standard datasets for evaluating SAT solvers. We utilize a range of datasets with varying distributions, from which SAT and UNSAT instances are generated for each distribution. The number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. More details on the datasets are provided in Table 1. In particular, to generate the required Weighted Max SAT instances, we assign weights to the clauses in each CNF file within the dataset. These weights are sampled from the integers in the range \\([1,10]\\) uniformly at random. Table 1. The parameters of the datasets. <table><tr><td>DATASET</td><td>INSTANCE</td><td>VARS</td><td>CLAUSES</td><td>TYPES</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>SAT</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>UNSAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>SAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>UNSAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>SAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>UNSAT</td></tr></table> Model Settings. Hyper GCN is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. The dimension of the cross- attention layer is the square root of the number of variables. The dropout used in the cross- attention layer is 0.1. The model is optimized using the Adam optimizer (Kingma, 2014) with a learning rate of \\(7 \\times 10^{- 2}\\) . The balancing hyperparameter of loss function \\(\\lambda\\) is \\(2 \\times 10^{- 3}\\) . The FFN consists of two linear transformations and a Re LU activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. An early stopping strategy is employed, with a tolerance of \\(10^{- 4}\\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. Finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained. Evaluation Configuration. All experiments are conducted on an NVIDIA A100- SXM4- 40GB GPU, using Python 3.9.30 and Py Torch 1.13.0.\n\n### 4.2. Analytical Experiment We first evaluate the performance of unsupervised methods, Hyper SAT and Hyp Op, with a focus on their convergence. We evaluate the convergence using the primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) from Eq. (8), which represents the sum of the weights of unsatisfied clauses. We illustrate the evolution curves of loss for Hyper SAT and Hyp Op on the uuf250- 1065 dataset in Figure 3 as an example. All models can converge within 300 epochs. Moreover, the loss of Hyper SAT is around 52, while the loss of Hyp Op is around 139. We can observe that Hyper SAT achieves better performance than Hyp Op. Specifically, Hyper SAT decreases the loss more quickly and achieves a lower loss value. The experimental results demonstrate that Hyper SAT can be used in learning to solve Weighted Max SAT problems. <center>Figure 3. The evolution of loss for Hyper SAT and Hyp Op during an inference process of 300 epochs on the uuf250-1065 dataset. </center> ### 4.3. Result We evaluate the performance of Hyper SAT against baseline algorithms Hyp Op and (Liu et al., 2023) on various datasets. The primary evaluation metric is the average weighted sum of unsatisfied clauses. The experiments are conducted on datasets with the number of variables ranging from 100 to 250, and the number of clauses varying between 430 and 1065. The results are shown in Table 2. As presented in the table, Hyper SAT consistently achieves lower values for the average weighted sum of unsatisfied clauses compared to the baseline algorithms (Liu et al., 2023) and Hyp Op across multiple datasets. For example, on the uf100- 430 dataset, Hyper SAT significantly outperforms both baselines with a result of 15.64. This result represents a substantial improvement over the results of (Liu et al., 2023) (32.48) and Hyp Op (99.15). This trend is observed across all datasets, where Hyper SAT always exceeds the Table 2. The average weighted sum of unsatisfied clauses of Weighted Max SAT problems. <table><tr><td>DATASET</td><td>LIU ET AL. (2023)</td><td>HYPOP</td><td>HYPERSAT</td></tr><tr><td>UF100-430</td><td>32.48</td><td>99.15</td><td>15.64</td></tr><tr><td>UUF100-430</td><td>41.65</td><td>102.44</td><td>20.46</td></tr><tr><td>UF200-860</td><td>67.38</td><td>158.46</td><td>28.98</td></tr><tr><td>UUF200-860</td><td>81.68</td><td>171.34</td><td>35.55</td></tr><tr><td>UF250-1065</td><td>79.06</td><td>170.60</td><td>33.24</td></tr><tr><td>UUF250-1065</td><td>100.04</td><td>182.39</td><td>41.64</td></tr></table> performance of the baselines. The average weight of unsatisfied clauses is reduced by approximately \\(50\\%\\) compared to (Liu et al., 2023) and over \\(80\\%\\) compared to Hyp Op. On the uf200- 860 and uuf200- 860 datasets, Hyper SAT achieves reductions of \\(56.99\\%\\) and \\(56.48\\%\\) compared to (Liu et al., 2023), respectively, and reductions of \\(81.71\\%\\) and \\(79.25\\%\\) compared to Hyp Op. Similarly, on the uf250- 1065 and uuf250- 1065 datasets, Hyper SAT continues to outperform the baselines. The reductions compared to (Liu et al., 2023) are \\(57.96\\%\\) and \\(58.38\\%\\) , respectively, while the reductions compared to Hyp Op are \\(80.52\\%\\) and \\(77.17\\%\\) . Importantly, even with the larger datasets, Hyper SAT maintains or even enhances its efficacy. This demonstrates that Hyper SAT scales well and performs better. These results show that Hyper SAT consistently provides substantial improvements over both baseline algorithms across a range of datasets, including those with larger problem sizes. As a result, it validates its robustness and effectiveness in reducing the weighted sum of unsatisfied clauses. ### 4.4. Ablation Study We conduct an ablation study to evaluate the contribution of each key component in our proposed model. By systematically removing components, we analyze their impact on performance and highlight the importance of each component. The experiments are designed to isolate the impact of the following components: (i) the hypergraph modeling of literal nodes rather than variable nodes; (ii) the transformer module with the cross- attention mechanism; (iii) the shared representation constraint loss. The results of the ablation study are shown in Table 3. Effect of Hypergraph Modeling of Literal Nodes: The performance drops by \\(52.77\\%\\) when the hypergraph modeling of variable nodes is used instead of literal nodes. The results demonstrate the importance and superiority of modeling the Weighted Max SAT instance as a hypergraph with literal nodes. Effect of Transformer with Cross- Attention: We disable the transformer module with the cross- attention mechanism to assess its importance. Without this module, the model experiences a performance drop of \\(11.54\\%\\) . This signifi\n\nTable 3. The results of the ablation study. We consider three components: (i) HGM-L: the hypergraph modeling of literal nodes rather than variable nodes; (ii) Transformer: the transformer module with the cross-attention mechanism; (iii) SRCL: the shared representation constraint loss. Specifically, the first row in the table represents the transformation of the Weighted Max SAT instance into the hypergraph of variables. <table><tr><td>HGM-L</td><td>TRANSFORMER</td><td>SRCL</td><td>RESULT</td></tr><tr><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td><td>182.39</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u00d7</td><td>86.14</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u00d7</td><td>64.08</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u221a</td><td>47.07</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u221a</td><td>41.64</td></tr></table> cantly reduces its ability to effectively capture dependencies across each pair of complementary literals, leading to lower overall accuracy. Effect of Share Representation Constraint Loss: We further remove the shared representation constraint loss and optimize the model with the primary task loss function to investigate the role of the unsupervised multi- objective loss. The results show that without the shared representation constraint loss, the model achieves a performance of 64.08. This performance is lower than that of the original configuration. Therefore, it highlights the importance of the shared representation constraint loss, which encourages the positive and negative literal nodes to develop distinct feature representations. In summary, the full model consistently outperforms the ablated versions, demonstrating the synergistic effect of integrating the hypergraph modeling of literal nodes, cross- attention mechanism, and shared representation constraint loss design. ## 5. Conclusion In this work, we propose Hyper SAT, a novel neural approach for Weighted Max SAT problems, addressing the challenges associated with learning the complex non- linear dependencies and sensitive objective function of this NP- hard problem. By modeling Weighted Max SAT instances as hypergraphs, a hypergraph convolutional network is introduced as the learning model, coupled with a cross- attention mechanism to capture the logical relationships between positive and negative literals. The proposed framework incorporates an unsupervised multi- objective loss design, which not only optimizes the intrinsic objectives of the Weighted Max SAT problem but also ensures that the representations of positive and negative literals remain distinct in the feature space. Extensive experiments demonstrate the potential of Hyper SAT. The experimental results show that Hyper SAT is effective in solving Weighted Max SAT instances and outperforms state- of- the- art competitors in terms of performance. This work offers a fresh perspective on solving the Weighted Max SAT problem through learning- based methods. Future work will focus on further integrating the model with heuristic solvers, improving the design of well- crafted solvers, and exploring its applications in other complex combinatorial problems. ## Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ## References Allouche, D., Traor\u00e9, S., Andr\u00e9, I., De Givry, S., Katsirelos, G., Barbe, S., and Schiex, T. Computational protein design as a cost function network optimization problem. In International Conference on Principles and Practice of Constraint Programming, pp. 840- 849. Springer, 2012. Audemard, G. and Simon, L. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27 (1):1- 25, 2018. Ba, J. L. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Biere, A. and Fleury, M. Gimsatul, isasat and kissat entering the sat competition 2022. Proceedings of SAT Competition, pp. 10- 11, 2022. Cai, S. and Zhang, X. Deep cooperation of cdc1 and local search for sat. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, pp. 64- 81. Springer, 2021. Cameron, C., Chen, R., Hartford, J., and Leyton- Brown, K. Predicting propositional satisfiability via end- to- end learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3324- 3331, 2020. Clarke, E., Biere, A., Raimi, R., and Zhu, Y. Bounded model checking using satisfiability solving. Formal methods in system design, 19:7- 34, 2001. Cook, S. A. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, pp. 151- 158, 1971. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers\n\nto 22 billion parameters. In International Conference on Machine Learning, pp. 7480- 7512. PMLR, 2023. Feng, Y., You, H., Zhang, Z., Ji, R., and Gao, Y. Hypergraph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 3558- 3565, 2019. Guo, W., Zhen, H.- L., Li, X., Luo, W., Yuan, M., Jin, Y., and Yan, J. Machine learning methods in solving the boolean satisfiability problem. Machine Intelligence Research, 20 (5):640- 655, 2023. Heydaribeni, N., Zhan, X., Zhang, R., Eliassi- Rad, T., and Koushanfar, F. Distributed constrained combinatorial optimization leveraging hypergraph neural networks. Nature Machine Intelligence, pp. 1- 9, 2024. Hoos, H. H. and St\u00fctzle, T. SATLIB: An online resource for research on sat. Sat, 2000:283- 292, 2000. Kautz, H. A., Selman, B., et al. Planning as satisfiability. In ECAI, volume 92, pp. 359- 363. Citeseer, 1992. Kingma, D. P. Adam: A method for stochastic optimization. ar Xiv preprint ar Xiv:1412.6980, 2014. Liu, M., Huang, P., Jia, F., Zhang, F., Sun, Y., Cai, S., Ma, F., and Zhang, J. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 16264- 16265, 2023. Ozolins, E., Freivalds, K., Draguns, A., Gaile, E., Zakovskis, R., and Kozlovics, S. Goal- aware neural sat solver. In 2022 International Joint Conference on Neural Networks, pp. 1- 8. IEEE, 2022. Selsam, D. and Bj\u00f8rner, N. Guiding high- performance sat solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, pp. 336- 353. Springer, 2019. Selsam, D., Lamm, M., B\u00fcnz, B., Liang, P., de Moura, L., and Dill, D. L. Learning a sat solver from single- bit supervision. In 7th International Conference on Learning Representations, 2019. Sun, L., Gerault, D., Benamira, A., and Peyrin, T. Neurogift: Using a machine learning based sat solver for cryptanalysis. In Cyber Security Cryptography and Machine Learning: Fourth International Symposium, Proceedings 4, pp. 62- 84. Springer, 2020. Thornton, J. and Sattar, A. Dynamic constraint weighting for over- constrained problems. In Pacific Rim International Conference on Artificial Intelligence, pp. 377- 388. Springer, 1998. Timm, N. and Botha, J. Synthesis of cost- optimal multiagent systems for resource allocation. ar Xiv preprint ar Xiv:2209.09473, 2022. Wang, W., Hu, Y., Tiwari, M., Khurshid, S., Mc Millan, K., and Miikkulainen, R. Neuroback: Improving CDCL SAT solving using graph neural networks. In The Twelfth International Conference on Learning Representations, 2024. Yang, Q., Wu, K., and Jiang, Y. Learning action models from plan examples using weighted max- sat. Artificial Intelligence, 171(2- 3):107- 143, 2007. Zhang, W., Sun, Z., Zhu, Q., Li, G., Cai, S., Xiong, Y., and Zhang, L. Nlocalsat: boosting local search with solution prediction. In Proceedings of the Twenty- Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 1177- 1183, 2021.",
      "level": 1,
      "line_start": 1,
      "line_end": 21
    },
    {
      "heading": "Introduction",
      "content": "The neural- guided solvers, on the other hand, integrate neural networks with traditional search frameworks to improve both the efficiency and solution quality of SAT solvers. Along this line, Neuro Core (Selsam & Bj\u00f8rner, 2019) leveraged Graph Neural Networks (GNNs) to predict unsatisfiable cores in SAT instances and further applied this prediction to periodically update the activity score of each variable in the Conflict- Driven Clause Learning (CDCL) solver. NLocal SAT (Zhang et al., 2021) employed a gated graph convolutional network (GGCN) to guide the initialization of assignments in the Stochastic Local Search (SLS) solver. Recently, Neuro Back (Wang et al., 2024) proposed a new approach by utilizing a graph transformer architecture to make offline neural predictions on backbone variable phases for refining the phase selection heuristic in CDCL solvers. For the Max SAT problem, (Liu et al., 2023) was the pioneering work in exploring the use of GNNs for solving the Max SAT problem. This work represented the Max SAT problem as two kinds of factor graphs and studied the capability of typical GNN models in solving the Max SAT problem from a theoretical perspective. Overall, it can be observed that GNNs have shown promising performance in solving both SAT and Max SAT problems, however, the extension of these methods to the Weighted Max SAT problem remains underdeveloped to our best knowledge. The main challenge comes from the uneven weight distribution in the Weighted Max SAT problem. The addition of weights increases the complexity of the search space and also makes the design of effective heuristics more difficult. The neural model needs to learn how to prioritize high- weight clauses over others. Moreover, since a small change in the assignment could result in a significant change in the weighted sum, the hidden structural patterns within Weighted Max SAT instances are much harder to learn. In this work, we consider the problem of designing a neural network solver for Weighted Max SAT problems. Considering the intrinsic difficulties in learning the non- linear dependency and sensitive objective function in the Weighted Max SAT problem, we formulate our learning- based framework Hyper SAT by integrating hypergraph representation, cross- attention mechanism, and multi- objective loss design. We model the Weighted Max SAT instance as a hypergraph. The positive and negative literals of a variable are represented as separate nodes, and each clause is represented as a hyperedge, with an associated weight equal to the weight of the clause. The hypergraph convolutional network (Hyper GCN) is utilized as the learning model, and a specific cross- attention mechanism is designed to capture the logical interplay between the positive and negative literal node features of the same variable. In addition, we design an unsupervised multi- objective loss function to optimize the learning model. Besides the intrinsic optimization objective of the Weighted Max SAT problem, a shared representation constraint ob jective is introduced to ensure that the representations of positive and negative literals of a variable are as distinct as possible in the feature space. We have tested our model on several random Weighted Max SAT datasets in different settings. The experimental results demonstrate that our model outperforms baseline methods across various datasets, achieving significantly better performance. This work provides a new perspective on solving the Weighted Max SAT problem using learning- based methods, with the hope that the results can offer preliminary knowledge into the capability of neural networks for solving Weighted Max SAT problems in the future. In summary, we make the following contributions: - We propose Hyper SAT, an innovative neural approach that uses an unsupervised hypergraph neural network model to solve Weighted Max SAT problems. This is the first work to predict the solution of the Weighted Max SAT problem with Hyper GCN in an end-to-end fashion.- We propose a hypergraph representation for Weighted Max SAT instances and design a cross-attention mechanism along with a shared representation constraint loss function to capture the logical relationships between positive and negative literal nodes within the hypergraph.- We conduct an extensive evaluation of the proposed Hyper SAT on multiple Weighted Max SAT datasets with different distributions. The experimental results demonstrate the superior performance of Hyper SAT. ## 2. Preliminaries We now briefly introduce some relevant preliminaries on Weighted Max SAT and hypergraph neural networks that will be leveraged in later sections. ### 2.1. Weighted Max SAT A Weighted Max SAT instance is represented by a triple \\(\\phi = (\\mathcal{X}, \\mathcal{C}, \\mathbf{w})\\) . Here, the set of variables is denoted by \\(\\mathcal{X} = \\{x_1, x_2, \\ldots , x_n\\}\\) , where each \\(x_i \\in \\{0, 1\\}\\) is a Boolean variable and \\(n\\) is the total number of variables involved in the instance. The set of clauses is given by \\(\\mathcal{C} = \\{C_1, C_2, \\ldots , C_m\\}\\) , where each \\(C_j \\in \\mathcal{C}\\) is a disjunction of literals and \\(m\\) is the number of clauses. The weight vector is given by \\(\\mathbf{w} = (w_1, w_2, \\ldots , w_m)\\) where \\(w_j\\) represents the weight of \\(C_j\\) . Take the clause \\(C_j \\in \\mathcal{C}\\) for example. Suppose it contains \\(k_j\\) literals. Then, it can be represented by \\(C_j = l_{j1} \\lor l_{j2} \\lor \\dots \\lor l_{jk_j}\\) , where each literal \\(l_{ji}\\) is either a variable or the negation of a variable in \\(\\mathcal{X}\\) . The clause \\(C_j\\) is satisfied if at least one of its literals evaluates to true. The clause \\(C_j\\) is unsatisfied if all its literals evaluate to false.\n\n<center>Figure 1. An overview of Hyper SAT framework. </center> An assignment \\(\\boldsymbol {A} = \\{a_{1},a_{2},\\ldots ,a_{n}\\}\\) is to assign each variable \\(x_{i}\\in \\mathcal{X}\\) with a value \\(a_{i}\\in \\{0,1\\}\\) . Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , the objective is to find an assignment \\(\\mathbf{A}\\) for the Boolean variables \\(\\mathcal{X}\\) such that the total weight of satisfied clauses is maximized. The optimization problem is given by \\[\\max_{A} \\sum_{j = 1}^{m} w_{j} \\cdot \\mathbf{1}(C_{j}(\\mathbf{A}))\\] \\[\\mathrm{s.t.} \\mathbf{A} \\in \\{0,1\\}^{n}.\\] Here, \\[\\mathbf{1}(C_{j}(\\mathbf{A})) = \\left\\{ \\begin{array}{ll}1, & \\mathrm{if} C_{j}\\mathrm{is~satisfied~by~}\\mathbf{A},\\\\ 0, & \\mathrm{otherwise}. \\end{array} \\right.\\] ### 2.2. Hypergraph Neural Networks A hypergraph is defined as \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) which includes a set of nodes \\(\\mathcal{V} = \\{v_{1},v_{2},\\dots ,v_{|\\mathcal{V}|}\\}\\) and a set of hyperedges \\(\\mathcal{E} = \\{e_{1},e_{2},\\dots ,e_{|\\mathcal{E}|}\\}\\) . \\(\\mathbf{W}\\) is a diagonal matrix of edge weights which assigns a weight to each hyperedge. Each hyperedge \\(e_{j}\\) is a non- empty subset of nodes (i.e., \\(\\emptyset \\neq e_{j}\\subseteq \\mathcal{V}\\) ). The hypergraph \\(\\mathcal{G}\\) can be represented as a \\(|\\mathcal{V}|\\times |\\mathcal{E}|\\) incidence matrix \\(\\boldsymbol{H}\\) , where \\(\\boldsymbol{H}_{i,j} = 1\\) if \\(v_{i}\\in e_{j}\\) and 0 otherwise. For a vertex \\(v_{i}\\in \\mathcal{V}\\) , its degree is defined as \\(d(v_{i}) = \\sum_{j = 1}^{|\\mathcal{E}|}\\boldsymbol{H}_{i,j}\\boldsymbol{W}_{j,j}\\) . For an edge \\(e_{j}\\in \\mathcal{E}\\) , its degree is defined as \\(\\delta (e_{j}) = \\sum_{i = 1}^{|\\mathcal{V}|}\\boldsymbol{H}_{i,j}\\) . Additionally, \\(\\boldsymbol{D}_{v}\\) and \\(\\boldsymbol{D}_{e}\\) denote the diagonal matrices of the vertex degrees and the edge degrees, respectively. Hypergraph Neural Networks (HGNN) (Feng et al., 2019) perform data representation learning by utilizing a hypergraph structure to capture higher- order dependencies between nodes. In HGNN, hyperedge convolution operations are used to extract features by leveraging the hypergraph Laplacian for spectral convolution. To reduce computational complexity, Chebyshev polynomials are applied to approximate the spectral convolution and avoid the need to compute high- order eigenvectors explicitly. Through the hyperedge convolution operation, the \\(l\\) - th layer of HGNN can be formulated by \\[\\boldsymbol{X}^{(l + 1)} = \\sigma \\left(\\boldsymbol{D}_{v}^{-1 / 2}\\boldsymbol {H}\\boldsymbol {W}\\boldsymbol{D}_{e}^{-1}\\boldsymbol {H}^{\\top}\\boldsymbol{D}_{v}^{-1 / 2}\\boldsymbol {X}^{(l)}\\boldsymbol {\\Theta}^{(l)}\\right),\\] where \\(\\boldsymbol{X}^{(l)}\\in \\mathbb{R}^{|\\mathcal{V}|\\times d_{l}}\\) is the signal of the hypergraph at \\(l\\) layer with \\(|\\mathcal{V}|\\) nodes and \\(d_{l}\\) dimensional features, \\(\\boldsymbol{X}^{(0)}\\) is the original signal of the hypergraph. \\(\\Theta^{(l)}\\) is the learnable filter parameter and \\(\\sigma\\) denotes the nonlinear activation function. ## 3. Hyper SAT In this section, we propose Hyper SAT, a neural approach for Weighted Max SAT problems. Figure 1 illustrates the workflow of Hyper SAT. It mainly consists of three modules: the hypergraph modeling module, the neural network module, and the probabilistic mapping module. Given a Weighted Max SAT instance, Hyper SAT first applies its hypergraph modeling component to represent the instance as a hypergraph. Then, the hypergraph is solved by the neural network module, which is responsible for processing and optimizing the hypergraph. Finally, through a mapping operation, the probabilistic output of the neural network module is mapped into Boolean values, which serves as a solution to the Weighted Max SAT instance. Note that the uneven weight of clauses increases the nonlinear dependency and sensitivity among variables, the neural network is required to learn how to prioritize the contributions of different clauses. To this end, we propose a specific cross- attention mechanism and introduce an unsupervised multi- objective loss function to capture the logical interplay between the positive and negative literal node representations. These two mechanisms are integrated with the hypergraph convolutional network to form the neural network module of our Hyper SAT.\n\n### 3.1. Hypergraph Modeling Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , we construct the hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) as follows: For the construction of the vertex set \\(\\nu\\) , each variable \\(x_{i}\\in \\mathcal{X}\\) is represented by two nodes \\(v_{i}\\) and \\(v_{n + i}\\) , which correspond to the positive and negative literals of the variable \\(x_{i}\\) (i.e., \\(x_{i}\\) and \\(\\neg x_{i}\\) ), respectively. For the construction of the hyperedge set \\(\\mathcal{E}\\) , each clause \\(C_{j}\\in \\mathcal{C}\\) is represented by a hyperedge \\(e_{j}\\in \\mathcal{E}\\) , which connects the nodes corresponding to the literals in \\(C_{j}\\) . Specifically, if clause \\(C_{j}\\) consists of the literals \\(l_{j1},l_{j2},\\ldots ,l_{jk_{j}}\\) , then the corresponding hyperedge \\(e_{j}\\) connects the nodes corresponding to these literals. For the construction of the weight matrix \\(\\mathbf{W}\\) , the weight of each hyperedge is equal to the weight of the corresponding clause, i.e., \\(W_{j,j} = w_{j}\\) . Through the above modeling process, the Weighted Max SAT instance can be uniquely represented by a hypergraph, where each literal is a node, and each clause is a hyperedge connecting the literals involved. The weight of each hyperedge is the weight of the corresponding clause in the instance. Figure 2 gives an illustration of the hypergraph modeling process. <center>Figure 2. Hypergraph Modeling of a Weighted Max SAT instance \\((\\neg x_{1}\\lor x_{2}\\lor \\neg x_{3})\\land (x_{1}\\lor x_{2})\\land (\\neg x_{2}\\lor x_{3}\\lor x_{4})\\land (\\neg x_{1}\\lor x_{3}\\lor \\neg x_{4})\\) with 8 literals, 4 clauses and weights \\(\\{w_{1},w_{2},w_{3},w_{4}\\}\\) . </center> In contrast to existing methods that represent Conjunctive Normal Form (CNF) formulas as factor graphs (Guo et al., 2023), which limit their ability to model relationships beyond pairwise connections, we construct Weighted Max SAT instances as hypergraphs. These hypergraphs can encode higher- order variable dependencies through their degree- free hyperedges, and thus offer more powerful representational capacity and enable more efficient handling of higher- order relationships in combinatorial optimization problems. In particular, we treat literals rather than variables as nodes, which addresses a major issue present in Hyp Op (Heydaribeni et al., 2024). Since the logical relationships between positive and negative literals are central to the Weighted Max SAT problem, it is essential to treat them as distinct nodes, rather than merging them into a single node. In addition, we encode the weights of the clauses in the Weighted Max SAT problem as the weights of the corresponding hyperedges in the hypergraph. ### 3.2. Neural Network Architecture The neural network architecture of our proposed Hyper SAT comprises the Hyper GCN, a transformer module with the cross- attention mechanism, and a softmax layer. #### 3.2.1. HYPERGRAPH CONVOLUTIONAL NETWORKS In Hyper SAT, we introduce a \\(T\\) - layer Hyper GCN for message passing among different nodes. The operation at the \\(l\\) - th layer of the Hyper GCN is formally expressed as follows: \\[\\begin{array}{r}{\\pmb{L}^{(l + 1)^{\\prime}} = \\sigma \\left(\\pmb{D}_{v}^{-\\frac{1}{2}}\\widetilde{\\pmb{Q}}\\pmb{D}_{v}^{-\\frac{1}{2}}\\pmb{L}^{(l)}\\pmb{R}^{(l)}\\right).} \\end{array} \\quad (2)\\] In this formulation, the matrix \\(\\pmb{L}^{(l + 1)^{\\prime}}\\) represents the output of the \\(l\\) - th layer; \\(\\pmb{D}_{v}\\) is the diagonal matrix of the vertex degrees in the hypergraph; \\(\\pmb{L}^{(l)}\\in \\mathbb{R}^{2n\\times d_{l}}\\) is the matrix of node representations at the \\(l\\) - th layer, where \\(d_{l}\\) is the dimension of the \\(l\\) - th layer node representations; \\(\\pmb{R}^{(l)}\\in\\) \\(\\mathbb{R}^{d_{l}\\times d_{l + 1}}\\) is the \\(l\\) - th layer learnable weight matrix; and \\(\\widetilde{\\pmb{Q}}\\) is given by \\[\\widetilde{\\pmb{Q}} = \\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top} - \\mathrm{diag}(\\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top}), \\quad (3)\\] where \\(\\pmb{H}\\) is the hypergraph incidence matrix, and \\(\\widetilde{\\pmb{D}}_{e} =\\) \\(\\pmb{D}_{e} - \\pmb{I}\\) . Specifically, \\(\\sigma\\) denotes the nonlinear activation function and \\(\\pmb{L}^{(0)}\\) is a learnable input embedding of Hyper GCN. Compared to the updating rule in Eq. (1) in traditional HGNN, our Hyper GCN focuses the convolutional layer's computation more on the influence of adjacent nodes by removing the diagonal elements of \\(\\widetilde{\\pmb{Q}}\\) . This adjustment allows the representation updates of each node to better align with the higher- order relationships of the adjacency structure. #### 3.2.2. CROSS-ATTENTION MECHANISM The core of the Weighted Max SAT problem lies in the logical constraints among variables. Positive and negative literal nodes (e.g., \\(x\\) and \\(\\neg x\\) ) are logically mutually exclusive and strongly correlated, and their relationships directly reflect the underlying structural characteristics of the problem.\n\nConsidering this, we leverage a cross- attention mechanism to dynamically assign importance weights to the relationships between each pair of complementary literals. This mechanism enables the model to adaptively capture critical logical properties such as mutual exclusivity, dependency, and the process of information exchange between positive and negative literal nodes. Therefore, after updating each node representation in the hypergraph convolution layer, a cross- attention layer is introduced to enhance the representations of positive and negative literal nodes. Specifically, given the \\(l\\) - th layer output of the Hyper GCN \\(\\pmb{L}^{(l + 1)}\\) , we divide it into two parts: \\(\\pmb{L}^{(l + 1)^{\\prime}} =\\) \\(\\left[\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\right]\\) , where \\(\\pmb{L}_{+}^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the positive literal node representations and \\(\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the negative literal node representations. The cross- attention mechanism is mathematically represented as follows: \\[\\begin{array}{r l} & {\\pmb{L}_{+}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{+}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{-}^{(l + 1)},}\\\\ & {\\pmb{L}_{-}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{-}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{+}^{(l + 1)}.} \\end{array} \\quad (4)\\] In this formulation, \\[\\begin{array}{r l} & {\\pmb{Q}_{+}^{(l + 1)} = \\pmb{W}_{Q}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{Q}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{K}_{+}^{(l + 1)} = \\pmb{W}_{K}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{K}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{V}_{+}^{(l + 1)} = \\pmb{W}_{V}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{V}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},} \\end{array} \\quad (5)\\] where \\(\\pmb{W}_{Q}^{(l + 1)}\\) , \\(\\pmb{W}_{K}^{(l + 1)}\\) , \\(\\pmb{W}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrices for the positive literal nodes at the \\(l\\) - th layer, and \\(\\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrix for the negative literal nodes at the \\(l\\) - th layer. The final node representation at the \\(l\\) - th layer is obtained as \\(\\pmb{L}^{(l + 1)} = [\\pmb{L}_{+}^{(l + 1)},\\pmb{L}_{- }^{(l + 1)}]\\) . The cross- attention mechanism explicitly constructs the interaction between positive and negative literal nodes by enabling each node to incorporate the features of its counterpart. This facilitates a more comprehensive representation of node features for Weighted Max SAT problems. Building upon this, the transformer module in Hyper SAT incorporates the previously discussed cross- attention layer, along with other components to further enhance the modeling capability. Inspired by the recent advancements in Vision Transformer architecture (Vi T- 22B (Dehghani et al., 2023)), the transformer module in Hyper SAT includes a Layer Norm layer (Ba, 2016), followed by a combination of a cross- attention layer and a Feed- Forward Network (FFN) layer, along with a residual connection and an additional Layer Norm layer. The FFN and cross- attention layers operate in parallel to enhance the efficiency. The residual connection is introduced by adding the outputs from the cross- attention layer, the FFN layer, and the Layer Norm layer. The resulting sum is then passed through another Layer Norm layer to stabilize the representations before proceeding to the next layer. The final layer of the network is a softmax layer. We reshape the iterated \\(\\pmb{L}^{(T)^{\\prime}}\\in \\mathbb{R}^{2n\\times 1}\\) into \\(\\hat{\\pmb{L}}^{(T)^{\\prime}}\\in \\mathbb{R}^{n\\times 2}\\) , which serves as the input to the softmax layer. After applying the softmax function, we obtain \\(\\hat{\\pmb{L}} = \\mathrm{softmax}(\\hat{\\pmb{L}}^{(T)^{\\prime}})\\) , which provides the soft node assignments for each literal, interpreted as class probabilities. The probability of assigning a variable to a truth value is recorded by \\(\\pmb{Y} = \\hat{\\pmb{L}}_{- 1}\\) , which is the first column of \\(\\hat{\\pmb{L}}\\) . #### 3.2.3. LOSS FUNCTION Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) and its hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) , we relax the Boolean variables \\(\\mathcal{X}\\) into continuous probability parameters \\(\\mathbf{Y}(\\gamma)\\) where \\(\\gamma = (\\pmb {R},\\pmb {L}^{(0)},\\pmb {W}_{Q},\\pmb {W}_{K},\\pmb {W}_{V},\\widetilde{\\pmb{W}}_{Q},\\widetilde{\\pmb{W}}_{K},\\widetilde{\\pmb{W}}_{V})\\) represent the learnable parameters. The relaxation is defined as follows: \\[\\mathcal{X}\\in \\{0,1\\}^{n}\\longrightarrow \\mathbf{Y}(\\gamma)\\in [0,1]^{n}. \\quad (6)\\] With this relaxation, we can design a differentiable loss function to optimize the learnable parameters \\(\\gamma\\) , enabling gradient- based optimization. In this paper, we propose an unsupervised multi- objective loss function that consists of two components: a primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) and a shared representation constraint loss \\(\\mathcal{L}_{\\mathrm{shared}}\\) . This unsupervised loss function obviates the necessity for large, labeled training datasets, which are conventionally indispensable in supervised learning paradigms. The specific form is as follows: \\[\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{task}} + \\lambda \\mathcal{L}_{\\mathrm{shared}}, \\quad (7)\\] where \\(\\lambda \\geq 0\\) is a balancing hyperparameter. The primary task loss function is the relaxed optimization objective of the Weighted Max SAT problem, as shown below: \\[\\mathcal{L}_{\\mathrm{task}}(\\mathbf{Y}) = \\sum_{j = 1}^{m}w_{j}V_{j}(\\mathbf{Y}), \\quad (8)\\] where \\[V_{j}(\\mathbf{Y}) = 1 - \\prod_{i\\in C_{j}^{+}}(1 - y_{i})\\prod_{i\\in C_{j}^{-}}y_{i}. \\quad (9)\\] Here, \\(C_{j}^{+}\\) and \\(C_{j}^{- }\\) are the index sets of variables appearing in the clause \\(C_{j}\\) in the positive and negative form, respectively. The term \\(V_{j}(\\mathbf{Y})\\) represents the satisfaction of clause \\(C_{j}\\) , where a value of 1 indicates that the clause is satisfied, and 0 indicates it is not. The weight \\(w_{j}\\) ensures that\n\nmore important clauses are prioritized during optimization. Minimizing the primary task loss function corresponds to maximizing the weighted sum of satisfied clauses in the Weighted Max SAT problem. The shared representation constraint loss function focuses on the representations of positive and negative literal nodes in the penultimate layer of the Hyper SAT network. Its form is given by \\[\\mathcal{L}_{\\mathrm{shared}} = \\left\\| \\pmb{L}_{+}^{(T - 1)} + \\pmb{L}_{-}^{(T - 1)}\\right\\|_{F}^{2}, \\quad (10)\\] where \\(\\left\\| \\cdot \\right\\|_{F}\\) denotes the Frobenius norm. By applying the shared representation constraint loss function, the network encourages the positive and negative literal nodes to develop distinct feature representations in the learning process. This constraint enhances the separation of the two types of nodes in the feature space. It should be noted that most neural network- based SAT methods employ supervised learning. However, supervised learning approaches are not particularly well- suited for solving the Weighted Max SAT problem. On the one hand, a Weighted Max SAT instance may have multiple satisfying assignments. On the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the Weighted Max SAT instance. From this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances. ### 3.3. Probabilistic Mapping The neural network module takes the hypergraph as input and produces a probability vector \\(\\mathbf{Y}\\) as output, which indicates the probability of each variable taking the truth value. Accordingly, to convert the probability vector \\(\\mathbf{Y}\\) into a Boolean assignment, we transform \\(\\mathbf{Y}\\) into \\(n\\) Bernoulli distributions, where the \\(i\\) - th distribution \\(B(y_{i})\\) is over the discrete values \\(\\{0,1\\}\\) . Finally, the node assignments are generated by sampling with the corresponding probability distributions. ## 4. Experiment ### 4.1. Experimental Settings We compare the performance of Hyper SAT against GNN- based methods for solving Weighted Max SAT problems. Baseline Algorithms. We compare our proposed Hyper SAT against GNN- based methods. The following algorithms are considered as baselines: (i) Hyp Op (Heydaribeni et al., 2024): an advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs; (ii) (Liu et al., 2023): an innovative supervised GNN- based approach that predicts solutions in an end- to- end manner by transforming CNF formulas into factor graphs. We apply these baselines to solve Weighted Max SAT problems. Datasets. We evaluate the algorithms using random 3- SAT benchmarks from the SATLIB dataset (Hoos & St\u00fctzle, 2000). The SATLIB dataset is one of the standard datasets for evaluating SAT solvers. We utilize a range of datasets with varying distributions, from which SAT and UNSAT instances are generated for each distribution. The number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. More details on the datasets are provided in Table 1. In particular, to generate the required Weighted Max SAT instances, we assign weights to the clauses in each CNF file within the dataset. These weights are sampled from the integers in the range \\([1,10]\\) uniformly at random. Table 1. The parameters of the datasets. <table><tr><td>DATASET</td><td>INSTANCE</td><td>VARS</td><td>CLAUSES</td><td>TYPES</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>SAT</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>UNSAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>SAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>UNSAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>SAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>UNSAT</td></tr></table> Model Settings. Hyper GCN is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. The dimension of the cross- attention layer is the square root of the number of variables. The dropout used in the cross- attention layer is 0.1. The model is optimized using the Adam optimizer (Kingma, 2014) with a learning rate of \\(7 \\times 10^{- 2}\\) . The balancing hyperparameter of loss function \\(\\lambda\\) is \\(2 \\times 10^{- 3}\\) . The FFN consists of two linear transformations and a Re LU activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. An early stopping strategy is employed, with a tolerance of \\(10^{- 4}\\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. Finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained. Evaluation Configuration. All experiments are conducted on an NVIDIA A100- SXM4- 40GB GPU, using Python 3.9.30 and Py Torch 1.13.0.\n\n### 4.2. Analytical Experiment We first evaluate the performance of unsupervised methods, Hyper SAT and Hyp Op, with a focus on their convergence. We evaluate the convergence using the primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) from Eq. (8), which represents the sum of the weights of unsatisfied clauses. We illustrate the evolution curves of loss for Hyper SAT and Hyp Op on the uuf250- 1065 dataset in Figure 3 as an example. All models can converge within 300 epochs. Moreover, the loss of Hyper SAT is around 52, while the loss of Hyp Op is around 139. We can observe that Hyper SAT achieves better performance than Hyp Op. Specifically, Hyper SAT decreases the loss more quickly and achieves a lower loss value. The experimental results demonstrate that Hyper SAT can be used in learning to solve Weighted Max SAT problems. <center>Figure 3. The evolution of loss for Hyper SAT and Hyp Op during an inference process of 300 epochs on the uuf250-1065 dataset. </center> ### 4.3. Result We evaluate the performance of Hyper SAT against baseline algorithms Hyp Op and (Liu et al., 2023) on various datasets. The primary evaluation metric is the average weighted sum of unsatisfied clauses. The experiments are conducted on datasets with the number of variables ranging from 100 to 250, and the number of clauses varying between 430 and 1065. The results are shown in Table 2. As presented in the table, Hyper SAT consistently achieves lower values for the average weighted sum of unsatisfied clauses compared to the baseline algorithms (Liu et al., 2023) and Hyp Op across multiple datasets. For example, on the uf100- 430 dataset, Hyper SAT significantly outperforms both baselines with a result of 15.64. This result represents a substantial improvement over the results of (Liu et al., 2023) (32.48) and Hyp Op (99.15). This trend is observed across all datasets, where Hyper SAT always exceeds the Table 2. The average weighted sum of unsatisfied clauses of Weighted Max SAT problems. <table><tr><td>DATASET</td><td>LIU ET AL. (2023)</td><td>HYPOP</td><td>HYPERSAT</td></tr><tr><td>UF100-430</td><td>32.48</td><td>99.15</td><td>15.64</td></tr><tr><td>UUF100-430</td><td>41.65</td><td>102.44</td><td>20.46</td></tr><tr><td>UF200-860</td><td>67.38</td><td>158.46</td><td>28.98</td></tr><tr><td>UUF200-860</td><td>81.68</td><td>171.34</td><td>35.55</td></tr><tr><td>UF250-1065</td><td>79.06</td><td>170.60</td><td>33.24</td></tr><tr><td>UUF250-1065</td><td>100.04</td><td>182.39</td><td>41.64</td></tr></table> performance of the baselines. The average weight of unsatisfied clauses is reduced by approximately \\(50\\%\\) compared to (Liu et al., 2023) and over \\(80\\%\\) compared to Hyp Op. On the uf200- 860 and uuf200- 860 datasets, Hyper SAT achieves reductions of \\(56.99\\%\\) and \\(56.48\\%\\) compared to (Liu et al., 2023), respectively, and reductions of \\(81.71\\%\\) and \\(79.25\\%\\) compared to Hyp Op. Similarly, on the uf250- 1065 and uuf250- 1065 datasets, Hyper SAT continues to outperform the baselines. The reductions compared to (Liu et al., 2023) are \\(57.96\\%\\) and \\(58.38\\%\\) , respectively, while the reductions compared to Hyp Op are \\(80.52\\%\\) and \\(77.17\\%\\) . Importantly, even with the larger datasets, Hyper SAT maintains or even enhances its efficacy. This demonstrates that Hyper SAT scales well and performs better. These results show that Hyper SAT consistently provides substantial improvements over both baseline algorithms across a range of datasets, including those with larger problem sizes. As a result, it validates its robustness and effectiveness in reducing the weighted sum of unsatisfied clauses. ### 4.4. Ablation Study We conduct an ablation study to evaluate the contribution of each key component in our proposed model. By systematically removing components, we analyze their impact on performance and highlight the importance of each component. The experiments are designed to isolate the impact of the following components: (i) the hypergraph modeling of literal nodes rather than variable nodes; (ii) the transformer module with the cross- attention mechanism; (iii) the shared representation constraint loss. The results of the ablation study are shown in Table 3. Effect of Hypergraph Modeling of Literal Nodes: The performance drops by \\(52.77\\%\\) when the hypergraph modeling of variable nodes is used instead of literal nodes. The results demonstrate the importance and superiority of modeling the Weighted Max SAT instance as a hypergraph with literal nodes. Effect of Transformer with Cross- Attention: We disable the transformer module with the cross- attention mechanism to assess its importance. Without this module, the model experiences a performance drop of \\(11.54\\%\\) . This signifi\n\nTable 3. The results of the ablation study. We consider three components: (i) HGM-L: the hypergraph modeling of literal nodes rather than variable nodes; (ii) Transformer: the transformer module with the cross-attention mechanism; (iii) SRCL: the shared representation constraint loss. Specifically, the first row in the table represents the transformation of the Weighted Max SAT instance into the hypergraph of variables. <table><tr><td>HGM-L</td><td>TRANSFORMER</td><td>SRCL</td><td>RESULT</td></tr><tr><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td><td>182.39</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u00d7</td><td>86.14</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u00d7</td><td>64.08</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u221a</td><td>47.07</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u221a</td><td>41.64</td></tr></table> cantly reduces its ability to effectively capture dependencies across each pair of complementary literals, leading to lower overall accuracy. Effect of Share Representation Constraint Loss: We further remove the shared representation constraint loss and optimize the model with the primary task loss function to investigate the role of the unsupervised multi- objective loss. The results show that without the shared representation constraint loss, the model achieves a performance of 64.08. This performance is lower than that of the original configuration. Therefore, it highlights the importance of the shared representation constraint loss, which encourages the positive and negative literal nodes to develop distinct feature representations. In summary, the full model consistently outperforms the ablated versions, demonstrating the synergistic effect of integrating the hypergraph modeling of literal nodes, cross- attention mechanism, and shared representation constraint loss design. ## 5. Conclusion In this work, we propose Hyper SAT, a novel neural approach for Weighted Max SAT problems, addressing the challenges associated with learning the complex non- linear dependencies and sensitive objective function of this NP- hard problem. By modeling Weighted Max SAT instances as hypergraphs, a hypergraph convolutional network is introduced as the learning model, coupled with a cross- attention mechanism to capture the logical relationships between positive and negative literals. The proposed framework incorporates an unsupervised multi- objective loss design, which not only optimizes the intrinsic objectives of the Weighted Max SAT problem but also ensures that the representations of positive and negative literals remain distinct in the feature space. Extensive experiments demonstrate the potential of Hyper SAT. The experimental results show that Hyper SAT is effective in solving Weighted Max SAT instances and outperforms state- of- the- art competitors in terms of performance. This work offers a fresh perspective on solving the Weighted Max SAT problem through learning- based methods. Future work will focus on further integrating the model with heuristic solvers, improving the design of well- crafted solvers, and exploring its applications in other complex combinatorial problems. ## Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ## References Allouche, D., Traor\u00e9, S., Andr\u00e9, I., De Givry, S., Katsirelos, G., Barbe, S., and Schiex, T. Computational protein design as a cost function network optimization problem. In International Conference on Principles and Practice of Constraint Programming, pp. 840- 849. Springer, 2012. Audemard, G. and Simon, L. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27 (1):1- 25, 2018. Ba, J. L. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Biere, A. and Fleury, M. Gimsatul, isasat and kissat entering the sat competition 2022. Proceedings of SAT Competition, pp. 10- 11, 2022. Cai, S. and Zhang, X. Deep cooperation of cdc1 and local search for sat. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, pp. 64- 81. Springer, 2021. Cameron, C., Chen, R., Hartford, J., and Leyton- Brown, K. Predicting propositional satisfiability via end- to- end learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3324- 3331, 2020. Clarke, E., Biere, A., Raimi, R., and Zhu, Y. Bounded model checking using satisfiability solving. Formal methods in system design, 19:7- 34, 2001. Cook, S. A. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, pp. 151- 158, 1971. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers\n\nto 22 billion parameters. In International Conference on Machine Learning, pp. 7480- 7512. PMLR, 2023. Feng, Y., You, H., Zhang, Z., Ji, R., and Gao, Y. Hypergraph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 3558- 3565, 2019. Guo, W., Zhen, H.- L., Li, X., Luo, W., Yuan, M., Jin, Y., and Yan, J. Machine learning methods in solving the boolean satisfiability problem. Machine Intelligence Research, 20 (5):640- 655, 2023. Heydaribeni, N., Zhan, X., Zhang, R., Eliassi- Rad, T., and Koushanfar, F. Distributed constrained combinatorial optimization leveraging hypergraph neural networks. Nature Machine Intelligence, pp. 1- 9, 2024. Hoos, H. H. and St\u00fctzle, T. SATLIB: An online resource for research on sat. Sat, 2000:283- 292, 2000. Kautz, H. A., Selman, B., et al. Planning as satisfiability. In ECAI, volume 92, pp. 359- 363. Citeseer, 1992. Kingma, D. P. Adam: A method for stochastic optimization. ar Xiv preprint ar Xiv:1412.6980, 2014. Liu, M., Huang, P., Jia, F., Zhang, F., Sun, Y., Cai, S., Ma, F., and Zhang, J. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 16264- 16265, 2023. Ozolins, E., Freivalds, K., Draguns, A., Gaile, E., Zakovskis, R., and Kozlovics, S. Goal- aware neural sat solver. In 2022 International Joint Conference on Neural Networks, pp. 1- 8. IEEE, 2022. Selsam, D. and Bj\u00f8rner, N. Guiding high- performance sat solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, pp. 336- 353. Springer, 2019. Selsam, D., Lamm, M., B\u00fcnz, B., Liang, P., de Moura, L., and Dill, D. L. Learning a sat solver from single- bit supervision. In 7th International Conference on Learning Representations, 2019. Sun, L., Gerault, D., Benamira, A., and Peyrin, T. Neurogift: Using a machine learning based sat solver for cryptanalysis. In Cyber Security Cryptography and Machine Learning: Fourth International Symposium, Proceedings 4, pp. 62- 84. Springer, 2020. Thornton, J. and Sattar, A. Dynamic constraint weighting for over- constrained problems. In Pacific Rim International Conference on Artificial Intelligence, pp. 377- 388. Springer, 1998. Timm, N. and Botha, J. Synthesis of cost- optimal multiagent systems for resource allocation. ar Xiv preprint ar Xiv:2209.09473, 2022. Wang, W., Hu, Y., Tiwari, M., Khurshid, S., Mc Millan, K., and Miikkulainen, R. Neuroback: Improving CDCL SAT solving using graph neural networks. In The Twelfth International Conference on Learning Representations, 2024. Yang, Q., Wu, K., and Jiang, Y. Learning action models from plan examples using weighted max- sat. Artificial Intelligence, 171(2- 3):107- 143, 2007. Zhang, W., Sun, Z., Zhu, Q., Li, G., Cai, S., Xiong, Y., and Zhang, L. Nlocalsat: boosting local search with solution prediction. In Proceedings of the Twenty- Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 1177- 1183, 2021.",
      "level": 2,
      "line_start": 4,
      "line_end": 21
    },
    {
      "heading": "3.1. Hypergraph Modeling Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) , we construct the hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) as follows: For the construction of the vertex set \\(\\nu\\) , each variable \\(x_{i}\\in \\mathcal{X}\\) is represented by two nodes \\(v_{i}\\) and \\(v_{n + i}\\) , which correspond to the positive and negative literals of the variable \\(x_{i}\\) (i.e., \\(x_{i}\\) and \\(\\neg x_{i}\\) ), respectively. For the construction of the hyperedge set \\(\\mathcal{E}\\) , each clause \\(C_{j}\\in \\mathcal{C}\\) is represented by a hyperedge \\(e_{j}\\in \\mathcal{E}\\) , which connects the nodes corresponding to the literals in \\(C_{j}\\) . Specifically, if clause \\(C_{j}\\) consists of the literals \\(l_{j1},l_{j2},\\ldots ,l_{jk_{j}}\\) , then the corresponding hyperedge \\(e_{j}\\) connects the nodes corresponding to these literals. For the construction of the weight matrix \\(\\mathbf{W}\\) , the weight of each hyperedge is equal to the weight of the corresponding clause, i.e., \\(W_{j,j} = w_{j}\\) . Through the above modeling process, the Weighted Max SAT instance can be uniquely represented by a hypergraph, where each literal is a node, and each clause is a hyperedge connecting the literals involved. The weight of each hyperedge is the weight of the corresponding clause in the instance. Figure 2 gives an illustration of the hypergraph modeling process. <center>Figure 2. Hypergraph Modeling of a Weighted Max SAT instance \\((\\neg x_{1}\\lor x_{2}\\lor \\neg x_{3})\\land (x_{1}\\lor x_{2})\\land (\\neg x_{2}\\lor x_{3}\\lor x_{4})\\land (\\neg x_{1}\\lor x_{3}\\lor \\neg x_{4})\\) with 8 literals, 4 clauses and weights \\(\\{w_{1},w_{2},w_{3},w_{4}\\}\\) . </center> In contrast to existing methods that represent Conjunctive Normal Form (CNF) formulas as factor graphs (Guo et al., 2023), which limit their ability to model relationships beyond pairwise connections, we construct Weighted Max SAT instances as hypergraphs. These hypergraphs can encode higher- order variable dependencies through their degree- free hyperedges, and thus offer more powerful representational capacity and enable more efficient handling of higher- order relationships in combinatorial optimization problems. In particular, we treat literals rather than variables as nodes, which addresses a major issue present in Hyp Op (Heydaribeni et al., 2024). Since the logical relationships between positive and negative literals are central to the Weighted Max SAT problem, it is essential to treat them as distinct nodes, rather than merging them into a single node. In addition, we encode the weights of the clauses in the Weighted Max SAT problem as the weights of the corresponding hyperedges in the hypergraph. ### 3.2. Neural Network Architecture The neural network architecture of our proposed Hyper SAT comprises the Hyper GCN, a transformer module with the cross- attention mechanism, and a softmax layer. #### 3.2.1. HYPERGRAPH CONVOLUTIONAL NETWORKS In Hyper SAT, we introduce a \\(T\\) - layer Hyper GCN for message passing among different nodes. The operation at the \\(l\\) - th layer of the Hyper GCN is formally expressed as follows: \\[\\begin{array}{r}{\\pmb{L}^{(l + 1)^{\\prime}} = \\sigma \\left(\\pmb{D}_{v}^{-\\frac{1}{2}}\\widetilde{\\pmb{Q}}\\pmb{D}_{v}^{-\\frac{1}{2}}\\pmb{L}^{(l)}\\pmb{R}^{(l)}\\right).} \\end{array} \\quad (2)\\] In this formulation, the matrix \\(\\pmb{L}^{(l + 1)^{\\prime}}\\) represents the output of the \\(l\\) - th layer; \\(\\pmb{D}_{v}\\) is the diagonal matrix of the vertex degrees in the hypergraph; \\(\\pmb{L}^{(l)}\\in \\mathbb{R}^{2n\\times d_{l}}\\) is the matrix of node representations at the \\(l\\) - th layer, where \\(d_{l}\\) is the dimension of the \\(l\\) - th layer node representations; \\(\\pmb{R}^{(l)}\\in\\) \\(\\mathbb{R}^{d_{l}\\times d_{l + 1}}\\) is the \\(l\\) - th layer learnable weight matrix; and \\(\\widetilde{\\pmb{Q}}\\) is given by \\[\\widetilde{\\pmb{Q}} = \\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top} - \\mathrm{diag}(\\pmb {H}\\widetilde{\\pmb{D}}_{e}^{-1}\\pmb {H}^{\\top}), \\quad (3)\\] where \\(\\pmb{H}\\) is the hypergraph incidence matrix, and \\(\\widetilde{\\pmb{D}}_{e} =\\) \\(\\pmb{D}_{e} - \\pmb{I}\\) . Specifically, \\(\\sigma\\) denotes the nonlinear activation function and \\(\\pmb{L}^{(0)}\\) is a learnable input embedding of Hyper GCN. Compared to the updating rule in Eq. (1) in traditional HGNN, our Hyper GCN focuses the convolutional layer's computation more on the influence of adjacent nodes by removing the diagonal elements of \\(\\widetilde{\\pmb{Q}}\\) . This adjustment allows the representation updates of each node to better align with the higher- order relationships of the adjacency structure. #### 3.2.2. CROSS-ATTENTION MECHANISM The core of the Weighted Max SAT problem lies in the logical constraints among variables. Positive and negative literal nodes (e.g., \\(x\\) and \\(\\neg x\\) ) are logically mutually exclusive and strongly correlated, and their relationships directly reflect the underlying structural characteristics of the problem.",
      "content": "Considering this, we leverage a cross- attention mechanism to dynamically assign importance weights to the relationships between each pair of complementary literals. This mechanism enables the model to adaptively capture critical logical properties such as mutual exclusivity, dependency, and the process of information exchange between positive and negative literal nodes. Therefore, after updating each node representation in the hypergraph convolution layer, a cross- attention layer is introduced to enhance the representations of positive and negative literal nodes. Specifically, given the \\(l\\) - th layer output of the Hyper GCN \\(\\pmb{L}^{(l + 1)}\\) , we divide it into two parts: \\(\\pmb{L}^{(l + 1)^{\\prime}} =\\) \\(\\left[\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\right]\\) , where \\(\\pmb{L}_{+}^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the positive literal node representations and \\(\\pmb{L}_{- }^{(l + 1)^{\\prime}}\\in \\mathbb{R}^{n\\times d_{l + 1}}\\) represent the negative literal node representations. The cross- attention mechanism is mathematically represented as follows: \\[\\begin{array}{r l} & {\\pmb{L}_{+}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{+}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{-}^{(l + 1)},}\\\\ & {\\pmb{L}_{-}^{(l + 1)} = \\mathrm{softmax}\\left(\\frac{\\pmb{Q}_{-}^{(l + 1)}(\\pmb{K}_{+}^{(l + 1)})^{\\top}}{\\sqrt{d_{l + 1}}}\\right)\\pmb{V}_{+}^{(l + 1)}.} \\end{array} \\quad (4)\\] In this formulation, \\[\\begin{array}{r l} & {\\pmb{Q}_{+}^{(l + 1)} = \\pmb{W}_{Q}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{Q}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{K}_{+}^{(l + 1)} = \\pmb{W}_{K}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{K}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},}\\\\ & {\\pmb{V}_{+}^{(l + 1)} = \\pmb{W}_{V}^{(l + 1)}\\pmb{L}_{+}^{(l + 1)^{\\prime}},\\pmb{V}_{-}^{(l + 1)} = \\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\pmb{L}_{-}^{(l + 1)^{\\prime}},} \\end{array} \\quad (5)\\] where \\(\\pmb{W}_{Q}^{(l + 1)}\\) , \\(\\pmb{W}_{K}^{(l + 1)}\\) , \\(\\pmb{W}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrices for the positive literal nodes at the \\(l\\) - th layer, and \\(\\widetilde{\\pmb{W}}_{Q}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{K}^{(l + 1)}\\) , \\(\\widetilde{\\pmb{W}}_{V}^{(l + 1)}\\) are the learnable query, key and value projection matrix for the negative literal nodes at the \\(l\\) - th layer. The final node representation at the \\(l\\) - th layer is obtained as \\(\\pmb{L}^{(l + 1)} = [\\pmb{L}_{+}^{(l + 1)},\\pmb{L}_{- }^{(l + 1)}]\\) . The cross- attention mechanism explicitly constructs the interaction between positive and negative literal nodes by enabling each node to incorporate the features of its counterpart. This facilitates a more comprehensive representation of node features for Weighted Max SAT problems. Building upon this, the transformer module in Hyper SAT incorporates the previously discussed cross- attention layer, along with other components to further enhance the modeling capability. Inspired by the recent advancements in Vision Transformer architecture (Vi T- 22B (Dehghani et al., 2023)), the transformer module in Hyper SAT includes a Layer Norm layer (Ba, 2016), followed by a combination of a cross- attention layer and a Feed- Forward Network (FFN) layer, along with a residual connection and an additional Layer Norm layer. The FFN and cross- attention layers operate in parallel to enhance the efficiency. The residual connection is introduced by adding the outputs from the cross- attention layer, the FFN layer, and the Layer Norm layer. The resulting sum is then passed through another Layer Norm layer to stabilize the representations before proceeding to the next layer. The final layer of the network is a softmax layer. We reshape the iterated \\(\\pmb{L}^{(T)^{\\prime}}\\in \\mathbb{R}^{2n\\times 1}\\) into \\(\\hat{\\pmb{L}}^{(T)^{\\prime}}\\in \\mathbb{R}^{n\\times 2}\\) , which serves as the input to the softmax layer. After applying the softmax function, we obtain \\(\\hat{\\pmb{L}} = \\mathrm{softmax}(\\hat{\\pmb{L}}^{(T)^{\\prime}})\\) , which provides the soft node assignments for each literal, interpreted as class probabilities. The probability of assigning a variable to a truth value is recorded by \\(\\pmb{Y} = \\hat{\\pmb{L}}_{- 1}\\) , which is the first column of \\(\\hat{\\pmb{L}}\\) . #### 3.2.3. LOSS FUNCTION Given a Weighted Max SAT instance \\(\\phi = (\\mathcal{X},\\mathcal{C},\\mathbf{w})\\) and its hypergraph \\(\\mathcal{G} = (\\mathcal{V},\\mathcal{E},\\mathbf{W})\\) , we relax the Boolean variables \\(\\mathcal{X}\\) into continuous probability parameters \\(\\mathbf{Y}(\\gamma)\\) where \\(\\gamma = (\\pmb {R},\\pmb {L}^{(0)},\\pmb {W}_{Q},\\pmb {W}_{K},\\pmb {W}_{V},\\widetilde{\\pmb{W}}_{Q},\\widetilde{\\pmb{W}}_{K},\\widetilde{\\pmb{W}}_{V})\\) represent the learnable parameters. The relaxation is defined as follows: \\[\\mathcal{X}\\in \\{0,1\\}^{n}\\longrightarrow \\mathbf{Y}(\\gamma)\\in [0,1]^{n}. \\quad (6)\\] With this relaxation, we can design a differentiable loss function to optimize the learnable parameters \\(\\gamma\\) , enabling gradient- based optimization. In this paper, we propose an unsupervised multi- objective loss function that consists of two components: a primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) and a shared representation constraint loss \\(\\mathcal{L}_{\\mathrm{shared}}\\) . This unsupervised loss function obviates the necessity for large, labeled training datasets, which are conventionally indispensable in supervised learning paradigms. The specific form is as follows: \\[\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{task}} + \\lambda \\mathcal{L}_{\\mathrm{shared}}, \\quad (7)\\] where \\(\\lambda \\geq 0\\) is a balancing hyperparameter. The primary task loss function is the relaxed optimization objective of the Weighted Max SAT problem, as shown below: \\[\\mathcal{L}_{\\mathrm{task}}(\\mathbf{Y}) = \\sum_{j = 1}^{m}w_{j}V_{j}(\\mathbf{Y}), \\quad (8)\\] where \\[V_{j}(\\mathbf{Y}) = 1 - \\prod_{i\\in C_{j}^{+}}(1 - y_{i})\\prod_{i\\in C_{j}^{-}}y_{i}. \\quad (9)\\] Here, \\(C_{j}^{+}\\) and \\(C_{j}^{- }\\) are the index sets of variables appearing in the clause \\(C_{j}\\) in the positive and negative form, respectively. The term \\(V_{j}(\\mathbf{Y})\\) represents the satisfaction of clause \\(C_{j}\\) , where a value of 1 indicates that the clause is satisfied, and 0 indicates it is not. The weight \\(w_{j}\\) ensures that\n\nmore important clauses are prioritized during optimization. Minimizing the primary task loss function corresponds to maximizing the weighted sum of satisfied clauses in the Weighted Max SAT problem. The shared representation constraint loss function focuses on the representations of positive and negative literal nodes in the penultimate layer of the Hyper SAT network. Its form is given by \\[\\mathcal{L}_{\\mathrm{shared}} = \\left\\| \\pmb{L}_{+}^{(T - 1)} + \\pmb{L}_{-}^{(T - 1)}\\right\\|_{F}^{2}, \\quad (10)\\] where \\(\\left\\| \\cdot \\right\\|_{F}\\) denotes the Frobenius norm. By applying the shared representation constraint loss function, the network encourages the positive and negative literal nodes to develop distinct feature representations in the learning process. This constraint enhances the separation of the two types of nodes in the feature space. It should be noted that most neural network- based SAT methods employ supervised learning. However, supervised learning approaches are not particularly well- suited for solving the Weighted Max SAT problem. On the one hand, a Weighted Max SAT instance may have multiple satisfying assignments. On the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the Weighted Max SAT instance. From this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances. ### 3.3. Probabilistic Mapping The neural network module takes the hypergraph as input and produces a probability vector \\(\\mathbf{Y}\\) as output, which indicates the probability of each variable taking the truth value. Accordingly, to convert the probability vector \\(\\mathbf{Y}\\) into a Boolean assignment, we transform \\(\\mathbf{Y}\\) into \\(n\\) Bernoulli distributions, where the \\(i\\) - th distribution \\(B(y_{i})\\) is over the discrete values \\(\\{0,1\\}\\) . Finally, the node assignments are generated by sampling with the corresponding probability distributions. ## 4. Experiment ### 4.1. Experimental Settings We compare the performance of Hyper SAT against GNN- based methods for solving Weighted Max SAT problems. Baseline Algorithms. We compare our proposed Hyper SAT against GNN- based methods. The following algorithms are considered as baselines: (i) Hyp Op (Heydaribeni et al., 2024): an advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs; (ii) (Liu et al., 2023): an innovative supervised GNN- based approach that predicts solutions in an end- to- end manner by transforming CNF formulas into factor graphs. We apply these baselines to solve Weighted Max SAT problems. Datasets. We evaluate the algorithms using random 3- SAT benchmarks from the SATLIB dataset (Hoos & St\u00fctzle, 2000). The SATLIB dataset is one of the standard datasets for evaluating SAT solvers. We utilize a range of datasets with varying distributions, from which SAT and UNSAT instances are generated for each distribution. The number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. More details on the datasets are provided in Table 1. In particular, to generate the required Weighted Max SAT instances, we assign weights to the clauses in each CNF file within the dataset. These weights are sampled from the integers in the range \\([1,10]\\) uniformly at random. Table 1. The parameters of the datasets. <table><tr><td>DATASET</td><td>INSTANCE</td><td>VARS</td><td>CLAUSES</td><td>TYPES</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>SAT</td></tr><tr><td>UF100-430</td><td>1000</td><td>100</td><td>430</td><td>UNSAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>SAT</td></tr><tr><td>UF200-860</td><td>100</td><td>200</td><td>860</td><td>UNSAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>SAT</td></tr><tr><td>UF250-1065</td><td>100</td><td>250</td><td>1065</td><td>UNSAT</td></tr></table> Model Settings. Hyper GCN is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. The dimension of the cross- attention layer is the square root of the number of variables. The dropout used in the cross- attention layer is 0.1. The model is optimized using the Adam optimizer (Kingma, 2014) with a learning rate of \\(7 \\times 10^{- 2}\\) . The balancing hyperparameter of loss function \\(\\lambda\\) is \\(2 \\times 10^{- 3}\\) . The FFN consists of two linear transformations and a Re LU activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. An early stopping strategy is employed, with a tolerance of \\(10^{- 4}\\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. Finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained. Evaluation Configuration. All experiments are conducted on an NVIDIA A100- SXM4- 40GB GPU, using Python 3.9.30 and Py Torch 1.13.0.",
      "level": 3,
      "line_start": 11,
      "line_end": 16
    },
    {
      "heading": "4.2. Analytical Experiment We first evaluate the performance of unsupervised methods, Hyper SAT and Hyp Op, with a focus on their convergence. We evaluate the convergence using the primary task loss \\(\\mathcal{L}_{\\mathrm{task}}\\) from Eq. (8), which represents the sum of the weights of unsatisfied clauses. We illustrate the evolution curves of loss for Hyper SAT and Hyp Op on the uuf250- 1065 dataset in Figure 3 as an example. All models can converge within 300 epochs. Moreover, the loss of Hyper SAT is around 52, while the loss of Hyp Op is around 139. We can observe that Hyper SAT achieves better performance than Hyp Op. Specifically, Hyper SAT decreases the loss more quickly and achieves a lower loss value. The experimental results demonstrate that Hyper SAT can be used in learning to solve Weighted Max SAT problems. <center>Figure 3. The evolution of loss for Hyper SAT and Hyp Op during an inference process of 300 epochs on the uuf250-1065 dataset. </center> ### 4.3. Result We evaluate the performance of Hyper SAT against baseline algorithms Hyp Op and (Liu et al., 2023) on various datasets. The primary evaluation metric is the average weighted sum of unsatisfied clauses. The experiments are conducted on datasets with the number of variables ranging from 100 to 250, and the number of clauses varying between 430 and 1065. The results are shown in Table 2. As presented in the table, Hyper SAT consistently achieves lower values for the average weighted sum of unsatisfied clauses compared to the baseline algorithms (Liu et al., 2023) and Hyp Op across multiple datasets. For example, on the uf100- 430 dataset, Hyper SAT significantly outperforms both baselines with a result of 15.64. This result represents a substantial improvement over the results of (Liu et al., 2023) (32.48) and Hyp Op (99.15). This trend is observed across all datasets, where Hyper SAT always exceeds the Table 2. The average weighted sum of unsatisfied clauses of Weighted Max SAT problems. <table><tr><td>DATASET</td><td>LIU ET AL. (2023)</td><td>HYPOP</td><td>HYPERSAT</td></tr><tr><td>UF100-430</td><td>32.48</td><td>99.15</td><td>15.64</td></tr><tr><td>UUF100-430</td><td>41.65</td><td>102.44</td><td>20.46</td></tr><tr><td>UF200-860</td><td>67.38</td><td>158.46</td><td>28.98</td></tr><tr><td>UUF200-860</td><td>81.68</td><td>171.34</td><td>35.55</td></tr><tr><td>UF250-1065</td><td>79.06</td><td>170.60</td><td>33.24</td></tr><tr><td>UUF250-1065</td><td>100.04</td><td>182.39</td><td>41.64</td></tr></table> performance of the baselines. The average weight of unsatisfied clauses is reduced by approximately \\(50\\%\\) compared to (Liu et al., 2023) and over \\(80\\%\\) compared to Hyp Op. On the uf200- 860 and uuf200- 860 datasets, Hyper SAT achieves reductions of \\(56.99\\%\\) and \\(56.48\\%\\) compared to (Liu et al., 2023), respectively, and reductions of \\(81.71\\%\\) and \\(79.25\\%\\) compared to Hyp Op. Similarly, on the uf250- 1065 and uuf250- 1065 datasets, Hyper SAT continues to outperform the baselines. The reductions compared to (Liu et al., 2023) are \\(57.96\\%\\) and \\(58.38\\%\\) , respectively, while the reductions compared to Hyp Op are \\(80.52\\%\\) and \\(77.17\\%\\) . Importantly, even with the larger datasets, Hyper SAT maintains or even enhances its efficacy. This demonstrates that Hyper SAT scales well and performs better. These results show that Hyper SAT consistently provides substantial improvements over both baseline algorithms across a range of datasets, including those with larger problem sizes. As a result, it validates its robustness and effectiveness in reducing the weighted sum of unsatisfied clauses. ### 4.4. Ablation Study We conduct an ablation study to evaluate the contribution of each key component in our proposed model. By systematically removing components, we analyze their impact on performance and highlight the importance of each component. The experiments are designed to isolate the impact of the following components: (i) the hypergraph modeling of literal nodes rather than variable nodes; (ii) the transformer module with the cross- attention mechanism; (iii) the shared representation constraint loss. The results of the ablation study are shown in Table 3. Effect of Hypergraph Modeling of Literal Nodes: The performance drops by \\(52.77\\%\\) when the hypergraph modeling of variable nodes is used instead of literal nodes. The results demonstrate the importance and superiority of modeling the Weighted Max SAT instance as a hypergraph with literal nodes. Effect of Transformer with Cross- Attention: We disable the transformer module with the cross- attention mechanism to assess its importance. Without this module, the model experiences a performance drop of \\(11.54\\%\\) . This signifi",
      "content": "Table 3. The results of the ablation study. We consider three components: (i) HGM-L: the hypergraph modeling of literal nodes rather than variable nodes; (ii) Transformer: the transformer module with the cross-attention mechanism; (iii) SRCL: the shared representation constraint loss. Specifically, the first row in the table represents the transformation of the Weighted Max SAT instance into the hypergraph of variables. <table><tr><td>HGM-L</td><td>TRANSFORMER</td><td>SRCL</td><td>RESULT</td></tr><tr><td>\u00d7</td><td>\u00d7</td><td>\u00d7</td><td>182.39</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u00d7</td><td>86.14</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u00d7</td><td>64.08</td></tr><tr><td>\u221a</td><td>\u00d7</td><td>\u221a</td><td>47.07</td></tr><tr><td>\u221a</td><td>\u221a</td><td>\u221a</td><td>41.64</td></tr></table> cantly reduces its ability to effectively capture dependencies across each pair of complementary literals, leading to lower overall accuracy. Effect of Share Representation Constraint Loss: We further remove the shared representation constraint loss and optimize the model with the primary task loss function to investigate the role of the unsupervised multi- objective loss. The results show that without the shared representation constraint loss, the model achieves a performance of 64.08. This performance is lower than that of the original configuration. Therefore, it highlights the importance of the shared representation constraint loss, which encourages the positive and negative literal nodes to develop distinct feature representations. In summary, the full model consistently outperforms the ablated versions, demonstrating the synergistic effect of integrating the hypergraph modeling of literal nodes, cross- attention mechanism, and shared representation constraint loss design. ## 5. Conclusion In this work, we propose Hyper SAT, a novel neural approach for Weighted Max SAT problems, addressing the challenges associated with learning the complex non- linear dependencies and sensitive objective function of this NP- hard problem. By modeling Weighted Max SAT instances as hypergraphs, a hypergraph convolutional network is introduced as the learning model, coupled with a cross- attention mechanism to capture the logical relationships between positive and negative literals. The proposed framework incorporates an unsupervised multi- objective loss design, which not only optimizes the intrinsic objectives of the Weighted Max SAT problem but also ensures that the representations of positive and negative literals remain distinct in the feature space. Extensive experiments demonstrate the potential of Hyper SAT. The experimental results show that Hyper SAT is effective in solving Weighted Max SAT instances and outperforms state- of- the- art competitors in terms of performance. This work offers a fresh perspective on solving the Weighted Max SAT problem through learning- based methods. Future work will focus on further integrating the model with heuristic solvers, improving the design of well- crafted solvers, and exploring its applications in other complex combinatorial problems. ## Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ## References Allouche, D., Traor\u00e9, S., Andr\u00e9, I., De Givry, S., Katsirelos, G., Barbe, S., and Schiex, T. Computational protein design as a cost function network optimization problem. In International Conference on Principles and Practice of Constraint Programming, pp. 840- 849. Springer, 2012. Audemard, G. and Simon, L. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27 (1):1- 25, 2018. Ba, J. L. Layer normalization. ar Xiv preprint ar Xiv:1607.06450, 2016. Biere, A. and Fleury, M. Gimsatul, isasat and kissat entering the sat competition 2022. Proceedings of SAT Competition, pp. 10- 11, 2022. Cai, S. and Zhang, X. Deep cooperation of cdc1 and local search for sat. In Theory and Applications of Satisfiability Testing- SAT 2021: 24th International Conference, pp. 64- 81. Springer, 2021. Cameron, C., Chen, R., Hartford, J., and Leyton- Brown, K. Predicting propositional satisfiability via end- to- end learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3324- 3331, 2020. Clarke, E., Biere, A., Raimi, R., and Zhu, Y. Bounded model checking using satisfiability solving. Formal methods in system design, 19:7- 34, 2001. Cook, S. A. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, pp. 151- 158, 1971. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers\n\nto 22 billion parameters. In International Conference on Machine Learning, pp. 7480- 7512. PMLR, 2023. Feng, Y., You, H., Zhang, Z., Ji, R., and Gao, Y. Hypergraph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 3558- 3565, 2019. Guo, W., Zhen, H.- L., Li, X., Luo, W., Yuan, M., Jin, Y., and Yan, J. Machine learning methods in solving the boolean satisfiability problem. Machine Intelligence Research, 20 (5):640- 655, 2023. Heydaribeni, N., Zhan, X., Zhang, R., Eliassi- Rad, T., and Koushanfar, F. Distributed constrained combinatorial optimization leveraging hypergraph neural networks. Nature Machine Intelligence, pp. 1- 9, 2024. Hoos, H. H. and St\u00fctzle, T. SATLIB: An online resource for research on sat. Sat, 2000:283- 292, 2000. Kautz, H. A., Selman, B., et al. Planning as satisfiability. In ECAI, volume 92, pp. 359- 363. Citeseer, 1992. Kingma, D. P. Adam: A method for stochastic optimization. ar Xiv preprint ar Xiv:1412.6980, 2014. Liu, M., Huang, P., Jia, F., Zhang, F., Sun, Y., Cai, S., Ma, F., and Zhang, J. Can graph neural networks learn to solve the maxsat problem? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 16264- 16265, 2023. Ozolins, E., Freivalds, K., Draguns, A., Gaile, E., Zakovskis, R., and Kozlovics, S. Goal- aware neural sat solver. In 2022 International Joint Conference on Neural Networks, pp. 1- 8. IEEE, 2022. Selsam, D. and Bj\u00f8rner, N. Guiding high- performance sat solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, pp. 336- 353. Springer, 2019. Selsam, D., Lamm, M., B\u00fcnz, B., Liang, P., de Moura, L., and Dill, D. L. Learning a sat solver from single- bit supervision. In 7th International Conference on Learning Representations, 2019. Sun, L., Gerault, D., Benamira, A., and Peyrin, T. Neurogift: Using a machine learning based sat solver for cryptanalysis. In Cyber Security Cryptography and Machine Learning: Fourth International Symposium, Proceedings 4, pp. 62- 84. Springer, 2020. Thornton, J. and Sattar, A. Dynamic constraint weighting for over- constrained problems. In Pacific Rim International Conference on Artificial Intelligence, pp. 377- 388. Springer, 1998. Timm, N. and Botha, J. Synthesis of cost- optimal multiagent systems for resource allocation. ar Xiv preprint ar Xiv:2209.09473, 2022. Wang, W., Hu, Y., Tiwari, M., Khurshid, S., Mc Millan, K., and Miikkulainen, R. Neuroback: Improving CDCL SAT solving using graph neural networks. In The Twelfth International Conference on Learning Representations, 2024. Yang, Q., Wu, K., and Jiang, Y. Learning action models from plan examples using weighted max- sat. Artificial Intelligence, 171(2- 3):107- 143, 2007. Zhang, W., Sun, Z., Zhu, Q., Li, G., Cai, S., Xiong, Y., and Zhang, L. Nlocalsat: boosting local search with solution prediction. In Proceedings of the Twenty- Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 1177- 1183, 2021.",
      "level": 3,
      "line_start": 17,
      "line_end": 21
    }
  ]
}