# Efficient Neural Networks for MNIST Classification

## Abstract

We propose a lightweight neural network architecture for MNIST digit classification implemented in PyTorch. Our approach achieves 99.1% accuracy while being trainable in just 2 hours on a single GPU.

## Introduction

MNIST digit classification is a classic benchmark in machine learning. While many complex models exist, we focus on efficiency and simplicity.

## Methodology

### Model Architecture

Our model uses a simple CNN architecture implemented in PyTorch:

```python
import torch
import torch.nn as nn

class MNISTNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3)
        self.conv2 = nn.Conv2d(32, 64, 3)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
```

### Training

We use supervised learning with cross-entropy loss and labeled training data. The model is optimized using SGD with momentum.

## Experiments

### Dataset

MNIST contains 70,000 grayscale images of handwritten digits (0-9). We use 60,000 for training and 10,000 for testing.

### Training Configuration

- Framework: PyTorch 1.12
- Hardware: Single GTX 1080 GPU  
- Training time: 2 hours
- Epochs: 50
- Batch size: 64

### Results

Our lightweight model achieves 99.1% test accuracy, demonstrating that simple architectures can be very effective on MNIST.

## Code Availability

Full implementation available at: https://github.com/author/mnist-pytorch

The repository includes:
- Complete model definition
- Training script
- Evaluation utilities
- Pretrained weights

## Conclusion

We show that efficient PyTorch models can achieve excellent performance on MNIST with minimal computational requirements.

## References

1. LeCun, Y., et al. "Gradient-based learning applied to document recognition." 1998.
2. Paszke, A., et al. "PyTorch: An imperative style, high-performance deep learning library." NeurIPS 2019.
