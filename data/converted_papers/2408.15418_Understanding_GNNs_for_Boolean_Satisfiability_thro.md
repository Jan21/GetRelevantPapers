# Understanding GNNs for Boolean Satisfiability thro


## Introduction


## S Supplementary Material ## S.1 Derivation of the SDP relaxation for MAX-2-SAT Here we provide further details about the definition of SDP relaxation for MAX- 2- SAT. The goal is to write an objective function for 2- CNF formulae, which consist of clauses \(c_{1},\ldots ,c_{k}\) over variables \(x_{1},\ldots ,x_{n}\) with at most two literals per clause. For each Boolean variable \(x_{i}\) (where \(i\in \{1,2,\ldots ,n\}\) ) a new variable \(y_{i}\in \{- 1,1\}\) is first instantiated and one additional variable \(y_{0}\in \{- 1,1\}\) is introduced. The additional variable is introduced to unambiguously assign the truth value in the original problem from values of relaxed problem. It is not possible to just assign True (False) to \(x_{i}\) if \(y_{i} = 1(0)\) because quadratic terms cannot distinguish between \(y_{i}\cdot y_{j}\) and \((- y_{i})\cdot (- y_{j})\) . Instead, the truth value of \(x_{i}\) is assigned by comparing \(y_{i}\) with \(y_{0}\colon x_{i}\) is True if and only if \(y_{i} = y_{0}\) otherwise it is False. The assignment is therefore invariant to negating all variables. To determine the value of a formula, we sum the value of its clauses \(c\) which are given by the value function \(v(c)\) . Here are examples of the value function for 3 different clauses: \[v(x_{i}) = \frac{1 + y_{0}\cdot y_{i}}{2}\] \[v(\neg x_{i}) = 1 - v(x_{i}) = \frac{1 - y_{0}\cdot y_{i}}{2}\] \[v(x_{i}\vee \neg x_{j}) = 1 - v(\neg x_{i}\wedge x_{j})\] \[\qquad = 1 - \frac{1 - y_{0}\cdot y_{i}}{2}\frac{1 + y_{0}\cdot y_{j}}{2}\] \[\qquad = \frac{1}{4} (1 + y_{0}\cdot y_{i}) + \frac{1}{4} (1 - y_{0}\cdot y_{j}) + \frac{1}{4} (1 + y_{i}\cdot y_{j})\] By summing over all clauses \(c\) in in the Boolean formula, the following integer quadratic program for MAX- 2- SAT is obtained: \[\mathrm{Maximize:}\quad \sum_{c\in C}v(c)\] \[\mathrm{Subject~to:}\quad y_{i}\in \{-1,1\} \mathrm{~for~all~}i\in \{0,1,\ldots ,n\} ,\] this can be rewritten by collecting coefficients of \(y_{i}\cdot y_{j}\) for \(i,j\in \{0,1,\ldots ,n\}\) and putting them symmetrically into a \((n + 1)\times (n + 1)\) coefficient matrix \(W\) . The terms \(y_{i}\cdot y_{j}\) can be collected in a matrix \(Y\) with same dimensions as \(W\) . The elements \(Y_{ij}\) correspond to \(y_{i}\cdot y_{j}\) for \(i,j\in \{0,1,\ldots ,n\}\) . Both matrices are symmetric, hence the sum of all elements in their element- wise product (which is the objective function) can be compactly expressed by using trace operation. This leads to the following version of the same integer program: \[\mathrm{Maximize:}\quad \mathrm{Tr}(W Y)\] \[\mathrm{Subject~to:}\quad Y_{ij}\in \{-1,1\} \mathrm{~for~all~}i,j\in \{0,1,\ldots ,n\} ,i\neq j.\] So far no relaxation has been made. To make the discrete program continuous, the value of the variables \(y_{i}\) is allowed to be any real number between \(- 1\) and 1. After solving a quadratic program with this relaxation, rounding can be used to obtain a value from \(\{- 1,1\}\) . Semi- definite programming goes further and allows variables to be \((n + 1)\) - dimensional unit vectors \((y_{0},\ldots ,y_{n})\longrightarrow (y_{0},\ldots ,y_{n})\) . schematically depicted in figure 5. This directly leads to the relaxation used in the main part of this study. Our aim was to show that solving SDP relaxation by optimization and rounding by separating the high- dimensional vectors closely resembles the behavior of GNN.

<center>Figure 5: Lifting the variables to a higher dimension, demonstrated on variables \(y_{1}, y_{2}, y_{3}\) . Initially, only integer values of \(-1\) and 1 could be assigned to them (integer program). Next, constraints are relaxed, allowing variables to take any real value between \(-1\) and 1. Finally, it is permitted for them to be unit vectors in a high-dimensional space (here, 3 dimensions). The hyperplane in the last picture would be used for rounding the variables at the end. This hyperplane can be randomly selected, and truth values for variables \(y_{1}, y_{2}, y_{3}\) are determined based on which side of the hyperplane they land after continuous optimization. </center> ## S.2 The Neuro SAT Architecture For completeness, we provide the update rules and voting rule from the original paper [29]: \[(C^{(t + 1)},C_{h}^{(t + 1)})\gets \mathbf{C}_{\mathbf{u}}(\left[C_{h}^{(t)},M^{\top}\mathbf{L}_{\mathbf{msg}}(L^{(t)})\right]) \quad (1)\] \[(L^{(t + 1)},L_{h}^{(t + 1)})\gets \mathbf{L}_{\mathbf{u}}(\left[L_{h}^{(t)},\mathrm{Flip}(L^{(t)}),M\mathbf{C}_{\mathbf{msg}}(C^{(t + 1)})\right]) \quad (2)\] \[L_{*}^{T}\gets \mathbf{L}_{\mathrm{vote}}(L^{(T)})\in \mathbb{R}^{2n}. \quad (3)\] The first rule is used to update the clause embedding matrix \(C^{(t)}\in \mathbb{R}^{m\times d}\) where \(d\) is the size of the hidden feature vector and \(m\) is the number of clauses, \(t\) is a discrete time step. The second rule is used to update literals whose embedding are stored in matrix \(L^{(t)}\in \mathbb{R}^{2n\times d}\) where \(n\) is number of variables (there are \(2n\) rows to cover both polarities of each literal). These two updates are consecutively repeated for \(T\) iterations. \(\mathbf{C}_{\mathbf{u}},\mathbf{L}_{\mathbf{u}}\) denote two Layer Norm LSTMs (initialized randomly) with hidden states \(C_{h}^{(t)}\in \mathbb{R}^{m\times d},L_{h}^{(t)}\in \mathbb{R}^{2n\times d}\) respectively, and \(\mathbf{L}_{\mathrm{msg}},\mathbf{C}_{\mathrm{msg}}\) are multilayer perceptrons (MLPs) processing messages from literals and clauses. The last trained component is \(\mathbf{L}_{\mathrm{vote}}\) , a voting MLP whose output is a single scalar for each literal. Edges of bipartite graph representation of the SAT formula are encoded in the bipartite adjacency matrix \(M(M(i,j)\) is 1 iff literal \(l_{i}\) is in clause \(c_{j}\) ). The flip operator swaps each pair of rows in matrix \(L\) , containing two polarities of the same literal. To update a representation of each clause, the representations of literals contained in this clause are processed by the MLP \(\mathbf{L}_{\mathrm{msg}}\) and the resulting vectors are summed together and taken as input by the LSTM \(\mathbf{C}_{\mathbf{u}}\) . We emphasize that for updating a representation of each literal, the process is analogous to the clause update, except that the LSTM takes as an input a concatenation of the summed messages from literals and the hidden- state representation of the literal of the same variable but opposite polarity (i.e., to update the hidden state of literal \(x_{i}\) , the LSTM takes as an input a concatenation of the aggregated message vector and a hidden state of literal \(\tilde{x}_{i}\) from the previous iteration). At the end, the output of the model is a \(2n\) dimensional vector, which is then averaged to a single logit on which a sigmoid activation cross- entropy is applied to compute the loss with respect to the ground truth label (SAT/UNSAT). Our model is a simplified version of the described architecture, achieved by omitting two MLPs, namely \(\mathbf{L}_{\mathrm{msg}},\mathbf{C}_{\mathrm{msg}}\) and replacing \(\mathbf{L}_{\mathrm{vote}}\) with just a single linear layer. Layer Norm is removed from LSTM and the dimensionality of the hidden states is reduced to 16 from 128. ## S.3 Datasets S.3.1 Random Problems. The generative model proposed by [29] samples formulas in sat/unsat pairs which differ only by a negation of a single literal in one clause. This is accomplished through the sequential sampling of clauses which are continuously added to the CNF formula until it becomes unsatisfiable. To create a new clause, the generative model first samples a small integer, \(k\) , and then randomly selects \(k\) variables without replacement. Each selected variable is independently negated with a probability 0.5 and the resulting literal is added to the clause. Satisfiability is determined by querying a solver right after the addition of a new clause. When the problem becomes unsatisfiable, it is paired with a satisfiable problem which is exactly the same except that in the last added clause, one literal is negated. The sampling of \(k\) is designed to vary the size of clauses while avoiding an excessive number of two-literal clauses, which would simplify the problem on average.

S.3.2 Structured Problems. While many works evaluate NN-based SAT solvers on randomly generated problems, it is far more compelling to understand their performance on problems representing facets of human reasoning. The objective is to generate data that is reflecting various degrees of difficulty of Boolean reasoning. Since real-world problems often produce a large number of variables and clauses, which can be easily reduced by preprocessing, we uniformly reduce all the instances by unit propagation, which can be realized in polynomial time (see following paragraph). Unit propagation. is one of the simplest operation for propositional logic, which propagates unit clauses in a CNF \(\phi\) . The process consists of identifying a unit clause \(\{l\} \in \phi\) , then removing all clauses from \(\phi\) that contain \(\phi\) and removing the complementary literal \(l\) from all the other clauses. This process may create new unit clauses, which are then propagated in the same manner. If the process produces the empty clause (semantically equivalent to false), then the formula \(\phi\) is unsatisfiable. We can say that a formula is solved by unit propagation if the unit operation derives the empty clause, or if all remaining clauses are unit clauses. Latin square. is an \(n\times n\) grid of numbers \(1..n\) , where each number appears exactly once in each row and in each column. We generate SAT instances by partially filling the Latin square—the individual values in the partially filled Latin square are referred to as hints. Then, the task is to decide whether the given hints can be completed in to a full Latin square (similarly to the Sudoku puzzle). In order to generate interesting instances, we generate instances that have a unique solution and are minimal in the sense that removing any of the hints leads to multiple solutions. This is generated as follows. First generate a valid random Latin square and then start removing values of individual squares, at random, while a unique solution exists—this is checked by a SAT solver. The resulting formula consists of the rules of the Latin square and a set of unit clauses representing the hints. This process generates a satisfiable SAT instance with a unique solution. An unsatisfiable instance is generated by adding a single random hint incongruent with the unique solution. Sudoku. is a popular puzzle, which is in fact an extension of latin squares Where we add additional constraints on smaller squares (aka boxes). We consider the standard format where the puzzle is composed of \(3\times 3\) boxes, which comprise \(3\times 3\) cells to be filled. We use the same method as in Latin squares to generate interesting puzzles. Logical circuits. Are one of the main means of modeling in SAT. Indeed, they enable modeling digital systems but also represent a powerful intermediate language for modeling propositional problems. An important application of SAT are bit- vector problems of a fixed bit- width. To represent this type of reasoning, we generate problems of the form \[c_{1}*r_{1} + c_{2}*r_{2}\neq c_{3}\mod 2^{n}\] We use the Model checker CBMC to convert these inequalities to CNF [7]. ## S.4 Visualizations of literal embeddings <center>Figure 6: This figure shows evolution of the embeddings of literals during the MP process. We selected 6 different time steps from the 30 time steps used for this example. The 16-dimensional vectors are projected to 2D by UMAP algorithm. </center>

## S.5 Analysis of the Evolution of Literal Embeddings To reinforce the assertion regarding the relationship between Neuro SAT and the SDP relaxation for Max SAT, we tested whether the evolution of literal embeddings in Neuro SAT actually corresponds to an optimization process that tries to maximize the SDP objective. We first sampled several hundred 2- CNF formulas and obtained the SDP objective function for each of them using the expressions mentioned in Appendix S.1. The objective function is a linear function of the Gram matrix \(Y\) corresponding to the inner products between the unit vectors (representing the lifted variables and one vector \(\mathbf{y}_0\) representing the value TRUE). An SDP solver optimizes the matrix \(Y\) while adhering to specified constraints (which ensure that the matrix can be obtained as a Gram matrix for some set of unit vectors). To observe the behavior of the same objective function with literal embeddings from Neuro SAT, we need to compute this Gram matrix \(Y\) after each MP iteration of the GNN. If the evolution of these embeddings would correspond to an optimization process maximizing the objective, then we should observe an increase in this objective after each MP step. When computing the matrix \(Y\) from the Neuro SAT embeddings, two details must be taken into account. First, only the positive literals are taken into account as the objective function automatically assumes that the embeddings of negative literals are obtained by negation of the positive ones. Second, Neuro SAT does not explicitly represent the embedding \(\mathbf{y}_0\) representing the value TRUE. Therefore, we estimate it by averaging all literals that are assigned to TRUE in the extracted solution. Before computing the matrix \(Y\) , we also center all vectors to 0 and normalize them to unit vectors. For each iteration \(t\) of the GNN, we obtain the matrix \(Y(t) = L^{(t)}L^{(t)T}\) where \(L^{(t)}\) represents the matrix of centered and normalized positive literal embeddings with the estimated vector \(\mathbf{y}_0\) (also normalized and centered) concatenated as its first row. In Figure 7 we show how the objective function changes after each iteration \(t\) for 10 randomly selected instances. We also include the objective value obtained with a SDP solver as a reference. <center>Figure 7: A plot showing how the SDP objective value computed from the Neuro SAT embeddings (in blue) changes after each iteration of MP. The horizontal red line represents the value of the same objective obtained with an SDP solver. </center> Figure 7 shows that the evolution of literal embeddings corresponds to an increase in the objective value of SDP. It is also visible that there is a gap between the highest value achieved and the objective value obtained with an SDP solver. In Figure 8 (a), we plot a histogram of these gaps computed for all generated problems. We hypothesized that the gap may be partially caused by the inappropriate choice of the vector \(\mathbf{y}_0\) . Therefore, we took the matrix \(Y^{(t)}\) from the last step of MP, \(t = 40\) , and further optimized it using a gradient- based SDP solver (implemented in Py Torch). This closed the gaps mentioned above in most instances, as visible in Figure 8 (b). In Figure 9, we show how the entries in the matrix \(Y^{(t)}\) change after further optimization for a random formula. As can be seen, the largest change in values occurs in the first row and in the first column, which correspond to inner products of each literal embedding with the vector \(\mathbf{y}_0\) . This supports our hypothesis that if we would be able to pick the vector \(\mathbf{y}_0\) in a more optimal way, the gaps in Figure 7 would be smaller.

<center>Figure 8: Histograms of relative gaps between the final SDP objective value obtained from Neuro SAT embeddings and the same objective value obtained with an SDP solver. The plot on the left depicts the results for the matrix \(Y^{(t)}\) with \(t = 40\) . The plot on the right depicts results for the case where this matrix is further optimized by an SDP solver. A negative gap means that the subsequent optimization found a solution with better objective value than the SDP solver, which was initialized randomly. </center> <center>Figure 9: A heat map (for a single MAX-2-SAT instance) showing the change of values in the symmetric matrix \(Y^{(t)}\) after further optimization. The largest change happens in the first row and in the first column, which correspond to inner products of each literal embedding with the vector \(y_0\) (corresponding to the value TRUE). </center>

## S.6 Training with the SDP objective function To further support the connection to SDP- based approximation algorithms, we tried to train the GNN with a loss function that is minimized when the maximum number of clauses is satisfied. Unlike the experiments in Section S.5, here we focus on general MAX- SAT for which we came up with the following (multilinear) objective function. Given a set of variables \(\{x_{1},x_{2},\ldots ,x_{n}\} = X\) associated to each Boolean variable, one special variable \(x_{0}\) , and a set of clauses \(\{c_{1},c_{2},\ldots ,c_{m}\} =\) \(C\) , where each clause \(c_{i}\) consists of (multiple) literals (variables with polarity), the objective function \(v(C)\) for an integer- valued problem can be defined as: \[v(C) = \sum_{c\in C}\left(\prod_{l\in c}\frac{1 - sgn(l)\cdot x_{l}\cdot x_{0}}{2}\right)\] where \(\operatorname {sgn}(l)\) is a variable polarity in clause \(c\) and evaluates to 1 for a positive occurrence of the variable \(l\) and \(- 1\) for negative, and \(x_{l}\in X\) can take the value \(- 1\) or 1. The product for a clause \(c\) is 0 if at least for one of the variables \(x_{l}\) in the clause, \(sgn(l)\cdot x_{l}\cdot x_{0} = 1\) , which is the case when this clause is satisfied. To use this as a loss function for the supervision of Neuro SAT, we lift the variables \(x_{0},x_{1},x_{2},\ldots ,x_{n}\) to be unit vectors in a high- dimensional space. The objective is to minimize the following differentiable expression by optimizing these unit vectors: \[v(C) = \sum_{c\in C}\left(\prod_{l\in c}\frac{1 - sgn(l)x_{l}\cdot x_{0}}{2}\right).\] The scalar product of the unit vectors \(x_{l}\cdot x_{0}\) is a real number between \(- 1\) and 1. The vector \(x_{0}\) is sampled randomly as a unit vector and is kept fixed, whereas other variable vectors are sampled randomly, fed into Neuro SAT as initial embeddings for positive literals, updated by MP iterations, and normalized after each update. We supervise the negative literals with the same objective with the exception that the \(\operatorname {sgn}(l)\) returns \(- 1\) for positive and 1 for a negative literal occurrence. To extract the assignment of individual variables, we compute an inner product with the vector \(x_{0}\) (representing the value true) and assign the variable to true if it is positive, and to false in the other case. Once we have the assignment, we can classify the formula as SAT/UNSAT by checking whether the solution satisfies the formula. The trained model accurately classifies only \(\sim 73\%\) of all problems (vs. \(\sim 85\%\) in the case of the Neuro SAT trained by the original loss). When optimizing the embeddings w.r.t. this objective directly with Autograd, the accuracy was \(\sim 65\%\) . On the other hand, when trained with this objective function, the model starts to quickly improve even when trained only on the formulas of the largest size (i.e. 40 variables) without a curriculum. This suggests that a possible combination of loss functions, one that tries to maximize the number of satisfied formulas and one that penalizes the model for incorrect classification, may be beneficial. We leave the investigation of this idea for future work. ## S.7 Results for Different Numbers of Decimation Steps In Table 2, we show the effect of running the decimation process multiple times. On our test sets, the decimation process did not result in any further improvement when repeated more than twice. The hyperparameters are the same as for the experiments in Table 1 (with 16 random init. samples per formula in the first pass and 1 random init. sample for each subsequent pass). We note that we did not try to optimize the decimation threshold, which could lead to further improvements. Table 2: This table shows a number of problems solved after subsequent application of the decimation procedure. <table><tr><td>Problem Type</td><td>#SAT problems</td><td>First pass</td><td>Second pass</td><td>Third pass</td></tr><tr><td>SR(40)</td><td>5000</td><td>4442 (88.8 %)</td><td>274 (5.4 %)</td><td>35 (0.7 %)</td></tr><tr><td>Latin Squares 9x9</td><td>200</td><td>186 (93 %)</td><td>14 (7 %)</td><td>4 (2 %)</td></tr><tr><td>Latin Squares 8x8</td><td>200</td><td>196 (98 %)</td><td>1 (0.5%)</td><td>0 (0 %)</td></tr><tr><td>Logical Circuits</td><td>344</td><td>319 (92.7 %)</td><td>0 (0 %)</td><td>0 (0 %)</td></tr><tr><td>Sudoku 9x9</td><td>200</td><td>83 (46 %)</td><td>11 (5.5 %)</td><td>3 (1.5 %)</td></tr></table> ## S.8 Results for Belief Propagation Here we report the results of Belief Propagation algorithm applied on the same problems as reported in the main paper. Neuro SAT results are without resampling and decimation (copied from Table 1). Belief Propagation is run once for a maximum of 1000 iterations.

Table 3: Comparison between Neuro SAT and Belief propagation on problems from Table 1 <table><tr><td>Problem Type</td><td>#SAT<br>problems</td><td>Neuro SAT</td><td>Belief<br>Propagation</td></tr><tr><td>SR(40)</td><td>5000</td><td>80.0%</td><td>42.34%</td></tr><tr><td>Latin Squares 9x9</td><td>200</td><td>47.5%</td><td>15.0%</td></tr><tr><td>Latin Squares 8x8</td><td>200</td><td>56.5%</td><td>32.5%</td></tr><tr><td>Logical Circuits</td><td>344</td><td>85.2%</td><td>0.5%</td></tr><tr><td>Sudoku 9x9</td><td>200</td><td>17.5%</td><td>6%</td></tr></table>

Another way of interpreting the MP process of a GNN is through the lens of Belief Propagation algorithms [23]. In the context of Boolean Satisfiability, Belief Propagation algorithms operate on similar literal- clause factor graphs as a GNN. A particular version called Survey Propagation also sends messages from literals to clauses in the form of 3- dimensional vectors. In the domain of random satisfiability, these algorithms have been proven very effective [3] and their theoretical properties are well- studied. In this work, we demonstrate that these connections could give us novel insights into how trained GNNs operate and bring about improvements in terms of their training speed and accuracy. To our knowledge, these connections have not been explored before. We present the following novel contributions: We demonstrate several similarities between the empirical behavior of trained GNN and two well- studied approximation algorithms for Boolean Satisfiability. Motivated by these connections, we design a training curriculum that speeds up the training process by an order of magnitude. For a trained model with fixed weights, we propose to sample different initializations of literal embeddings and to apply a decimation procedure (inspired by Belief Propagation). This substantially increases the number of solved problems (problem is considered solved if the network correctly predicts satisfiability and produces a satisfying assignment for satisfiable instances). In Section 2, we provide relevant background information; Sections 3 and 4 describe our contribution; Section 5 contains experimental results and is followed by related work in Section 6, conclusion in Section 7 and limitations in Section 8. ## 2 Background ### 2.1 Boolean Satisfiability Basic background knowledge of propositional logic is assumed, cf. [2]. Boolean variables are denoted by \(x_{1},x_{2},\ldots\) ; disjunction by \(\vee\) , conjunction by \(\wedge\) , and negation by \(\neg\) . A literal is a variable or its negation; a clause is a disjunction of literals. For a literal \(l\) we write \(\bar{l}\) for the complementary literal of \(l\) , i.e. \(\bar{x}\) is \(\neg x\) and \(\neg \bar{x}\) is \(x\) . A formula in conjunctive normal form (CNF) is a conjunction of clauses. Whenever convenient, a clause is treated as a set of literals and a CNF formula as a set of sets of literals. A clause is called unit iff it consists of a single literal. An assignment is a total mapping from variables to \(\{0,1\}\) , representing true/false. An assignment \(\sigma\) is satisfying a formula \(\phi\) iff \(\phi\) evaluates to true under the standard semantics of Boolean connectives. In particular, an assignment satisfies a CNF \(\phi\) iff it satisfies at least one literal in each clause. There exist multiple representations of CNF formulas in the form of a graph. In this work, we use the literal- clause factor graph, which is an undirected bipartite graph of clauses and literals. Each node of a literal in this graph is connected to nodes of clauses that contain this literal. Max SAT is an optimization version of SAT where one is given a CNF \(\phi\) and the objective is to find a variable assignment that maximizes the number of satisfied clauses. For instance, in \(\{x_{1}\vee x_{2},\neg x_{1},\neg x_{2}\}\) the assignment \(x_{1} = 1,x_{2} = 0\) satisfies the first 2 clauses but not the last one, and it is optimal because the 3 clauses cannot be satisfied simultaneously. The problem is NP- hard even for formulas that have only 2 literals in each clause [22]. ### 2.2 Random Satisfiability and Message-passing Algorithms Random satisfiability provides a natural and simplified setting to study the computational hardness of finding a satisfying assignment and the structure of the space of satisfying assignments. Typically, it is assumed that each clause in the formula is sampled randomly and has the same number of variables \((k)\) . A random formula is parametrized by a parameter \(\alpha\) denoting the clause- to- variable ratio. As the problem size increases asymptotically, the solution space undergoes several phase transitions as the parameter \(\alpha\) changes. When random clauses are added to the formula, it becomes increasingly challenging to find a satisfying assignments until it reaches a point where the formula becomes unsatisfiable. This occurs at the satisfiability threshold whose value for \(k > 2\) is known only through upper and lower bounds and numerical estimates [34]. Before reaching the phase transition to unsatisfiability, the geometry of the solution space undergoes several other phase transitions, during which the set of solutions breaks into well- separated clusters (in terms of the Hamming distance) [15]. Each cluster corresponds to a set of solutions in which specific variables are fixed (to a value 0 or 1) and the values of the remaining variables could vary (this is denoted by the value \(\ast\) ). Unlike real- world formulas, random formulas could be efficiently solved by message- passing algorithms [3]. These algorithms could be viewed as algorithms that compute the marginal probability of individual variables using a belief propagation algorithm (BP) [15]. The marginal probability that a variable \(x_{i}\) will have a value 1 in a satisfying assignment is given by a proportion of assignments where \(x_{i} = 1\) among all possible satisfiable assignments. Belief propagation can compute these marginal probabilities exactly for formulas whose factor graphs are devoid of loops. Clearly, computing these marginal probabilities exactly for a generic formula is much harder than finding a single satisfying assignment. For factor graphs with loops, BP can be regarded as an algorithm that tries to approximate these marginal probabilities [20]. To obtain an algorithm for finding a satisfying assignment with BP, one can employ a decimation procedure in which variables with the most extreme estimated marginal values are fixed \(^{1}\) and the whole process is repeated by running the BP process again on the reduced formula until no variable has a sufficiently high estimated marginal. At this point, the resulting formula would be solved by a local search. Empirically, it has been observed that BP starts to fail as the parameter \(\alpha\) approaches the satisfiability threshold. This phenomenon is frequently attributed to the evolving structure of the solution space [20]. As the parameter \(\alpha\) increases, long- range correlations between variables start to appear \(^{2}\) and this breaks the assumption needed for BP to work properly.

Braunstein et al. [3] overcome this problem by drawing on concepts from statistical physics to design an algorithm called Survey Propagation (SP). It can be viewed as a BP running on an augmented factor graph of the original formula in which each variable can take one of three values: 0, 1, \(*\) , where the value \(*\) corresponds to undecided. SP was empirically shown to find satisfying assignments of problems with parameter \(\alpha\) very close to the satisfiability threshold. Similarly to BP, SP works by iteratively sending messages on a literal-clause factor graph until the convergence threshold is reached. Unlike BP, the messages from literals to clauses have the form of 3- dimensional vectors expressing the marginals for the three possible values. ### 2.3 Semidefinite Programming for Boolean Satisfiability Semidefinite programming (SDP) is a mathematical optimization technique that is primarily used for problems involving positive semidefinite matrices. In SDP, a linear objective function is optimized over a feasible region given by a spectrahedron (an intersection of a convex cone formed by positive semidefinite matrices and an affine subspace) [25]. Along with the broad scope of applications, SDP has also been used to design approximation algorithms for discrete NP- hard problems [10]. This is achieved by lifting variables of a problem to a vector space and optimizing a loss function expressed in terms of these vectors. Here we illustrate this process on a Semidefinite Relaxation of a MAX- 2- SAT problem. MAX- 2- SAT is a version of MAX- SAT in which each clause contains at most two literals. The semidefinite relaxation of a MAX- 2- SAT problem can be formulated as follows [13]: To each Boolean variable \(x_{i}\) (where \(i\in \{1,2,\ldots ,n\}\) ), a new variable \(y_{i}\in \{- 1,1\}\) is associated, and an additional variable \(y_{0}\) is introduced (this variable can be understood as representing the value true). By definition, \(x_{i}\) is true if and only if \(y_{i} = y_{0}\) , otherwise, it is false. Using these new variables, we can represent each clause by an expression that is maximized when the clause is satisfied (considering only values in \(\{- 1,1\}\) ). Each expression contains binary products between variables used in the given clause (more on this in the Supplementary material S.2). By summing the expressions of all the clauses in the formula, we obtain a quadratic objective function that gives a maximal value when the maximum number of clauses is satisfied. Therefore, the whole problem may be stated as an integer quadratic program where the constraints restrict the values of the variables to \(\{- 1,1\}\) . The SDP relaxation is obtained by lifting each variable \(y_{i}\) to a \((n + 1)\) - dimensional unit vector \(\mathbf{y}_{i}\) . Therefore, the binary products \(y_{i}\cdot y_{j}\) in the objective function are replaced by inner products \(\langle \mathbf{y}_{i},\mathbf{y}_{j}\rangle\) . This can be compactly represented in matrix form if we substitute each inner product \(\langle \mathbf{y}_{i},\mathbf{y}_{j}\rangle\) by a scalar \(Y_{ij}\) of a matrix \(Y\) . The fact that these scalars correspond to inner products could be encoded by the restriction to positive- semidefinite matrices \(Y\) . We can thus represent the original MAX- 2- SAT problem as the following SDP: Maximize: \(\operatorname {Tr}(WY)\) Subject to: \(Y_{ii} = 1\) for all \(i\in \{0,1,\ldots ,n\}\) \(Y\ge 0,\) where Tr denotes the trace of a matrix. Both \(Y\) and \(W\) are \((n+\) \(1)\times (n + 1)\) matrices. Matrix \(W\) is a coefficient matrix of the objective function derived from the clauses. A more detailed derivation is available in the Supplementary material S.1. Positive semidefinitness of matrix \(Y\) assures that the matrix can be uniquely factorized as \(Y = Y^{\frac{1}{2}}(Y^{\frac{1}{2}})^T\) . Rows of the matrix \(Y^{\frac{1}{2}}\) are real vectors \(\mathbf{y}_{i}\) for all \(i\in \{0,\ldots ,n\}\) and values in the original matrix \(Y_{ij}\) are their inner products \(\langle \mathbf{y}_{i},\mathbf{y}_{j}\rangle\) for all \(i,j\in \{0,\ldots ,n\}\) The constraints \(Y_{ii} = 1\) assures that all vectors \(\mathbf{y}_{i}\) lie on \((n + 1)\) dimensional unit sphere. The solver for this SDP optimizes the numbers in the matrix \(Y\) but using the factorization, we can possibly visualize what happens with the vectors \(\mathbf{y}_{i}\) . The process starts with random unit vectors which are continuously updated in order to maximize the objective function. If we would further fix the position of the vector \(\mathbf{y}_{0}\) (corresponding to the value true) we would see that the vectors of variables that will be set to true in the final assignment are getting closer to the vector \(\mathbf{y}_{0}\) and the vectors \(\mathbf{y}_{j}\) of variables that will be set to false will be moving away from it so that the inner product \(\langle \mathbf{y}_{0},\mathbf{y}_{j}\rangle\) is close to \(- 1\) . If the formula is satisfiable, the objective function drives the vectors to form two well- separated clusters. However, if only a few clauses could be satisfied at the same time, the vectors would end up being scattered. A simple way to round the resulting vectors \((\mathbf{y}_{1},\ldots ,\mathbf{y}_{n})\) and get the assignment for the original Boolean variables is to compute an inner product \(\langle \mathbf{y}_{0},\mathbf{y}_{i}\rangle\) and assign the value according to its sign. It is also possible to assign the values by picking a random separating hyperplane and it can be shown that this rounding gives 0.8785- approximation of the integer program optimum [12]. Similar SDPs can be obtained for different versions of MAX- SAT (with larger clauses). From an empirical observation, the convergence threshold of the SDP solver needs to be decreased significantly compared to MAX- 2- SAT in order to obtain a good approximation for these more complicated versions, which is related to our curriculum training procedure we introduce. We mention that the expressions of the clauses reach their maximum at 1 (when a clause is satisfied by the assignment). This means that the whole formula is satisfiable if the objective function achieves a value that is equal to the number of clauses in the formula. Another way to check satisfiability is to plug the obtained solution into the formula and check whether it is satisfied by it. Therefore, we can obtain an incomplete SAT solver from this SDP. In Section 4, we empirically demonstrate that the behavior of a trained GNN resembles the optimization process described above. With this intuition, we propose several improvements that lead to faster training time and higher accuracy. ### 2.4 Graph Neural Networks for Boolean Satisfiability GNNs constitute a flexible tool for learning representations of graph- structured data. Representing the input data in the form of a graph

allows one to encode complex relations and sparsity structures. GNNs then allow to encode inductive biases such as invariance to various transformations [4]. For these reasons, GNNs are frequently used in applications of machine learning to combinatorial optimization [6, 11, 19] where optimization problems are often amenable to graph- based representations. Typically, a GNN would enhance a manually designed solver by replacing various decision heuristics with their predictions after being trained either in a supervised or reinforcement learning mode [1, 11]. Another area of research focuses on end- to- end approaches where the GNN is trained to produce the final answer [29]. From a practical point of view, these end- to- end approaches are interesting because they can potentially find more efficient solutions than those proposed by algorithm designers [31]. As other data- driven algorithms, GNNs used for combinatorial optimization make a trade- off between performance on some restricted subset of inputs and generalization to the whole domain of possible inputs. In the extreme case, the input distribution may be skewed to the extent that the GNN only needs to recognize superficial features of the input graph. In this work, we focus on the end- to- end approaches. We demonstrate these improvements with the popular Neuro SAT architecture [29], which has demonstrated the ability to exhibit nontrivial behavior resembling a search in a continuous space, rather than mere classification based on superficial statistics. The Neuro SAT Architecture. We demonstrate our enhancement using the Neuro SAT architecture with several simplifications. Neuro SAT is a GNN that operates on an undirected bipartite graph of literals and clauses. In this graph, each literal is connected to clauses that contain this literal. The MP process alternates between two steps that update the representations of clauses and literals, respectively. The embeddings of literals and clauses are updated by two independent LSTMs. The messages from clause to literals are produced with a 3- layer MLP that takes the embeddings of a clause as an input, and similarly in the opposite direction. After several rounds of MP iterations, the vector representation of each literal is passed into another MLP used to produce a vote for the satisfiability of the formula. These votes are averaged across all literals to produce the final prediction. A more detailed description is provided in the Supplementary material S.2. ## 3 Curriculum for Training GNNs An important feature of the Neuro SAT architecture is that the number of MP iterations does not need to be fixed because each round of MP is realized with the same function. In the original paper, the authors demonstrated that the model trained with 26 MP iterations on problems with up to 40 variables was able to generalize to much larger problems with up to 200 variables. This is achieved just by iterating MP for more steps (hundreds or even thousands). Therefore, we can view the architecture as an iterative algorithm with an adaptive number of steps that could depend on the difficulty of the problem. During training, the number of iterations needs to be fixed so that the problems can be solved in batches, but during inference, each problem can run for a different number of steps. As was already shown in the original paper, when the problem is satisfiable and the model correctly predicts it, the vectors of literals form two well- separated clusters. Empirically, once the vectors form two well- separate clusters, subsequent updates do not change the vectors significantly. Informally speaking, MP iterations can be viewed as optimization steps of an implicit energy function of the trained model [14]. Unsatisfied clauses should increase the energy and the minimum energy should be achieved when the maximum number of clauses are satisfied. For satisfiable formulas, this occurs when the vectors form two well- separated clusters, which makes the whole process qualitatively similar to the optimization of the SDP relaxation described in Section 2.3. In the experimental section 5 (and in the Supplementary material S.5), we further verify the connection to SDP by visualizing the evolution of the SDP objective evaluated on the Neuro SAT embeddings after each MP round. Figure 4 shows that this objective function increases until it reaches a fixed point. Therefore, we can set up a stopping criterion that stops the MP process once the vectors stop to change significantly. This could be viewed as an analog of a convergence threshold of iterative solvers for continuous problems or BP. As mentioned in Section 2.3, the number of iterations required is well correlated with the difficulty of the problem. This motivates our curriculum training procedure, which trains the model by incrementally enlarging the training set with bigger problems and increasing the number of MP operations. For each new problem size, the model is trained until a certain accuracy is reached, and after that, larger problems are added to the training set and the number of MP rounds is incremented accordingly. With this procedure and several simplifications of the original model, we achieve almost an order of magnitude faster convergence to the same accuracy as reported in the original paper (85%). A similar observation was recently made by Garg et al. [9] in a study of the in- context learning capabilities of a trained transformer. The authors observe that the trained model is able to perform optimization of a loss function (a high- dimensional regression) as the input passes through individual layers. They also experimentally demonstrated that it is possible to significantly accelerate the emergence of this capability if the model is trained incrementally by increasing the dimensionality of the regression. In our case, we also incrementally increase the number of MP iterations together with the number of variables within the formula, which speeds up the training even further. In the experimental section 5 (and in the Supplementary material S.6), we also describe an experiment in which we trained Neuro SAT with an SDP- like loss function instead of the classification loss. The trained model reached a smaller accuracy but was not as dependent on the curriculum as the original model, because the MAX- SAT objective gives more information than the 1- bit supervision of SAT. ## 4 Sampling and Decimation Selsam et al. [29] observe that for formulas that the model correctly classified as satisfiable, the embeddings of literals form two well- separated clusters. In Figure 1 and in Supplementary Figure 6, we recapitulate their visualization of embeddings with UMAP instead of PCA (the final clusters are more distinct when visualized with

<center>Figure 1: This figure shows how the embeddings of literals change during the MP process. We selected 3 different time steps from the 30 time steps used for this example. The 16-dimensional vectors are projected to 2D by UMAP algorithm. The colors correspond to the truth values of the final solution recovered for this formula. As can be seen, the literals progressively form two well-separated clusters of literals with the same truth value. </center> UMAP). The authors showed that for a large portion of correctly classified satisfiable formulas, they were able to recover a satisfying assignment by clustering the embeddings and assigning the same Boolean value to all literals within one cluster. They needed to test both possible ways of assigning Boolean values because they did not know in advance which cluster corresponds to the value true and which to the value false. We first confirmed their finding by running an experiment in which we removed the final voting layer and classified the formula using a Silhouette score [27] of the embeddings (capturing the quality of the discovered clusters). Concretely, we first run K- means on the embeddings of literals to assign them to two clusters and then we compute the Silhouette score with the assigned labels. On the training set, we estimate a threshold for this score and classify the test set according to this threshold (i.e., we classify a formula as satisfiable if its score is above this threshold). With this procedure, we achieve the same accuracy as the original model with the voting layer (85%), which means that the observation of cluster formation is robust. Next, we tested whether the model can produce different clusterings if we sample initial embeddings of literals multiple times (this corresponds to different initializations of the SDP solver). This turned out to be the case; for a large portion of satisfiable problems, using multiple random initializations of the embeddings would produce a diverse set of solutions. This enables us to substantially improve the classification accuracy by taking a majority vote over multiple initializations. The sampling of different solutions is also trivially parallelizable. Motivated by similarities with the SDP relaxation, we tested whether it is possible to recover the vector representing the value true and use this vector to assign a value to each literal. We selected all the formulas that the model correctly classified as satisfiable and checked whether the formula could be satisfied by one of the two possible assignments of Boolean values to the resulting clusters. Thus, we obtain a set of literal embeddings that correspond to the value true and another set corresponding to the value false, aggregated over all problems. Finally, we computed an average vector for both sets and also a distribution of Euclidean distances to these two vectors. Concretely, for each literal that was assigned the value true, we compute the \(l_2\) distance of its embedding to the average true vector and also to the average false vector. Similarly, for each literal that was assigned the value false. <center>Figure 2: A histogram of Euclidean distances to the average true vector and average false vector. 0 to 0 center are distances between embeddings of literals assigned to false to the average false vector, etc. The figure clearly demonstrates that literals that take the value false in the final assignment move to the same area of the vector space and the same is true for literals that take the value true. </center> Figure 2 shows that all vectors assigned to the value true are close to the average true vector and far from the average false vector, and vice versa. Therefore, we can assign each literal of a formula according to its distance to these two average vectors. Relying on the intuition from BP, we may try to treat these two distances as marginal probabilities over the two possible truth values, and therefore obtain a decimation algorithm. This algorithm fixes variables whose embeddings are close to one of these average vectors, simplifies the formula, and runs the MP of the GNN again. The results with these improvements are presented in the following section. ## 5 Experiments In this section, we describe the datasets we use for evaluation, model/training hyperparameters and experimental results. The experimental results are divided into qualitative findings where we present general observations supporting the connection to SDP and quantitative findings where we compare our improvements to the original Neuro SAT model.

<center>Figure 3: Validation accuracy during training. Our model with a curriculum achieves reaches \(85\%\) in approximately 30 minutes, whereas the original Neuro SAT implementation needs over 5 hours. For comparison, we also add our implementation trained on the same data, but without a curriculum. The training of each model stops once it achieves an accuracy of \(85\%\) on a validation set. </center> ### 5.1 Data and hyperparameters 5.1.1 Data Generation. Selsam et al. [29] demonstrates that a well- structured distribution of training data is essential to prevent the model from overfitting to superficial features. A recent theoretical study [26] also explains why input diversity is important in order for the model to transition to a regime where it is performing optimization over inputs. We therefore reuse Selsam's generative model of random SAT formulas which makes sure that no superficial features exist. The generative model samples random clauses until the formula becomes unsatisfiable. Once it finds such unsatisfiable formula, it flips one literal in the lastly generated clause, and this will produce another formula which will differ by one literal and will be satisfiable. However, Selsam's generation procedure is largely random and therefore does not capture any human- like reasoning skills. Therefore we also generate structured problems (Latin squares, Sudoku, and logical circuits). For the interested reader, we describe the details of the data generation process in the Supplementary materials S3.1 and S3.2. In the reference implementation of Neuro SAT, the model is trained on 100 000 formulas where for each formula the number of variables is sampled uniformly from the interval [10, 40]. The model is then evaluated on problems with 40 variables. Similarly as in the original paper, this test set is here referred to as \(SR(40)\) . In our case, the size of the test set is the same, but we train only on 10 000 formulas in total and sample the number of variables in the formula from the interval [5, 40]. We emphasize that in the experimental results, all models are trained on the same training data. 5.1.2 Model architecture. When experimenting with the original Neuro SAT architecture, we found that it is unnecessarily complex without any clear rationale and therefore we tried to simplify it as much as possible. We managed to significantly simplify the model without sacrificing the final accuracy. Here is the list of simplifications in our model: We completely removed the two 3- layer MLPs that produce the messages from the hidden states of the two LSTMs. The messages sent are, therefore, the hidden states themselves. We replace the final voting MLP with a linear layer. We do not use Layer Norm within LSTMs. We reduce the dimension of the hidden state of the LSTMs from 128 to 16. 5.1.3 Training loop. We train the model using the curriculum described in Section 3. In the following text, we consider the size of the formula to be given by the number of variables it contains. The training starts with formulas of size 5 and this size is incremented by 2 every time the validation accuracy (for a given size) reaches a given threshold or the maximum number of 200 epochs is reached. For each increment, we add the problems from the four previous increments 3 which makes the training more stable. The thresholds used to increment the size are obtained by interpolating the values between 0.65 (for the first size) and 0.85 (for the last size). We note that the values could be set to a fixed number but this may waste time during learning on the intermediate sizes. Empirically, the model spends most of the time on the first 3 and 5 last sizes. For the other training hyperparameters, we follow the original implementation except that we change the learning rate to \(2 \cdot 10^{- 3}\) .

### 5.2 Qualitative Results 5.2.1 Evolution of Literal Embeddings. To further support our claim about the connection to SDP, we tested whether the evolution of literal embeddings in Neuro SAT actually corresponds to an optimization process that maximizes the SDP objective (2.3). The test was performed on several hundred 2- CNF random formulas, since their SDP formulation is simple to state. An SDP solver optimizes the matrix \(Y\) (whose entries could be interpreted as dot products of vectors corresponding to Boolean variables). To observe the behavior of the SDP objective function on the evolving literal embeddings from Neuro SAT, we need to compute matrix \(Y\) after each MP iteration of the GNN. For each iteration \(t\) of the GNN, we obtain the matrix \(Y(t) = L^{(t)}L^{(t)T}\) where \(L^{(t)}\) represents the matrix of centered and normalized embeddings of positive literals. The first row of \(L^{(t)}\) corresponds to the vector \(y_{0}\) representing the value true and therefore is not present in the MP graph for Neuro SAT. Thus, we therefore set \(y_{0}\) to the average true vector described in Section 4. In Figure 4 we show how the objective function changes after each iteration \(t\) for 3 randomly selected instances. We also include the objective value obtained with an SDP solver as a reference. As can be seen, the <center>Figure 4: A plot showing how the SDP objective value computed from the Neuro SAT embeddings (in blue) changes after each iteration of MP. The horizontal red line represents the value of the same objective obtained with an SDP solver. </center> 5.2.2 Training directly with the MAX- SAT SDP objective function. To outline one possible future work direction, we tried to train the GNN with a loss function that is minimized when the maximum number of clauses is satisfied. Given a set of variables \(\{x_{1},x_{2},\ldots ,x_{n}\} = X\) corresponding to each variable in the boolean formula, one special variable \(x_{0}\) corresponding to value true, and a set of clauses \(\{c_{1},c_{2},\ldots ,c_{m}\} = C\) , where each clause \(c_{i}\) is a set of literals (variables with a polarity), the objective function \(v(C)\) for an integer- valued problem can be defined as: \[v(C) = \sum_{c\in C}\left(\prod_{l\in c}\frac{1 - sgn(l)\cdot x_{l}\cdot x_{0}}{2}\right)\] \(\operatorname {sgn}(l)\) is a variable polarity in clause \(c\) and evaluates to 1 for a positive occurrence of the variable \(x_{l}\) and to \(- 1\) for a negative occurrence. The variable \(x_{l}\in X\) can take the value \(- 1\) or 1 (corresponding to false and true). If we set \(x_{0}\) to 1 then the product for a clause \(c\) will yield 0 if at least for one of the variables \(x_{l}\) in the clause, \(sgn(l)\cdot x_{l}\cdot x_{0} = 1\) , which is the case when this clause is satisfied. To use this as a loss function for the supervision of Neuro SAT, we lift boolean variables to be unit vectors \(y_{0},y_{1},y_{2},\ldots ,y_{n}\) in a high- dimensional space. The objective is to minimize the following differentiable expression by optimizing these unit vectors: \[v(C) = \sum_{c\in C}\left(\prod_{l\in c}\frac{1 - sgn(l)y_{l}\cdot y_{0}}{2}\right).\] The scalar product of the unit vectors \(y_{l}\cdot y_{0}\) is a real number between \(- 1\) and 1. The vector \(y_{0}\) is sampled randomly as a unit vector and is kept fixed, whereas other variable vectors are sampled randomly, fed into Neuro SAT as initial embeddings for positive literals, updated by MP iterations, and normalized after each update. We supervise the negative literals with the same objective with the exception that the \(\operatorname {sgn}(l)\) returns \(- 1\) for positive and 1 for a negative literal occurrence. In order to compare the network trained with the SDP MAX- SAT objective with Neuro SAT trained with classification loss, we need a Boolean variable assignment. To extract the assignment of individual variables, we compute an inner product with the vector \(y_{0}\) (representing the value true) and assign the variable to true if it is positive and to false in the other case. Once we have the assignment, we can classify the formula as SAT/UNSAT by checking whether the solution satisfies the formula. The model trained with MAX- SAT SDP objective accurately classifies only \(\sim 73\%\) of all problems (vs. \(\sim 85\%\) in the case of the Neuro SAT trained by the classification loss). On the other hand, when trained with this objective function, the model starts to quickly improve even when trained only on the formulas of the largest size (i.e. 40 variables) without a curriculum. This suggests that a possible combination of loss functions, one that tries to maximize the number of satisfied formulas and one that penalizes the model for incorrect classification, may be beneficial. We leave the investigation of this idea for future work. ### 5.3 Quantitative Results 5.3.1 Training Convergence with the Curriculum. To demonstrate the effectiveness of the proposed curriculum, we compare the training process with two baselines. The first is the publicly available implementation of Neuro SAT 4 and the second is our model without curriculum. We stop training each model once it reaches the validation accuracy reported in the original paper (85%). As visible in Figure 3, our model with the curriculum reaches this accuracy in approximately 30 minutes, while the other two baselines need to be trained for several hours. All models were trained on 1 GPU (NVIDIA A100). 5.3.2 Sampling and Decimation. In Table 1, we show the increase in accuracy due to the enhancements described in Section 4. Together with the results on randomly generated problems, we also show results on three different structured problems whose details are described in the Supplementary material S3.2. The results show a noticeable increase in the number of solved problems for both enhancements (sampling and decimation). For decimation, we use only two passes, which means that if the first application of the

Table 1: Improvements obtained due to the sampling and decimation procedure. We test how many satisfiable problems could be solved with the model. SR(40) is the test set with random problems; the other problems are describe in the Supplementary meterial S3.2. For each problem type, we include the average number of variables. For larger problems, we run the model for more MP iterations. When decimation is used, we count the number of problems solved during the first and the second pass separately. #samples refers to the number of different initializations of literal embeddings. <table><tr><td>Problem type</td><td>#SAT problems</td><td>Avg. #var</td><td>First pass</td><td>Second pass</td><td>#MP iterations</td><td>#samples</td><td>Decimation</td><td>Solved</td></tr><tr><td rowspan="3">SR(40)</td><td rowspan="3">5000</td><td rowspan="3">40</td><td>4442</td><td>274</td><td rowspan="3">100</td><td>16</td><td>Yes</td><td>94 %</td></tr><tr><td>3990</td><td>-</td><td>1</td><td>No</td><td>80 %</td></tr><tr><td>4457</td><td>-</td><td>32</td><td>No</td><td>89.1 %</td></tr><tr><td rowspan="3">Latin Squares 9x9</td><td rowspan="3">200</td><td rowspan="3">196.9</td><td>186</td><td>14</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>100 %</td></tr><tr><td>95</td><td>-</td><td>1</td><td>No</td><td>47.5 %</td></tr><tr><td>192</td><td>-</td><td>32</td><td>No</td><td>96 %</td></tr><tr><td rowspan="3">Latin Squares 8x8</td><td rowspan="3">200</td><td rowspan="3">133.5</td><td>196</td><td>1</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>98.5 %</td></tr><tr><td>113</td><td>-</td><td>1</td><td>No</td><td>56.5 %</td></tr><tr><td>197</td><td>-</td><td>32</td><td>No</td><td>98.5 %</td></tr><tr><td rowspan="3">Logical Circuits</td><td rowspan="3">344</td><td rowspan="3">131.1</td><td>319</td><td>0</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>92.7 %</td></tr><tr><td>293</td><td>-</td><td>1</td><td>No</td><td>85.2 %</td></tr><tr><td>319</td><td>-</td><td>32</td><td>No</td><td>92.73 %</td></tr><tr><td rowspan="3">Sudoku 9x9</td><td rowspan="3">200</td><td rowspan="3">245.6</td><td>92</td><td>11</td><td rowspan="3">1000</td><td>16</td><td>Yes</td><td>51.5 %</td></tr><tr><td>35</td><td>-</td><td>1</td><td>No</td><td>17.5 %</td></tr><tr><td>94</td><td>-</td><td>32</td><td>No</td><td>47 %</td></tr></table> GNN did not solve the formula, we fix the variables whose distances to the average vectors were below a threshold, simplify the formula, and then process it with the GNN again. For the second pass, we used only one initialization sample for each decimated formula. Therefore, if the first pass uses 16 samples, the second pass can also produce a maximum of 16 samples. To see the effects of decimation, we show results of runs with the same number of samples in total (32) but without decimation. We included results with 3 passes and comparison to BP in the Supplementary materials S.7 and S.8. We note, as should be obvious, that our method cannot certify unsatisfiability. ## 6 Related Work Most of the related work was already mentioned in Section 2. In this section, we describe related work in the context of GNNs and Boolean Satisfiability. In the domain of Boolean Satisfiability, applications of GNNs can be divided into hybrid or end- to- end approaches. In the hybrid approaches, the GNN is used to guide a discrete search. We can further distinguish between applications where the GNN guides a simple heuristic and applications where the predictions of the GNN are used inside an industrial SAT solver. For the case of heuristics, Yolcu and Poczos [33] use GNNs trained by Reinforcement Learning to select variables and values in a local search. Zhang et al. [35] also use GNN for local search, but train it with supervised learning. For the case of SAT solvers, Kurin et al. [17] introduce a branching heuristic for SAT solvers trained using value- based reinforcement learning (RL) with GNNs for function approximation. They incorporate the heuristic with the Mini Sat solver and manage to reduce the number of iterations required to solve SAT problems by 2- 3X. Similarly, Wang et al. [32] use GNN as a variable selection heuristic and manage to improve Mini Sat in terms of the number of solved problems on the SATCOMP- 2021 competition problem set. On the end- to- end front, the most relevant work is the one by Selsam et al. [29] who introduced the Neuro SAT architecture, which was our starting point. Similar to Neuro SAT were the models introduced by Cameron et al. [5] who used different GNN architecture and Shi et al. [30] who used a Transformer. Freivalds and Kozlovics [8] use a Denoising Diffusion model to learn to sample multiple solutions and Ozolins et al. [21] propose an approach in which the GNN can take feedback from solution trials. Apart from work focused on Boolean Satisfiability, we also mention the work by Kuck et al. [16] who use GNN to improve Belief Propagation. ## 7 Conclusion We uncover a connection between GNNs trained on combinatorial problems and two well- known approximation algorithms, SDP and BP. Using this connection, we enhance their training and inference procedure. In particular, we focus on the well- known NP- complete problem of Boolean Satisfiability (SAT). We introduce a curriculum training procedure, which enables a significantly faster iteration over experiments. Further, we apply a decimation procedure and initial- value sampling, which significantly increase the number of solved problems. For a problem to be considered solved, we not only require the correct prediction whether it is satisfiable or not, but we also require the GNN to produce a satisfying assignment for satisfiable problem instances. Even though the enhancements were presented in the domain of Boolean Satisfiability, we believe that they can easily be generalized to other domains where these approximation algorithms are used.

In future work, we plan to explore these similarities more closely and reverse engineer the algorithm learned by the GNN. ## 8 Limitations As already mentioned in the main text, we demonstrate the efficacy of our model solely on datasets with a low number of variables, without incorporating any "real- world" problems. It is important to note that end- to- end trained machine learning models such as GNNs cannot compete with specialized solvers such as CDCL SAT solvers. We did not aim to compete with state- of- the- art SAT solvers on realistic benchmarks; instead, our focus is on understanding the reason why GNNs are able to correctly predict satisfiability. Furthermore, more research is still needed to determine the precise mechanism by which the GNN "optimizes" the embeddings during the message- passing process (i.e. how to interpret the messages and the updates of the embeddings provided by the LSTMs). Our study mainly focused on an empirical evaluation rather than a precise theoretical model. ## Acknowledgments The results were supported by the Ministry of Education, Youth and Sports within the dedicated program ERC CZ under the project POSTMAN no. LL1902. The work was also co- funded by the European Union under the project ROBOPROX (reg. no. CZ.02.01.01/00/22_008/0004590) and by the REFRESH - Research Excellence For Region Sustainability and High- tech Industries project number CZ.10.03.01/00/22_003/0000048 via the Operational Programme Just Transition. The computing resources required were provided with the support of the Ministry of Education, Youth and Sports of the Czech Republic through the e- INFRA CZ (ID:90254) ## References [1] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. 2021. Machine learning for combinatorial optimization: a methodological tour d'horizon. European Journal of Operational Research 290, 2 (2021), 405- 421. [2] Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh (Eds.). 2021. Handbook of Satisfiability - Second Edition. Frontiers in Artificial Intelligence and Applications, Vol. 336. IOS Press. 437- 462 pages. https://doi.org/10.3233/FAIA200993 [3] Alfredo Braunstein, Marc Mezard, and Riccardo Zecchina. 2005. Survey propagation: An algorithm for satisfiability. Random Structures & Algorithms 27, 2 (2005), 201- 226. [4] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. 2021. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. ar Xiv preprint ar Xiv:2104.13478 (2021). [5] Chris Cameron, Rex Chen, Jason Hartford, and Kevin Leyton- Brown. 2020. Predicting propositional satisfiability via end- to- end learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 3324- 3331. [6] Quentin Cappart, Didier Chetelat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. 2023. Combinatorial optimization and reasoning with graph neural networks. J. Mach. Learn. Res. 24 (2023), 130- 1. [7] Edmund Clarke, Daniel Kroening, and Flavio Lerda. 2004. A tool for checking ANSI- C programs. In Tools and Algorithms for the Construction and Analysis of Systems: 10th International Conference, TACAS 2004, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2004, Barcelona, Spain, March 29- April 2, 2004. Proceedings 10. Springer, 168- 176. [8] Karlis Freivalds and Sergejs Kozlovics. 2022. Denoising Diffusion for Sampling SAT Solutions. ar Xiv preprint ar Xiv:2212.00121 (2022). [9] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. 2022. What can transformers learn in- context? A case study of simple function classes. Advances in Neural Information Processing Systems 35 (2022), 30583- 30598. [10] Bernd Gartner and Jiri Matousek. 2012. Approximation algorithms and semidefinite programming. Springer Science & Business Media. [11] Maxime Gasse, Didier Chetelat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. 2019. Exact combinatorial optimization with graph convolutional neural networks. Advances in neural information processing systems 32 (2019). [12] Michel X Goemans and David P Williamson. 1995. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM) 42, 6 (1995), 1115- 1145. [13] Carla P Gomes, Willem- Jan Van Hoeve, and Lucian Leahu. 2006. The power of semidefinite programming relaxations for max- sat. In International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) Techniques in Constraint Programming. Springer, 104- 118. [14] Stephen Gould, Richard Hartley, and Dylan Campbell. 2021. Deep declarative networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 8 (2021), 3988- 4004. [15] Lukas Kroc, Ashish Sabharwal, and Bart Selman. 2012. Survey propagation revisited. ar Xiv preprint ar Xiv:1206.5273 (2012). [16] Jonathan Kuck, Shuvam Chakraborty, Hao Tang, Rachel Luo, Jiaming Song, Ashish Sabharwal, and Stefano Ermon. 2020. Belief propagation neural networks. Advances in Neural Information Processing Systems 33 (2020), 667- 678. [17] Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. 2020. Can Q- learning with graph networks learn a generalizable branching heuristic for a SAT solver? Advances in Neural Information Processing Systems 33 (2020), 9608- 9621. [18] Anastasios Kyrillidis, Anshumali Shrivastava, Moshe Vardi, and Zhiwei Zhang. 2021. Fourier SAT: A Fourier expansion- based algebraic framework for solving hybrid boolean constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1552- 1560. [19] Luis C Lamb, Artur Garcez, Marco Gori, Marcelo Prates, Pedro Avelar, and Moshe Vardi. 2020. Graph neural networks meet neural- symbolic computing: A survey and perspective. ar Xiv preprint ar Xiv:2003.00330 (2020). [20] Elitza Maneva, Elchanan Mossel, and Martin J Wainwright. 2007. A new look at survey propagation and its generalizations. Journal of the ACM (JACM) 54, 4 (2007), 17- es. [21] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. 2022. Goal- aware neural SAT solver. In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 1- 8. [22] Christos H Papadimitriou. 1994. Computational complexity. Addison- Wesley. [23] Sejun Park and Jinwoo Shin. 2014. Max- Product Belief Propagation for Linear Programming: Applications to Combinatorial Optimization. In Conference on Uncertainty in Artificial Intelligence. https://api.semanticscholar.org/Corpus ID: 15136373 [24] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. 2019. Physics- informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys. 378 (2019), 686- 707. https://api.semanticscholar.org/Corpus ID:57379996 [25] Motakuri Ramana and Alan J Goldman. 1995. Some geometric results in semidefinite programming. Journal of Global Optimization 7, 1 (1995), 33- 50. [26] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. 2023. Pretraining task diversity and the emergence of non- Bayesian in- context learning for regression. ar Xiv preprint ar Xiv:2306.15063 (2023). [27] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics 20 (1987), 53- 65. [28] Daniel Selsam and Nikolaj Bjorner. 2019. Guiding high- performance SAT solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, Lisbon, Portugal, July 9- 12, 2019, Proceedings 22. Springer, 336- 353. [29] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L. Dill. 2019. Learning a SAT Solver from Single- Bit Supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6- 9, 2019. Open Review.net. https://openreview.net/forum?id= HJMc_i A5tm [30] Zhengyuan Shi, Min Li, Sadaf Khan, Hui- Ling Zhen, Mingxuan Yuan, and Qiang Xu. 2022. Satformer: Transformers for SAT solving. ar Xiv preprint ar Xiv:2209.00953 (2022). [31] Petar Velickovic and Charles Blundell. 2021. Neural algorithmic reasoning. Patterns 2, 7 (2021). [32] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth Mc Millan, and Risto Miikkulainen. 2021. Neuro Comb: Improving SAT solving with graph neural networks. ar Xiv preprint ar Xiv:2110.14053 (2021). [33] Emre Yolcu and Barnabas Poczos. 2019. Learning local search heuristics for boolean satisfiability. Advances in Neural Information Processing Systems 32 (2019). [34] Lenka Zdeborova and Florent Krzakala. 2016. Statistical physics of inference: Thresholds and algorithms. Advances in Physics 65, 5 (2016), 453- 552. [35] Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, and Lu Zhang. 2020. NLocal SAT: Boosting local search with solution prediction. ar Xiv preprint ar Xiv:2001.09398 (2020). [12] Michel X Goemans and David P Williamson. 1995. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM) 42, 6 (1995), 1115- 1145. [13] Carla P Gomes, Willem- Jan Van Hoeve, and Lucian Leahu. 2006. The power of semidefinite programming relaxations for max- sat. In International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) Techniques in Constraint Programming. Springer, 104- 118. [14] Stephen Gould, Richard Hartley, and Dylan Campbell. 2021. Deep declarative networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 8 (2021), 3988- 4004. [15] Lukas Kroc, Ashish Sabharwal, and Bart Selman. 2012. Survey propagation revisited. ar Xiv preprint ar Xiv:1206.5273 (2012). [16] Jonathan Kuck, Shuvam Chakraborty, Hao Tang, Rachel Luo, Jiaming Song, Ashish Sabharwal, and Stefano Ermon. 2020. Belief propagation neural networks. Advances in Neural Information Processing Systems 33 (2020), 667- 678. [17] Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. 2020. Can Q- learning with graph networks learn a generalizable branching heuristic for a SAT solver? Advances in Neural Information Processing Systems 33 (2020), 9608- 9621. [18] Anastasios Kyrillidis, Anshumali Shrivastava, Moshe Vardi, and Zhiwei Zhang. 2021. Fourier SAT: A Fourier expansion- based algebraic framework for solving hybrid boolean constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1552- 1560. [19] Luis C Lamb, Artur Garcez, Marco Gori, Marcelo Prates, Pedro Avelar, and Moshe Vardi. 2020. Graph neural networks meet neural- symbolic computing: A survey and perspective. ar Xiv preprint ar Xiv:2003.00330 (2020). [20] Elitza Maneva, Elchanan Mossel, and Martin J Wainwright. 2007. A new look at survey propagation and its generalizations. Journal of the ACM (JACM) 54, 4 (2007), 17- es. [21] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. 2022. Goal- aware neural SAT solver. In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 1- 8. [22] Christos H Papadimitriou. 1994. Computational complexity. Addison- Wesley. [23] Sejun Park and Jinwoo Shin. 2014. Max- Product Belief Propagation for Linear Programming: Applications to Combinatorial Optimization. In Conference on Uncertainty in Artificial Intelligence. https://api.semanticscholar.org/Corpus ID: 15136373 [24] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. 2019. Physics- informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys. 378 (2019), 686- 707. https://api.semanticscholar.org/Corpus ID:57379996 [25] Motakuri Ramana and Alan J Goldman. 1995. Some geometric results in semidefinite programming. Journal of Global Optimization 7, 1 (1995), 33- 50. [26] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. 2023. Pretraining task diversity and the emergence of non- Bayesian in- context learning for regression. ar Xiv preprint ar Xiv:2306.15063 (2023). [27] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics 20 (1987), 53- 65. [28] Daniel Selsam and Nikolaj Bjorner. 2019. Guiding high- performance SAT solvers with unsat- core predictions. In Theory and Applications of Satisfiability Testing- SAT 2019: 22nd International Conference, SAT 2019, Lisbon, Portugal, July 9- 12, 2019, Proceedings 22. Springer, 336- 353. [29] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L. Dill. 2019. Learning a SAT Solver from Single- Bit Supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6- 9, 2019. Open Review.net. https://openreview.net/forum?id= HJMc_i A5tm [30] Zhengyuan Shi, Min Li, Sadaf Khan, Hui- Ling Zhen, Mingxuan Yuan, and Qiang Xu. 2022. Satformer: Transformers for SAT solving. ar Xiv preprint ar Xiv:2209.00953 (2022). [31] Petar Velickovic and Charles Blundell. 2021. Neural algorithmic reasoning. Patterns 2, 7 (2021). [32] Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth Mc Millan, and Risto Miikkulainen. 2021. Neuro Comb: Improving SAT solving with graph neural networks. ar Xiv preprint ar Xiv:2110.14053 (2021). [33] Emre Yolcu and Barnabas Poczos. 2019. Learning local search heuristics for boolean satisfiability. Advances in Neural Information Processing Systems 32 (2019). [34] Lenka Zdeborova and Florent Krzakala. 2016. Statistical physics of inference: Thresholds and algorithms. Advances in Physics 65, 5 (2016), 453- 552. [35] Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, and Lu Zhang. 2020. NLocal SAT: Boosting local search with solution prediction. ar Xiv preprint ar Xiv:2001.09398 (2020).