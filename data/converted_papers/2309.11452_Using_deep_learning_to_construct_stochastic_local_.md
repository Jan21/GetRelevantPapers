# Using deep learning to construct stochastic local 


## Introduction


In Fig. 4 and Fig. 5, we display more detailed information of the experiments. We find that, in terms of \(\alpha\) , access to the GNN- based oracle “shifts the curve to the right” for \(\alpha\) around 3.0 to 3.2 for the boosted MT algorithm. A similar behaviour is found for the boosted Walk SAT algorithm. In particular, averaged across solvers, the average value of \(\alpha\) for which a solution was found increased by \(17\%\) . Interestingly, it is evident from the plots that for both solvers, the “hybrid” variant that switches to uniform updating quickly loses its initial advantage over the purely uniform variant. This exhibits the advantage of giving an SLS solver continuous access to an oracle. Since our loss consists of two terms (that we weigh equally in the experiments), an obvious question to ask is whether both of them contribute to the improvement in the performance of the algorithm. In Fig. 6 and Fig. 7 we plot the same statistics as above, however comparing the uniform variant with the full boosted variant, however with the model being trained on only one, the other, and both loss terms. Overall, we find that the combination of the two loss terms produces the best results, however, the Lovász local loss contributes significantly more to the improvement than the Gibbs loss. This is especially surprising for the oracle- based Walk SAT since there is no theoretical result or argument known to the authors that would connect the Walk SAT algorithm to the Lovász Local loss or the conditions in Propositions 2.1 and 2.2. In section A.2 of the appendix, we provide plots with additional details on the experiments. <center>Figure 6: Comparison of the MT algorithm when switching on and off the individual loss terms in the training of the underlying model. The uniform MT algorithm is printed as benchmark. </center> <center>Figure 7: Comparison of the Walk SAT algorithm when switching on and off the individual loss terms in the training of the underlying model. The uniform Walk SAT algorithm is printed as benchmark. </center>

## 5 Conclusion and future work In this work, we have presented an approach to use deep learning to construct SLS- based SAT solvers. The main contribution is that we design the algorithms and loss functions in such a way that the training process is guaranteed, by theoretical results, to, in expectation, produce a stronger solver. The hope is that with this approach we can create solvers for SAT and constraint optimization more generally, that are highly performant on for specific applications, while also being provably robust in terms of their average performance. We have empirically investigated this promise on a random 3- SAT dataset, with promising results. However, the practical potential of our approach remains largely unchartered, as more thorough experiments on larger and application- oriented datasets, more research on suitable deep learning architectures, as well as the application to more mature and sophisticated SLS solvers, needs to be carried out, since the resulting solvers are currently far from being competitive with state of the art solvers. We leave these experiments for future research and hope that the preliminary results presented in this paper nevertheless establish the promise of the idea. ## Acknowledgments P.B. thanks Mahdi Manesh for stimulating discussions. Both authors thank Jens Eisert for introducing them to the results of the Moser- Tardos algorithm and related works, which instilled the idea for this work. They thank Porsche Digital Gmb H for the possibility to work on this research project. M.K. thanks the BMBF (Hybrid) and the BMWK (Eni Qm A) for their support. ## References [1] Stephen A. Cook. The complexity of theorem- proving procedures. In Proceedings of the Third Annual ACM Symposium on Theory of Computing, STOC '71, pages 151- 158, New York, NY, USA, 1971. Association for Computing Machinery. [2] L. A. Levin. Problems of information transmission. 1973. [3] Wenxuan Guo, Junchi Yan, Hui- Ling Zhen, Xijun Li, Mingxuan Yuan, and Yaohui Jin. Machine learning methods in solving the boolean satisfiability problem, 2022. [4] Benedikt Bunz and Matthew Lamm. Graph neural networks and boolean satisfiability, 2017. [5] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a sat solver from single- bit supervision, 2019. [6] Emils Ozolins, Karlis Freivalds, Andis Draguns, Eliza Gaile, Ronalds Zakovskis, and Sergejs Kozlovics. Goal- aware neural SAT solver. In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, jul 2022. [7] Daniel Selsam and Nikolaj Bjorner. Guiding high- performance sat solvers with unsat- core predictions, 2019. [8] Sebastian Jaszczur, Michal Łuszczyk, and Henryk Michalewski. Neural heuristics for sat solving, 2020. [9] Jesse Michael Han. Enhancing sat solvers with glue variable predictions, 2020. [10] Emre Yolcu and Barnabas Poczos. Learning local search heuristics for boolean satisfiability. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [11] Wenjie Zhang, Zeyu Sun, Qihao Zhu, Ge Li, Shaowei Cai, Yingfei Xiong, and Lu Zhang. NLocal SAT: Boosting local search with solution prediction. In Proceedings of the Twenty- Ninth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, jul 2020. [12] Robin A. Moser. A constructive proof of the lovasz local lemma, 2008. [13] Robin A. Moser and Gábor Tardos. A constructive proof of the general lovasz local lemma, 2009. [14] David G. Harris and Aravind Srinivasan. Algorithmic and enumerative aspects of the moser- tardos distribution, 2017. [15] Dimitris Achlioptas, Fotis Iliopoulos, and Alistair Sinclair. Beyond the lovasz local lemma: Point to set correlations and their algorithmic applications, 2020.

Using deep learning to construct Stochastic Local Search SAT solvers with performance bounds [16] Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, and Koray Kavukcuoglu. Interaction networks for learning about objects, relations and physics, 2016. [17] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid von Glehn, Pawel Lichocki, Ivan Lobov, Brendan O'Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja, Pengming Wang, Ravichandra Addanki, Tharindi Hapuarachchi, Thomas Keck, James Keeling, Pushmeet Kohli, Ira Ktena, Yujia Li, Oriol Vinyals, and Yori Zwols. Solving mixed integer programs using neural networks, 2020. [18] P. Erdős and L. Lovász. Problems and results on 3- chromatic hypergraphs and some related questions. Infinite and finite sets, 2:609- 627, 1975. [19] Nicholas Harvey and Jan Vondrak. An algorithmic proof of the lovasz local lemma via resampling oracles, 2015. [20] T. Schöning. A probabilistic algorithm for k- sat and constraint satisfaction problems. In 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), pages 410- 414, 1999. [21] C.H. Papadimitriou. On selecting a satisfying truth assignment. In [1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science, pages 163- 169, 1991. [22] B. Selman, H. Kautz, and B. Cohen. Local search strategies for satisfiability testing. In D. S. Johnson and M. A. Trick, editors, Cliques, Coloring, and Satisfiability: the Second DIMACS Implementation Challenge in Discrete Mathematics and Theoretical Computer Science, volume 26, pages 521- 532. American Mathematical Society, 1996. [23] Adrian Balint and Uwe Schöning. Choosing probability distributions for stochastic local search and the role of make versus break. volume 7317, pages 16- 29, 06 2012. [24] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez- Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks, 2018. [25] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [26] Abien Fred Agarap. Deep learning using rectified linear units (relu). ar Xiv preprint ar Xiv:1803.08375, 2018. [27] Robin A. Moser. Exact Algorithms for Constraint Satisfaction Problems. Ph D thesis, 2012. [28] Jan Dean Catarata, Scott Corbett, Harry Stern, Mario Szegedy, Tomas Vyskočil, and Zheng Zhang. The Moser- Tardos Resample algorithm: Where is the limit? (an experimental inquiry), pages 159- 171. [29] Scott Kirkpatrick and Bart Selman. Critical behavior in the satisfiability of random boolean expressions. Science, 264(5163):1297- 1301, 1994. [30] Massimo Lauria, Jan Elffers, Jakob Nordström, and Marc Vinyals. Massimo Lauria/cnfgen: CNFgen registered with Zenodo, November 2019. [31] Gilles Audemard and Laurent Simon. On the glucose sat solver. International Journal on Artificial Intelligence Tools, 27:1840001, 02 2018. [32] Alexey Ignatiev, Antonio Morgado, and Joao Marques- Silva. Py SAT: A Python toolkit for prototyping with SAT oracles. In SAT, pages 428- 437, 2018. [33] Ashiqur Khudabukhsh, Lin Xu, Holger Hoos, and Kevin Leyton- Brown. Satenstein: Automatically building local search sat solvers from components. volume 232, pages 517- 524, 01 2009. [34] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander Plas, Skye Wanderman- Milne, and Qiao Zhang. JAX: composable transformations of Python+Num Py programs, 2018. [35] Jonathan Godwin\*, Thomas Keck\*, Peter Battaglia, Victor Bapst, Thomas Kipf, Yujia Li, Kimberly Stachenfeld, Petar Veličković, and Alvaro Sanchez- Gonzalez. Jraph: A library for graph neural networks in jax., 2020.

## A Appendix ## A.1 Evaluating the loss in practice In Sec.2.4 of the main text, we motivated the goal to find an oracle factory that minimises the total loss (12). However, in practice we don't have access to \(\mathcal{P}\) . As is common practice, we instead estimate the loss using samples from a training set \(\mathcal{X} = (X_{i},\phi_{i})_{i = 1}^{N}\) of \(N\) instances that we assume to be sampled independently from \(\mathcal{P}\) . Here, \(X_{i} = \{x^{i,j}\}_{j = 1}^{N_{i}}\) describes a set of unique \(N_{i}\) assignments for the problem instance \(\phi_{i}\) . For each assignment \(x^{i,j}\) , let \(e^{i,j}\) denote the number of clauses it violates. For every instance, we can calculate \(LL\) loss by first explicitly constructing the dependency graph \(\mathcal{D}_{F_{\theta}(\phi)}(\phi)\) . This is relatively simple in our case because, due to the simple Bernoulli structure of our output oracles, we know that the neighbourhood of a given clause node \(j\) is simply the set of clause nodes, whose corresponding variable sets intersect with that of \(c_{j}\) , i.e. \(\Gamma (j)\coloneqq \{j^{\prime}\in [m]|j^{\prime}\neq j,V(c_{j^{\prime}})\cap V(c_{j})\neq \emptyset \}\) . Given a batch \(B\subset [|\mathcal{X}|]\) of instances, we estimate the LL loss by calculating the \(z\) - norm of the whole loss vector, creating the dependency graph of the batch as the union of the dependency graphs of batch elements. Regarding the Gibbs loss, we follow Ref. [17] and generate, for a given batch \(B\) , an estimator \[\hat{L}_{G i b b s}(\theta) = -\sum_{i\in B}\sum_{j}^{N_{i}}\omega_{i j}\log \left(P_{F_{\theta}(\phi)}(x^{i,j})\right) \quad (20)\] with the Gibbs weights: \[\omega_{i j} = \frac{\exp\left(-\beta\phi(x^{i,j})\right)}{\sum_{k = 1}^{N_{i}}\exp\left(-\beta\phi(x^{i,k})\right)} \quad (21)\] ## A.2 Detailed plots for the experiments Fig. 8 and Fig. 9 provide more detail of the experiments. We provide them in this technical appendix for the interested reader.

<center>Figure 8: Benchmark of our ml-boosted version of the Moser-Tardos Resample algorithm (orange) against its uniform version (blue) and Walk SAT algorithm (red). a) + b): mean (see a)) and median (see b)) number of steps needed (y-axis) for instances of the evaluation dataset with a certain clauses to variables ratio \(\alpha\) (x-axis). Note that \(10^{6}\) steps means that the algorithm has reached its time limit and has not found a solution. c): percentage of instances of the evaluation dataset solved within \(10^{6}\) steps (y-axis) with a certain clauses to variables ratio \(\alpha\) (x-axis). d): benchmark of the steps taken by uniform version of Moser-Tardos Resample algorithm (x-axis) against the ml-boosted version of Moser-Tardos Resample algorithm (y-axis). The color of the dots indicates the corresponding clauses-to-variables ratio \(\alpha\) . e): mean number of clauses violated (y-axis) as a function of the number of steps the algorithm has taken (x-axis). f): ratio of solved instances (y-axis) as a function of the number of steps the algorithm has taken. </center>

<center>Figure 9: Benchmark of our ml-boosted version of Walk SAT algorithm (green) against its uniform version (orange) a) + b): mean (see a)) and median (see b)) number of steps needed (y-axis) for instances of the evaluation dataset with a certain clauses to variables ratio \(\alpha\) (x-axis). Note that \(10^{6}\) steps means that the algorithm has reached its time limit and has not found a solution. c): percentage of instances of the evaluation dataset solved within \(10^{6}\) steps (y-axis) with a certain clauses to variables ratio \(\alpha\) (x-axis). d): benchmark of the steps taken by uniform version of Walk SAT algorithm (x-axis) against the ml-boosted version of it (y-axis). The color of the dots indicates the corresponding clauses-to-variables ratio \(\alpha\) . e): mean number of clauses violated (y-axis) as a function of the number of steps the algorithm has taken (x-axis). f): ratio of solved instances (y-axis) as a function of the number of steps the algorithm has taken. </center>

<center>Figure 1: Illustration of the general idea of this work. Left: A simple SLS solver finds a solution to a SAT instance by repeatedly and randomly updating a small subset of the variables. Middle: An oracle-based SLS solver uses samples from an oracle \(O\) that is provided as part of the input to update the variables at each iteration. Right: We use a deep learning model to train an oracle factory \(F_{\theta}\) that maps an incoming instance to an oracle which is then fed into an oracle-based. This approach is motivated by results that provide sufficient conditions for an oracle-based SLS solver to find a solution efficiently, based on properties of the oracle. </center> The main motivation for the approach presented in this paper is a series of breakthrough results from theoretical computer science that imply sufficient conditions for an SLS solver with access to a suitable oracle to efficiently find a solution to a SAT instance from Refs. [12, 13, 14, 15]. ### 1.1 Contributions This work provides several contributions: - We establish the promise of application-specific solvers with performance guarantees, by connecting "hands-on" research from machine learning with theoretical results from computer science.- We introduce a new, theoretically motivated loss function, which we call the Lovász Local loss, which rewards an oracle's exploitation of the local structure of SAT instances. We are not aware of any work in the field whose loss is motivated by known bounds on the resulting solver performance.- We construct and train two oracle-based SLS solvers using an Interaction network [16].- We empirically investigate the ability of these solvers to solve random 3-SAT at varying levels of difficulty (as measured by the common \(\alpha\) -ratio between clauses and variables). Our experiments show significant boosts in performance, with the ML-based solvers solving instances that have, on average, a \(17\%\) higher value of \(\alpha\) and doing so in \(35\%\) fewer steps, with a larger than \(8x\) improvement in the median number of steps.- We show that, in our experiments, providing continuous access to an oracle produces significantly better results (both in number of steps needed and number of instances solved) while an algorithm that only uses the oracle to initialise a candidate, only solves instances in fewer steps and is not able to solve more instances. ### 1.2 Related Work #### 1.2.1 Deep learning based SAT solving Ref. [3] provides a recent review paper on machine learning- based SAT solvers. Focussing here on the literature on SLS solvers, in Ref. [11] the authors train a Graph Neural Network to generate an initial candidate for various SLS solvers. However, they don't use this GNN as an oracle in the course of running these solvers. Ref. [10] uses a GNN to learn a variable selection heuristic in the course of a Walk SAT type SLS solver. However, their model is based on a learned policy and hence doesn't act as an oracle in the sense of this work. The cross- entropy loss with respect to a Gibbs distribution that we use in this work is also used in Ref. [17] in the context of Mixed Integer Program solving. #### 1.2.2 SLS algorithms with solution guarantees As described above, one core contribution of this work is to choose a solver and loss function for training such that we are guaranteed by theoretical results that a smaller loss will lead to better performance. The body of work we have in mind here is that around the seminal Lovász Local Lemma (LLL) [18]. In particular, Ref. [12, 13] proved the results that underlie our Proposition 2.1. Ref. [19] explicitly introduces the notion of an oracle- based SLS algorithm, even though their resampling oracles are special cases of the oracles we consider here. Ref. [14] proves the results stated below as Proposition 2.2 and that we consider as crucial for motivating our work in settings where an oracle does not satisfy the conditions of the LLL.

While no runtime results other than those in Ref. [20, 21] are known to us for the Walk SAT algorithm [22], we include it here for its strong performance in competitions, especially the oracle- like working of the Prob SAT algorithm from Ref. [23] that served as an inspiration for Algorithm 2. ## 2 Methods ### 2.1 SAT The SAT decision problem is to decide whether a given formula \(\phi\) , that is, a formula consisting of \(n\) Boolean variables connected by the Boolean operators (conjunction \(\wedge\) , disjunction \(\vee\) and negation \(\neg\) ), admits a truth value assignment \(x \in \{0,1\}^{n}\) to the \(n\) variables such that the formula, as a whole, evaluates to true under this assignment. For every formula, there exists an equivalent formula that is in conjunctive normal form (CNF) which is a conjunction of clauses, \[\phi = (c_{1}\wedge c_{2}\wedge \ldots \wedge c_{m}), \quad (1)\] where each clause is a disjunction of literals (i.e. variables or their negation), \[c_{j} = (l_{1}^{(j)}\vee l_{2}^{(j)}\vee \ldots \vee l_{k}^{(j)}). \quad (2)\] SAT is NP- complete (see Refs. [1, 2]) and has many applications in industry. For example, it can be used to verify hardware and software, to plan and schedule, and to solve problems in cryptography. Notationwise, in the following we write \([n] \equiv \{1,\ldots ,n\}\) , \(c \in \phi\) to denote that the clause \(c\) is present in a CNF- formula \(\phi\) , \(V(c) \subseteq [n]\) to denote the set of variables present in \(c\) . We further split \(V(c)\) into disjoint subsets \(V^{+}(c)\) and \(V^{- }(c)\) , depending on whether the variables appear in \(c\) with or without negation, respectively. For any assignment \(x\) and any subset \(V \subseteq [n]\) of variables, we write \(x|_{V} = (x_{v})_{v \in V}\) to denote the truncated assignment on \(V\) , \(c(x) \in \{0,1\}\) to indicate whether a clause is violated or not, and \(\phi (x) = \sum_{j} c_{j}(x)\) to denote the number of violated clauses. Finally, we write \(X_{n} \equiv \{0,1\}^{n}\) and, for an instance \(\phi\) , we denote as \(\Pi (\phi) \subset X_{n}\) the set of assignments with the minimal number of violated constraints across \(I_{n}\) . An instance is satisfiable if there exists an \(x\) such that \(\phi (x) = 0\) and unsatisfiable otherwise. ### 2.2 Oracle-based SLS There exist many solvers to address the general SAT problem. One class of them are Stochastic Local Search (SLS) algorithms. In their simplest form, they work as follows: 1. Given a SAT instance \(\phi\) in conjunctive normal form (CNF), generate an initial candidate \(x\) via an initialise sub-routine. 2. If \(x\) violates any clause, choose one violated clause \(c\) at random and update \(x\) on the variables that appear in \(c\) via a sub-routine update, keeping the remainder of \(x\) unchanged. 3. Repeat step 2 until no clause is violated or a stopping criterion is met. Output \(x\) . Hence, to specify a simple SLS solver amounts to specifying the initialise and update sub-routines. Possibly the two simplest non- trivial SLS algorithms are the Moser- Tardos (MT) algorithm and the simple Walk SAT algorithm. The MT algorithm initialises an assignment randomly and updates an assignment \(x\) by randomly sampling a new assignment \(x^{\prime}\) and setting \(x\) to \(x^{\prime}\) on all variables in the clause \(c\) . Walk SAT also initialises an assignment randomly, however to update it randomly chooses one of the variables that appear in \(c\) and flips the assignment of that variable. Both of the above algorithms heavily use samples drawn uniformly at random. However, it seems intuitively clear that they would perform better if instead, they sampled from distributions that are fine- tuned to the instance \(\phi\) and the current state \(x\) of the solver. To formalise this, we introduce the notion of a (sampling) oracle. A sampling oracle for instance \(\phi\) is a random variable \(O\) over the sample space \(\{0,1\}^{n}\) . Oracle- based SLS solvers are then SLS solvers who accept an oracle as part of the input and whose sub- routines utilise either samples \(x \sim O\) from \(O\) or the values of the latter's probability measure \(P_{O}\) in its definition. We define the oracle- based MT and the oracle- based Walk SAT algorithms in Algorithm 1 and Algorithm 2 respectively. Both of these algorithms generalise their non oracle- based counterpart, since the latter are simply the special cases of the former for the case that the uniform oracle is used. The choice of sampling probabilities in the update set of

Algorithm 1 Oracle- based Moser- Tardos algorithm Input: \(\phi ,O\) Output: \(x\) 1: \(x\sim O\) 2: while \(\exists c\in \phi\) : \(c(x) = 1\) do 3: \(x^{\prime}\sim O\) 4: pick a violated clause \(c\in \phi\) at random 5: \(x|_{V(c)}\gets x^{\prime}|_{V(c)}\) 6: end while Algorithm 2 Oracle- based Walk SAT algorithm Input: \(\phi ,O\) Output: \(x\) 1: \(x\sim O\) 2: while \(\exists c\in \phi\) : \(c(x) = 1\) do 3: pick a violated clause \(c\in \phi\) at random 4: pick variable \(v\) from \(c\) with probability \(\frac{P_{O}(\neg x_{v})}{\sum_{v^{\prime}\in c}P_{O}(\neg x_{v^{\prime}})}\) 5: \(x_{v}\gets \neg x_{v}\) 6: end while Algorithm 2 are inspired by the Prob SAT algorithm in Ref. [23] as they put more weight on choosing a variable \(v\) , the higher the likelihood under \(P_{O}\) of sampling a state with a flipped value assignment. Here and in the remainder, for any set of variables \(V\) and any \(x_{V}\in \{0,1\}^{|V|}\) , we write \[P_{O}(x_{V}) = \sum_{x\in \{0,1\}^{n}:x|_{V} = x_{V}}P_{O}(x). \quad (3)\] ### 2.3 Performance guarantees for the MT algorithm It is clear that an oracle can be more or less suited to an instance. For example, for a given instance \(\phi\) , a "perfect" oracle, one that samples an element from \(\Pi (\phi)\) with unit probability, leads to convergence of both of the above algorithms in a single step. Of course, perfect oracles are hard to come by, since they imply having solved the problem that we address. Fortunately, over the past 15 years, researchers from theoretical computer science have been able to prove that oracle- based SLS with access to a "sufficiently good" oracle are guaranteed to find a solution efficiently. To formulate these results, we need to introduce some additional concepts and notation. Let \(O\) be an oracle, \(\phi\) be a given Boolean formula in CNF with \(n\) variables and \(m\) clauses and let \[P_{O}(J|\phi) = \sum_{x\in X_{n}:c_{j}(x) = 1} P_{O}(x) \quad (4)\] be the probability that a sample \(x\sim O\) violates the clauses \(J\subseteq [m]\) . We define the dependency graph \(\mathcal{D}_{O}(\phi)\) induced by this distribution as the graph that has \(m\) nodes and with two nodes \(j,j^{\prime}\) being connected by an edge whenever they are not statistically independent, i.e. \[P_{O}(\{j,j^{\prime}\} |\phi)\neq P_{O}(j|\phi)\cdot P_{O}(j^{\prime}|\phi). \quad (5)\] Given this graph, let \(\Gamma (j)\) denote the exclusive neighbourhood of node \(j\) in this graph and \(\Gamma^{+}(j) = \Gamma (j)\cup \{j\}\) as the inclusive neighbourhood. We then have the following proposition, which is implied by the main result of Ref. [13]: Proposition 2.1. Given a formula \(\phi\) in CNF- form, if there exists a map \(\mu :[m]\to [0,\infty)\) such that, for all \(j\in [m]\) \[P_{O}(j|\phi)\cdot \Pi_{j^{\prime}\in \Gamma^{+}(j)}(1 + \mu (j^{\prime}))\leq \mu (j), \quad (6)\] then \(\phi\) is satisfiable and the MT algorithm with access to \(O\) will find a solution to \(\phi\) in an expected \(\sum_{j}\mu (j)\) number of steps. The "perfect" oracle clearly satisfies this proposition, as \(P_{O}(j|\phi) = 0\) for all \(j\) , so that we can simply set \(\mu (j) = 0\) for all \(j\) . Importantly, however, there can be imperfect oracles that still satisfy this condition. For instance, consider a

\(k\) - SAT instance \(\phi\) in which each clause carries at most \(k\) literals and each variable appears in at most \(\frac{2^{k}}{(k + 1)\epsilon}\) clauses. Then, by setting \(\mu (j) = e / 2^{k}\) for all \(j\) and using the fact that \((1 + 1 / r)^{r}\leq e\) for all \(r > 0\) , we find that the uniform oracle over \(I_{n}\) satisfies (6) and hence the MT algorithm would find a solution to \(\phi\) despite it being not tuned to \(\phi\) at all. As such, Proposition 2.1 strongly motivates the search for oracles that satisfy (6). Moreover, more recent results further motivate the construction of oracles that satisfy (6) only approximately or for instances that are in fact unsatisfiable. In particular, results in Ref. [14] imply the following: Proposition 2.2. Given a formula \(\phi\) in CNF- form, and a map \(\mu :[m]\to [0,\infty)\) , there exists a simple extension to the MT algorithm that takes as input \(\phi\) , \(\mu\) and an oracle \(O\) , that runs an expected \(\sum_{j}\mu (j)\) number of steps and whose output \(x\) , for every \(j\in [m]\) , violates clause \(c_{j}\) with probability \[\mathrm{Prob}(c_{j}(x) = 1)\leq \max (0,\epsilon_{O,\mu}(j)), \quad (7)\] where \(\epsilon_{O,\mu}(j) = P_{O}(j|\phi)\cdot \Pi_{j^{\prime}\in \Gamma^{+}(j)}(1 + \mu (j^{\prime})) - \mu (j)\) . This proposition implies that it is in our interest to construct oracles that minimize the set of \(\epsilon_{O,\mu}(j)\) , even if we're not able to satisfy condition (6) or work on satisfiable instances, such as in the case of the Max SAT optimization problem, which is concerned with solvers that return elements in \(\Pi (\phi)\) also for unsatisfiable instances. ### 2.4 Learning oracle factories with the LLL loss So far we've been concerned with single instances and oracles for them. Of course, in practice we're often confronted instead with a whole class of instances that is distributed with respect to some measure \(\mathcal{P}\) over the space of possible instances \(\Phi\) . What we're then really interested in is a solver that performs well when fed with samples from \(\mathcal{P}\) . As such, when using oracle- based SLS solvers, we're not primarily interested in single oracles but instead oracle factories, which we define as functions \(F\) from \(\Phi\) to the space of random variables, such that, for any \(\phi\) , \(F(\phi)\) is an oracle for \(\phi\) . Given a practical SAT application, here represented abstractly by \(\mathcal{P}\) , and an SLS solver \(S\) , we can then formulate our main goal as finding \[\arg \max_{F}\mathrm{Prob}_{\phi \sim \mathcal{P},x\sim S(\phi ,F(\phi))}(x\in \Pi (\phi)), \quad (8)\] that is, that oracle factory that maximises the probability that \(S\) returns a solution. We take a variational approach to this task and optimize a parametrized oracle factory \(F_{\theta}\) , where \(\theta\) are some set of parameters in a parameter space \(\Theta\) . In particular, we introduce a novel loss function that is inspired by the propositions above: For a given value of \(\theta\) , introduce an additional parametrized functional \(\mu_{\theta}[\phi ]\) that maps an instance \(\phi\) to a function from \([m]\) to the reals. We then define the Lovász Local (LLL) loss as \[L_{LLL,z}(\theta ,\phi) = \| (\max (0,\epsilon_{F_{\theta}(\phi),\mu_{\theta}[\phi ]}(j)))_{j\in [m]}\|_{z}, \quad (9)\] where \(\| \cdot \|_{z}\) indicates the usual \(p\) - norm. This loss is motivated by the propositions above, which imply that a smaller loss leads to better performance of (variants of) the MT algorithm.2 The second loss term, that we here use mostly to avoid the model's overfitting on solution instances, is the cross entropy between the thermal distribution \[\gamma_{\beta ,\phi}(x) = \frac{e^{-\beta\phi(x)}}{Z_{\beta}(\phi)},\quad Z_{\beta}(\phi) = \sum_{x\in I_{n}}e^{-\beta \phi (x)} \quad (10)\] with respect to some inverse temperature \(\beta >0\) and the output distribution, i.e. \[L_{Gibbs}(\theta ,\phi) = -\mathbb{E}_{\gamma_{\beta ,\phi}}\log (P_{F_{\theta}(\phi)}) \quad (11)\] The total loss is then a linear combination of these two loss functions, \[L(\theta ,\phi) = \gamma_{1}L_{Gibbs}(\theta ,\phi) + \gamma_{2}L_{LLL,z}(\theta ,\phi), \quad (12)\] where \(\gamma_{1},\gamma_{2}\geq 0\) are hyperparameters and the effective goal becomes to find \[\arg \min_{\theta \in \Theta}\mathbb{E}_{\phi \sim \mathcal{P}}L(\theta ,\phi). \quad (13)\] In section 1 of the technical appendix, we provide details on how we evaluate this loss in practice.

# Algorithm 3 A single IN layer 1: for \(k\in \{1\dots |E|\}\) do 2: \(\mathbf{e}_{k}^{\prime}\leftarrow \eta^{E}\left(\mathbf{e}_{k},\mathbf{n}_{r_{k}},\mathbf{n}_{s_{k}}\right)\) 3: end for 4: for \(i\in \{1\dots |N|\}\) do 5: let \(E_{i}^{\prime} = \left\{(\mathbf{e}_{k}^{\prime},r_{k},s_{k})\right\}_{r_{k} = i, k = 1:|E|}\) 6: \(\overline{{\mathbf{e}}}_{i}^{\prime}\leftarrow \rho^{E}\left(E_{i}^{\prime}\right)\) 7: \(\mathbf{n}_{i}^{\prime}\leftarrow \eta^{N}\left(\overline{{\mathbf{e}}}_{i}^{\prime},\mathbf{n}_{i},\right)\) 8: end for 9: let \(N^{\prime} = \{\mathbf{n}^{\prime}\}_{i = 1:|N|}\) 10: let \(E^{\prime} = \left\{(\mathbf{e}_{k}^{\prime},r_{k},s_{k})\right\}_{k = 1:|E|}\) 11: return \((E^{\prime},N^{\prime})\) ### 2.5 Graph Neural Networks as oracle factories In principle, many different ML models qualify as parametrized oracle factories. All that is required to train an ML model with our loss is for it to return both an oracle factory \(F_{\theta}\) as well as the parametrized functional \(\mu_{\theta}\) , in order to evaluate the LLL loss. Here we follow the common strategy to use a GNN as a deep learning model. In particular, we use an Interaction Network [16, 24]. Like all other GNNs, this type of network maps graphs to graphs. Consider a graph \(G = (N,E)\) with a set of nodes \(N = \{\mathbf{n}_{i}\}_{i = 1:|N|}\) and a set of edges \(E = \left\{(\mathbf{e}_{k},r_{k},s_{k})\right\}_{k = 1:|E|}\) , where \(\mathbf{n}_{i}\in \mathbb{R}^{d_{N}}\) are feature vectors of the nodes, for some \(d_{N}\in \mathbb{N}\) , \(\mathbf{e}_{k}\in \mathbb{R}^{d_{E}}\) are feature vectors of the edges, for some \(d_{E}\in \mathbb{N}\) , and \(r_{k},s_{k}\in [|N|]\) are indices of the receiver and source nodes for the \(k\) - th edge, respectively. A single layer of the Interaction Network takes \(G\) as input and updates both edge and node features based on a message passing algorithm whose pseudocode is shown as Algorithm 3. This algorithm requires the specification of a node update function \(\eta^{N}\) , an edge update function \(\eta^{E}\) as well as an edge aggregation function \(\rho^{E}\) . Note that, depending on the update and aggregation functions, the dimensions of the features in \(G^{\prime}\) might be different from those of \(G\) . Here, we use as update functions \(\eta^{N}\) and \(\eta^{V}\) simple neural networks of the form \[\eta = L N\circ R L\circ F L_{d_{u}}\circ R L\circ F L_{d_{u - 1}}\circ \ldots \circ F L_{d_{1}}, \quad (14)\] where \(L N\) is a layer normalisation layer [25], \(R L\) is a rectified linear unit layer [26] and \(F L_{d}\) is a fully connected layer from an input feature dimension to an output dimension \(d\) . Hence, specifying an update function in our case requires a list \((d_{t})_{t = 1}^{u}\) of layer dimensions, one for each of the \(u\) layers. As an edge aggregation function we simply use summation over the elements of \(E_{i}^{\prime}\) . A full application of an IN consists of the consecutive application of several single IN layers, followed by a fully connected layer \(F L_{1}\) to ensure that outgoing features across nodes and edges are one- dimensional. That is, for an IN consisting of \(l\) layers, the output of the IN is \[I N_{\theta}(G) = F L_{1}\circ I N_{\theta_{l}}\circ I N_{\theta_{l - 1}}\circ \ldots I N_{\theta_{1}}(G). \quad (15)\] Here, \(\theta_{r}\) denotes the parameter vector for the \(r\) - th layer, whose values specify the entries of the fully connected layers in its update functions. We have \(\theta = (\theta_{F L},\theta_{l},\theta_{l - 1},\ldots ,\theta_{1})\) , where \(\theta_{F L}\) are the parameters in the last layer. #### 2.5.1 Representing SAT instances as graphs Since, GNNs map graphs to graphs, in order to feed a SAT instance into a Graph network, we need to represent it as a graph. Fortunately, there exist various simple graph representations of Boolean formulas. We use the common LCG representation:3 Let \(\phi\) be a SAT formula in CNF form with \(n\) variables \((v_{i})_{i = 1}^{n}\) and \(m\) clauses \((c_{j})_{j = 1}^{m}\) . We introduce a directed tripartite graph \(G = (N,E)\) , consisting of one set \(N_{C}\) of \(m\) nodes, one per constraint, as well as two sets \(N_{V}^{+}\) and \(N_{V}^{- }\) of \(n\) nodes each, one per variable. The initial node embeddings \(\mathbf{n}\) are one- hot encodings for which of these three sets a given node belongs to. The edge set \(E\) further consists of two groups. The first group connects the \(i\) - th node in \(N_{V}^{+}\) with the \(i\) - th node in \(N_{V}^{- }\) , for \(i\in [n]\) . The second group connects, for each clause \(c_{j}\in \phi\) and every variable \(v_{i}\in V^{+}(c_{j})\) , the \(j\) - th node in \(V_{C}\) with the \(i\) - th node in \(N_{V}^{+}\) , and also for every variable \(v_{i}\in V^{- }(c_{j})\) , the \(j\) - th node in

<center>Figure 2: Example for the encoding of a single clause into LCG representation. </center> \(V_{C}\) with the \(i\) - th node in \(N_{V}^{- }\) . The initial edge embeddings e are, again, simply one- hot encodings of which of these two groups a given edge falls into. Figure 2 contains a graphical depiction of the LCG representation. #### 2.5.2 Defining the GNN's output Let \(I N_{\theta}(\phi)\) denote the output of the IN when feeding an instance \(\phi\) into it using the above LCG representation. Recall that we require an oracle \(F_{\theta}(\phi)\) as well as the functional \(\mu_{\theta}(\phi)\) as output from the GNN in order to evaluate the loss (12). Due to the final layer, the output features across all nodes are one- dimensional. Let \(\mathbf{n}^{C}\) denote the vector of outgoing node features for the \(m\) clause nodes, and similarly \(\mathbf{n}^{+}\) and \(\mathbf{n}^{- }\) for the \(n\) nodes in \(N_{V}^{+}\) and \(N_{V}^{- }\) respectively. We then define the oracle \(F_{\theta}(\phi)\) the random variable over \(\{0,1\}\) with measure \[P_{F_{\theta}(\phi)}(x) = \Pi_{i}\left(\widetilde{w}_{i}^{x_{i}}(1 - \widetilde{w}_{i})^{(1 - x_{i})}\right), \quad (16)\] where \[\widetilde{w}_{i} = \frac{\mathbf{n}_{i}^{+}}{\mathbf{n}_{i}^{+} + \mathbf{n}_{i}^{-}}. \quad (17)\] In other words, the output oracle generates an assignment \(x\) simply by sampling the value of the \(i\) - th variable as a Bernoulli trial with success probability \(\widetilde{w}_{i}\) , and does so independently for each variable. We further define the functional \(\mu_{\theta}[\phi ]\) via the mapping \[\mu_{\theta}[\phi ](j) = \frac{\widetilde{w}_{j}}{1 - \widetilde{w}_{j}}, \quad (18)\] where4 \[\widetilde{w}_{j} = \mathrm{sigmoid}(\mathbf{n}_{j}^{C}). \quad (19)\] ## 3 Experiments We train the above model on a training set of random 3- SAT instances, in which each clause holds at most 3 literals, of varying difficulty. Here, our measure of difficulty is the ratio \(\alpha = m / n\) . Random instances with lower \(\alpha\) are more likely to be satisfiable and also, in general, easier to decide. We evaluate the resulting models on a test set of random 3- SAT instances, again of varying difficulty. These experiments are motivated by the fact that numerical results have shown that the original MT- algorithm experiences a barrier at around \(\alpha = 2.45\) . For lower values it tends to find solutions quickly, while taking exponential time for higher values [28], see Fig. 3. A similar statement seems to hold true for the Walk SAT algorithm around \(\alpha = 2.8\) . We were interested in the ability of the GNN- boosted variants of these algorithms to solve instances in the "hard" regime.

<center>Figure 3: Simple illustration of the “hardness regimes” for 3-SAT: For \(\alpha < 0.55\) the uniform MT algorithm is guaranteed to solve instances efficiently (green), while in practice, the uniform algorithm is found to solve instances efficiently up until around 2.5 to 2.7, but not guaranteed to do so (yellow) [28]. Our experiments indicate that the oracle-boosted MT algorithm solves instances until around 3 to 3.2 efficiently (blue). Finally, it is an open question whether this cutoff can be increased to 4.27 using better oracle factories (orange), at which point a phase transition occurs and instances are typically unsatisfiable (red) [29]. </center> <table><tr><td>Variant</td><td>#</td><td>M1/2(#)</td><td>%</td><td>α</td></tr><tr><td>Uniform MT</td><td>36e3</td><td>1163</td><td>69.1</td><td>1.50</td></tr><tr><td>Hybrid MT</td><td>36e3</td><td>801</td><td>68.6</td><td>1.49</td></tr><tr><td>Boosted MT</td><td>22e3</td><td>134</td><td>80.3</td><td>1.90</td></tr><tr><td>Uniform Walk SAT</td><td>23e3</td><td>462</td><td>81.2</td><td>1.96</td></tr><tr><td>Hybrid Walk SAT</td><td>23e3</td><td>161</td><td>82.2</td><td>1.96</td></tr><tr><td>Boosted Walk SAT</td><td>16e3</td><td>93</td><td>87.0</td><td>2.12</td></tr></table> Table 1: Results from our experiments. The best performing algorithm variant is highlighted. \(\#\) is the average number of steps before finding a solution, across all instances and runs and \(M_{1 / 2}(\#)\) is the outer median (across instances) over the inner median (across runs) of steps before finding a solution. Furthermore, we report the total fraction \(\%\) of solved instances and the average value \(\bar{\alpha}\) of \(\alpha\) for which an algorithm was able to find a solution. We trained our model on a dataset of 396 satisfiable instances equally distributed between \(n \in [100, 200, 300]\) and \(1 \leq \alpha \leq 4.82\) . We generated these instances with the python library CNFgen (see Ref. [30]) and post- selected on satisfiable instances. For generating the solutions, we used the Glucose3 solver [31] implemented in the Pysat library in Ref. [32]. As it turns out, this number of training instances is already enough for our model to learn the important characteristics of these SAT- instances. For the Interaction network, we used a number of \(l = 5\) message passing steps and 2 layers of dimension \(d = 200\) for every fully connected layer, in every message passing step. As hyperparameters for the loss, we used \(z = 2\) , \(\beta = 1\) , \(\gamma_{1} = 1\) , \(\gamma_{2} = 1\) . For the optimization, we used the ADAM- optimizer with a learning rate of \(10^{- 3}\) , a batchsize of 1, and trained for 50 epochs. While experimenting with hyperparameters, we have learned that the model leads to the best results when using a batchsize of 1. We have not experimented too much with changing the learning rate. However, we have included in our code the possibility to use a dynamic learning rate with an exponential decay since we have seen that this was beneficial for overfitting highly structured small scale problems. We are convinced that by tuning the hyperparameters one can further boost the performance of the pre- trained model. It seems to be the case that the learning rate has to be adapted when using another dataset. To evaluate the resulting model, we ran the oracle- based Walk SAT and MT algorithms on an evaluation set of 2052 instances, also containing an equal number of instances across \(n \in [100, 200, 300]\) and \(1 \leq \alpha \leq 4.82\) . We ran each algorithm for up to \(10^{6}\) steps and a total of 5 runs per test instance. Following Refs. [10, 33], we evaluated each algorithm using three metrics: i) the average number of steps \(\#\) before finding a solution, across all instances and runs, ii) the outer median (across instances) over the inner median (across runs) of steps before finding a solution, \(M_{1 / 2}(\#)\) , iii) the total fraction \(\%\) of solved instances, where we considered an instance solved if any of the runs had returned by the time of the cutoff. As an additional metric, we measure the average value \(\bar{\alpha}\) of \(\alpha\) for which an algorithm was able to find a solution. We implemented our experiments in Python, using the JAX- framework [34] and its graph extension JRAPH [35] for the GNN and optimization. We implemented the oracle- based SLS algorithms in Rust. We ran the experiments on an AWS g4dn.4xlarge instance, using a Deep Learning AMI for Ubuntu 20.04 (ami- 094950f08c57b4f62). All code and datasets used are made available as part of this publication in a Git Hub repository.

## 4 Results We compare three different variants of both for the MT- algorithm and for Walk SAT, namely - the original version that uses the uniform oracle,- a "hybrid" version that uses the trained oracle just for initialization and then switches to uniform updating, and- the full oracle-based algorithm that uses the oracle for both initialization and updating. In Table 1, we provide a summary of the results. We find that for both solvers, the full oracle- based algorithm outperforms both others across the board. Moreover, the Walk SAT algorithm dominates the MT algorithm on each of the variants. A particularly dramatic improvement is seen in terms of the median steps required, where the boosted MT algorithm is faster by a factor of more than 8, while the boosted Walk SAT provides a speed- up by a factor of around 5. This indicates that the boosted solvers are especially powerful when solving relatively simple instances. A less dramatic but still significant improvement is seen in the mean number of steps, where we see an average decrease of \(35\%\) across solvers. <center>Figure 4: Benchmark of three different variants of the oracle-based MT algorithm, namely the uniform version, the "hybrid" version, and the "full" version. Left: Number of steps needed on average for instances of the evaluation data set for given \(\alpha\) . Center: Number of clauses violated on average as a function of the number of steps the algorithm has taken. Right: Ratio of solved instances as a function of the number of steps the algorithm has taken. </center> <center>Figure 5: Benchmark of three different variants of the oracle-based Walk SAT, using the same plots as in Fig. 4 </center>