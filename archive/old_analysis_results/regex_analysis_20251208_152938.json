{
  "results": [
    {
      "doc_id": "5296fb85e84d6890d37f97710f30f93a",
      "title": "Large-Scale Self-Supervised Learning with TensorFlow",
      "overall_score": 0.185,
      "recommendation": "Exclude",
      "summary": "Large-Scale Self-Supervised Learning with TensorFlow - Exclude (Score: 0.18)",
      "evaluations": {
        "pytorch": {
          "answer": "No",
          "confidence": 0.8,
          "evidence": "Found negative: tensorflow, tensorflow"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (7) than negative (5)"
        },
        "small_dataset": {
          "answer": "No",
          "confidence": 0.9,
          "evidence": "Found negative: imagenet, million images, large-scale"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "No",
          "confidence": 0.8,
          "evidence": "Found negative: will be released, will be released"
        }
      }
    },
    {
      "doc_id": "2978d81eacedcc22568390d974b63d9b",
      "title": "Deep Learning for Image Classification with PyTorch",
      "overall_score": 0.875,
      "recommendation": "Include",
      "summary": "Deep Learning for Image Classification with PyTorch - Include (Score: 0.88)",
      "evaluations": {
        "pytorch": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: pytorch, pytorch"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: supervised, supervised, classification"
        },
        "small_dataset": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: cifar, cifar"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: single nvidia v100 gpu, single nvidia v100 gpu"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.8,
          "evidence": "Found: github.com, github.com"
        }
      }
    },
    {
      "doc_id": "c5c51bb0ce0b29139bc147b16406a611",
      "title": "Efficient Neural Networks for MNIST Classification",
      "overall_score": 0.8799999999999999,
      "recommendation": "Include",
      "summary": "Efficient Neural Networks for MNIST Classification - Include (Score: 0.88)",
      "evaluations": {
        "pytorch": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: pytorch, pytorch, torch."
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: supervised, labeled training data, cross-entropy"
        },
        "small_dataset": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: mnist, mnist"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.7,
          "evidence": "Found: single gtx 1080 gpu"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: github.com, github.com, repository"
        }
      }
    },
    {
      "doc_id": "db18ac8883db1a9144cb82763df8b8b0",
      "title": "Efficient CNN Architecture for CIFAR-10 Classification",
      "overall_score": 0.89,
      "recommendation": "Include",
      "summary": "Efficient CNN Architecture for CIFAR-10 Classification - Include (Score: 0.89)",
      "evaluations": {
        "pytorch": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: pytorch, pytorch"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: supervised, supervised, cross-entropy"
        },
        "small_dataset": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: cifar, cifar, small dataset"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.8,
          "evidence": "Found: single gpu, single nvidia rtx 3080 gpu"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: github.com, github.com, code is publicly available"
        }
      }
    },
    {
      "doc_id": "c315cbf5fc60285b9e90d40ff7090b5a",
      "title": "Graph Neural Networks for Propositional Model Coun",
      "overall_score": 0.405,
      "recommendation": "Exclude",
      "summary": "Graph Neural Networks for Propositional Model Coun - Exclude (Score: 0.41)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: fast to generate (differently from other distributions) and the fine-tuning phase requires only a few tens of distribution-specific labeled samples. these results are also relevant from the time-efficiency perspective, as only retraining, fast to obtain, and then eventually fine- tuned towards the specific test distribution. the architectural improvements guarantee a sensibly better performance in terms of scalability and generalizability than the bpnn model, as shown in section b of the supplementary material. we believe that the attentional layer allows the network to focus on regions of the input formulae which are more significant for the #sat problem. ## 6 conclusions we presented bpgat, an extension of the bpnn architecture presented in [15], which combines the algorithmic structure of belief propagation and the learning paradigm of graph attention networks. we conducted several experiments to investigate the scalability and generalization abilities of our network, showing that it is able to achieve a performance comparable to (and in some settings higher than) state- of- the- art approximate #sat solvers, albeit the lack of any theoretical guarantees on the quality of the solution. finally, we highlighted the efficiency of our model, both in terms of required training"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "818d75376cda814e0f95d7b14883ea8d",
      "title": "STRCMP Integrating Graph Structural Priors with La",
      "overall_score": 0.2975,
      "recommendation": "Exclude",
      "summary": "STRCMP Integrating Graph Structural Priors with La - Exclude (Score: 0.30)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "No",
          "confidence": 0.6,
          "evidence": "More negative (7) than positive (6)"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.7,
          "evidence": "Found: faster than existing evolutionary- based algorithm discovery frameworks. furthermore, the framework <center>figure 4: convergence comparison (w.r.t. par-2) between evolutionary-based algorithm discovery frameworks on zankeller dataset of sat domain. </center> attains markedly higher- quality convergence points compared to baseline methods auto sat and llm4solver. notably, strcmp achieves stable convergence while auto sat exhibits persistent oscillations even after convergence. these findings validate that our proposed strcmp substantially reduces computational overhead in evolutionary- based algorithm discovery frameworks. ablation studies (answer to rq3). to address rq3, we conduct comprehensive ablation studies on our composite model by systematically deactivating individual components and evaluating their impact on optimization performance and computational efficiency. representative results are shown in figure 5, with full ablation studies detailed in appendix g.3. analysis of figure 5 reveals that the structural prior provides measurable benefits for code generation in combinatorial optimization tasks: the strcmp w/o gnn variant (lacking structural guidance) exhibits consistently inferior optimization performance compared to counterparts incorporating the prior. furthermore, this variant demonstrates increased solution variability during algorithmic search iterations, potentially resulting in higher computational costs. counterintuitively, the full strcmp model (with complete post- training) does not uniformly outperform its ablations strcmp (sft only) and strcmp (dpo only) across all benchmarks, suggesting underlying conflicts within the post- training"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "7c9d87a54a294365ad41a782a0200f78",
      "title": "Addressing Variable Dependency in GNN based SAT So",
      "overall_score": 0.6325000000000001,
      "recommendation": "Review",
      "summary": "Addressing Variable Dependency in GNN based SAT So - Review (Score: 0.63)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (10) than negative (3)"
        },
        "small_dataset": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (4) than negative (2)"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.8,
          "evidence": "Found: faster than neuro sat, although their parameter counts are roughly in the same scale (around 200k). actually, the authors of neuro sat reported that it would take \\(10^{7}\\) sat problems to train a full- fledged neuro sat [selsam, 2018]. although our training set is much smaller due to limitations of computing resources, we didn't observe the overfitting problem in the experiments judging from the training loss. comparison on the \\(sr(n)\\) problems. we use \\(8k\\) \\(sr(n)\\) problems sampled uniformly from \\(sr(u(3,8))\\) to train the three models. the test set contains \\(1.5k\\) \\(sr(n)\\) problems \\((n\\) is from 3 to 10). for asym sat and dg- dagrnn, cnf formulas are first converted into circuits to serve as the model input. table 2 summarizes the performance measured on the \\(sr(n)\\) problem. the result shows that asym sat model has a better performance compared to neuro sat and dg- dagrnn on this dataset. overall, asym sat can reach more than \\(90\\%\\) solution rate (averaged across \\(sr(3)\\) to \\(sr(10)\\) ), while neuro sat can only reach \\(60\\%\\) . furthermore, we supply more experimental results regarding larger \\(sr(n)\\) problems. when trained from \\(sr(3)\\) to \\(sr(10)\\) , asym sat outperforms neuro sat on \\(sr(20)\\) , \\(sr(40)\\) , \\(sr(60)\\) and \\(sr(80)\\) . the result is shown in table 6. in our experiment, the performance of dg- dagrnn is non- competitive to the other two. we conjecture that the unsupervised learning method in dg- dagrnn suffers from the vanishing gradient problem if trained on circuits converted from cnf. we provide a detailed analysis of dg- dagrnn in the appendix. comparison on the \\(v(n)\\) problems. the training, faster than neuro sat, although their parameter counts are roughly in the same scale (around 200k). actually, the authors of neuro sat reported that it would take \\(10^{7}\\) sat problems to train a full- fledged neuro sat [selsam, 2018]. although our training set is much smaller due to limitations of computing resources, we didn't observe the overfitting problem in the experiments judging from the training loss. comparison on the \\(sr(n)\\) problems. we use \\(8k\\) \\(sr(n)\\) problems sampled uniformly from \\(sr(u(3,8))\\) to train the three models. the test set contains \\(1.5k\\) \\(sr(n)\\) problems \\((n\\) is from 3 to 10). for asym sat and dg- dagrnn, cnf formulas are first converted into circuits to serve as the model input. table 2 summarizes the performance measured on the \\(sr(n)\\) problem. the result shows that asym sat model has a better performance compared to neuro sat and dg- dagrnn on this dataset. overall, asym sat can reach more than \\(90\\%\\) solution rate (averaged across \\(sr(3)\\) to \\(sr(10)\\) ), while neuro sat can only reach \\(60\\%\\) . furthermore, we supply more experimental results regarding larger \\(sr(n)\\) problems. when trained from \\(sr(3)\\) to \\(sr(10)\\) , asym sat outperforms neuro sat on \\(sr(20)\\) , \\(sr(40)\\) , \\(sr(60)\\) and \\(sr(80)\\) . the result is shown in table 6. in our experiment, the performance of dg- dagrnn is non- competitive to the other two. we conjecture that the unsupervised learning method in dg- dagrnn suffers from the vanishing gradient problem if trained on circuits converted from cnf. we provide a detailed analysis of dg- dagrnn in the appendix. comparison on the \\(v(n)\\) problems. the training"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: github.com, code for the original dg- dagrnn model is not publicly available, code for the original dg- dagrnn model is not publicly available"
        }
      }
    },
    {
      "doc_id": "83ab2bd7b585c97101b99d26fafebf19",
      "title": "Neural heuristics for SAT solving",
      "overall_score": 0.3125,
      "recommendation": "Exclude",
      "summary": "Neural heuristics for SAT solving - Exclude (Score: 0.31)",
      "evaluations": {
        "pytorch": {
          "answer": "No",
          "confidence": 0.7,
          "evidence": "Found negative: tensorflow"
        },
        "supervised": {
          "answer": "No",
          "confidence": 0.6,
          "evidence": "More negative (7) than positive (6)"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: github.com, github.com, code including hyperparameters is published at https://bit.ly/neurheur. our code is based on tensor flow [aab+15]. it uses a cdcl implementation by [zho18]. we access mini sat through py sat interface [imm18]. <center>figure 5: comparison of policy error with and without attention. the presented values are mean and standard deviation over 3-5 trained models calculated on the evaluation set. </center> 5 conclusions and future workin this work we have shown three experiments confirming that sat- solving can be augmented by neural networks. the message- passing architecture augmented by attention performs competitively comparing with standard heuristics when evaluated on relatively large propositional problems, including problems with more than a hundred variables (see section 4). from the ablation presented in experiment 3 follows that the message- passing architecture that uses the attention mechanism overall performs better than the same architecture without attention and we attribute it to a selective acceptance of incoming messages made possible by the attention mechanism. we believe that using an appropriately large computing infrastructure the learning process can be extended to more complex examples and that in the near future parallelization combined with a variant of the message- passing architecture can be used to train models which will tackle larger sr problems, and possibly sat problem classes currently beyond the reach of sat- solvers. as a future step we consider extending our improved heuristics so that a neural network would be able to control other aspects of the sat solver behavior, like restarting and backtracking. eventually, other prediction targets, including expected number of steps, may be beneficial. once we exhaust the pool of available"
        }
      }
    },
    {
      "doc_id": "a0ac4c54c1df325b9494211e467bfc4d",
      "title": "Learning Branching Heuristics from Graph Neural Ne",
      "overall_score": 0.26249999999999996,
      "recommendation": "Exclude",
      "summary": "Learning Branching Heuristics from Graph Neural Ne - Exclude (Score: 0.26)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "No",
          "confidence": 0.6,
          "evidence": "More negative (4) than positive (2)"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "da14976f78933e5c4c39d53db9705621",
      "title": "One Model Any CSP Graph Neural Networks as Fast Gl",
      "overall_score": 0.425,
      "recommendation": "Exclude",
      "summary": "One Model Any CSP Graph Neural Networks as Fast Gl - Exclude (Score: 0.42)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "No",
          "confidence": 0.9,
          "evidence": "Found negative: reinforcement, reinforcement"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (3) than negative (1)"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: github.com, github.com"
        }
      }
    },
    {
      "doc_id": "71cb5cd4a4e3658c9e73df930df6a0e1",
      "title": "Neural Approaches to SAT Solving Design Choices an",
      "overall_score": 0.35,
      "recommendation": "Exclude",
      "summary": "Neural Approaches to SAT Solving Design Choices an - Exclude (Score: 0.35)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "960a3cc47a619e0ae16407fb45a58435",
      "title": "A unified pre training and adaptation framework fo",
      "overall_score": 0.5425,
      "recommendation": "Review",
      "summary": "A unified pre training and adaptation framework fo - Review (Score: 0.54)",
      "evaluations": {
        "pytorch": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: torch., torch."
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (74) than negative (28)"
        },
        "small_dataset": {
          "answer": "No",
          "confidence": 0.6,
          "evidence": "More negative (9) than positive (5)"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.8,
          "evidence": "Found: faster and achieve better generalization performance. concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on mlp and \\(\\mathrm{bip - gnn}(\\cdot)\\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \\(\\mathrm{dis}(\\cdot)\\) is to classify the domain labels for each sample. the source domain and target domain share the same feature extraction backbone but have different classification networks. the domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training, faster and achieve better generalization performance. concretely, the fine- tuning network contains three parts: the pre- trained feature extraction backbones based on mlp and \\(\\mathrm{bip - gnn}(\\cdot)\\) are used to extract the features from both source and target domains, the prediction or classification layers are used to predict the labels of the source and target domains, and the discriminator network \\(\\mathrm{dis}(\\cdot)\\) is to classify the domain labels for each sample. the source domain and target domain share the same feature extraction backbone but have different classification networks. the domain adaptation framework in our work follows the supervised setting where data from both source and target domains contains labels and can be used for training"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "c0d56776510218968a38f9f4a9d4e0e9",
      "title": "Attn JGNN Attention Enhanced Join Graph Neural Net",
      "overall_score": 0.35,
      "recommendation": "Exclude",
      "summary": "Attn JGNN Attention Enhanced Join Graph Neural Net - Exclude (Score: 0.35)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "fc136d6364c6e0fa586846bd1bdc6408",
      "title": "G4SATBench Benchmarking and Advancing SAT Solving",
      "overall_score": 0.5874999999999999,
      "recommendation": "Review",
      "summary": "G4SATBench Benchmarking and Advancing SAT Solving - Review (Score: 0.59)",
      "evaluations": {
        "pytorch": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: pytorch, pytorch"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (66) than negative (22)"
        },
        "small_dataset": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (12) than negative (4)"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "46b65358d382a78f15182fe583348ec2",
      "title": "Circuit Aware SAT Solving Guiding CDCL via Conditi",
      "overall_score": 0.2975,
      "recommendation": "Exclude",
      "summary": "Circuit Aware SAT Solving Guiding CDCL via Conditi - Exclude (Score: 0.30)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "small_dataset": {
          "answer": "No",
          "confidence": 0.6,
          "evidence": "More negative (6) than positive (3)"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "9dfe8a7b63cb43b47cb5e188d19e214f",
      "title": "HyperSAT Unsupervised Hypergraph Neural Networks f",
      "overall_score": 0.31499999999999995,
      "recommendation": "Exclude",
      "summary": "HyperSAT Unsupervised Hypergraph Neural Networks f - Exclude (Score: 0.31)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "No",
          "confidence": 0.9,
          "evidence": "Found negative: multiple satisfying assignments. on the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the weighted max sat instance. from this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances. ### 3.3. probabilistic mapping the neural network module takes the hypergraph as input and produces a probability vector \\(\\mathbf{y}\\) as output, which indicates the probability of each variable taking the truth value. accordingly, to convert the probability vector \\(\\mathbf{y}\\) into a boolean assignment, we transform \\(\\mathbf{y}\\) into \\(n\\) bernoulli distributions, where the \\(i\\) - th distribution \\(b(y_{i})\\) is over the discrete values \\(\\{0,1\\}\\) . finally, the node assignments are generated by sampling with the corresponding probability distributions. ## 4. experiment ### 4.1. experimental settings we compare the performance of hyper sat against gnn- based methods for solving weighted max sat problems. baseline algorithms. we compare our proposed hyper sat against gnn- based methods. the following algorithms are considered as baselines: (i) hyp op (heydaribeni et al., 2024): an advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs; (ii) (liu et al., 2023): an innovative supervised gnn- based approach that predicts solutions in an end- to- end manner by transforming cnf formulas into factor graphs. we apply these baselines to solve weighted max sat problems. datasets. we evaluate the algorithms using random 3- sat benchmarks from the satlib dataset (hoos & st\u00fctzle, 2000). the satlib dataset is one of the standard datasets for evaluating sat solvers. we utilize a range of datasets with varying distributions, from which sat and unsat instances are generated for each distribution. the number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. more details on the datasets are provided in table 1. in particular, to generate the required weighted max sat instances, we assign weights to the clauses in each cnf file within the dataset. these weights are sampled from the integers in the range \\([1,10]\\) uniformly at random. table 1. the parameters of the datasets. <table><tr><td>dataset</td><td>instance</td><td>vars</td><td>clauses</td><td>types</td></tr><tr><td>uf100-430</td><td>1000</td><td>100</td><td>430</td><td>sat</td></tr><tr><td>uf100-430</td><td>1000</td><td>100</td><td>430</td><td>unsat</td></tr><tr><td>uf200-860</td><td>100</td><td>200</td><td>860</td><td>sat</td></tr><tr><td>uf200-860</td><td>100</td><td>200</td><td>860</td><td>unsat</td></tr><tr><td>uf250-1065</td><td>100</td><td>250</td><td>1065</td><td>sat</td></tr><tr><td>uf250-1065</td><td>100</td><td>250</td><td>1065</td><td>unsat</td></tr></table> model settings. hyper gcn is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. the dimension of the cross- attention layer is the square root of the number of variables. the dropout used in the cross- attention layer is 0.1. the model is optimized using the adam optimizer (kingma, 2014) with a learning rate of \\(7 \\times 10^{- 2}\\) . the balancing hyperparameter of loss function \\(\\lambda\\) is \\(2 \\times 10^{- 3}\\) . the ffn consists of two linear transformations and a re lu activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. an early stopping strategy is employed, with a tolerance of \\(10^{- 4}\\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained. evaluation configuration. all experiments are conducted on an nvidia a100- sxm4- 40gb gpu, multiple satisfying assignments. on the other hand, the non- uniform distribution of weight across the clauses makes it challenging for supervised learning models to detect the hidden structural patterns within the weighted max sat instance. from this perspective, our proposed unsupervised learning approach offers an effective alternative approach that enhances generalization capabilities while eliminating the need for labeled instances. ### 3.3. probabilistic mapping the neural network module takes the hypergraph as input and produces a probability vector \\(\\mathbf{y}\\) as output, which indicates the probability of each variable taking the truth value. accordingly, to convert the probability vector \\(\\mathbf{y}\\) into a boolean assignment, we transform \\(\\mathbf{y}\\) into \\(n\\) bernoulli distributions, where the \\(i\\) - th distribution \\(b(y_{i})\\) is over the discrete values \\(\\{0,1\\}\\) . finally, the node assignments are generated by sampling with the corresponding probability distributions. ## 4. experiment ### 4.1. experimental settings we compare the performance of hyper sat against gnn- based methods for solving weighted max sat problems. baseline algorithms. we compare our proposed hyper sat against gnn- based methods. the following algorithms are considered as baselines: (i) hyp op (heydaribeni et al., 2024): an advanced unsupervised learning framework that solves constrained combinatorial optimization problems using hypergraphs; (ii) (liu et al., 2023): an innovative supervised gnn- based approach that predicts solutions in an end- to- end manner by transforming cnf formulas into factor graphs. we apply these baselines to solve weighted max sat problems. datasets. we evaluate the algorithms using random 3- sat benchmarks from the satlib dataset (hoos & st\u00fctzle, 2000). the satlib dataset is one of the standard datasets for evaluating sat solvers. we utilize a range of datasets with varying distributions, from which sat and unsat instances are generated for each distribution. the number of variables in the dataset ranges from 100 to 250, while the number of clauses varies between 430 and 1065. more details on the datasets are provided in table 1. in particular, to generate the required weighted max sat instances, we assign weights to the clauses in each cnf file within the dataset. these weights are sampled from the integers in the range \\([1,10]\\) uniformly at random. table 1. the parameters of the datasets. <table><tr><td>dataset</td><td>instance</td><td>vars</td><td>clauses</td><td>types</td></tr><tr><td>uf100-430</td><td>1000</td><td>100</td><td>430</td><td>sat</td></tr><tr><td>uf100-430</td><td>1000</td><td>100</td><td>430</td><td>unsat</td></tr><tr><td>uf200-860</td><td>100</td><td>200</td><td>860</td><td>sat</td></tr><tr><td>uf200-860</td><td>100</td><td>200</td><td>860</td><td>unsat</td></tr><tr><td>uf250-1065</td><td>100</td><td>250</td><td>1065</td><td>sat</td></tr><tr><td>uf250-1065</td><td>100</td><td>250</td><td>1065</td><td>unsat</td></tr></table> model settings. hyper gcn is a two- layer network, with the input dimension set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. the dimension of the cross- attention layer is the square root of the number of variables. the dropout used in the cross- attention layer is 0.1. the model is optimized using the adam optimizer (kingma, 2014) with a learning rate of \\(7 \\times 10^{- 2}\\) . the balancing hyperparameter of loss function \\(\\lambda\\) is \\(2 \\times 10^{- 3}\\) . the ffn consists of two linear transformations and a re lu activation function, with both the input and output dimensions set to the square root of the number of variables, and the hidden layer dimension set to half of the square root of the number of variables. an early stopping strategy is employed, with a tolerance of \\(10^{- 4}\\) and a patience of 50 iterations, terminating training if no improvement is observed over this period. finally, in the random sampling stage, we perform 5 independent samplings and return the best solution obtained. evaluation configuration. all experiments are conducted on an nvidia a100- sxm4- 40gb gpu, distributed"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "38a6100f8a1976a22dbac38a66b0421e",
      "title": "GraSS Combining Graph Neural Networks with Expert",
      "overall_score": 0.635,
      "recommendation": "Review",
      "summary": "GraSS Combining Graph Neural Networks with Expert - Review (Score: 0.64)",
      "evaluations": {
        "pytorch": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: pytorch, pytorch"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (32) than negative (4)"
        },
        "small_dataset": {
          "answer": "No",
          "confidence": 0.9,
          "evidence": "Found negative: large- scale, large number of optimization steps during logic synthesis, and at each one of the steps, it is necessary to verify that the circuits before and after optimization are functionally equivalent. this is done by verifying that the two circuits produce the same outputs for all possible inputs, which is equivalent to solving a sat problem [17, 21]. we collected logic equivalence checks from the optimization of 30 industrial circuits, yielding a total of 78,727 sat instances. a summary of dataset statistics is provided as table 1a. sat competition (sc). this is a subset of the anniversary track benchmark of the 2022 sat competition [2], which itself was created by collecting all instances from the main, crafted and application tracks of the previous sat competitions up to that year. we ran each instance of the anniversary benchmark through each of the seven solvers in the portfolio, and excluded those that could not be solved within 1,500 seconds by any solver, as well as those with more than 20,000 variables, yielding 2,088 sat instances. a summary of dataset statistics is provided as table 1b. ### 4.3 baselines we compare our approach with the following baselines. best base solver. the individual solver among the portfolio of seven that had the best performance on the training data, measured in average runtime over all instances. in practice, this was the bulky solver for both datasets. satzilla07 [46]. we adapt this landmark sat solving machine learning model, based on a linear ridge regression model trained to predict runtimes based on global handcrafted features that summarize sat instance characteristics. since our work focuses on sat solver selection, we omit the presolving process in the original satzilla pipeline. we also remove features in the original model that require probing. this leaves 33 global features (#1- 33 in the original article). the model is trained from the ridge class in the scikit- learn [36] library with default settings. we convert the approach into a sat solver selection model by selecting the solver with shortest predicted runtime. satzilla12 [45]. we also adapt the updated satzilla model from 2012, which was based on random forest classification. again, we remove the presolving process, and only use the features that do not require probing (features #1- 55 in the original article). we train a random forest model between each pair of solvers, weighting each training instance by the absolute difference in runtime between the two solvers, for \\(7(7 - 1) / 2 = 21\\) models in total. each model is trained from the random forest classifier class in scikit- learn with 99 trees and \\([log_2(55)] + 1 = 7\\) sampled features in each tree. at test- time, each model is used to vote which solver it prefers in its pair for solving an instance, and the final solver choice is made from the solver that has received the most votes. argo sm ar t [34]. this is an approach based on a \\(k\\) - nearest neighbors model trained for classification. we used the same 29 features as in the original paper, which form a subset of the 33 features used in our adaptation of satzilla07. we use the kneighbors classifier class in scikit- learn with \\(k = 9\\) neighbors. cnn [31]. we reimplement the approach of loreggia et al. [31], where the cnf formula is interpreted as text, converted to its ascii values and then to a grayscale"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (4) than negative (2)"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.8,
          "evidence": "Found: code instances, although with learned initial node embeddings, and uses a very deep lstm- gnn hybrid architecture with 26 layers and custom graph convolution operations. we implement the same, but replace the final layer which computes a scalar \"vote\" for every literal, and takes the average vote before a sigmoid activation, by an averaging of the literal embeddings, followed by a linear layer and a softmax activation. we also use 4 layers instead of 26 for tractability on our dataset, whose instances are substantially larger than those in the original paper. as can be seen in table 5, our approach improves over these alternatives in every metric. we next evaluate our choice of node features. we compare it against random normal values (random), as in selsam et al. [38]); a one- hot vector indicating whether the node represents a clause, positive or negative literal (node- type), as used in li et al. [29], yolcu and p\u00f3czos [48] and you et al. [49]; and laplacian positional encodings (laplacian pe), as introduced in dwivedi et al. [13]. we also compare against a variant of our approach consisting of the same table 5: exploration of alternative architectures on the lec benchmark. we report the average and standard deviation over 5 train-test folds. <table><tr><td></td><td>avg runtime (s) \u2193</td><td>solved (%) \u2191</td><td>acc \u2191</td></tr><tr><td>homogeneous</td><td>343.339\u00b10.987</td><td>77.4\u00b10.2</td><td>0.464\u00b10.009</td></tr><tr><td>neuro sat variant</td><td>383.132\u00b15.284</td><td>74.3\u00b11.0</td><td>0.423\u00b10.008</td></tr><tr><td>gra ss (ours)</td><td>341.549\u00b11.440</td><td>77.7\u00b10.2</td><td>0.480\u00b10.006</td></tr></table> table 6: exploration of alternative node features on the lec benchmark. we report the average and standard deviation over 5 train-test folds. <table><tr><td></td><td>avg runtime (s) \u2193</td><td>solved (%) \u2191</td><td>acc \u2191</td></tr><tr><td>random</td><td>352.258\u00b12.114</td><td>76.2\u00b10.5</td><td>0.444\u00b10.008</td></tr><tr><td>node-type</td><td>344.088\u00b11.603</td><td>77.1\u00b10.3</td><td>0.474\u00b10.002</td></tr><tr><td>laplacian pe</td><td>347.632\u00b11.274</td><td>76.9\u00b10.3</td><td>0.454\u00b10.003</td></tr><tr><td>custom</td><td>343.143\u00b11.621</td><td>77.3\u00b10.3</td><td>0.476\u00b10.003</td></tr><tr><td>custom + pe (ours)</td><td>341.549\u00b11.440</td><td>77.7\u00b10.2</td><td>0.480\u00b10.006</td></tr></table> hand- designed features, but without the clause positional embeddings (custom). as can be seen in table 6, our choices outperform these alternative approaches in every metric. ## 5 limitations although our experiments strongly establish the superiority of our approach in the presented scenario, several limitations can be noted. deep learning methods are well- known to be data hungry, and perform best in regimes where training sets are large. it is plausible that in scenarios where a limited number of timed instances are available, code instances, although with learned initial node embeddings, and uses a very deep lstm- gnn hybrid architecture with 26 layers and custom graph convolution operations. we implement the same, but replace the final layer which computes a scalar \"vote\" for every literal, and takes the average vote before a sigmoid activation, by an averaging of the literal embeddings, followed by a linear layer and a softmax activation. we also use 4 layers instead of 26 for tractability on our dataset, whose instances are substantially larger than those in the original paper. as can be seen in table 5, our approach improves over these alternatives in every metric. we next evaluate our choice of node features. we compare it against random normal values (random), as in selsam et al. [38]); a one- hot vector indicating whether the node represents a clause, positive or negative literal (node- type), as used in li et al. [29], yolcu and p\u00f3czos [48] and you et al. [49]; and laplacian positional encodings (laplacian pe), as introduced in dwivedi et al. [13]. we also compare against a variant of our approach consisting of the same table 5: exploration of alternative architectures on the lec benchmark. we report the average and standard deviation over 5 train-test folds. <table><tr><td></td><td>avg runtime (s) \u2193</td><td>solved (%) \u2191</td><td>acc \u2191</td></tr><tr><td>homogeneous</td><td>343.339\u00b10.987</td><td>77.4\u00b10.2</td><td>0.464\u00b10.009</td></tr><tr><td>neuro sat variant</td><td>383.132\u00b15.284</td><td>74.3\u00b11.0</td><td>0.423\u00b10.008</td></tr><tr><td>gra ss (ours)</td><td>341.549\u00b11.440</td><td>77.7\u00b10.2</td><td>0.480\u00b10.006</td></tr></table> table 6: exploration of alternative node features on the lec benchmark. we report the average and standard deviation over 5 train-test folds. <table><tr><td></td><td>avg runtime (s) \u2193</td><td>solved (%) \u2191</td><td>acc \u2191</td></tr><tr><td>random</td><td>352.258\u00b12.114</td><td>76.2\u00b10.5</td><td>0.444\u00b10.008</td></tr><tr><td>node-type</td><td>344.088\u00b11.603</td><td>77.1\u00b10.3</td><td>0.474\u00b10.002</td></tr><tr><td>laplacian pe</td><td>347.632\u00b11.274</td><td>76.9\u00b10.3</td><td>0.454\u00b10.003</td></tr><tr><td>custom</td><td>343.143\u00b11.621</td><td>77.3\u00b10.3</td><td>0.476\u00b10.003</td></tr><tr><td>custom + pe (ours)</td><td>341.549\u00b11.440</td><td>77.7\u00b10.2</td><td>0.480\u00b10.006</td></tr></table> hand- designed features, but without the clause positional embeddings (custom). as can be seen in table 6, our choices outperform these alternative approaches in every metric. ## 5 limitations although our experiments strongly establish the superiority of our approach in the presented scenario, several limitations can be noted. deep learning methods are well- known to be data hungry, and perform best in regimes where training sets are large. it is plausible that in scenarios where a limited number of timed instances are available"
        }
      }
    },
    {
      "doc_id": "ac3f9f5343377be2d869bdfaf7a87622",
      "title": "On the Hardness of Learning GNN based SAT Solvers",
      "overall_score": 0.7949999999999999,
      "recommendation": "Review",
      "summary": "On the Hardness of Learning GNN based SAT Solvers - Review (Score: 0.79)",
      "evaluations": {
        "pytorch": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: pytorch, pytorch"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: supervised, supervised, classification"
        },
        "small_dataset": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: small value \\(\\delta > 0\\) could be used in theorem 4 of topping et al. [47] such that \\(ric(i,j) \\leq - 2 + \\delta\\) , leading to an exponentially decaying jacobian of the node representations around \\(i \\sim j\\) . this result leads us to the conclusion that gnn- based solvers are limited by both these parameters and suffer from two distinct hardness types: the algorithmic hardness inherent to sat and the hardness of learning representations for long range communication. the interplay between \\(k\\) and \\(\\alpha\\) in theorem 3.1 provides additional insights. indeed, for problems with large values of \\(k\\) or large values of \\(\\alpha\\) , highly negatively curved edges are guaranteed to exist on average, and this quantity will concentrate. on the other hand, for large values of \\(\\alpha\\) and relatively small values of \\(k\\) , the latter becomes crucial in deciding how well a gnn- based solver will be able to learn, i.e., it should be easier to learn a solver for smaller values of \\(k\\) . we confirm this fact empirically in section 4. the intuition we provide for a more complete understanding of this crucial result is the following: at increasing connectivity, literals become very distant on the interaction network, i.e., the number of long- range codependencies increases. in this scenario, the gnn will not be able to learn a fixed length representation that can \"remember\" the information of reachable, but not directly adjacent nodes. this means that the ability to learn a solver is compromised by an oversquashing phenomenon. for large values of \\(k\\) , this problem becomes prevalent even before the hardness of exploring the solution space, due to the effect of \\(k\\) on the bfc. our theory motivates therefore how increasing values of \\(k\\) in random \\(k\\) - sat would lead to worse oversquashing and performance, even for what would be considered simple problems in terms of \\(\\alpha\\) . to visually understand the aforementioned concepts, we can again refer to figure 1. as the value of \\(k\\) grows, the gap between the flatter (yellow) and highly negatively curved problems (violet) gets smaller. the same holds for increasing values of \\(\\alpha\\) , as expected. we provide additional visual depictions of this aforementioned explanation in appendix a.5, where we plot two input graphs for random 3- sat at small (figure 5) and large (figure 6) \\(\\alpha\\) . in the following section, we will show that our results can be empirically confirmed for different gnn- based solvers. ## 4 experiments experimental setting. to validate our theory, we perform different experiments on various dataset, small plot in the bottom-left corner provides the model's solution probability curve in terms of \\(\\alpha\\) , where it is possible to clearly notice the algorithmic transition from satisfiable to unsatisfiable problems. (b) probability of finding a satisfying assignment of the same problems as in (a) with neuro sat as a function of the variance and average of the bfc. notice how as \\(\\alpha\\) grows, the average curvature not only gets more negative, but also concentrates. we can see from the empirical results in (a) that in this case, the model is unable to produce a satisfying assignment. as \\(\\alpha\\) becomes smaller, the input graphs have less negative edges on average, the associated variance naturally grows, and so does the solving probability. using the first two moments of the bfc, we are able to observe a similar transition-like phenomenon as the small plot in the bottom-left corner of (a). </center> results, we then propose two heuristics to understand how hard a given sat dataset will be to solve for a gnn- based solver. we will focus on the assignment scenario, as it includes the decision scenario as well. given that all the dataset"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.7,
          "evidence": "Found: github.com"
        }
      }
    },
    {
      "doc_id": "87ad0dd8785bdc058db3d41ee492de21",
      "title": "Can Graph Neural Networks Learn to Solve MaxSAT Pr",
      "overall_score": 0.31499999999999995,
      "recommendation": "Exclude",
      "summary": "Can Graph Neural Networks Learn to Solve MaxSAT Pr - Exclude (Score: 0.31)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "No",
          "confidence": 0.9,
          "evidence": "Found negative: distributed, distributed"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "08d3123d0d72b2e7090c85f5894a4d99",
      "title": "DeepGate2 Functionality Aware Circuit Representati",
      "overall_score": 0.41500000000000004,
      "recommendation": "Exclude",
      "summary": "DeepGate2 Functionality Aware Circuit Representati - Exclude (Score: 0.42)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (7) than negative (2)"
        },
        "small_dataset": {
          "answer": "No",
          "confidence": 0.8,
          "evidence": "Found negative: large- scale, large- scale"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: single nvidia v100 gpu, single nvidia v100 gpu, fast chip design,\" nature, vol. 594, no. 7862, pp. 207- 212, 2021. [3] w. l. neto, m. austin, s. temple, l. amaru, x. tang, and p.- e. gaillardon, \"losaic: a logic synthesis framework driven by artificial intelligence,\" in 2019 ieee/acm international conference on computer- aided design (iccad). ieee, 2019, pp. 1- 6. [4] w. haaswijk, e. collins, b. seguin, m. soeken, f. kaplan, s. susstrunk, and g. de micheli, \"deep learning for logic optimization algorithms,\" in 2018 ieee international symposium on circuits and systems (iscas). ieee, 2018, pp. 1- 4. [5] z. shi, m. li, s. khan, l. wang, n. wang, y. huang, and q. xu, \"deeptpi: test point insertion with deep reinforcement learning,\" in 2022 ieee international test conference (itc). ieee, 2022, pp. 194- 203. [6] j. huang, h.- l. zhen, n. wang, h. mao, m. yuan, and y. huang, \"neural fault analysis for sat- based atpg,\" in 2022 ieee international test conference (itc). ieee, 2022, pp. 36- 45. [7] m. li, s. khan, z. shi, n. wang, h. yu, and q. xu, \"deepgate: learning neural representations of logic gates,\" in proceedings of the 59th acm/ieee design automation conference, 2022, pp. 667- 672. [8] z. wang, c. bai, z. he, g. zhang, q. xu, t.- y. ho, b. yu, and y. huang, \"functionality matters in netlist representation learning,\" in proceedings of the 59th acm/ieee design automation conference, 2022, pp. 61- 66. [9] k. zhu, h. chen, w. j. turner, g. f. kokai, p.- h. wei, d. z. pan, and h. ren, \"tag: learning circuit spatial embedding from layouts,\" in proceedings of the 41st ieee/acm international conference on computer- aided design, 2022, pp. 1- 9. [10] y. lai, y. mu, and p. luo, \"maskplace: fast chip placement via reinforced visual representation learning,\" ar xiv preprint ar xiv:2211.13382, 2022. [11] a. fayyazi, s. shababi, p. nuzzo, s. nazarian, and m. pedram, \"deep learning- based circuit recognition using sparse mapping and level- dependent decaying sum circuit representations,\" in 2019 design, automation & test in europe conference & exhibition (date). ieee, 2019, pp. 638- 641. [12] z. he, z. wang, c. bail, h. yang, and b. yu, \"graph learning- based arithmetic block identification,\" in 2021 ieee/acm international conference on computer aided design (iccad). ieee, 2021, pp. 1- 8. [13] m. li, z. shi, q. lai, s. khan, and q. xu, \"deepsat: an eda- driven learning framework for sat,\" ar xiv preprint ar xiv:2205.13745, 2022. [14] a. mishchenko, s. chatterjee, r. jiang, and r. k. brayton, \"fraigs: a unifying representation for logic synthesis and verification,\" erl technical report, tech. rep., 2005. [15] s. d. queue, \"cadical at the sat race 2019,\" sat race 2019, p. 8, 2019. [16] t. brown, b. mann, n. ryder, m. subbiah, j. d. kaplan, p. dhariwal, a. neelakantan, p. shyam, g. sastry, a. askell et al., \"language models are few- shot learners,\" advances in neural information processing systems, vol. 33, pp. 1877- 1901, 2020. [17] j. devlin, m.- w. chang, k. lee, and k. toutanova, \"bert: pre- training"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "9d9cf73ce3ea598240dec3d0490f78a4",
      "title": "Learning from Algorithm Feedback One Shot SAT Solv",
      "overall_score": 0.4125,
      "recommendation": "Exclude",
      "summary": "Learning from Algorithm Feedback One Shot SAT Solv - Exclude (Score: 0.41)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (5) than negative (3)"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "a9cb544ef31b581fdf9860eab55512bf",
      "title": "Using deep learning to construct stochastic local",
      "overall_score": 0.33999999999999997,
      "recommendation": "Exclude",
      "summary": "Using deep learning to construct stochastic local - Exclude (Score: 0.34)",
      "evaluations": {
        "pytorch": {
          "answer": "No",
          "confidence": 0.8,
          "evidence": "Found negative: jax, jax"
        },
        "supervised": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "No",
          "confidence": 0.8,
          "evidence": "Found negative: distributed, distributed"
        },
        "has_repo": {
          "answer": "Yes",
          "confidence": 0.8,
          "evidence": "Found: code the possibility to use a dynamic learning rate with an exponential decay since we have seen that this was beneficial for overfitting highly structured small scale problems. we are convinced that by tuning the hyperparameters one can further boost the performance of the pre- trained model. it seems to be the case that the learning rate has to be adapted when using another dataset. to evaluate the resulting model, we ran the oracle- based walk sat and mt algorithms on an evaluation set of 2052 instances, also containing an equal number of instances across \\(n \\in [100, 200, 300]\\) and \\(1 \\leq \\alpha \\leq 4.82\\) . we ran each algorithm for up to \\(10^{6}\\) steps and a total of 5 runs per test instance. following refs. [10, 33], we evaluated each algorithm using three metrics: i) the average number of steps \\(\\#\\) before finding a solution, across all instances and runs, ii) the outer median (across instances) over the inner median (across runs) of steps before finding a solution, \\(m_{1 / 2}(\\#)\\) , iii) the total fraction \\(\\%\\) of solved instances, where we considered an instance solved if any of the runs had returned by the time of the cutoff. as an additional metric, we measure the average value \\(\\bar{\\alpha}\\) of \\(\\alpha\\) for which an algorithm was able to find a solution. we implemented our experiments in python, using the jax- framework [34] and its graph extension jraph [35] for the gnn and optimization. we implemented the oracle- based sls algorithms in rust. we ran the experiments on an aws g4dn.4xlarge instance, using a deep learning ami for ubuntu 20.04 (ami- 094950f08c57b4f62). all code and datasets used are made available, repository"
        }
      }
    },
    {
      "doc_id": "276c92a9f75ec42d28cf72510257deda",
      "title": "Graph Neural Reasoning May Fail in Certifying Bool",
      "overall_score": 0.36749999999999994,
      "recommendation": "Exclude",
      "summary": "Graph Neural Reasoning May Fail in Certifying Bool - Exclude (Score: 0.37)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "small_dataset": {
          "answer": "Yes",
          "confidence": 0.7,
          "evidence": "Found: small unsatisfiable core (minimal number of clauses that is enough to cause unsatisfiability). in fact, some previous work [1] even completely removed unsatisfiable formulas from the training dataset"
        },
        "quick_training": {
          "answer": "No",
          "confidence": 0.8,
          "evidence": "Found negative: distributed, distributed"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "247555210734f65824867d1d2260c5c0",
      "title": "Understanding GNNs for Boolean Satisfiability thro",
      "overall_score": 0.4675,
      "recommendation": "Exclude",
      "summary": "Understanding GNNs for Boolean Satisfiability thro - Exclude (Score: 0.47)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (18) than negative (8)"
        },
        "small_dataset": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: 5 hours. for comparison, we also add our implementation trained on the same data, but without a curriculum. the training of each model stops once it achieves an accuracy of \\(85\\%\\) on a validation set. </center> ### 5.1 data and hyperparameters 5.1.1 data generation. selsam et al. [29] demonstrates that a well- structured distribution of training data is essential to prevent the model from overfitting to superficial features. a recent theoretical study [26] also explains why input diversity is important in order for the model to transition to a regime where it is performing optimization over inputs. we therefore reuse selsam's generative model of random sat formulas which makes sure that no superficial features exist. the generative model samples random clauses until the formula becomes unsatisfiable. once it finds such unsatisfiable formula, it flips one literal in the lastly generated clause, and this will produce another formula which will differ by one literal and will be satisfiable. however, selsam's generation procedure is largely random and therefore does not capture any human- like reasoning skills. therefore we also generate structured problems (latin squares, sudoku, and logical circuits). for the interested reader, we describe the details of the data generation process in the supplementary materials s3.1 and s3.2. in the reference implementation of neuro sat, the model is trained on 100 000 formulas where for each formula the number of variables is sampled uniformly from the interval [10, 40]. the model is then evaluated on problems with 40 variables. similarly as in the original paper, this test set is here referred to as \\(sr(40)\\) . in our case, the size of the test set is the same, but we train only on 10 000 formulas in total and sample the number of variables in the formula from the interval [5, 40]. we emphasize that in the experimental results, all models are trained on the same training data. 5.1.2 model architecture. when experimenting with the original neuro sat architecture, we found that it is unnecessarily complex without any clear rationale and therefore we tried to simplify it as much as possible. we managed to significantly simplify the model without sacrificing the final accuracy. here is the list of simplifications in our model: we completely removed the two 3- layer mlps that produce the messages from the hidden states of the two lstms. the messages sent are, therefore, the hidden states themselves. we replace the final voting mlp with a linear layer. we do not use layer norm within lstms. we reduce the dimension of the hidden state of the lstms from 128 to 16. 5.1.3 training loop. we train the model using the curriculum described in section 3. in the following text, we consider the size of the formula to be given by the number of variables it contains. the training starts with formulas of size 5 and this size is incremented by 2 every time the validation accuracy (for a given size) reaches a given threshold or the maximum number of 200 epochs is reached. for each increment, we add the problems from the four previous increments 3 which makes the training more stable. the thresholds used to increment the size are obtained by interpolating the values between 0.65 (for the first size) and 0.85 (for the last size). we note that the values could be set to a fixed number but this may waste time during learning on the intermediate sizes. empirically, the model spends most of the time on the first 3 and 5 last sizes. for the other training, 5 hours. for comparison, we also add our implementation trained on the same data, but without a curriculum. the training of each model stops once it achieves an accuracy of \\(85\\%\\) on a validation set. </center> ### 5.1 data and hyperparameters 5.1.1 data generation. selsam et al. [29] demonstrates that a well- structured distribution of training data is essential to prevent the model from overfitting to superficial features. a recent theoretical study [26] also explains why input diversity is important in order for the model to transition to a regime where it is performing optimization over inputs. we therefore reuse selsam's generative model of random sat formulas which makes sure that no superficial features exist. the generative model samples random clauses until the formula becomes unsatisfiable. once it finds such unsatisfiable formula, it flips one literal in the lastly generated clause, and this will produce another formula which will differ by one literal and will be satisfiable. however, selsam's generation procedure is largely random and therefore does not capture any human- like reasoning skills. therefore we also generate structured problems (latin squares, sudoku, and logical circuits). for the interested reader, we describe the details of the data generation process in the supplementary materials s3.1 and s3.2. in the reference implementation of neuro sat, the model is trained on 100 000 formulas where for each formula the number of variables is sampled uniformly from the interval [10, 40]. the model is then evaluated on problems with 40 variables. similarly as in the original paper, this test set is here referred to as \\(sr(40)\\) . in our case, the size of the test set is the same, but we train only on 10 000 formulas in total and sample the number of variables in the formula from the interval [5, 40]. we emphasize that in the experimental results, all models are trained on the same training data. 5.1.2 model architecture. when experimenting with the original neuro sat architecture, we found that it is unnecessarily complex without any clear rationale and therefore we tried to simplify it as much as possible. we managed to significantly simplify the model without sacrificing the final accuracy. here is the list of simplifications in our model: we completely removed the two 3- layer mlps that produce the messages from the hidden states of the two lstms. the messages sent are, therefore, the hidden states themselves. we replace the final voting mlp with a linear layer. we do not use layer norm within lstms. we reduce the dimension of the hidden state of the lstms from 128 to 16. 5.1.3 training loop. we train the model using the curriculum described in section 3. in the following text, we consider the size of the formula to be given by the number of variables it contains. the training starts with formulas of size 5 and this size is incremented by 2 every time the validation accuracy (for a given size) reaches a given threshold or the maximum number of 200 epochs is reached. for each increment, we add the problems from the four previous increments 3 which makes the training more stable. the thresholds used to increment the size are obtained by interpolating the values between 0.65 (for the first size) and 0.85 (for the last size). we note that the values could be set to a fixed number but this may waste time during learning on the intermediate sizes. empirically, the model spends most of the time on the first 3 and 5 last sizes. for the other training, faster training"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "d718c5c662baa3c007cdb0ed1cad77db",
      "title": "IB Net Initial Branch Network for Variable Decisio",
      "overall_score": 0.535,
      "recommendation": "Review",
      "summary": "IB Net Initial Branch Network for Variable Decisio - Review (Score: 0.54)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (5) than negative (1)"
        },
        "small_dataset": {
          "answer": "Yes",
          "confidence": 0.8,
          "evidence": "Found: small cost comparing to the solving time. it's worth noting that our neural network is efficient enough to run on cpus. consequently, our method maintains the solver's completeness while significantly enhancing its efficiency and speed. ## 4 experimental settings ### 4.1 datasets we utilize two datasets to train and evaluate our framework and previous approaches. the first one is the industrial lec circuit sat instances from real- life chip design in 2023. they table 1: detail statistic of lec circuit dataset and sat competition dataset. noted that over \\(99\\%\\) of instance in lec dataset is result in unsat. <table><tr><td>data</td><td>#cnf</td><td>avg#var</td><td>avg#clause</td></tr><tr><td rowspan=\"4\">lec</td><td>circuit 1</td><td>22,050</td><td>1,002</td></tr><tr><td>circuit 2</td><td>14,840</td><td>952</td></tr><tr><td>circuit 3</td><td>25,591</td><td>1,563</td></tr><tr><td>all</td><td>62,481</td><td>1,220</td></tr><tr><td rowspan=\"4\">sat comp</td><td>sat</td><td>1,115</td><td>3,065</td></tr><tr><td>unsat</td><td>985</td><td>3,784</td></tr><tr><td>halted</td><td>1,171</td><td>3,623</td></tr><tr><td>all</td><td>3,271</td><td>3,481</td></tr></table> are extracted from the real chip development process to reflect the need for lec process. to fit the data into gpu easily and model the real- life production constraint, we first run state- of- the- art sat solver (kissat [biere and fleury, 2022]) on these problems and filter the instance that solver can solve within 1,000 seconds (the instances that solver cannot solve within 1,000 seconds will be regarded as hard cases and sent to redesign in real- life). apart from the industrial dataset, we also prepare the sat competition dataset as previous works are mostly target open datasets. we adopt the anniversary track of sat competition in 2022 [balyo et al., 2022]. the anniversary track is comprised of all benchmark instances used in previous sat competitions to ensure the coverage of problems. we also filter the anniversary track dataset in the same way with lec dataset. the details of filtered datasets can be found in table 1. halted here means instances unsolved within given time. after preparing the datasets, we use kissat and drat- trim [wetzler et al., 2014] to find variable assignments for sat problems and unsat- core for unsat problems, which serves as the supervision of our model training. when kissat try to solve a unsat instance, it will produce a proof for the unsatisfiability, which will be taken by drat- trim to produce the unsat- core variable. though the unsat- core found by drat- trim is not the minimal core, it still helps in solving unsat instances, so we will take these unsat- core as all the unsat- core refered in following sections. the supervision production just cost the same time of solving given dataset by kissat, which can consider as easy to follow and efficient to produce. ### 4.2 experimental setup our experiment design commences by segregating the two previously mentioned dataset, small cost comparing to the solving time. it's worth noting that our neural network is efficient enough to run on cpus. consequently, our method maintains the solver's completeness while significantly enhancing its efficiency and speed. ## 4 experimental settings ### 4.1 datasets we utilize two datasets to train and evaluate our framework and previous approaches. the first one is the industrial lec circuit sat instances from real- life chip design in 2023. they table 1: detail statistic of lec circuit dataset and sat competition dataset. noted that over \\(99\\%\\) of instance in lec dataset is result in unsat. <table><tr><td>data</td><td>#cnf</td><td>avg#var</td><td>avg#clause</td></tr><tr><td rowspan=\"4\">lec</td><td>circuit 1</td><td>22,050</td><td>1,002</td></tr><tr><td>circuit 2</td><td>14,840</td><td>952</td></tr><tr><td>circuit 3</td><td>25,591</td><td>1,563</td></tr><tr><td>all</td><td>62,481</td><td>1,220</td></tr><tr><td rowspan=\"4\">sat comp</td><td>sat</td><td>1,115</td><td>3,065</td></tr><tr><td>unsat</td><td>985</td><td>3,784</td></tr><tr><td>halted</td><td>1,171</td><td>3,623</td></tr><tr><td>all</td><td>3,271</td><td>3,481</td></tr></table> are extracted from the real chip development process to reflect the need for lec process. to fit the data into gpu easily and model the real- life production constraint, we first run state- of- the- art sat solver (kissat [biere and fleury, 2022]) on these problems and filter the instance that solver can solve within 1,000 seconds (the instances that solver cannot solve within 1,000 seconds will be regarded as hard cases and sent to redesign in real- life). apart from the industrial dataset, we also prepare the sat competition dataset as previous works are mostly target open datasets. we adopt the anniversary track of sat competition in 2022 [balyo et al., 2022]. the anniversary track is comprised of all benchmark instances used in previous sat competitions to ensure the coverage of problems. we also filter the anniversary track dataset in the same way with lec dataset. the details of filtered datasets can be found in table 1. halted here means instances unsolved within given time. after preparing the datasets, we use kissat and drat- trim [wetzler et al., 2014] to find variable assignments for sat problems and unsat- core for unsat problems, which serves as the supervision of our model training. when kissat try to solve a unsat instance, it will produce a proof for the unsatisfiability, which will be taken by drat- trim to produce the unsat- core variable. though the unsat- core found by drat- trim is not the minimal core, it still helps in solving unsat instances, so we will take these unsat- core as all the unsat- core refered in following sections. the supervision production just cost the same time of solving given dataset by kissat, which can consider as easy to follow and efficient to produce. ### 4.2 experimental setup our experiment design commences by segregating the two previously mentioned dataset"
        },
        "quick_training": {
          "answer": "Yes",
          "confidence": 0.9,
          "evidence": "Found: single sat problem from one of the three targeted circuits. the plots illustrate a clear trend: across a wide range of problem sizes, ib- net consistently outperforms kissat. this performance advantage of ib- net holds true when facing substantial variability in size of problems within these circuits, which indicate that our method is not only faster but also scalable and capable of handling large and complex problems efficiently. these findings underscore the potential of ib- net to improve the performance of sat solver in lec, confirming the practical applicability of our method in a real- world setting. ### 5.4 scalability we plot scatter graphs in fig. 6, showing memory consumption and training time of varying sizes of data samples under different computation units. wgcn uses significantly less memory across all sizes compared to lstm, around only \\(30\\%\\) . this stark difference suggests that model with wgcn is memory- efficient, potentially allowing for large or complex problem- solving without compromising system resources like gpu, single sat problem from one of the three targeted circuits. the plots illustrate a clear trend: across a wide range of problem sizes, ib- net consistently outperforms kissat. this performance advantage of ib- net holds true when facing substantial variability in size of problems within these circuits, which indicate that our method is not only faster but also scalable and capable of handling large and complex problems efficiently. these findings underscore the potential of ib- net to improve the performance of sat solver in lec, confirming the practical applicability of our method in a real- world setting. ### 5.4 scalability we plot scatter graphs in fig. 6, showing memory consumption and training time of varying sizes of data samples under different computation units. wgcn uses significantly less memory across all sizes compared to lstm, around only \\(30\\%\\) . this stark difference suggests that model with wgcn is memory- efficient, potentially allowing for large or complex problem- solving without compromising system resources like gpu, faster but also scalable and capable of handling large and complex problems efficiently. these findings underscore the potential of ib- net to improve the performance of sat solver in lec, confirming the practical applicability of our method in a real- world setting. ### 5.4 scalability we plot scatter graphs in fig. 6, showing memory consumption and training time of varying sizes of data samples under different computation units. wgcn uses significantly less memory across all sizes compared to lstm, around only \\(30\\%\\) . this stark difference suggests that model with wgcn is memory- efficient, potentially allowing for large or complex problem- solving without compromising system resources like gpu. wgcn also demonstrates faster training speeds across all data sample sizes. on average, wgcn is found to be approximately \\(40\\%\\) quicker than lstm. this speed advantage can reduce the time required for training models, thereby increasing the efficiency of model development process. <center>figure 6: the scalability: the memory cost (a) and training"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    },
    {
      "doc_id": "85db9e5aac62b5b1758102f95edbd8ee",
      "title": "NeuroBack Improving CDCL SAT Solving using Graph N",
      "overall_score": 0.36,
      "recommendation": "Exclude",
      "summary": "NeuroBack Improving CDCL SAT Solving using Graph N - Exclude (Score: 0.36)",
      "evaluations": {
        "pytorch": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "supervised": {
          "answer": "Yes",
          "confidence": 0.6,
          "evidence": "More positive (34) than negative (4)"
        },
        "small_dataset": {
          "answer": "No",
          "confidence": 0.6,
          "evidence": "More negative (5) than positive (2)"
        },
        "quick_training": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        },
        "has_repo": {
          "answer": "Unknown",
          "confidence": 0.3,
          "evidence": "No clear evidence"
        }
      }
    }
  ],
  "summary": {
    "total": 26,
    "include": 3,
    "exclude": 17,
    "review": 6,
    "method": "Regex"
  }
}